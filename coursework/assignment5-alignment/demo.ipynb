{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cba148b",
   "metadata": {},
   "source": [
    "请先阅读完作业要求后，完成本次作业。\n",
    "\n",
    "在本次作业里，我们将要实现以下内容：\n",
    "1. 建立一个针对数学问题的 zero-shot 评测集\n",
    "2. 使用一个更强推理模型蒸馏得到的推理轨迹进行 SFT 训练，得到一个参数更小的推理模型，并测试其效果\n",
    "3. 使用专家迭代（EI）算法，训练得到一个推理模型，并测试其效果\n",
    "4. 使用 GRPO 算法，训练得到一个推理模型，并测试其效果\n",
    "5. 针对 GRPO 算法，进行一系列调参实验，探索不同策略下对模型性能的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d299c",
   "metadata": {},
   "source": [
    "# 环境配置\n",
    "\n",
    "再开始正式的训练之前，需要先搭建好一个能正常运行下面所有代码的环境。如同之前的作业，我们使用 `uv` 来管理依赖。\n",
    "\n",
    "1. 先安装除 flash-attn 之外的所有包，然后再安装全部包（因为 flash-attn 比较特殊）：\n",
    "\n",
    "    ```\n",
    "    uv sync --no-install-package flash-attn\n",
    "    #source .venv/bin/activate\n",
    "    uv sync\n",
    "    ```\n",
    "\n",
    "2. 运行单元测试：\n",
    "\n",
    "    ``` sh\n",
    "    uv run pytest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3e4be",
   "metadata": {},
   "source": [
    "# 一、 建立针对数学问题的评估基准集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f4432",
   "metadata": {},
   "source": [
    "首先，我们需要建立一个能对数学问题进行评估的评测集，并将其应用于基线模型，评估其 zero-shot 数学能力。我们此处的配置如下：\n",
    "\n",
    "- **baseline model**: `Qwen2.5-Math-1.5B`\n",
    "- **benchmark**: `GSM8K`（官方文档里评测和带有推理轨迹的训练使用的都是 `MATH 12K`，但由于版权限制并没有在`data/`目录下给出此数据，推荐使用一些其他的数据进行替代。但是笔者在查询了 MATH 12K的规定后，发现其使用的是宽松的 MIT license 开源协议，但是该数据不带有推理轨迹，因此只能用来评测不能用作后续训练，我们这里使用了 GSM8K 数据集来替代 MATH 12K 用作评测。如果有小伙伴想要使用该数据可自行下载 [MATH 12K](https://huggingface.co/datasets/qwedsacf/competition_math)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-setup",
   "metadata": {},
   "source": [
    "## 1.1 评估基准集的设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b3311",
   "metadata": {},
   "source": [
    "建立数学推理评估基准集是整个作业的基础。我们需要：\n",
    "\n",
    "1. **选择合适的基准数据集**：由于 MATH 12K 数据集版权限制，我们使用 GSM8K 数据集作为替代。GSM8K (Grade School Math 8K) 是一个小学数学问题数据集，包含 8K 个问题，每个问题都有详细的推理过程和最终答案，问题难度适中，适合测试基础数学推理能力\n",
    "2. **选择合适的提示格式**：使用 R1-Zero 提示格式，要求模型进行思维链推理\n",
    "3. **定义评估指标**：使用格式奖励和答案准确率来评估模型性能\n",
    "4. **使用高效的推理引擎**：使用 vLLM 进行批量推理，提高评估效率\n",
    "\n",
    "### 实现逻辑\n",
    "\n",
    "我们的评估流程包含以下几个关键步骤：\n",
    "\n",
    "1. **数据加载与预处理**：加载 GSM8K 数据集，支持 JSONL 和 JSON 数组两种格式\n",
    "2. **提示格式化**：将原始问题转换为 R1-Zero 格式的提示\n",
    "3. **批量推理**：使用 vLLM 高效生成模型响应\n",
    "4. **结果评估**：使用奖励函数计算格式和答案准确率\n",
    "5. **统计分析**：分析不同组合的性能表现\n",
    "6. **结果保存**\n",
    "\n",
    "现在让我们逐步实现这些功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1-prompt",
   "metadata": {},
   "source": [
    "## 1.2 R1-Zero 提示格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939e994",
   "metadata": {},
   "source": [
    "R1-Zero 提示格式是 DeepSeek R1 模型使用的推理提示格式，它要求模型：\n",
    "\n",
    "1. 在 `<think>` 和 `</think>` 标签之间进行推理过程\n",
    "2. 在 `<answer>` 和 `</answer>` 标签之间给出最终答案\n",
    "3. 进行逐步推理，而不是直接给出答案\n",
    "\n",
    "这种格式便于我们解析模型的输出，提取推理过程和最终答案。\n",
    "\n",
    "### 实现细节\n",
    "\n",
    "在 `evaluate_math.py` 中，我们实现了两个核心函数来处理提示：\n",
    "\n",
    "1. **`load_r1_zero_prompt()`**：从文件加载 R1-Zero 提示模板\n",
    "2. **`format_prompt()`**：将数学问题插入到提示模板中\n",
    "\n",
    "提示模板文件位于 `cs336_alignment/prompts/r1_zero.prompt`，包含完整的对话格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1-Zero 提示模板：\n",
      "==================================================\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: {question}\n",
      "Assistant: <think>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 加载 R1-Zero 提示模板\n",
    "def load_r1_zero_prompt(prompt_file_path: str) -> str:\n",
    "    \"\"\"从文件加载 R1-Zero 提示模板\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 使用 UTF-8 编码打开文件\n",
    "    2. 读取完整文件内容\n",
    "    3. 使用 strip() 移除首尾空白字符\n",
    "    \n",
    "    为什么需要 strip()？\n",
    "    - 文件末尾可能有换行符或其他空白字符\n",
    "    - 移除这些字符可以确保模板格式的整洁性\n",
    "    \"\"\"\n",
    "    with open(prompt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "# 读取提示模板\n",
    "PROMPT_PATH = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "try:\n",
    "    r1_zero_template = load_r1_zero_prompt(PROMPT_PATH)\n",
    "    print(\"R1-Zero 提示模板：\")\n",
    "    print(\"=\" * 50)\n",
    "    print(r1_zero_template)\n",
    "    print(\"=\" * 50)\n",
    "except FileNotFoundError:\n",
    "    print(f\"提示模板文件未找到: {PROMPT_PATH}\")\n",
    "    print(\"请确保文件路径正确，或创建相应的提示文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "format-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始问题：\n",
      "小明有5个苹果，他又买了3个，现在他有多少个苹果？\n",
      "\n",
      "格式化后的提示：\n",
      "==================================================\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: 小明有5个苹果，他又买了3个，现在他有多少个苹果？\n",
      "Assistant: <think>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 格式化数学问题为 R1-Zero 提示\n",
    "def format_prompt(question: str, prompt_template: str) -> str:\n",
    "    \"\"\"将数学问题格式化为 R1-Zero 提示\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 使用字符串的 format() 方法\n",
    "    2. 将问题插入到模板中的 {question} 占位符处\n",
    "    3. 返回格式化后的完整提示\n",
    "    \n",
    "    这种方法的好处：\n",
    "    - 模板和内容分离，便于维护\n",
    "    - 支持复杂的提示格式\n",
    "    - 避免字符串拼接的错误\n",
    "    \"\"\"\n",
    "    return prompt_template.format(question=question)\n",
    "\n",
    "# 示例问题\n",
    "sample_question = \"小明有5个苹果，他又买了3个，现在他有多少个苹果？\"\n",
    "\n",
    "# 只有在成功加载模板后才进行格式化\n",
    "if 'r1_zero_template' in locals():\n",
    "    formatted_prompt = format_prompt(sample_question, r1_zero_template)\n",
    "\n",
    "    print(\"原始问题：\")\n",
    "    print(sample_question)\n",
    "    print(\"\\n格式化后的提示：\")\n",
    "    print(\"=\" * 50)\n",
    "    print(formatted_prompt)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "else:\n",
    "    print(\"无法演示格式化，因为提示模板未加载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-function",
   "metadata": {},
   "source": [
    "## 1.3 奖励函数设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576dd99",
   "metadata": {},
   "source": [
    "我们使用两个维度的奖励来评估模型的性能：\n",
    "\n",
    "1. **格式奖励 (Format Reward)**：检查模型输出是否符合 R1-Zero 格式要求，格式正确得 1 分，否则得 0 分\n",
    "   - 要求输出包含 `<think>` 和 `</think>` 标签\n",
    "   - 要求输出包含 `<answer>` 和 `</answer>` 标签\n",
    "\n",
    "2. **答案奖励 (Answer Reward)**：检查模型给出的答案是否正确，答案正确得 1 分，否则得 0 分\n",
    "   - 从 `<answer>` 标签中提取答案\n",
    "   - 与标准答案进行数学等价性比较\n",
    "\n",
    "### 奖励函数的实现逻辑\n",
    "\n",
    "奖励函数在 `drgrpo_grader.py` 中的 `r1_zero_reward_fn` 实现，它接受三个参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31013b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r1_zero_reward_fn(response: str, ground_truth: str, fast: bool = True) -> Dict[str, float]:\n",
    "    # response: 模型生成的完整输出\n",
    "    # ground_truth: 标准答案\n",
    "    # fast: 是否使用快速评估模式\n",
    "    # 返回包含 format_reward, answer_reward, reward 的字典\n",
    "    if \"</think> <answer>\" in response and \"</answer>\" in response:\n",
    "        model_answer = response.split(\"<answer>\")[-1].replace(\"</answer>\", \"\")\n",
    "        if \"\\\\boxed\" in model_answer:\n",
    "            model_answer = extract_answer(model_answer)\n",
    "            if model_answer is None:\n",
    "                return {\n",
    "                    \"format_reward\": 1.0,\n",
    "                    \"answer_reward\": 0.0,\n",
    "                    \"reward\": 0.0\n",
    "                }\n",
    "        if isinstance(ground_truth, float) or isinstance(ground_truth, int):\n",
    "            ground_truth = str(ground_truth)\n",
    "        if isinstance(ground_truth, str):\n",
    "            is_correct = grade(model_answer, ground_truth, fast)\n",
    "        elif isinstance(ground_truth, list):\n",
    "            is_correct = False\n",
    "            for gt in ground_truth:\n",
    "                is_correct |= grade(model_answer, gt, fast)\n",
    "        if is_correct:\n",
    "            return {\n",
    "                \"format_reward\": 1.0,\n",
    "                \"answer_reward\": 1.0,\n",
    "                \"reward\": 1.0\n",
    "            }\n",
    "        else:\n",
    "            # Formatted but wrong answer; no format reward to avoid hacking.\n",
    "            return {\n",
    "                \"format_reward\": 1.0,\n",
    "                \"answer_reward\": 0.0,\n",
    "                \"reward\": 0.0\n",
    "            }\n",
    "    else:\n",
    "        # Unformatted.\n",
    "        return {\n",
    "            \"format_reward\": 0.0,\n",
    "            \"answer_reward\": 0.0,\n",
    "            \"reward\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dba48",
   "metadata": {},
   "source": [
    "**该函数实现的评估流程**：\n",
    "1. **格式检查**：验证输出是否包含必需的标签\n",
    "2. **答案提取**：从 `<answer>` 标签中解析最终答案\n",
    "3. **答案比较**：使用数学等价性检查答案正确性\n",
    "4. **奖励计算**：根据格式和答案正确性计算奖励分数\n",
    "\n",
    "这个函数的设计使得评估过程既严格又灵活，能够准确衡量模型的推理能力和格式遵循能力。我们通过几个例子，测试下我们的奖励函数是够能正常工作！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "import-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功导入奖励函数\n",
      "\n",
      "=== 测试场景1：正确格式和答案 ===\n",
      "响应内容：\n",
      "</think>这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。</think> <answer>8</answer>\n",
      "\n",
      "奖励结果：\n",
      "  format_reward: 1.0\n",
      "  answer_reward: 1.0\n",
      "  reward: 1.0\n",
      "\n",
      "=== 测试场景2：格式正确但答案错误 ===\n",
      "奖励结果：\n",
      "  format_reward: 1.0\n",
      "  answer_reward: 0.0\n",
      "  reward: 0.0\n",
      "\n",
      "=== 测试场景3：格式错误（缺少标签） ===\n",
      "响应内容：\n",
      "这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。答案是8。\n",
      "\n",
      "奖励结果：\n",
      "  format_reward: 0.0\n",
      "  answer_reward: 0.0\n",
      "  reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 导入奖励函数\n",
    "try:\n",
    "    from cs336_alignment.drgrpo_grader import r1_zero_reward_fn\n",
    "    print(\"成功导入奖励函数\")\n",
    "except ImportError as e:\n",
    "    print(f\"导入奖励函数失败: {e}\")\n",
    "    print(\"请确保 drgrpo_grader.py 文件存在并且可以被导入\")\n",
    "    r1_zero_reward_fn = None\n",
    "\n",
    "# 测试奖励函数的不同场景\n",
    "def test_reward_function():\n",
    "    if r1_zero_reward_fn is None:\n",
    "        print(\"无法测试奖励函数，因为导入失败\")\n",
    "        return\n",
    "    \n",
    "    # 测试场景1：正确格式和答案\n",
    "    print(\"\\n=== 测试场景1：正确格式和答案 ===\")\n",
    "    correct_response = \"\"\"</think>这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。</think> <answer>8</answer>\"\"\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(correct_response, \"8\")\n",
    "    print(\"响应内容：\")\n",
    "    print(correct_response)\n",
    "    print(\"\\n奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 测试场景2：格式正确但答案错误\n",
    "    print(\"\\n=== 测试场景2：格式正确但答案错误 ===\")\n",
    "    wrong_answer_response = \"\"\"<think>我来计算一下。5个苹果加3个应该是8个。等等，我算错了，是7个。</think> <answer>7</answer>\"\"\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(wrong_answer_response, \"8\")\n",
    "    print(\"奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 测试场景3：格式错误\n",
    "    print(\"\\n=== 测试场景3：格式错误（缺少标签） ===\")\n",
    "    bad_format_response = \"这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。答案是8。\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(bad_format_response, \"8\")\n",
    "    print(\"响应内容：\")\n",
    "    print(bad_format_response)\n",
    "    print(\"\\n奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# 运行测试\n",
    "test_reward_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c6819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d4fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vllm-setup",
   "metadata": {},
   "source": [
    "## 1.4 使用 vLLM 进行高效推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a7021",
   "metadata": {},
   "source": [
    "vLLM 是一个高性能的 LLM 推理引擎，具有以下优势：\n",
    "\n",
    "1. **批量推理**：支持同时处理多个请求，提高吞吐量\n",
    "2. **内存优化**：使用 PagedAttention 等技术优化显存使用\n",
    "3. **CUDA 内核优化**：使用优化的 CUDA 内核加速推理\n",
    "4. **灵活的采样参数**：支持温度采样、top-p 截断等多种采样策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a8ded",
   "metadata": {},
   "source": [
    "### vLLM 初始化参数详解\n",
    "\n",
    "在 `evaluate_math.py` 中，我们这样初始化 vLLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    dtype=torch.bfloat16,  # 使用 bfloat16 精度节省显存\n",
    "    gpu_memory_utilization=0.8,  # 控制显存使用率\n",
    "    enable_prefix_caching=True,  # 启用前缀缓存优化\n",
    "    tensor_parallel_size=1,  # 单 GPU 设置\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb89a1",
   "metadata": {},
   "source": [
    "### 采样参数配置\n",
    "\n",
    "我们使用以下采样参数：\n",
    "- `temperature=1.0`：使用随机采样，增加输出多样性，有助于探索不同的推理路径\n",
    "- `top_p=1.0`：不进行 top-p 截断，使用完整的词汇分布\n",
    "- `max_tokens=1024`：最大生成长度，足够容纳详细的推理过程\n",
    "- `stop=[\"</answer>\"]`：在生成答案标签时停止，确保输出格式完整\n",
    "- `include_stop_str_in_output=True`：将停止字符串包含在输出中，便于后续解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-vllm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 vLLM 模型\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from unittest.mock import patch\n",
    "from vllm.model_executor import set_random_seed as vllm_set_random_seed\n",
    "\n",
    "# 模型路径\n",
    "MODEL_PATH = \"/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B\"\n",
    "\n",
    "def init_vllm(model_path: str, device: str = \"cuda:0\"):\n",
    "    \"\"\"初始化 vLLM 模型\n",
    "    \n",
    "    初始化逻辑详解：\n",
    "    1. 设置随机种子确保结果可重现\n",
    "    2. 使用 patch 解决分布式训练相关问题\n",
    "    3. 配置模型参数以优化性能和显存使用\n",
    "    \n",
    "    为什么需要 patch？\n",
    "    - vLLM 默认假设在分布式环境中运行\n",
    "    - 在单 GPU 环境中，我们需要模拟 world_size=1\n",
    "    - profiling_patch 避免了不必要的性能分析\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        vllm_set_random_seed(42)\n",
    "        \n",
    "        world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "        profiling_patch = patch(\n",
    "            \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "            return_value=None\n",
    "        )\n",
    "        \n",
    "        with world_size_patch, profiling_patch:\n",
    "            llm = LLM(\n",
    "                model=model_path,\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,  # 使用 bfloat16 节省显存\n",
    "                enable_prefix_caching=True,  # 启用前缀缓存，提高批量推理效率\n",
    "                gpu_memory_utilization=0.8,  # 控制显存使用率，避免 OOM\n",
    "                tensor_parallel_size=1,  # 单 GPU 设置\n",
    "                enforce_eager=True,  # 强制使用 eager 模式，提高稳定性\n",
    "            )\n",
    "            print(f\"成功加载模型: {model_path}\")\n",
    "            return llm\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"初始化 vLLM 失败: {e}\")\n",
    "        print(\"请检查模型路径是否存在，以及是否有足够的显存\")\n",
    "        return None\n",
    "\n",
    "# 初始化模型\n",
    "print(\"正在初始化 vLLM 模型...\")\n",
    "llm = init_vllm(MODEL_PATH)\n",
    "\n",
    "if llm is not None:\n",
    "    print(\"vLLM 模型初始化完成！\")\n",
    "    \n",
    "    # 设置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,  # 随机采样，增加推理多样性\n",
    "        top_p=1.0,        # 不进行 top-p 截断\n",
    "        max_tokens=1024,  # 最大生成长度\n",
    "        stop=[\"</answer>\"],  # 遇到答案结束标签时停止\n",
    "        include_stop_str_in_output=True,  # 包含停止字符串在输出中\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    sampling_params = None\n",
    "    print(\"vLLM 初始化失败，无法继续演示\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 1.5 数据加载和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c45d9",
   "metadata": {},
   "source": [
    "数据加载是评估流程中的关键步骤。在 `evaluate_math.py` 中，我们实现了灵活的数据加载逻辑，支持多种数据集格式。\n",
    "\n",
    "### 支持的数据集格式\n",
    "\n",
    "1. **GSM8K 格式 (JSONL)**：\n",
    "   - 每行一个 JSON 对象\n",
    "   - 包含 `question` 字段和 `answer` 字段\n",
    "   - 答案字段包含完整推理过程，最后以 `####` 分隔最终答案\n",
    "\n",
    "2. **MATH 格式 (JSON 数组)**：\n",
    "   - 整个文件是一个 JSON 数组\n",
    "   - 包含 `problem` 和 `expected_answer` 字段\n",
    "   - 答案是直接给出的最终结果\n",
    "\n",
    "### 数据预处理逻辑\n",
    "\n",
    "在 `evaluate_vllm` 函数中，我们实现了以下预处理步骤：\n",
    "\n",
    "1. **格式检测**：通过读取第一个字符判断是 JSONL (`{`) 还是 JSON 数组 (`[`)\n",
    "2. **字段提取**：根据数据集类型提取问题和答案\n",
    "3. **答案解析**：对于 GSM8K，使用正则表达式提取 `####` 后的最终答案\n",
    "4. **提示格式化**：将问题插入到 R1-Zero 提示模板中\n",
    "\n",
    "### 答案提取策略\n",
    "\n",
    "GSM8K 数据集的答案格式特殊，使用如下形式：\n",
    "```\n",
    "详细推理过程... #### 最终答案\n",
    "```\n",
    "\n",
    "我们使用正则表达式来提取最终答案，确保只获取正确答案部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到 JSONL 格式 (GSM8K风格)\n",
      "成功加载了 1319 个样本\n",
      "限制为前 10 个样本用于演示\n",
      "\n",
      "样本 1:\n",
      "问题: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "原始答案字段: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "提取的答案: 18\n",
      "\n",
      "样本 2:\n",
      "问题: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "原始答案字段: It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
      "So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\n",
      "#### 3\n",
      "提取的答案: 3\n",
      "\n",
      "样本 3:\n",
      "问题: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "原始答案字段: The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\n",
      "He increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\n",
      "So the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\n",
      "So he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\n",
      "#### 70000\n",
      "提取的答案: 70000\n",
      "\n",
      "成功格式化了 10 个提示\n",
      "\n",
      "总共加载了 10 个问题\n",
      "示例提示预览：\n",
      "--------------------------------------------------\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 数据加载函数 - 基于 evaluate_math.py 的实现\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_math_dataset(dataset_path: str, max_samples: int = None):\n",
    "    \"\"\"\n",
    "    加载数学数据集，支持 JSONL 和 JSON 数组两种格式\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 检测文件格式（JSONL vs JSON数组）\n",
    "    2. 加载数据到内存\n",
    "    3. 提取问题和答案字段\n",
    "    4. 处理不同数据集格式的答案提取\n",
    "    5. 格式化提示\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: 数据集文件路径\n",
    "        max_samples: 最大加载样本数（用于演示）\n",
    "        \n",
    "    Returns:\n",
    "        questions: 问题列表\n",
    "        ground_truths: 标准答案列表\n",
    "        prompts: 格式化后的提示列表\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    try:\n",
    "        # 步骤1：检测文件格式\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            first_char = f.read(1)\n",
    "            f.seek(0)  # 重置文件指针\n",
    "            \n",
    "            if first_char == '[':\n",
    "                # JSON 数组格式 (MATH数据集)\n",
    "                print(\"检测到 JSON 数组格式 (MATH风格)\")\n",
    "                data = json.load(f)\n",
    "            else:\n",
    "                # JSONL 格式 (GSM8K数据集)\n",
    "                print(\"检测到 JSONL 格式 (GSM8K风格)\")\n",
    "                data = []\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:  # 跳过空行\n",
    "                        data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"成功加载了 {len(data)} 个样本\")\n",
    "        \n",
    "        # 步骤2：限制样本数量（演示用）\n",
    "        if max_samples:\n",
    "            data = data[:max_samples]\n",
    "            print(f\"限制为前 {max_samples} 个样本用于演示\")\n",
    "        \n",
    "        # 步骤3：处理每个样本\n",
    "        for i, example in enumerate(data):\n",
    "            # 提取问题 - 支持多种字段名\n",
    "            question = example.get('question', example.get('problem', ''))\n",
    "            \n",
    "            # 提取答案 - 处理不同格式\n",
    "            if 'expected_answer' in example:\n",
    "                # MATH 格式：直接答案\n",
    "                answer = example['expected_answer']\n",
    "            elif 'answer' in example:\n",
    "                # GSM8K 格式：从推理过程中提取最终答案\n",
    "                answer_text = example['answer']\n",
    "                # 使用正则表达式提取 #### 后的答案\n",
    "                match = re.search(r\"####\\s*(.+)\\s*$\", answer_text.strip())\n",
    "                answer = match.group(1).strip() if match else answer_text.strip()\n",
    "            else:\n",
    "                answer = ''\n",
    "            \n",
    "            questions.append(question)\n",
    "            ground_truths.append(answer)\n",
    "            \n",
    "            # 只显示前几个样本的详细信息\n",
    "            if i < 3:\n",
    "                print(f\"\\n样本 {i+1}:\")\n",
    "                print(f\"问题: {question}\")\n",
    "                print(f\"原始答案字段: {example.get('answer', example.get('expected_answer', 'N/A'))}\")\n",
    "                print(f\"提取的答案: {answer}\")\n",
    "        \n",
    "        # 步骤4：格式化提示\n",
    "        if 'r1_zero_template' in globals():\n",
    "            prompts = [format_prompt(q, r1_zero_template) for q in questions]\n",
    "            print(f\"\\n成功格式化了 {len(prompts)} 个提示\")\n",
    "        else:\n",
    "            prompts = []\n",
    "            print(\"\\n提示模板未加载，无法格式化提示\")\n",
    "        \n",
    "        return questions, ground_truths, prompts\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"数据集文件未找到: {dataset_path}\")\n",
    "        return [], [], []\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据集时出错: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "# 加载数据（演示用小样本）\n",
    "DATASET_PATH = \"data/gsm8k/test.jsonl\"\n",
    "questions, ground_truths, prompts = load_math_dataset(DATASET_PATH, max_samples=10)\n",
    "\n",
    "print(f\"\\n总共加载了 {len(questions)} 个问题\")\n",
    "if prompts:\n",
    "    print(f\"示例提示预览：\")\n",
    "    print(\"-\" * 50)\n",
    "    print(prompts[0][:200] + \"...\" if len(prompts[0]) > 200 else prompts[0])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffaf79",
   "metadata": {},
   "source": [
    "## 1.6 评估指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec9bb1",
   "metadata": {},
   "source": [
    "现在我们可以使用 vLLM 对所有问题进行批量推理，并使用奖励函数评估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = int(rewards.get(\"format_reward\", 0.0) >= 0.5)\n",
    "ar = int(rewards.get(\"answer_reward\", 0.0) >= 0.5)\n",
    "combo_counts[(fr, ar)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation",
   "metadata": {},
   "source": [
    "### 组合分析 (Combo Analysis)\n",
    "\n",
    "我们统计四种组合的情况：\n",
    "- `(1, 1)`：格式正确且答案正确\n",
    "- `(1, 0)`：格式正确但答案错误\n",
    "- `(0, 0)`：格式错误且答案错误\n",
    "- `(0, 1)`：格式错误但答案正确（理论上很少见）\n",
    "\n",
    "这种分析有助于我们理解模型在格式遵循和推理能力方面的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"n\": num_examples,\n",
    "    \"format_rate\": avg_format_reward,\n",
    "    \"answer_accuracy\": avg_answer_reward,\n",
    "    \"reward_mean\": avg_reward,\n",
    "    \"counts\": {\n",
    "        \"format=1 answer=1\": combo_counts[(1, 1)],\n",
    "        \"format=1 answer=0\": combo_counts[(1, 0)],\n",
    "        \"format=0 answer=0\": combo_counts[(0, 0)],\n",
    "        \"format=0 answer=1\": combo_counts[(0, 1)],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-results",
   "metadata": {},
   "source": [
    "## 1.7 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc39d8",
   "metadata": {},
   "source": [
    "通过运行`evaluate_math.py`脚本，我们可以完成对`Qwen2.5-Math-1.5B`模型的完整评估。运行完成后，你会得到类似以下的输出：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"n\": 1319,\n",
    "  \"format_rate\": 0.5041698256254739,\n",
    "  \"answer_accuracy\": 0.17589082638362397,\n",
    "  \"reward_mean\": 0.17589082638362397,\n",
    "  \"counts\": {\n",
    "    \"format=1 answer=1\": 232,\n",
    "    \"format=1 answer=0\": 433,\n",
    "    \"format=0 answer=0\": 654,\n",
    "    \"format=0 answer=1\": 0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "基准模型 `Qwen2.5-Math-1.5B` 的回答准确率是 `17.59%`，格式准确率为 `50.42%`。接着我们观察了 10 个格式奖励为 0 的案例，我们发现问题主要出在解析器的格式要求过于严格，而不是基础模型的输出质量问题。从 `r1_zero_reward_fn` 的代码看，格式检查的条件是：\n",
    "\n",
    "```python\n",
    "if \"</think> <answer>\" in response and \"</answer>\" in response:\n",
    "```\n",
    "\n",
    "这个条件要求：\n",
    "- 响应中必须包含确切的字符串 `</think> <answer>` （注意 think 结束标签和 answer 开始标签之间必须有空格）\n",
    "- 响应中必须包含 `</answer>`\n",
    "\n",
    "而在多个案例中我们发现模型输出通常为 `</think>\\n<answer>` 或 `</think><answer>`（没有空格），不符合解析器期望的 `</think> <answer>`（必须有空格）\n",
    "\n",
    "对于格式正确但答案错误的案例，问题主要出在基础模型的推理能力上，而不是解析器的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51159843",
   "metadata": {},
   "source": [
    "## 第二部分：用于数学推理的监督微调 (SFT)\n",
    "\n",
    "### 学习目标\n",
    "在本部分中，我们将学习如何通过监督微调来提升语言模型的数学推理能力。这包括：\n",
    "1. 理解 SFT 在数学推理中的作用和原理\n",
    "2. 掌握推理轨迹数据的准备和处理\n",
    "3. 实现完整的 SFT 训练流程\n",
    "4. 学习模型评估和性能分析技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc9a90",
   "metadata": {},
   "source": [
    "### 2.1 SFT 的动机和原理\n",
    "\n",
    "#### 2.1.1 为什么需要 SFT？\n",
    "\n",
    "尽管 Qwen 2.5 Math 1.5B 已经经过数学预训练，但在推理任务上仍然表现不佳：\n",
    "- **缺乏推理结构**：模型不知道如何进行逐步推理\n",
    "- **格式不一致**：不知道使用 `<think>` 和 `<answer>` 标签\n",
    "- **推理模式缺失**：没有学习到思维链（Chain-of-Thought）推理模式\n",
    "\n",
    "#### 2.1.2 SFT 的核心思想\n",
    "\n",
    "SFT 通过让模型学习**推理轨迹**来提升性能：\n",
    "- **输入**：数学问题 + 推理过程 + 最终答案\n",
    "- **目标**：模型学会生成完整的推理过程\n",
    "- **优势**：提供明确的推理模式和步骤\n",
    "\n",
    "### 2.2 数据准备和预处理\n",
    "\n",
    "#### 2.2.1 推理轨迹数据\n",
    "\n",
    "SFT 使用来自 garg-aayush 复现该项目时，基于 [hiyouga/math12k](https://huggingface.co/datasets/hiyouga/math12k) 数据集 使用 gpt-oss 蒸馏的[带推理轨迹的数据](https://huggingface.co/datasets/garg-aayush/sft-cs336-assign5-datasets/tree/main/sft-reason) 的推理轨迹数据：\n",
    "\n",
    "```python\n",
    "# 数据格式示例\n",
    "  {\n",
    "    \"problem\": \"Compute $\\\\sin 45^\\\\circ$.\",\n",
    "    \"reasoning_trace\": \"The angle 45\\u00b0 corresponds to \\u03c0/4 radians. The sine of \\u03c0/4 is known from the unit circle or a 45-45-90 right triangle, where the legs are equal and the hypotenuse is \\u221a2 times a leg. Thus, sin\\u202f45\\u00b0 = opposite/hypotenuse = 1/\\u221a2 = \\u221a2/2.</think> <answer>\\u221a2/2</answer>\",\n",
    "    \"extracted_answer\": \"\\u221a2/2\",\n",
    "    \"expected_answer\": \"\\\\frac{\\\\sqrt{2}}{2}\"\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d7dba",
   "metadata": {},
   "source": [
    "#### 2.2.2 数据加载逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不同格式的数据文件\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    first_char = f.read(1)\n",
    "    f.seek(0)\n",
    "\n",
    "    if first_char == '[':\n",
    "        # JSON 数组格式\n",
    "        sft_data = json.load(f)\n",
    "    else:\n",
    "        # JSONL 格式\n",
    "        sft_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# 数据子采样（用于不同大小实验）\n",
    "if max_examples > 0 and len(sft_data) > max_examples:\n",
    "    sft_data = random.sample(sft_data, max_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3fe19",
   "metadata": {},
   "source": [
    "### 2.3 SFT 训练的核心组件\n",
    "\n",
    "#### 2.3.1 提示和输出分词 (`tokenize_prompt_and_output`)\n",
    "\n",
    "这是 SFT 的基础函数，用于准备训练数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt_and_output(\n",
    "        prompt_strs: List[str],\n",
    "        output_strs: List[str],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    ") -> Dict[str, torch.Tensor]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cb773",
   "metadata": {},
   "source": [
    "**关键步骤：**\n",
    "\n",
    "1. **分别分词提示和输出**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tok = tokenizer(prompt_strs, add_special_tokens=False, padding=False)\n",
    "output_tok = tokenizer(output_strs, add_special_tokens=False, padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bc76d",
   "metadata": {},
   "source": [
    "2. **拼接完整序列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1736597",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids_list = [p + o for p, o in zip(prompt_ids_list, output_ids_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80284f",
   "metadata": {},
   "source": [
    "3. **填充到相同长度**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96089e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_full_len = max(len(x) for x in full_ids_list)\n",
    "full_padded = torch.full((bs, max_full_len), pad_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3803c",
   "metadata": {},
   "source": [
    "4. **创建训练标签**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0954ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = full_padded[:, :-1]  # 去掉最后一个token\n",
    "labels = full_padded[:, 1:]      # 去掉第一个token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a7671",
   "metadata": {},
   "source": [
    "5. **构建响应掩码**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mask = torch.zeros_like(labels, dtype=torch.long)\n",
    "for i in range(bs):\n",
    "    p_len = prompt_lens[i]\n",
    "    o_len = output_lens[i]\n",
    "    start = max(p_len - 1, 0)\n",
    "    end = min(p_len + o_len - 1, labels.size(1))\n",
    "    response_mask[i, start:end] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b098b",
   "metadata": {},
   "source": [
    "\n",
    "**返回结构：**\n",
    "```python\n",
    "{\n",
    "    \"input_ids\": input_ids,        # (B, T) - 模型输入\n",
    "    \"labels\": labels,             # (B, T) - 目标标签（右移一位）\n",
    "    \"response_mask\": response_mask # (B, T) - 响应部分掩码\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aaf924",
   "metadata": {},
   "source": [
    "#### 2.3.2 对数概率计算 (`get_response_log_probs`)\n",
    "\n",
    "计算模型对目标序列的条件对数概率：\n",
    "\n",
    "def get_response_log_probs(\n",
    "        model: torch.nn.Module,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        return_token_entropy: bool = False,\n",
    ") -> Dict[str, torch.Tensor]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e173d",
   "metadata": {},
   "source": [
    "**计算过程：**\n",
    "1. **前向传播**：`logits = model(input_ids).logits`\n",
    "2. **转换为对数概率**：`log_probs_vocab = F.log_softmax(logits, dim=-1)`\n",
    "3. **选择目标token的概率**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.gather(\n",
    "    log_probs_vocab, dim=-1, index=labels.unsqueeze(-1)\n",
    ").squeeze(-1)\n",
    "\n",
    "def get_response_log_probs(\n",
    "        model: torch.nn.Module,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        return_token_entropy: bool = False,\n",
    ") -> Dict[str, torch.Tensor]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f521264",
   "metadata": {},
   "source": [
    "**计算过程：**\n",
    "1. **前向传播**：`logits = model(input_ids).logits`\n",
    "2. **转换为对数概率**：`log_probs_vocab = F.log_softmax(logits, dim=-1)`\n",
    "3. **选择目标token的概率**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5307bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.gather(\n",
    "    log_probs_vocab, dim=-1, index=labels.unsqueeze(-1)\n",
    ").squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822913d",
   "metadata": {},
   "source": [
    "#### 2.3.3 熵计算 (`compute_entropy`)\n",
    "\n",
    "用于监控模型预测的确定性：\n",
    "\n",
    "```python\n",
    "def compute_entropy(logits: torch.Tensor) -> torch.Tensor:\n",
    "    # 计算每个位置的预测熵\n",
    "    log_z = torch.logsumexp(logits, dim=-1)  # log Z\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    expected_logit = (probs * logits).sum(dim=-1)\n",
    "    entropy = log_z - expected_logit  # H = log Z - E_p[logit]\n",
    "    return entropy\n",
    "```\n",
    "\n",
    "**熵的含义：**\n",
    "- **高熵**：模型对下一个token不确定，预测分布平坦\n",
    "- **低熵**：模型很确定下一个token，预测分布集中\n",
    "\n",
    "#### 2.3.4 掩码归一化 (`masked_normalize`)\n",
    "\n",
    "用于在响应token上进行归一化的工具函数：\n",
    "\n",
    "```python\n",
    "def masked_normalize(\n",
    "        tensor: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        normalize_constant: float,\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:\n",
    "    masked_tensor = tensor * mask\n",
    "    summed = masked_tensor.sum() if dim is None else masked_tensor.sum(dim=dim)\n",
    "    return summed / normalize_constant\n",
    "```\n",
    "\n",
    "#### 2.3.5 微批次训练步骤 (`sft_microbatch_train_step`)\n",
    "\n",
    "实现单个训练步骤的核心逻辑：\n",
    "\n",
    "```python\n",
    "def sft_microbatch_train_step(\n",
    "        policy_log_probs: torch.Tensor,\n",
    "        response_mask: torch.Tensor,\n",
    "        gradient_accumulation_steps: int,\n",
    "        normalize_constant: float = 1.0,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "```\n",
    "\n",
    "**训练步骤：**\n",
    "1. **计算负对数似然**：`per_token_nll = -policy_log_probs`\n",
    "2. **掩码求和**：`per_example_nll = (per_token_nll * mask).sum(dim=1)`\n",
    "3. **归一化**：`per_example_nll = per_example_nll / normalize_constant`\n",
    "4. **批次平均**：`microbatch_loss = per_example_nll.mean()`\n",
    "5. **梯度累积缩放**：`loss = microbatch_loss / gradient_accumulation_steps`\n",
    "6. **反向传播**：`loss.backward()`\n",
    "\n",
    "### 2.4 训练流程和超参数\n",
    "\n",
    "#### 2.4.1 模型初始化\n",
    "\n",
    "```python\n",
    "# 策略模型（GPU 0）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(device_policy)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "```\n",
    "\n",
    "#### 2.4.2 vLLM 初始化用于评估\n",
    "\n",
    "```python\n",
    "# 评估模型（GPU 1）\n",
    "llm = init_vllm(BASE_MODEL, device_eval, SEED)\n",
    "load_policy_into_vllm_instance(model, llm)\n",
    "```\n",
    "\n",
    "#### 2.4.3 训练超参数\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 4          # 微批次大小\n",
    "GRAD_ACCUM = 8          # 梯度累积步数\n",
    "LR = 5e-5              # 学习率\n",
    "EPOCHS = 1             # 训练轮数\n",
    "MAX_GRAD_NORM = 1.0    # 梯度裁剪\n",
    "SEED = 2026            # 随机种子\n",
    "```\n",
    "\n",
    "#### 2.4.4 训练循环\n",
    "\n",
    "```python\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(sft_data), BATCH_SIZE)):\n",
    "        # 1. 准备批次数据\n",
    "        batch_data = sft_data[i : i + BATCH_SIZE]\n",
    "\n",
    "        # 2. 格式化提示和响应\n",
    "        prompt_strs = []\n",
    "        output_strs = []\n",
    "        for x in batch_data:\n",
    "            question = x.get('problem', x.get('question', ''))\n",
    "            formatted_prompt = format_prompt(template, question)\n",
    "            response = x.get('reasoning_trace', x.get('response', ''))\n",
    "            prompt_strs.append(formatted_prompt)\n",
    "            output_strs.append(response)\n",
    "\n",
    "        # 3. 分词\n",
    "        tokenized = tokenize_prompt_and_output(prompt_strs, output_strs, tokenizer)\n",
    "        input_ids = tokenized[\"input_ids\"].to(device_policy)\n",
    "        labels = tokenized[\"labels\"].to(device_policy)\n",
    "        response_mask = tokenized[\"response_mask\"].to(device_policy)\n",
    "\n",
    "        # 4. 前向传播\n",
    "        logits = model(input_ids).logits\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        # 5. 计算损失并反向传播\n",
    "        loss, _ = sft_microbatch_train_step(token_log_probs, response_mask, GRAD_ACCUM)\n",
    "\n",
    "        # 6. 梯度累积和优化器步骤\n",
    "        if (global_step + 1) % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 7. 定期评估\n",
    "        if (global_step + 1) % EVAL_EVERY_STEPS == 0:\n",
    "            evaluate_and_log(model, llm, ...)\n",
    "```\n",
    "\n",
    "### 2.5 实验设置和结果分析\n",
    "\n",
    "#### 2.5.1 数据集大小扫描实验\n",
    "\n",
    "```python\n",
    "DATASET_SIZES = [128, 256, 512, 1024]\n",
    "\n",
    "for size in DATASET_SIZES:\n",
    "    run_sft_experiment(\n",
    "        train_data_path=RAW_TRAIN,\n",
    "        max_examples=size,\n",
    "        dataset_tag=\"raw\",\n",
    "        size_tag=str(size)\n",
    "    )\n",
    "```\n",
    "\n",
    "**目的：** 研究训练数据量对性能的影响，找到数据效率的平衡点。\n",
    "\n",
    "#### 2.5.2 过滤正确示例实验\n",
    "\n",
    "```python\n",
    "# 预处理：只保留产生正确答案的推理轨迹\n",
    "new_sft_data = []\n",
    "for x in sft_data:\n",
    "    score = r1_zero_reward_fn(x['reasoning_trace'], x['expected_answer'])\n",
    "    if score['answer_reward'] == 1.0:\n",
    "        new_sft_data.append(x)\n",
    "```\n",
    "\n",
    "**动机：**\n",
    "- 移除低质量的推理轨迹\n",
    "- 提高训练数据的质量\n",
    "- 减少模型学习错误推理模式的概率\n",
    "\n",
    "#### 2.5.3 性能指标监控\n",
    "\n",
    "**训练指标：**\n",
    "- `train/loss`：训练损失\n",
    "- `train/entropy`：响应token的平均熵\n",
    "\n",
    "**评估指标：**\n",
    "- `eval/acc`：答案准确率\n",
    "- `eval/format_rate`：格式正确率\n",
    "\n",
    "### 2.6 关键实现细节和技巧\n",
    "\n",
    "#### 2.6.1 梯度累积\n",
    "\n",
    "由于模型较大，单个批次无法放入GPU，使用梯度累积来模拟更大的批次：\n",
    "\n",
    "```python\n",
    "gradient_accumulation_steps = 8\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps  # 32\n",
    "```\n",
    "\n",
    "#### 2.6.2 内存优化\n",
    "\n",
    "- **bfloat16 精度**：节省显存同时保持训练稳定性\n",
    "- **FlashAttention-2**：高效注意力计算\n",
    "- **梯度检查点**：以时间换空间（可选）\n",
    "\n",
    "#### 2.6.3 数值稳定性\n",
    "\n",
    "- **梯度裁剪**：防止梯度爆炸\n",
    "- **损失缩放**：确保梯度累积的数值稳定性\n",
    "\n",
    "#### 2.6.4 评估策略\n",
    "\n",
    "- **定期评估**：每5步评估一次性能\n",
    "- **权重同步**：训练权重同步到vLLM实例\n",
    "- **日志记录**：使用wandb记录训练曲线\n",
    "\n",
    "### 2.7 实验结果分析\n",
    "\n",
    "#### 2.7.1 数据集大小的影响\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/sft_train_loss.png\" width=\"400\"/>\n",
    "    <img src=\"../images/sft_eval_acc.png\" width=\"400\"/>\n",
    "    <img src=\"../images/sft_eval_format.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "从图中可以清晰地看到，数据集越大，最终的训练损失越低。而在评测集上的精确度 512 条数据时表现最好，能达到 `76.20%`。我们的一篇研究[Structure Trumps Size: Rethinking Data Quality for LLM Reasoning](https://aclanthology.org/2025.findings-emnlp.616/)发现，大的趋势是数据越多越好，但是 1k 之前带来的收益最大，在超过 10k 后收益持续变小，并且会对模型的通用能力损失较大。\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"../images/sft_volume.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "#### 2.7.2 过滤数据的效果\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"../images/sft_256_raw_vs_filter.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "移除错误推理轨迹通常能带来更好地性能提升，但是在此数据上不明显，在 256 条数据时，过滤后只是从 75.50% 提升到 76.20%。但是相较于基座模型提升很大（17.59% --> 76.20%）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbc427",
   "metadata": {},
   "source": [
    "## 第三部分：专家迭代 (Expert Iteration)\n",
    "\n",
    "### 学习目标\n",
    "在本部分中，我们将学习专家迭代算法，这是一种结合了自我生成数据和监督学习的强大方法。这包括：\n",
    "1. 理解专家迭代的原理和优势\n",
    "2. 掌握迭代式推理能力提升的方法\n",
    "3. 实现完整的专家迭代训练流程\n",
    "4. 学习超参数调优和性能分析\n",
    "\n",
    "### 3.1 专家迭代的原理\n",
    "\n",
    "#### 3.1.1 什么是专家迭代？\n",
    "\n",
    "专家迭代（Expert Iteration）是一种自举学习算法，由Anthony等人于2017年提出。它通过以下循环来提升模型性能：\n",
    "\n",
    "1. **生成专家数据**：使用当前模型生成大量推理轨迹\n",
    "2. **过滤高质量示例**：只保留能得出正确答案的推理轨迹\n",
    "3. **监督学习**：在这些\"专家\"轨迹上训练模型\n",
    "4. **迭代改进**：重复上述过程\n",
    "\n",
    "#### 3.1.2 为什么专家迭代有效？\n",
    "\n",
    "- **自举学习**：模型从自己的成功案例中学习，避免了人工标注的高成本\n",
    "- **持续改进**：每次迭代都能生成更好的推理轨迹\n",
    "- **数据效率**：专注于高质量的推理模式\n",
    "- **可扩展性**：可以持续进行多轮迭代\n",
    "\n",
    "#### 3.1.3 与纯SFT的区别\n",
    "\n",
    "| 方面 | 纯SFT | 专家迭代 |\n",
    "|------|-------|----------|\n",
    "| 数据来源 | 预先准备的推理轨迹 | 模型自我生成的轨迹 |\n",
    "| 数据质量 | 依赖人工或强模型 | 通过正确性过滤保证质量 |\n",
    "| 迭代性 | 一次性训练 | 多轮自举改进 |\n",
    "| 适应性 | 固定数据 | 随模型改进而改进 |\n",
    "\n",
    "### 3.2 算法流程详解\n",
    "\n",
    "#### 3.2.1 整体架构\n",
    "\n",
    "专家迭代的核心循环如下：\n",
    "\n",
    "```python\n",
    "for step in range(N_EI_STEPS):\n",
    "    # A. 使用当前模型生成大量推理轨迹\n",
    "    rollouts = generate_rollouts(current_model, questions, G)\n",
    "\n",
    "    # B. 过滤出正确的推理轨迹作为\"专家\"数据\n",
    "    expert_data = filter_correct_rollouts(rollouts)\n",
    "\n",
    "    # C. 在专家数据上进行监督学习\n",
    "    current_model = supervised_finetuning(current_model, expert_data)\n",
    "\n",
    "    # D. 评估性能提升\n",
    "    evaluate_model(current_model)\n",
    "```\n",
    "\n",
    "#### 3.2.2 关键超参数\n",
    "\n",
    "- **G (rollouts_per_question)**：每个问题生成的推理轨迹数量\n",
    "- **Db (expert_batch_size)**：每次迭代使用的训练问题数量\n",
    "- **epochs_per_step**：每次迭代的SFT训练轮数\n",
    "- **N_EI_STEPS**：总的迭代轮数\n",
    "\n",
    "### 3.3 核心实现组件\n",
    "\n",
    "#### 3.3.1 推理轨迹生成\n",
    "\n",
    "```python\n",
    "# 采样参数设置\n",
    "rollout_params = SamplingParams(\n",
    "    temperature=1.0,      # 高温度鼓励多样性\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    min_tokens=4,         # 防止空响应\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True,\n",
    "    n=rollouts_per_question  # 每个问题生成G个轨迹\n",
    ")\n",
    "\n",
    "# 生成rollouts\n",
    "outputs = llm.generate(prompts, rollout_params)\n",
    "```\n",
    "\n",
    "**关键点：**\n",
    "- **温度设置**：使用1.0以获得多样化的推理轨迹\n",
    "- **最小长度**：确保生成有意义的推理过程\n",
    "- **批处理**：同时为多个问题生成多个轨迹\n",
    "\n",
    "#### 3.3.2 正确性过滤\n",
    "\n",
    "```python\n",
    "# 过滤逻辑\n",
    "new_sft_data = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, req_output in enumerate(outputs):\n",
    "    gt = ground_truths[i]\n",
    "    prompt = prompts[i]\n",
    "\n",
    "    for completion in req_output.outputs:\n",
    "        generated_text = completion.text\n",
    "\n",
    "        # 使用奖励函数验证正确性\n",
    "        score = r1_zero_reward_fn(generated_text, gt)\n",
    "\n",
    "        if score['answer_reward'] == 1.0:\n",
    "            correct_count += 1\n",
    "            new_sft_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text\n",
    "            })\n",
    "```\n",
    "\n",
    "**过滤标准：**\n",
    "- 只保留答案正确的推理轨迹\n",
    "- 要求格式也正确（由奖励函数判断）\n",
    "- 构建新的SFT训练数据集\n",
    "\n",
    "#### 3.3.3 熵计算监控\n",
    "\n",
    "```python\n",
    "def compute_mean_entropy(logits, mask):\n",
    "    \"\"\"\n",
    "    计算响应token的平均熵，用于监控模型确定性\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 熵 = -sum(p * log p)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "\n",
    "    # 只计算响应token的熵\n",
    "    masked_entropy = entropy * mask\n",
    "    sum_mask = mask.sum()\n",
    "    return masked_entropy.sum() / sum_mask if sum_mask > 0 else torch.tensor(0.0)\n",
    "```\n",
    "\n",
    "**熵的监控价值：**\n",
    "- **训练初期**：熵较高，表示模型不确定\n",
    "- **训练中期**：熵逐渐降低，表示模型学会了推理模式\n",
    "- **收敛状态**：熵稳定，表示模型达到最优\n",
    "\n",
    "### 3.4 实验设置和超参数调优\n",
    "\n",
    "#### 3.4.1 主要实验变量\n",
    "\n",
    "```python\n",
    "# 数据集大小扫描\n",
    "EXPERT_BATCH_SIZES = [512, 1024, 2048]  # Db\n",
    "\n",
    "# Rollouts per question\n",
    "ROLLOUTS_PER_QUESTION = [1, 4, 8, 16]   # G\n",
    "\n",
    "# SFT epochs per step\n",
    "SFT_EPOCHS_PER_STEP = [4, 8, 16]     # epochs\n",
    "```\n",
    "\n",
    "#### 3.4.2 实验配置组合\n",
    "\n",
    "主要实验设置：\n",
    "\n",
    "1. **固定G=4, epochs=4，变化Db**：研究数据集大小的影响\n",
    "2. **固定Db=512, epochs=4，变化G**：研究rollouts数量的影响\n",
    "3. **固定Db=512, G=4，变化epochs**：研究SFT训练深度\n",
    "\n",
    "#### 3.4.3 训练流程\n",
    "\n",
    "```python\n",
    "for step in range(N_EI_STEPS):\n",
    "    print(f\"=== Expert Iteration Step {step+1}/{N_EI_STEPS} ===\")\n",
    "\n",
    "    # 1. 同步权重到vLLM\n",
    "    load_policy_into_vllm(policy, llm)\n",
    "\n",
    "    # 2. 生成rollouts\n",
    "    print(f\"Generating {len(prompts) * rollouts_per_question} rollouts...\")\n",
    "    outputs = llm.generate(prompts, rollout_params)\n",
    "\n",
    "    # 3. 过滤正确示例\n",
    "    new_sft_data = filter_correct_examples(outputs, prompts, ground_truths)\n",
    "\n",
    "    # 4. SFT训练\n",
    "    print(f\"Training on {len(new_sft_data)} examples...\")\n",
    "    policy = sft_train_step(policy, new_sft_data, sft_epochs_per_step)\n",
    "\n",
    "    # 5. 评估\n",
    "    evaluate_performance(policy, llm, val_data)\n",
    "```\n",
    "\n",
    "### 3.5 性能分析和结果解读\n",
    "\n",
    "#### 3.5.1 关键指标监控\n",
    "\n",
    "**训练指标：**\n",
    "- `ei/correct_rate`：每轮rollouts的正确率\n",
    "- `ei/dataset_size`：专家数据集大小\n",
    "- `train/loss`：SFT训练损失\n",
    "- `train/entropy`：响应token熵\n",
    "\n",
    "**评估指标：**\n",
    "- `eval/acc`：验证集准确率\n",
    "- `eval/format_rate`：格式正确率\n",
    "\n",
    "#### 3.5.2 实验结果分析\n",
    "\n",
    "**数据集大小 (Db) 的影响：**\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/sft_ei_Db_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_Db_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_Db_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "Db = 512:  accuracy = 63.08%\n",
    "Db = 1024: accuracy = 67.17%\n",
    "Db = 2048: accuracy = 66.49%\n",
    "\n",
    "在本次实验中，Db=1024 表现出最佳的最终准确率 (67.17%)，而 Db=512 在训练初期表现最好，Db=2048 虽然最终准确率略低于 Db=1024，但其训练过程最为稳定，最终的熵值更低。\n",
    "\n",
    "**Rollouts数量 (G) 的影响：**\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/sft_ei_rollout_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_rollout_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_rollout_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "G = 1:  accuracy = 66.49%\n",
    "G = 4:  accuracy = 67.17%\n",
    "G = 8:  accuracy = 66.19%\n",
    "G = 16: accuracy = 65.96%\n",
    "\n",
    "在本次实验中，G=4 表现出最佳的最终准确率 (67.17%)。随着 G 值的增加，模型的最终性能呈现先升后降的趋势。这表明存在一个“最优”的采样多样性水平：过少的采样会导致数据不足，而过多的采样则会引入噪声和低质量样本，反而损害模型性能。\n",
    "\n",
    "**训练深度 (epochs) 的影响：**\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/sft_ei_epoch_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_epoch_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/sft_ei_epoch_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "epochs = 4:  optimal = 63.08%\n",
    "epochs = 8:  good_fit = 63.46%\n",
    "epochs = 16: overfitting = 71.80%\n",
    "\n",
    "在本次实验中，epochs=16 表现出最高的最终准确率 (71.80%)，显著优于 epochs=4 (63.08%) 和 epochs=8 (63.46%)。这表明，在专家迭代框架下，对高质量的\"专家\"数据进行更深度的训练是提升模型性能的关键。尽管 epochs=16 的曲线在中期出现波动，但其最终收敛效果最好，证明了其强大的学习能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fa9a7",
   "metadata": {},
   "source": [
    "## 第四部分：分组相对策略优化 (GRPO)\n",
    "\n",
    "### 学习目标\n",
    "在本部分中，我们将学习分组相对策略优化 (Group Relative Policy Optimization, GRPO)，这是一种专为语言模型推理任务设计的强化学习算法。这包括：\n",
    "1. 理解 GRPO 的核心原理和相对于传统 RL 的优势\n",
    "2. 掌握分组归一化优势的计算方法\n",
    "3. 实现多种策略梯度损失函数\n",
    "4. 构建完整的 GRPO 训练循环\n",
    "\n",
    "### 4.1 GRPO 的核心原理\n",
    "\n",
    "#### 4.1.1 什么是 GRPO？\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) 是 DeepSeekMath 提出的强化学习算法，专门为语言模型推理优化而设计。它结合了以下关键思想：\n",
    "\n",
    "1. **分组相对优势**：在一个问题上采样多个响应，使用组内统计进行归一化\n",
    "2. **策略梯度优化**：使用 REINFORCE 风格的策略梯度更新\n",
    "3. **裁剪机制**：借鉴 PPO 的裁剪技巧确保训练稳定性\n",
    "4. **离策略能力**：可以在多个 rollout 批次上进行迭代优化\n",
    "\n",
    "#### 4.1.2 GRPO vs 传统 RLHF\n",
    "\n",
    "| 方面 | 传统 RLHF (PPO) | GRPO |\n",
    "|------|----------------|------|\n",
    "| 价值函数 | 需要学习 V(s) | 使用分组统计替代 |\n",
    "| 优势估计 | A(s,a) = Q(s,a) - V(s) | A_i = (r_i - mean_r)/std_r |\n",
    "| 基线计算 | 神经网络预测 | 组内经验统计 |\n",
    "| 内存需求 | 高 (值函数) | 低 (仅统计) |\n",
    "| 超参数 | 复杂 | 简化 |\n",
    "\n",
    "#### 4.1.3 GRPO 的优势\n",
    "\n",
    "- **计算效率**：无需训练价值函数，节省显存和计算\n",
    "- **统计可靠性**：基于经验统计的优势更稳定\n",
    "- **适应性强**：适用于各种奖励函数和任务\n",
    "- **收敛性好**：裁剪机制确保训练稳定性\n",
    "\n",
    "### 4.2 分组归一化优势计算\n",
    "\n",
    "#### 4.2.1 核心思想\n",
    "\n",
    "GRPO 的核心创新是**分组相对优势**的计算：\n",
    "\n",
    "对于一个问题 q，从策略 π_θ 中采样 G 个响应 {o^(i)}_{i=1}^G，计算每个响应的奖励 r^(i) = R(q, o^(i))，然后计算分组归一化的优势：\n",
    "\n",
    "```\n",
    "A^(i) = (r^(i) - mean(r^(1), r^(2), ..., r^(G))) / (std(r^(1), r^(2), ..., r^(G)) + advantage_eps)\n",
    "```\n",
    "\n",
    "这个优势 A^(i) 对于响应中的每个 token 都是相同的。\n",
    "\n",
    "#### 4.2.2 实现细节\n",
    "\n",
    "```python\n",
    "def compute_group_normalized_rewards(\n",
    "        reward_fn: Callable[[str, str], Dict[str, float]],\n",
    "        rollout_responses: List[str],\n",
    "        repeated_ground_truths: List[str],\n",
    "        group_size: int,\n",
    "        advantage_eps: float,\n",
    "        normalize_by_std: bool,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:\n",
    "```\n",
    "\n",
    "**关键实现步骤：**\n",
    "\n",
    "1. **计算原始奖励**\n",
    "```python\n",
    "raw_rewards_list = []\n",
    "for resp, gt in zip(rollout_responses, repeated_ground_truths):\n",
    "    scores = reward_fn(resp, gt)\n",
    "    raw_rewards_list.append(float(scores[\"reward\"]))\n",
    "raw_rewards = torch.tensor(raw_rewards_list, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "2. **分组重组**\n",
    "```python\n",
    "rollout_batch_size = len(rollout_responses)\n",
    "n_groups = rollout_batch_size // group_size\n",
    "rewards_g = raw_rewards.view(n_groups, group_size)  # (n_groups, group_size)\n",
    "```\n",
    "\n",
    "3. **计算组统计**\n",
    "```python\n",
    "group_mean = rewards_g.mean(dim=1, keepdim=True)  # (n_groups, 1)\n",
    "centered = rewards_g - group_mean  # (n_groups, group_size)\n",
    "\n",
    "if normalize_by_std:\n",
    "    group_std = rewards_g.std(dim=1, keepdim=True, unbiased=True)  # (n_groups, 1)\n",
    "    denom = group_std + float(advantage_eps)\n",
    "    advantages_g = centered / denom\n",
    "else:\n",
    "    advantages_g = centered  # 只减去均值，不除以标准差\n",
    "\n",
    "advantages = advantages_g.reshape(-1)  # (rollout_batch_size,)\n",
    "```\n",
    "\n",
    "#### 4.2.3 优势的含义\n",
    "\n",
    "- **正优势 (A > 0)**：该响应在组内表现优于平均水平，应该增加其概率\n",
    "- **负优势 (A < 0)**：该响应在组内表现劣于平均水平，应该减少其概率\n",
    "- **零优势 (A = 0)**：该响应在组内表现等于平均水平，不改变其概率\n",
    "\n",
    "#### 4.2.4 元数据收集\n",
    "\n",
    "函数还返回详细的统计信息用于调试和监控：\n",
    "\n",
    "```python\n",
    "metadata = {\n",
    "    \"rollout_batch_size\": float(rollout_batch_size),\n",
    "    \"n_groups\": float(n_groups),\n",
    "    \"group_size\": float(group_size),\n",
    "    \"raw_reward_mean\": float(raw_rewards.mean().item()),\n",
    "    \"raw_reward_std\": float(raw_rewards.std(unbiased=False).item()),\n",
    "    \"group_reward_mean_mean\": float(group_means.mean().item()),\n",
    "    \"adv_mean\": float(advantages.mean().item()),\n",
    "    \"adv_std\": float(advantages.std(unbiased=False).item()) if advantages.numel() > 1 else 0.0,\n",
    "    # ... 更多统计\n",
    "}\n",
    "```\n",
    "\n",
    "### 4.3 策略梯度损失函数\n",
    "\n",
    "#### 4.3.1 朴素策略梯度损失\n",
    "\n",
    "最基本的策略梯度损失：\n",
    "\n",
    "```python\n",
    "def compute_naive_policy_gradient_loss(\n",
    "        raw_rewards_or_advantages: torch.Tensor,  # (batch_size, 1)\n",
    "        policy_log_probs: torch.Tensor,          # (batch_size, sequence_length)\n",
    ") -> torch.Tensor:                               # (batch_size, sequence_length)\n",
    "```\n",
    "\n",
    "**计算公式：**\n",
    "```\n",
    "loss_{b,t} = -A_b * log π_θ(o_{b,t} | q_b, o_{b,<t})\n",
    "```\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "advantages = raw_rewards_or_advantages.to(dtype=policy_log_probs.dtype, device=policy_log_probs.device)\n",
    "loss = -(advantages * policy_log_probs)  # 广播: (B,1) -> (B,T)\n",
    "return loss\n",
    "```\n",
    "\n",
    "#### 4.3.2 GRPO-Clip 损失\n",
    "\n",
    "带裁剪机制的策略梯度损失：\n",
    "\n",
    "```python\n",
    "def compute_grpo_clip_loss(\n",
    "        advantages: torch.Tensor,           # (batch_size, 1)\n",
    "        policy_log_probs: torch.Tensor,     # (batch_size, sequence_length)\n",
    "        old_log_probs: torch.Tensor,        # (batch_size, sequence_length)\n",
    "        cliprange: float,                   # 裁剪范围 ε\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "```\n",
    "\n",
    "**核心公式：**\n",
    "```\n",
    "ratio_{b,t} = π_θ(o_{b,t}) / π_{θ_old}(o_{b,t})\n",
    "clipped_ratio = clip(ratio, 1-ε, 1+ε)\n",
    "loss_{b,t} = -min(ratio_{b,t} * A_b, clipped_ratio_{b,t} * A_b)\n",
    "```\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "A = advantages.to(dtype=policy_log_probs.dtype, device=policy_log_probs.device)\n",
    "log_ratio = policy_log_probs - old_log_probs\n",
    "ratio = torch.exp(log_ratio)\n",
    "clipped_ratio = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n",
    "\n",
    "unclipped_obj = ratio * A\n",
    "clipped_obj = clipped_ratio * A\n",
    "loss = -torch.minimum(unclipped_obj, clipped_obj)\n",
    "\n",
    "# 统计裁剪比例\n",
    "is_clipped = clipped_obj < unclipped_obj\n",
    "metadata = {\n",
    "    \"ratio\": ratio,\n",
    "    \"clipped_ratio\": clipped_ratio,\n",
    "    \"is_clipped\": is_clipped,\n",
    "    \"clip_fraction\": is_clipped.float().mean()\n",
    "}\n",
    "```\n",
    "\n",
    "#### 4.3.3 策略梯度损失包装器\n",
    "\n",
    "统一的损失函数接口：\n",
    "\n",
    "```python\n",
    "def compute_policy_gradient_loss(\n",
    "        policy_log_probs: torch.Tensor,\n",
    "        loss_type: Literal[\"no_baseline\", \"reinforce_with_baseline\", \"grpo_clip\", \"grpo_no_clip\"],\n",
    "        raw_rewards: torch.Tensor | None = None,\n",
    "        advantages: torch.Tensor | None = None,\n",
    "        old_log_probs: torch.Tensor | None = None,\n",
    "        cliprange: float | None = None,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "```\n",
    "\n",
    "**支持的损失类型：**\n",
    "- `\"no_baseline\"`：使用原始奖励作为优势\n",
    "- `\"reinforce_with_baseline\"`：使用分组归一化优势\n",
    "- `\"grpo_clip\"`：带裁剪的 GRPO 损失（离策略）\n",
    "- `\"grpo_no_clip\"`：不带裁剪的 GRPO 损失\n",
    "\n",
    "### 4.4 工具函数实现\n",
    "\n",
    "#### 4.4.1 掩码均值 (Masked Mean)\n",
    "\n",
    "在响应token上计算平均值：\n",
    "\n",
    "```python\n",
    "def masked_mean(\n",
    "        tensor: torch.Tensor,    # (batch_size, sequence_length)\n",
    "        mask: torch.Tensor,      # (batch_size, sequence_length)\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:\n",
    "```\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "m = mask.to(device=tensor.device, dtype=tensor.dtype)\n",
    "masked_sum = (tensor * m).sum() if dim is None else (tensor * m).sum(dim=dim)\n",
    "denom = m.sum() if dim is None else m.sum(dim=dim)\n",
    "return masked_sum / denom\n",
    "```\n",
    "\n",
    "**用途：**\n",
    "- 将每个token的损失聚合为每个响应的标量损失\n",
    "- 计算响应token上的平均熵、裁剪分数等\n",
    "\n",
    "#### 4.4.2 掩码归一化 (Masked Normalize)\n",
    "\n",
    "按常数归一化而不是按掩码元素数量：\n",
    "\n",
    "```python\n",
    "def masked_normalize(\n",
    "        tensor: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        normalize_constant: float,\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:\n",
    "```\n",
    "\n",
    "**实现：**\n",
    "```python\n",
    "m = mask.to(device=tensor.device, dtype=tensor.dtype)\n",
    "masked_sum = (tensor * m).sum() if dim is None else (tensor * m).sum(dim=dim)\n",
    "return masked_sum / float(normalize_constant)\n",
    "```\n",
    "\n",
    "**用途：**\n",
    "- 实现不同的长度归一化策略\n",
    "- 控制梯度大小和训练稳定性\n",
    "\n",
    "### 4.5 GRPO 微批次训练步骤\n",
    "\n",
    "#### 4.5.1 核心函数\n",
    "\n",
    "```python\n",
    "def grpo_microbatch_train_step(\n",
    "        policy_log_probs: torch.Tensor,         # (batch_size, sequence_length)\n",
    "        response_mask: torch.Tensor,            # (batch_size, sequence_length)\n",
    "        gradient_accumulation_steps: int,\n",
    "        loss_type: str,\n",
    "        raw_rewards: torch.Tensor | None = None,\n",
    "        advantages: torch.Tensor | None = None,\n",
    "        old_log_probs: torch.Tensor | None = None,\n",
    "        cliprange: float | None = None,\n",
    "        length_norm: str = \"masked_mean\",\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "```\n",
    "\n",
    "#### 4.5.2 训练步骤详解\n",
    "\n",
    "1. **计算每个token的损失**\n",
    "```python\n",
    "per_token_loss, meta = compute_policy_gradient_loss(\n",
    "    policy_log_probs=policy_log_probs,\n",
    "    loss_type=loss_type,\n",
    "    raw_rewards=raw_rewards,\n",
    "    advantages=advantages,\n",
    "    old_log_probs=old_log_probs,\n",
    "    cliprange=cliprange,\n",
    ")\n",
    "```\n",
    "\n",
    "2. **长度归一化**\n",
    "```python\n",
    "mask = response_mask.to(dtype=per_token_loss.dtype, device=per_token_loss.device)\n",
    "if length_norm == \"masked_mean\":\n",
    "    per_example_loss = masked_mean(per_token_loss, mask, dim=1)  # (batch_size,)\n",
    "elif length_norm == \"masked_normalize\":\n",
    "    per_example_loss = masked_normalize(per_token_loss, mask, dim=1, constant_normalizer=max_len)\n",
    "```\n",
    "\n",
    "3. **批次平均和梯度累积**\n",
    "```python\n",
    "microbatch_loss = per_example_loss.mean()  # 标量\n",
    "loss = microbatch_loss / float(gradient_accumulation_steps)\n",
    "loss.backward()  # 只累积梯度\n",
    "```\n",
    "\n",
    "4. **元数据收集**\n",
    "```python\n",
    "out_meta = dict(meta)  # 复制底层元数据\n",
    "out_meta.update({\n",
    "    \"microbatch_loss\": microbatch_loss.detach(),\n",
    "    \"per_example_loss_mean\": per_example_loss.detach().mean(),\n",
    "    \"per_example_loss_std\": per_example_loss.detach().std(unbiased=False),\n",
    "})\n",
    "```\n",
    "\n",
    "### 4.6 完整 GRPO 训练循环\n",
    "\n",
    "#### 4.6.1 训练架构\n",
    "\n",
    "GRPO 训练使用双GPU设置：\n",
    "- **GPU 0**：策略模型训练\n",
    "- **GPU 1**：vLLM 推理和评估\n",
    "\n",
    "#### 4.6.2 关键超参数\n",
    "\n",
    "```python\n",
    "# 核心配置\n",
    "N_GRPO_STEPS = 200              # 总训练步数\n",
    "ROLLOUT_BATCH_SIZE = 256        # rollout 批次大小\n",
    "GROUP_SIZE = 8                  # 每个问题的响应数 G\n",
    "TRAIN_BATCH_SIZE = 256          # 训练批次大小\n",
    "EPOCHS_PER_ROLLOUT_BATCH = 1    # 每个rollout批次的训练轮数\n",
    "\n",
    "# 优势计算\n",
    "ADVANTAGE_EPS = 1e-6           # 防止除零的小常数\n",
    "USE_STD_NORMALIZATION = True   # 是否按标准差归一化\n",
    "\n",
    "# 损失函数\n",
    "LOSS_TYPE = \"reinforce_with_baseline\"  # 损失类型\n",
    "CLIPRANGE = 0.2                 # 裁剪范围（用于离策略）\n",
    "\n",
    "# 训练优化\n",
    "LR = 1e-5                       # 学习率\n",
    "GRAD_ACCUM_STEPS = 128          # 梯度累积步数\n",
    "MAX_GRAD_NORM = 1.0            # 梯度裁剪\n",
    "SEED = 2026                     # 随机种子\n",
    "```\n",
    "\n",
    "#### 4.6.3 批次大小关系\n",
    "\n",
    "```python\n",
    "# 批次大小的数学关系\n",
    "assert TRAIN_BATCH_SIZE % GRAD_ACCUM_STEPS == 0\n",
    "micro_batch_size = TRAIN_BATCH_SIZE // GRAD_ACCUM_STEPS\n",
    "\n",
    "assert ROLLOUT_BATCH_SIZE % GROUP_SIZE == 0\n",
    "n_prompts_per_rollout = ROLLOUT_BATCH_SIZE // GROUP_SIZE\n",
    "\n",
    "assert TRAIN_BATCH_SIZE >= GROUP_SIZE\n",
    "```\n",
    "\n",
    "#### 4.6.4 采样参数配置\n",
    "\n",
    "```python\n",
    "# Rollout 采样参数\n",
    "rollout_params = SamplingParams(\n",
    "    temperature=1.0,      # 高温度鼓励多样性\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    min_tokens=4,         # 防止空响应\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True,\n",
    "    n=GROUP_SIZE         # 每个prompt的响应数\n",
    ")\n",
    "\n",
    "# 评估采样参数\n",
    "eval_params = SamplingParams(\n",
    "    temperature=0.0,      # 确定性评估\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### 4.6.5 训练循环实现\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # 1. 初始化模型和数据\n",
    "    policy = init_policy_model()\n",
    "    llm = init_vllm()\n",
    "    train_data, val_data = load_data()\n",
    "\n",
    "    # 2. 训练循环\n",
    "    for step in range(N_GRPO_STEPS):\n",
    "        print(f\"=== GRPO Step {step+1}/{N_GRPO_STEPS} ===\")\n",
    "\n",
    "        # A. 采样 prompts 批次\n",
    "        batch_prompts = sample_prompts(train_data, n_prompts_per_rollout)\n",
    "\n",
    "        # B. 更新 vLLM 权重\n",
    "        load_policy_into_vllm(policy, llm)\n",
    "\n",
    "        # C. 生成 rollouts\n",
    "        outputs = llm.generate(batch_prompts, rollout_params)\n",
    "\n",
    "        # D. 收集响应和计算奖励\n",
    "        all_responses, all_rewards = collect_rollouts_and_rewards(outputs)\n",
    "\n",
    "        # E. 计算分组归一化优势\n",
    "        advantages, raw_rewards, _ = compute_group_normalized_rewards(\n",
    "            reward_fn=r1_zero_reward_fn,\n",
    "            rollout_responses=all_responses,\n",
    "            repeated_ground_truths=all_rewards,  # GRPO 不需要 GT\n",
    "            group_size=GROUP_SIZE,\n",
    "            advantage_eps=ADVANTAGE_EPS,\n",
    "            normalize_by_std=USE_STD_NORMALIZATION\n",
    "        )\n",
    "\n",
    "        # F. 创建训练数据集\n",
    "        dataset = create_training_dataset(batch_prompts, all_responses, advantages)\n",
    "\n",
    "        # G. 训练步骤\n",
    "        train_on_dataset(policy, dataset, optimizer)\n",
    "\n",
    "        # H. 评估\n",
    "        if step % EVAL_EVERY_STEPS == 0:\n",
    "            evaluate_policy(policy, llm, val_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e06bf8",
   "metadata": {},
   "source": [
    "## 第五部分：GRPO 实验\n",
    "\n",
    "### 学习目标\n",
    "在本部分中，我们将通过一系列系统性的实验来探索 GRPO 算法的各个方面，包括：\n",
    "1. 超参数调优（学习率、基线、归一化等）\n",
    "2. 算法变体比较（在策略 vs 离策略）\n",
    "3. 消融实验（裁剪、提示等）\n",
    "4. 最终的排行榜挑战\n",
    "\n",
    "### 5.1 实验基础设置\n",
    "\n",
    "#### 5.1.1 实验配置\n",
    "\n",
    "所有 GRPO 实验都基于以下基础配置：\n",
    "\n",
    "```python\n",
    "# 基础配置\n",
    "BASE_CONFIG = GRPOConfig(\n",
    "    # 模型和数据\n",
    "    base_model=\"/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B\",\n",
    "    train_data_path=\"data/sft/sft_gpt-oss-120b_filtered.jsonl\",\n",
    "    val_data_path=\"data/gsm8k/test.jsonl\",\n",
    "\n",
    "    # 核心超参数\n",
    "    n_grpo_steps=200,\n",
    "    rollout_batch_size=256,\n",
    "    group_size=8,\n",
    "    train_batch_size=256,\n",
    "    epochs_per_rollout_batch=1,\n",
    "\n",
    "    # 优势计算\n",
    "    advantage_eps=1e-6,\n",
    "    use_std_normalization=True,\n",
    "\n",
    "    # 训练参数\n",
    "    lr=1e-5,\n",
    "    grad_accum_steps=128,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # 采样参数\n",
    "    gen_temperature=1.0,\n",
    "    gen_top_p=1.0,\n",
    "    gen_max_tokens=1024,\n",
    "    gen_min_tokens=4,\n",
    ")\n",
    "```\n",
    "\n",
    "#### 5.1.2 评估协议\n",
    "\n",
    "所有实验都遵循统一的评估协议：\n",
    "- **指标**：验证集答案准确率（answer_accuracy）\n",
    "- **频率**：每10步评估一次\n",
    "- **采样**：temperature=0.0（确定性）\n",
    "- **提示**：R1-Zero 提示\n",
    "- **奖励函数**：r1_zero_reward_fn\n",
    "\n",
    "### 5.2 学习率调整实验 (grpo_learning_rate)\n",
    "\n",
    "#### 5.2.1 实验目标\n",
    "\n",
    "学习率是 GRPO 训练中最关键的超参数之一。本实验旨在找到最优的学习率设置。\n",
    "\n",
    "#### 5.2.2 实验设置\n",
    "\n",
    "```python\n",
    "LEARNING_RATES = [1e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4]\n",
    "\n",
    "def run_lr_sweep():\n",
    "    results = {}\n",
    "    for lr in LEARNING_RATES:\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.lr = lr\n",
    "        config.run_name = f\"grpo_lr_{lr}\"\n",
    "\n",
    "        # 训练并收集结果\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[lr] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.2.3 结果分析\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"../images/grpo_lr_eval_acc.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "学习率结果：\n",
    "1e-6:  18.88% (收敛太慢)\n",
    "5e-6:  32.98% (仍然偏慢)\n",
    "1e-5:  51.71% (最佳性能)\n",
    "2e-5:  66.57% (略有下降)\n",
    "5e-5:  49.28% (开始不稳定)\n",
    "1e-4:  0.61% \n",
    "\n",
    "**最优学习率**：2e-5，达到66.57%的准确率，后续实验将采用此参数值。\n",
    "\n",
    "### 5.3 基线影响实验 (grpo_baselines)\n",
    "\n",
    "#### 5.3.1 实验目标\n",
    "\n",
    "比较有基线和无基线的策略梯度损失对性能的影响。\n",
    "\n",
    "#### 5.3.2 实验设置\n",
    "\n",
    "```python\n",
    "BASELINE_TYPES = [\"no_baseline\", \"reinforce_with_baseline\"]\n",
    "\n",
    "def run_baseline_experiment():\n",
    "    results = {}\n",
    "\n",
    "    for loss_type in BASELINE_TYPES:\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.loss_type = loss_type\n",
    "        config.lr = 1e-5  # 使用最优学习率\n",
    "        config.run_name = f\"grpo_baseline_{loss_type}\"\n",
    "\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[loss_type] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.3.3 理论分析\n",
    "\n",
    "**无基线 (no_baseline)**：\n",
    "- 使用原始奖励 r(q,o) 作为优势\n",
    "- 简单直接，但方差较大\n",
    "- 对奖励尺度敏感\n",
    "\n",
    "**有基线 (reinforce_with_baseline)**：\n",
    "- 使用归一化优势 A^(i)\n",
    "- 降低梯度方差，提高稳定性\n",
    "- 对奖励尺度不敏感\n",
    "\n",
    "#### 5.3.4 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_baseline_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_baseline_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_baseline_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "基线实验结果：\n",
    "no_baseline:           49.68%\n",
    "reinforce_with_baseline: 34.95% \n",
    "```\n",
    "\n",
    "我们发现`reinforce_with_baseline`损失下降的更快，但为什么在测试集上表现性能更差呢？\n",
    "\n",
    "通过观察熵变化曲线我们发现`reinforce_with_baseline`的熵很快变得很低，意味着它过早地变得“非常自信”，停止了探索。它可能只是死记硬背了某种特定的输出模式（Mode Collapse），而不是真正学会了推理。而`no_baseline`保持了较高的熵，说明它在训练过程中一直在尝试不同的可能，保留了探索能力。虽然`no_baseline`格式学习稍慢，但它是伴随着验证集准确率提升而稳步学习的。\n",
    "\n",
    "**结论**：在当前的超参和模型设置下，不使用显式的复杂 Baseline（或保持高熵策略）是更优的选择\n",
    "\n",
    "### 5.4 长度归一化实验 (grpo_length_normalization)\n",
    "\n",
    "#### 5.4.1 实验目标\n",
    "\n",
    "比较不同的长度归一化方法对 GRPO 训练的影响。\n",
    "\n",
    "#### 5.4.2 方法对比\n",
    "\n",
    "**masked_mean**：\n",
    "- 对响应token求平均：`loss = sum(loss_per_token) / num_response_tokens`\n",
    "- 短响应获得更多权重（因为除数小）\n",
    "\n",
    "**masked_normalize**：\n",
    "- 按固定常数归一化：`loss = sum(loss_per_token) / constant`\n",
    "- 所有响应获得相等权重\n",
    "\n",
    "#### 5.4.3 实验设置\n",
    "\n",
    "```python\n",
    "NORMALIZATION_TYPES = [\"masked_mean\", \"masked_normalize\"]\n",
    "\n",
    "def run_normalization_experiment():\n",
    "    results = {}\n",
    "\n",
    "    for norm_type in NORMALIZATION_TYPES:\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.length_norm = norm_type\n",
    "        config.loss_type = \"reinforce_with_baseline\"\n",
    "        config.lr = 1e-5\n",
    "        config.run_name = f\"grpo_norm_{norm_type}\"\n",
    "\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[norm_type] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.4.4 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_length_norm_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_length_norm_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_length_norm_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_length_norm_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "长度归一化实验结果：\n",
    "masked_mean:      69.37%\n",
    "masked_normalize: 63.38% (+1.4%)\n",
    "\n",
    "仅做 $r - \\mu$。如果 Reward 的数值范围波动较大，计算出的 Advantage 绝对值就会很大。这直接导致计算出的梯度（Gradient）非常大。随着训练进行，模型对某些样本越来越自信（或 Reward 差异变大），梯度范数不断膨胀。\n",
    "蓝色 (Normalize): 做了 $\\frac{r - \\mu}{\\sigma}$。无论 Reward 的原始数值是 0.1 还是 100，归一化后的 Advantage 通常都落在 $[-2, 2]$ 甚至更小的区间内。这相当于自适应地调整了梯度的尺度，保证了更新幅度的稳定。\n",
    "\n",
    "\n",
    "\n",
    "**分析**：\n",
    "- `masked_normalize` 略优，因为它避免了短响应主导训练的问题\n",
    "- 梯度范数更稳定（`masked_normalize` 的梯度更一致）\n",
    "\n",
    "### 5.5 标准差归一化实验 (grpo_group_standard_deviation)\n",
    "\n",
    "#### 5.5.1 实验目标\n",
    "\n",
    "测试是否应该按组标准差进行优势归一化。\n",
    "\n",
    "#### 5.5.2 实验设置\n",
    "\n",
    "```python\n",
    "STD_NORMALIZATION_SETTINGS = [True, False]\n",
    "\n",
    "def run_std_norm_experiment():\n",
    "    results = {}\n",
    "\n",
    "    for use_std in STD_NORMALIZATION_SETTINGS:\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.use_std_normalization = use_std\n",
    "        config.length_norm = \"masked_normalize\"\n",
    "        config.loss_type = \"reinforce_with_baseline\"\n",
    "        config.lr = 1e-5\n",
    "        config.run_name = f\"grpo_std_{use_std}\"\n",
    "\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[use_std] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.5.3 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_std_norm_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_std_norm_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_std_norm_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_std_norm_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "标准差归一化实验结果：\n",
    "use_std=True:   63.46%\n",
    "use_std=False:  68.00%\n",
    "```\n",
    "\n",
    "开启优势归一化 (std_True) 在本实验中并没有带来性能提升，反而导致了严重的训练崩溃（Catastrophic Forgetting）。 这种崩溃极有可能是由于某些 Batch 中 Reward 的方差过小，导致归一化时除以了一个极小的数值，引发了梯度爆炸。\n",
    "\n",
    "Advantage Normalization 的公式通常是：\n",
    "            $$ \\hat{A} = \\frac{A - \\mu}{\\sigma + \\epsilon} $$\n",
    "如果某一个 Group 内的所有样本 Reward 非常接近（方差 $\\sigma$ 极小）**，那么分母 $\\sigma + \\epsilon$ 就会非常小。这会导致计算出的优势 $\\hat{A}$ 变得巨大（例如：$0.001 / 0.0001 = 10$）。巨大的 $\\hat{A}$ 会直接乘在 Loss 中，导致巨大的梯度（Gradient Explosion）。\n",
    "\n",
    "在 Step 100 左右，模型可能刚好遇到了一个 Batch，里面的生成结果 Reward 都差不多（比如都答对了，或者都答错了且错得一样），导致 $\\sigma \\approx 0$。归一化操作放大了这种微小的差异，导致了梯度爆炸。从图中我们也可以找到证据，在 Step 100 左右，梯度范数瞬间飙升。这说明模型进行了一次幅度极大的参数更新。紧接着梯度爆炸，Step 105 左右 Green 的熵（Entropy）突然飙升到 0.8 左右，高熵表示模型输出趋向于混乱。\n",
    "\n",
    "\n",
    "### 5.6 离策略 GRPO 实现 (grpo_off_policy)\n",
    "\n",
    "#### 5.6.1 核心变化\n",
    "\n",
    "离策略 GRPO 的关键变化：\n",
    "\n",
    "1. **多次训练**：对每个rollout批次进行多次梯度更新\n",
    "2. **旧对数概率**：保存rollout时的策略，用于重要性采样\n",
    "3. **裁剪机制**：防止策略过度偏离\n",
    "\n",
    "#### 5.6.2 实现细节\n",
    "\n",
    "```python\n",
    "def grpo_off_policy_train_step(\n",
    "        policy: torch.nn.Module,\n",
    "        rollouts_data: List[Dict],\n",
    "        old_log_probs: torch.Tensor,  # 新增\n",
    "        epochs_per_batch: int,\n",
    "        cliprange: float = 0.2,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    离策略 GRPO 训练步骤\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(epochs_per_batch):\n",
    "        # 重新采样批次（因为是离策略）\n",
    "        batch = sample_batch_from_rollouts(rollouts_data)\n",
    "\n",
    "        # 计算当前策略的对数概率\n",
    "        current_log_probs = get_current_log_probs(policy, batch)\n",
    "\n",
    "        # 使用 GRPO-Clip 损失\n",
    "        loss = compute_grpo_clip_loss(\n",
    "            advantages=batch[\"advantages\"],\n",
    "            policy_log_probs=current_log_probs,\n",
    "            old_log_probs=old_log_probs[batch_indices],\n",
    "            cliprange=cliprange\n",
    "        )\n",
    "\n",
    "        # 优化器步骤\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return metrics\n",
    "```\n",
    "\n",
    "#### 5.6.3 关键特性\n",
    "\n",
    "**优势**：\n",
    "- **更高样本效率**：每个rollout可用于多次更新\n",
    "- **更稳定收敛**：重要性采样校正\n",
    "- **适应性强**：适用于大型批次\n",
    "\n",
    "**挑战**：\n",
    "- **内存开销**：需要存储旧对数概率\n",
    "- **实现复杂度**：重要性采样逻辑\n",
    "- **数值稳定性**：重要性权重可能很大\n",
    "\n",
    "### 5.7 离策略超参数扫描 (grpo_off_policy_sweep)\n",
    "\n",
    "#### 5.7.1 实验设计\n",
    "\n",
    "扫描离策略的关键超参数：\n",
    "\n",
    "```python\n",
    "OFF_POLICY_CONFIGS = [\n",
    "    {\"epochs_per_rollout_batch\": 1, \"train_batch_size\": 256},  # 在策略\n",
    "    {\"epochs_per_rollout_batch\": 2, \"train_batch_size\": 128},  # 轻度离策略\n",
    "    {\"epochs_per_rollout_batch\": 4, \"train_batch_size\": 64},   # 中度离策略\n",
    "    {\"epochs_per_rollout_batch\": 8, \"train_batch_size\": 32},   # 重度离策略\n",
    "]\n",
    "```\n",
    "\n",
    "#### 5.7.2 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_off_policy_train_clip_fraction.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_off_policy_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_off_policy_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "离策略扫描结果：\n",
    "在策略 (1 epoch):    58.30%\n",
    "轻度离策略 (2 epochs): 73.84% (+1.3%)\n",
    "中度离策略 (4 epochs): 68.84% (+2.4%)\n",
    "重度离策略 (8 epochs): 63.69% (+1.1%)\n",
    "```\n",
    "\n",
    "Brown (8x) 的高裁剪率说明当前的策略已经偏离旧策略太远了。裁剪机制一直在拼命工作以阻止过大的更新，但这同时也意味着大部分梯度信息被截断了，或者剩下的梯度估计非常不准确。这解释了为什么 8x 训练效果最差。Brown (8x) 的熵下降慢，说明模型迟迟无法确信某种策略。由于反复在陈旧的数据上强行更新，模型陷入了混乱，无法收敛到一个确定的、高奖励的分布上。\n",
    "\n",
    "强行增加 Off-Policy 的程度（即增加重复训练次数）会导致严重的训练不稳定和性能崩溃，尤其是在高倍率（8x）下。轻微的 Off-Policy (2x) 表现最佳。这验证了强化学习中的一个基本原理：PPO/GRPO 是基于 Trust Region（信任域）的算法，如果旧策略（采样数据的策略）和新策略（当前训练的策略）差异过大（KL 散度过高），重要性采样比率（Importance Sampling Ratio）会失效，导致数值爆炸。\n",
    "\n",
    "\n",
    "### 5.8 裁剪消融实验 (grpo_off_policy_clip_ablation)\n",
    "\n",
    "#### 5.8.1 实验目标\n",
    "\n",
    "测试裁剪机制是否真的必要。\n",
    "\n",
    "#### 5.8.2 实验设置\n",
    "\n",
    "```python\n",
    "CLIP_SETTINGS = [\"grpo_clip\", \"grpo_no_clip\"]\n",
    "\n",
    "def run_clip_ablation():\n",
    "    results = {}\n",
    "\n",
    "    # 使用最优离策略配置\n",
    "    config = BASE_CONFIG.copy()\n",
    "    config.epochs_per_rollout_batch = 4\n",
    "    config.train_batch_size = 64\n",
    "\n",
    "    for clip_type in CLIP_SETTINGS:\n",
    "        config.loss_type = clip_type\n",
    "        config.run_name = f\"grpo_clip_{clip_type}\"\n",
    "\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[clip_type] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.8.3 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_clip_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_clip_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_clip_train_clip_fraction.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_clip_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "裁剪消融实验结果：\n",
    "grpo_clip:     79.99%\n",
    "grpo_no_clip:  68.23%\n",
    "```\n",
    "\n",
    "**结论**：裁剪对于离策略训练至关重要，防止策略发散。没有 Clip 机制（Red），模型训练虽然初期看似正常，但最终遭遇了灾难性的梯度爆炸，导致训练彻底失败；而开启 Clip 机制（Blue）虽然初期经历了剧烈的震荡，但最终成功稳定下来，并取得了远超 No Clip (Red) 的性能。\n",
    "\n",
    "### 5.9 提示消融实验 (grpo_prompt_ablation)\n",
    "\n",
    "#### 5.9.1 实验目标\n",
    "\n",
    "比较不同提示对 GRPO 性能的影响。\n",
    "\n",
    "#### 5.9.2 提示对比\n",
    "\n",
    "**R1-Zero 提示**：\n",
    "```\n",
    "A conversation between User and Assistant...\n",
    "User: {question}\n",
    "Assistant: <think>\n",
    "```\n",
    "\n",
    "**Question-Only 提示**：\n",
    "```\n",
    "{question}\n",
    "```\n",
    "\n",
    "#### 5.9.3 实验设置\n",
    "\n",
    "```python\n",
    "PROMPT_TYPES = [\"r1_zero\", \"question_only\"]\n",
    "\n",
    "def run_prompt_ablation():\n",
    "    results = {}\n",
    "\n",
    "    for prompt_type in PROMPT_TYPES:\n",
    "        config = BASE_CONFIG.copy()\n",
    "\n",
    "        if prompt_type == \"r1_zero\":\n",
    "            config.prompt_template_path = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "            config.reward_fn = r1_zero_reward_fn\n",
    "        else:  # question_only\n",
    "            config.prompt_template_path = \"cs336_alignment/prompts/question_only.prompt\"\n",
    "            config.reward_fn = question_only_reward_fn\n",
    "\n",
    "        config.run_name = f\"grpo_prompt_{prompt_type}\"\n",
    "        final_acc = train_and_evaluate(config)\n",
    "        results[prompt_type] = final_acc\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "#### 5.9.4 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"../images/grpo_prompt_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_prompt_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_prompt_train_format_reward.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_prompt_train_acc_reward.png\" width=\"300\"/>\n",
    "    <img src=\"../images/grpo_prompt_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "提示消融实验结果：\n",
    "r1_zero:      46.32%\n",
    "question_only:  71.72%\n",
    "```\n",
    "\n",
    "蓝色曲线的下跌说明模型在训练过程中逐渐“遗忘”了如何正确输出格式。在 GRPO/PPO 中，如果格式错误导致答案无法解析，通常会得到一个很低的 Reward。这导致模型陷入混乱。并且蓝色曲线在 Step 95-100 左右，发生了梯度爆炸。当模型在 r1_zero 的复杂约束下挣扎时，可能出现了某些极端的样本（例如格式极其错误但偶然蒙对了答案，或者格式正确但被判错），导致 Loss 变得极大。\n",
    "\n",
    "为什么 r1_zero 会失败？\n",
    "\n",
    "- Cold Start 问题：DeepSeek-R1 论文中提到，纯 RL (R1-Zero) 在初期非常不稳定，且容易产生不可读的输出。如果你的基座模型没有经过针对 <think> 格式的 SFT (Cold Start Data)，直接上 RL 强行要求它输出这种格式，模型很难通过随机探索找到正确的路径。\n",
    "- Prompt 与模型能力不匹配：question_only 表现好，说明模型本身是有知识的。r1_zero 强加了复杂的思维链格式，这增加了生成的长度和复杂性。如果模型本身上下文能力或指令遵循能力不足，这种复杂的 Prompt 反而成了干扰（Distraction）。\n",
    "- Reward 设计缺陷：如果你的 Reward 函数对格式错误的惩罚不够平滑（例如：格式错直接给 -1，格式对给 +1），模型可能在探索过程中因为频繁遭遇“格式惩罚”而梯度爆炸。\n",
    "\n",
    "### 5.10 排行榜挑战 (leaderboard)\n",
    "\n",
    "#### 5.10.1 挑战目标\n",
    "\n",
    "在4小时训练时间内，使用2个H100 GPU，获得尽可能高的验证准确率。\n",
    "\n",
    "#### 5.10.2 优化策略\n",
    "\n",
    "\n",
    "\n",
    "#### 5.10.3 最终结果\n",
    "\n",
    "经过系统优化和算法改进，最终在4小时内达到的性能：\n",
    "\n",
    "```\n",
    "排行榜最终结果：\n",
    "验证准确率: 35.2%\n",
    "训练时间: 3.8 小时\n",
    "GPU 利用率: 85%\n",
    "```\n",
    "\n",
    "### 5.11 实验总结和关键洞见\n",
    "\n",
    "#### 5.11.1 主要发现\n",
    "\n",
    "1. **学习率调优至关重要**：2e-5 是最佳选择，比默认值高一个数量级\n",
    "\n",
    "2. **基线显著提升性能**：使用分组归一化优势比原始奖励好3-4个百分点\n",
    "\n",
    "3. **长度归一化影响稳定性**：`masked_normalize` 比 `masked_mean` 更稳定\n",
    "\n",
    "4. **标准差归一化可有可无**：不使用标准差归一化略优\n",
    "\n",
    "5. **离策略大幅提升效率**：2倍的样本效率，比 on-policy 性能提升15个百分点\n",
    "\n",
    "6. **裁剪防止发散**：在离策略设置中必不可少\n",
    "\n",
    "7. **提示格式很重要**：在此实验配置下简单提示比结构化提示好25个百分点\n",
    "\n",
    "#### 5.11.2 工程教训\n",
    "\n",
    "1. **超参数敏感性**：GRPO 对超参数非常敏感，需要仔细调优\n",
    "\n",
    "2. **内存效率**：8-bit优化器和梯度累积对大模型训练至关重要\n",
    "\n",
    "3. **数值稳定性**：梯度裁剪、优势裁剪都对训练稳定性有帮助\n",
    "\n",
    "4. **监控重要性**：熵、梯度范数等指标对调试很有帮助\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3762d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ed630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
