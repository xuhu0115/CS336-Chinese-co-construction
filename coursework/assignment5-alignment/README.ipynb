{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cba148b",
   "metadata": {},
   "source": [
    "请先阅读完作业要求后，完成本次作业。\n",
    "\n",
    "在本次作业里，我们将要实现以下内容：\n",
    "1. 建立一个针对数学问题的 zero-shot 评测集\n",
    "2. 使用一个更强推理模型蒸馏得到的推理轨迹进行 SFT 训练，得到一个参数更小的推理模型，并测试其效果\n",
    "3. 使用专家迭代（EI）算法，训练得到一个推理模型，并测试其效果\n",
    "4. 使用 GRPO 算法，训练得到一个推理模型，并测试其效果\n",
    "5. 针对 GRPO 算法，进行一系列调参实验，探索不同策略下对模型性能的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d299c",
   "metadata": {},
   "source": [
    "# 环境配置\n",
    "\n",
    "再开始正式的训练之前，需要先搭建好一个能正常运行下面所有代码的环境。如同之前的作业，我们使用 `uv` 来管理依赖。\n",
    "\n",
    "1. 先安装除 flash-attn 之外的所有包，然后再安装全部包（因为 flash-attn 比较特殊）：\n",
    "\n",
    "    ```\n",
    "    uv sync --no-install-package flash-attn\n",
    "    #source .venv/bin/activate\n",
    "    uv sync\n",
    "    ```\n",
    "\n",
    "2. 运行单元测试：\n",
    "\n",
    "    ``` sh\n",
    "    uv run pytest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3e4be",
   "metadata": {},
   "source": [
    "# 一、 建立针对数学问题的评估基准集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f4432",
   "metadata": {},
   "source": [
    "首先，我们需要建立一个能对数学问题进行评估的评测集，并将其应用于基线模型，评估其 zero-shot 数学能力。我们此处的配置如下：\n",
    "\n",
    "- **baseline model**: `Qwen2.5-Math-1.5B`\n",
    "- **benchmark**: `GSM8K`（官方文档里评测和带有推理轨迹的训练使用的都是 `MATH 12K`，但由于版权限制并没有在`data/`目录下给出此数据，推荐使用一些其他的数据进行替代。但是笔者在查询了 MATH 12K的规定后，发现其使用的是宽松的 MIT license 开源协议，但是该数据不带有推理轨迹，因此只能用来评测不能用作后续训练，我们这里使用了 GSM8K 数据集来替代 MATH 12K 用作评测。如果有小伙伴想要使用该数据可自行下载 [MATH 12K](https://huggingface.co/datasets/qwedsacf/competition_math)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-setup",
   "metadata": {},
   "source": [
    "## 1.1 评估基准集的设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b3311",
   "metadata": {},
   "source": [
    "建立数学推理评估基准集是整个作业的基础。我们需要：\n",
    "\n",
    "1. **选择合适的基准数据集**：由于 MATH 12K 数据集版权限制，我们使用 GSM8K 数据集作为替代。GSM8K (Grade School Math 8K) 是一个小学数学问题数据集，包含 8K 个问题，每个问题都有详细的推理过程和最终答案，问题难度适中，适合测试基础数学推理能力\n",
    "2. **选择合适的提示格式**：使用 R1-Zero 提示格式，要求模型进行思维链推理\n",
    "3. **定义评估指标**：使用格式奖励和答案准确率来评估模型性能\n",
    "4. **使用高效的推理引擎**：使用 vLLM 进行批量推理，提高评估效率\n",
    "\n",
    "### 实现逻辑\n",
    "\n",
    "我们的评估流程包含以下几个关键步骤：\n",
    "\n",
    "1. **数据加载与预处理**：加载 GSM8K 数据集，支持 JSONL 和 JSON 数组两种格式\n",
    "2. **提示格式化**：将原始问题转换为 R1-Zero 格式的提示\n",
    "3. **批量推理**：使用 vLLM 高效生成模型响应\n",
    "4. **结果评估**：使用奖励函数计算格式和答案准确率\n",
    "5. **统计分析**：分析不同组合的性能表现\n",
    "6. **结果保存**\n",
    "\n",
    "现在让我们逐步实现这些功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1-prompt",
   "metadata": {},
   "source": [
    "## 1.2 R1-Zero 提示格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939e994",
   "metadata": {},
   "source": [
    "R1-Zero 提示格式是 DeepSeek R1 模型使用的推理提示格式，它要求模型：\n",
    "\n",
    "1. 在 `<think>` 和 `</think>` 标签之间进行推理过程\n",
    "2. 在 `<answer>` 和 `</answer>` 标签之间给出最终答案\n",
    "3. 进行逐步推理，而不是直接给出答案\n",
    "\n",
    "这种格式便于我们解析模型的输出，提取推理过程和最终答案。\n",
    "\n",
    "### 实现细节\n",
    "\n",
    "在 `evaluate_math.py` 中，我们实现了两个核心函数来处理提示：\n",
    "\n",
    "1. **`load_r1_zero_prompt()`**：从文件加载 R1-Zero 提示模板\n",
    "2. **`format_prompt()`**：将数学问题插入到提示模板中\n",
    "\n",
    "提示模板文件位于 `cs336_alignment/prompts/r1_zero.prompt`，包含完整的对话格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1-Zero 提示模板：\n",
      "==================================================\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: {question}\n",
      "Assistant: <think>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 加载 R1-Zero 提示模板\n",
    "def load_r1_zero_prompt(prompt_file_path: str) -> str:\n",
    "    \"\"\"从文件加载 R1-Zero 提示模板\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 使用 UTF-8 编码打开文件\n",
    "    2. 读取完整文件内容\n",
    "    3. 使用 strip() 移除首尾空白字符\n",
    "    \n",
    "    为什么需要 strip()？\n",
    "    - 文件末尾可能有换行符或其他空白字符\n",
    "    - 移除这些字符可以确保模板格式的整洁性\n",
    "    \"\"\"\n",
    "    with open(prompt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "# 读取提示模板\n",
    "PROMPT_PATH = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "try:\n",
    "    r1_zero_template = load_r1_zero_prompt(PROMPT_PATH)\n",
    "    print(\"R1-Zero 提示模板：\")\n",
    "    print(\"=\" * 50)\n",
    "    print(r1_zero_template)\n",
    "    print(\"=\" * 50)\n",
    "except FileNotFoundError:\n",
    "    print(f\"提示模板文件未找到: {PROMPT_PATH}\")\n",
    "    print(\"请确保文件路径正确，或创建相应的提示文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "format-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始问题：\n",
      "小明有5个苹果，他又买了3个，现在他有多少个苹果？\n",
      "\n",
      "格式化后的提示：\n",
      "==================================================\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: 小明有5个苹果，他又买了3个，现在他有多少个苹果？\n",
      "Assistant: <think>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 格式化数学问题为 R1-Zero 提示\n",
    "def format_prompt(question: str, prompt_template: str) -> str:\n",
    "    \"\"\"将数学问题格式化为 R1-Zero 提示\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 使用字符串的 format() 方法\n",
    "    2. 将问题插入到模板中的 {question} 占位符处\n",
    "    3. 返回格式化后的完整提示\n",
    "    \n",
    "    这种方法的好处：\n",
    "    - 模板和内容分离，便于维护\n",
    "    - 支持复杂的提示格式\n",
    "    - 避免字符串拼接的错误\n",
    "    \"\"\"\n",
    "    return prompt_template.format(question=question)\n",
    "\n",
    "# 示例问题\n",
    "sample_question = \"小明有5个苹果，他又买了3个，现在他有多少个苹果？\"\n",
    "\n",
    "# 只有在成功加载模板后才进行格式化\n",
    "if 'r1_zero_template' in locals():\n",
    "    formatted_prompt = format_prompt(sample_question, r1_zero_template)\n",
    "\n",
    "    print(\"原始问题：\")\n",
    "    print(sample_question)\n",
    "    print(\"\\n格式化后的提示：\")\n",
    "    print(\"=\" * 50)\n",
    "    print(formatted_prompt)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "else:\n",
    "    print(\"无法演示格式化，因为提示模板未加载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-function",
   "metadata": {},
   "source": [
    "## 1.3 奖励函数设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576dd99",
   "metadata": {},
   "source": [
    "我们使用两个维度的奖励来评估模型的性能：\n",
    "\n",
    "1. **格式奖励 (Format Reward)**：检查模型输出是否符合 R1-Zero 格式要求，格式正确得 1 分，否则得 0 分\n",
    "   - 要求输出包含 `<think>` 和 `</think>` 标签\n",
    "   - 要求输出包含 `<answer>` 和 `</answer>` 标签\n",
    "\n",
    "2. **答案奖励 (Answer Reward)**：检查模型给出的答案是否正确，答案正确得 1 分，否则得 0 分\n",
    "   - 从 `<answer>` 标签中提取答案\n",
    "   - 与标准答案进行数学等价性比较\n",
    "\n",
    "### 奖励函数的实现逻辑\n",
    "\n",
    "奖励函数在 `drgrpo_grader.py` 中的 `r1_zero_reward_fn` 实现，它接受三个参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31013b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r1_zero_reward_fn(response: str, ground_truth: str, fast: bool = True) -> Dict[str, float]:\n",
    "    # response: 模型生成的完整输出\n",
    "    # ground_truth: 标准答案\n",
    "    # fast: 是否使用快速评估模式\n",
    "    # 返回包含 format_reward, answer_reward, reward 的字典\n",
    "    if \"</think> <answer>\" in response and \"</answer>\" in response:\n",
    "        model_answer = response.split(\"<answer>\")[-1].replace(\"</answer>\", \"\")\n",
    "        if \"\\\\boxed\" in model_answer:\n",
    "            model_answer = extract_answer(model_answer)\n",
    "            if model_answer is None:\n",
    "                return {\n",
    "                    \"format_reward\": 1.0,\n",
    "                    \"answer_reward\": 0.0,\n",
    "                    \"reward\": 0.0\n",
    "                }\n",
    "        if isinstance(ground_truth, float) or isinstance(ground_truth, int):\n",
    "            ground_truth = str(ground_truth)\n",
    "        if isinstance(ground_truth, str):\n",
    "            is_correct = grade(model_answer, ground_truth, fast)\n",
    "        elif isinstance(ground_truth, list):\n",
    "            is_correct = False\n",
    "            for gt in ground_truth:\n",
    "                is_correct |= grade(model_answer, gt, fast)\n",
    "        if is_correct:\n",
    "            return {\n",
    "                \"format_reward\": 1.0,\n",
    "                \"answer_reward\": 1.0,\n",
    "                \"reward\": 1.0\n",
    "            }\n",
    "        else:\n",
    "            # Formatted but wrong answer; no format reward to avoid hacking.\n",
    "            return {\n",
    "                \"format_reward\": 1.0,\n",
    "                \"answer_reward\": 0.0,\n",
    "                \"reward\": 0.0\n",
    "            }\n",
    "    else:\n",
    "        # Unformatted.\n",
    "        return {\n",
    "            \"format_reward\": 0.0,\n",
    "            \"answer_reward\": 0.0,\n",
    "            \"reward\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dba48",
   "metadata": {},
   "source": [
    "**该函数实现的评估流程**：\n",
    "1. **格式检查**：验证输出是否包含必需的标签\n",
    "2. **答案提取**：从 `<answer>` 标签中解析最终答案\n",
    "3. **答案比较**：使用数学等价性检查答案正确性\n",
    "4. **奖励计算**：根据格式和答案正确性计算奖励分数\n",
    "\n",
    "这个函数的设计使得评估过程既严格又灵活，能够准确衡量模型的推理能力和格式遵循能力。我们通过几个例子，测试下我们的奖励函数是够能正常工作！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "import-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功导入奖励函数\n",
      "\n",
      "=== 测试场景1：正确格式和答案 ===\n",
      "响应内容：\n",
      "</think>这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。</think> <answer>8</answer>\n",
      "\n",
      "奖励结果：\n",
      "  format_reward: 1.0\n",
      "  answer_reward: 1.0\n",
      "  reward: 1.0\n",
      "\n",
      "=== 测试场景2：格式正确但答案错误 ===\n",
      "奖励结果：\n",
      "  format_reward: 1.0\n",
      "  answer_reward: 0.0\n",
      "  reward: 0.0\n",
      "\n",
      "=== 测试场景3：格式错误（缺少标签） ===\n",
      "响应内容：\n",
      "这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。答案是8。\n",
      "\n",
      "奖励结果：\n",
      "  format_reward: 0.0\n",
      "  answer_reward: 0.0\n",
      "  reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 导入奖励函数\n",
    "try:\n",
    "    from cs336_alignment.drgrpo_grader import r1_zero_reward_fn\n",
    "    print(\"成功导入奖励函数\")\n",
    "except ImportError as e:\n",
    "    print(f\"导入奖励函数失败: {e}\")\n",
    "    print(\"请确保 drgrpo_grader.py 文件存在并且可以被导入\")\n",
    "    r1_zero_reward_fn = None\n",
    "\n",
    "# 测试奖励函数的不同场景\n",
    "def test_reward_function():\n",
    "    if r1_zero_reward_fn is None:\n",
    "        print(\"无法测试奖励函数，因为导入失败\")\n",
    "        return\n",
    "    \n",
    "    # 测试场景1：正确格式和答案\n",
    "    print(\"\\n=== 测试场景1：正确格式和答案 ===\")\n",
    "    correct_response = \"\"\"</think>这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。</think> <answer>8</answer>\"\"\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(correct_response, \"8\")\n",
    "    print(\"响应内容：\")\n",
    "    print(correct_response)\n",
    "    print(\"\\n奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 测试场景2：格式正确但答案错误\n",
    "    print(\"\\n=== 测试场景2：格式正确但答案错误 ===\")\n",
    "    wrong_answer_response = \"\"\"<think>我来计算一下。5个苹果加3个应该是8个。等等，我算错了，是7个。</think> <answer>7</answer>\"\"\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(wrong_answer_response, \"8\")\n",
    "    print(\"奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 测试场景3：格式错误\n",
    "    print(\"\\n=== 测试场景3：格式错误（缺少标签） ===\")\n",
    "    bad_format_response = \"这是一个简单的加法问题。小明原来有5个苹果，又买了3个。所以总数是5 + 3 = 8个苹果。答案是8。\"\n",
    "    \n",
    "    rewards = r1_zero_reward_fn(bad_format_response, \"8\")\n",
    "    print(\"响应内容：\")\n",
    "    print(bad_format_response)\n",
    "    print(\"\\n奖励结果：\")\n",
    "    for key, value in rewards.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# 运行测试\n",
    "test_reward_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vllm-setup",
   "metadata": {},
   "source": [
    "## 1.4 使用 vLLM 进行高效推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a7021",
   "metadata": {},
   "source": [
    "vLLM 是一个高性能的 LLM 推理引擎，具有以下优势：\n",
    "\n",
    "1. **批量推理**：支持同时处理多个请求，提高吞吐量\n",
    "2. **内存优化**：使用 PagedAttention 等技术优化显存使用\n",
    "3. **CUDA 内核优化**：使用优化的 CUDA 内核加速推理\n",
    "4. **灵活的采样参数**：支持温度采样、top-p 截断等多种采样策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a8ded",
   "metadata": {},
   "source": [
    "### vLLM 初始化参数详解\n",
    "\n",
    "在 `evaluate_math.py` 中，我们这样初始化 vLLM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    dtype=torch.bfloat16,  # 使用 bfloat16 精度节省显存\n",
    "    gpu_memory_utilization=0.8,  # 控制显存使用率\n",
    "    enable_prefix_caching=True,  # 启用前缀缓存优化\n",
    "    tensor_parallel_size=1,  # 单 GPU 设置\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb89a1",
   "metadata": {},
   "source": [
    "### 采样参数配置\n",
    "\n",
    "我们使用以下采样参数：\n",
    "- `temperature=1.0`：使用随机采样，增加输出多样性，有助于探索不同的推理路径\n",
    "- `top_p=1.0`：不进行 top-p 截断，使用完整的词汇分布\n",
    "- `max_tokens=1024`：最大生成长度，足够容纳详细的推理过程\n",
    "- `stop=[\"</answer>\"]`：在生成答案标签时停止，确保输出格式完整\n",
    "- `include_stop_str_in_output=True`：将停止字符串包含在输出中，便于后续解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-vllm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 vLLM 模型\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from unittest.mock import patch\n",
    "from vllm.model_executor import set_random_seed as vllm_set_random_seed\n",
    "\n",
    "# 模型路径\n",
    "MODEL_PATH = \"/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B\"\n",
    "\n",
    "def init_vllm(model_path: str, device: str = \"cuda:0\"):\n",
    "    \"\"\"初始化 vLLM 模型\n",
    "    \n",
    "    初始化逻辑详解：\n",
    "    1. 设置随机种子确保结果可重现\n",
    "    2. 使用 patch 解决分布式训练相关问题\n",
    "    3. 配置模型参数以优化性能和显存使用\n",
    "    \n",
    "    为什么需要 patch？\n",
    "    - vLLM 默认假设在分布式环境中运行\n",
    "    - 在单 GPU 环境中，我们需要模拟 world_size=1\n",
    "    - profiling_patch 避免了不必要的性能分析\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        vllm_set_random_seed(42)\n",
    "        \n",
    "        world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "        profiling_patch = patch(\n",
    "            \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "            return_value=None\n",
    "        )\n",
    "        \n",
    "        with world_size_patch, profiling_patch:\n",
    "            llm = LLM(\n",
    "                model=model_path,\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,  # 使用 bfloat16 节省显存\n",
    "                enable_prefix_caching=True,  # 启用前缀缓存，提高批量推理效率\n",
    "                gpu_memory_utilization=0.8,  # 控制显存使用率，避免 OOM\n",
    "                tensor_parallel_size=1,  # 单 GPU 设置\n",
    "                enforce_eager=True,  # 强制使用 eager 模式，提高稳定性\n",
    "            )\n",
    "            print(f\"成功加载模型: {model_path}\")\n",
    "            return llm\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"初始化 vLLM 失败: {e}\")\n",
    "        print(\"请检查模型路径是否存在，以及是否有足够的显存\")\n",
    "        return None\n",
    "\n",
    "# 初始化模型\n",
    "print(\"正在初始化 vLLM 模型...\")\n",
    "llm = init_vllm(MODEL_PATH)\n",
    "\n",
    "if llm is not None:\n",
    "    print(\"vLLM 模型初始化完成！\")\n",
    "    \n",
    "    # 设置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,  # 随机采样，增加推理多样性\n",
    "        top_p=1.0,        # 不进行 top-p 截断\n",
    "        max_tokens=1024,  # 最大生成长度\n",
    "        stop=[\"</answer>\"],  # 遇到答案结束标签时停止\n",
    "        include_stop_str_in_output=True,  # 包含停止字符串在输出中\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    sampling_params = None\n",
    "    print(\"vLLM 初始化失败，无法继续演示\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 1.5 数据加载和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c45d9",
   "metadata": {},
   "source": [
    "数据加载是评估流程中的关键步骤。在 `evaluate_math.py` 中，我们实现了灵活的数据加载逻辑，支持多种数据集格式。\n",
    "\n",
    "### 支持的数据集格式\n",
    "\n",
    "1. **GSM8K 格式 (JSONL)**：\n",
    "   - 每行一个 JSON 对象\n",
    "   - 包含 `question` 字段和 `answer` 字段\n",
    "   - 答案字段包含完整推理过程，最后以 `####` 分隔最终答案\n",
    "\n",
    "2. **MATH 格式 (JSON 数组)**：\n",
    "   - 整个文件是一个 JSON 数组\n",
    "   - 包含 `problem` 和 `expected_answer` 字段\n",
    "   - 答案是直接给出的最终结果\n",
    "\n",
    "### 数据预处理逻辑\n",
    "\n",
    "在 `evaluate_vllm` 函数中，我们实现了以下预处理步骤：\n",
    "\n",
    "1. **格式检测**：通过读取第一个字符判断是 JSONL (`{`) 还是 JSON 数组 (`[`)\n",
    "2. **字段提取**：根据数据集类型提取问题和答案\n",
    "3. **答案解析**：对于 GSM8K，使用正则表达式提取 `####` 后的最终答案\n",
    "4. **提示格式化**：将问题插入到 R1-Zero 提示模板中\n",
    "\n",
    "### 答案提取策略\n",
    "\n",
    "GSM8K 数据集的答案格式特殊，使用如下形式：\n",
    "```\n",
    "详细推理过程... #### 最终答案\n",
    "```\n",
    "\n",
    "我们使用正则表达式来提取最终答案，确保只获取正确答案部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到 JSONL 格式 (GSM8K风格)\n",
      "成功加载了 1319 个样本\n",
      "限制为前 10 个样本用于演示\n",
      "\n",
      "样本 1:\n",
      "问题: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "原始答案字段: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "提取的答案: 18\n",
      "\n",
      "样本 2:\n",
      "问题: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "原始答案字段: It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
      "So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\n",
      "#### 3\n",
      "提取的答案: 3\n",
      "\n",
      "样本 3:\n",
      "问题: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "原始答案字段: The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\n",
      "He increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\n",
      "So the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\n",
      "So he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\n",
      "#### 70000\n",
      "提取的答案: 70000\n",
      "\n",
      "成功格式化了 10 个提示\n",
      "\n",
      "总共加载了 10 个问题\n",
      "示例提示预览：\n",
      "--------------------------------------------------\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 数据加载函数 - 基于 evaluate_math.py 的实现\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_math_dataset(dataset_path: str, max_samples: int = None):\n",
    "    \"\"\"\n",
    "    加载数学数据集，支持 JSONL 和 JSON 数组两种格式\n",
    "    \n",
    "    这个函数的实现逻辑：\n",
    "    1. 检测文件格式（JSONL vs JSON数组）\n",
    "    2. 加载数据到内存\n",
    "    3. 提取问题和答案字段\n",
    "    4. 处理不同数据集格式的答案提取\n",
    "    5. 格式化提示\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: 数据集文件路径\n",
    "        max_samples: 最大加载样本数（用于演示）\n",
    "        \n",
    "    Returns:\n",
    "        questions: 问题列表\n",
    "        ground_truths: 标准答案列表\n",
    "        prompts: 格式化后的提示列表\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    try:\n",
    "        # 步骤1：检测文件格式\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            first_char = f.read(1)\n",
    "            f.seek(0)  # 重置文件指针\n",
    "            \n",
    "            if first_char == '[':\n",
    "                # JSON 数组格式 (MATH数据集)\n",
    "                print(\"检测到 JSON 数组格式 (MATH风格)\")\n",
    "                data = json.load(f)\n",
    "            else:\n",
    "                # JSONL 格式 (GSM8K数据集)\n",
    "                print(\"检测到 JSONL 格式 (GSM8K风格)\")\n",
    "                data = []\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:  # 跳过空行\n",
    "                        data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"成功加载了 {len(data)} 个样本\")\n",
    "        \n",
    "        # 步骤2：限制样本数量（演示用）\n",
    "        if max_samples:\n",
    "            data = data[:max_samples]\n",
    "            print(f\"限制为前 {max_samples} 个样本用于演示\")\n",
    "        \n",
    "        # 步骤3：处理每个样本\n",
    "        for i, example in enumerate(data):\n",
    "            # 提取问题 - 支持多种字段名\n",
    "            question = example.get('question', example.get('problem', ''))\n",
    "            \n",
    "            # 提取答案 - 处理不同格式\n",
    "            if 'expected_answer' in example:\n",
    "                # MATH 格式：直接答案\n",
    "                answer = example['expected_answer']\n",
    "            elif 'answer' in example:\n",
    "                # GSM8K 格式：从推理过程中提取最终答案\n",
    "                answer_text = example['answer']\n",
    "                # 使用正则表达式提取 #### 后的答案\n",
    "                match = re.search(r\"####\\s*(.+)\\s*$\", answer_text.strip())\n",
    "                answer = match.group(1).strip() if match else answer_text.strip()\n",
    "            else:\n",
    "                answer = ''\n",
    "            \n",
    "            questions.append(question)\n",
    "            ground_truths.append(answer)\n",
    "            \n",
    "            # 只显示前几个样本的详细信息\n",
    "            if i < 3:\n",
    "                print(f\"\\n样本 {i+1}:\")\n",
    "                print(f\"问题: {question}\")\n",
    "                print(f\"原始答案字段: {example.get('answer', example.get('expected_answer', 'N/A'))}\")\n",
    "                print(f\"提取的答案: {answer}\")\n",
    "        \n",
    "        # 步骤4：格式化提示\n",
    "        if 'r1_zero_template' in globals():\n",
    "            prompts = [format_prompt(q, r1_zero_template) for q in questions]\n",
    "            print(f\"\\n成功格式化了 {len(prompts)} 个提示\")\n",
    "        else:\n",
    "            prompts = []\n",
    "            print(\"\\n提示模板未加载，无法格式化提示\")\n",
    "        \n",
    "        return questions, ground_truths, prompts\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"数据集文件未找到: {dataset_path}\")\n",
    "        return [], [], []\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据集时出错: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "# 加载数据（演示用小样本）\n",
    "DATASET_PATH = \"data/gsm8k/test.jsonl\"\n",
    "questions, ground_truths, prompts = load_math_dataset(DATASET_PATH, max_samples=10)\n",
    "\n",
    "print(f\"\\n总共加载了 {len(questions)} 个问题\")\n",
    "if prompts:\n",
    "    print(f\"示例提示预览：\")\n",
    "    print(\"-\" * 50)\n",
    "    print(prompts[0][:200] + \"...\" if len(prompts[0]) > 200 else prompts[0])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffaf79",
   "metadata": {},
   "source": [
    "## 1.6 评估指标计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec9bb1",
   "metadata": {},
   "source": [
    "现在我们可以使用 vLLM 对所有问题进行批量推理，并使用奖励函数评估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = int(rewards.get(\"format_reward\", 0.0) >= 0.5)\n",
    "ar = int(rewards.get(\"answer_reward\", 0.0) >= 0.5)\n",
    "combo_counts[(fr, ar)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation",
   "metadata": {},
   "source": [
    "### 组合分析 (Combo Analysis)\n",
    "\n",
    "我们统计四种组合的情况：\n",
    "- `(1, 1)`：格式正确且答案正确\n",
    "- `(1, 0)`：格式正确但答案错误\n",
    "- `(0, 0)`：格式错误且答案错误\n",
    "- `(0, 1)`：格式错误但答案正确（理论上很少见）\n",
    "\n",
    "这种分析有助于我们理解模型在格式遵循和推理能力方面的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"n\": num_examples,\n",
    "    \"format_rate\": avg_format_reward,\n",
    "    \"answer_accuracy\": avg_answer_reward,\n",
    "    \"reward_mean\": avg_reward,\n",
    "    \"counts\": {\n",
    "        \"format=1 answer=1\": combo_counts[(1, 1)],\n",
    "        \"format=1 answer=0\": combo_counts[(1, 0)],\n",
    "        \"format=0 answer=0\": combo_counts[(0, 0)],\n",
    "        \"format=0 answer=1\": combo_counts[(0, 1)],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-results",
   "metadata": {},
   "source": [
    "## 1.7 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc39d8",
   "metadata": {},
   "source": [
    "通过运行`evaluate_math.py`脚本，我们可以完成对`Qwen2.5-Math-1.5B`模型的完整评估。运行完成后，你会得到类似以下的输出：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"n\": 1319,\n",
    "  \"format_rate\": 0.5041698256254739,\n",
    "  \"answer_accuracy\": 0.17589082638362397,\n",
    "  \"reward_mean\": 0.17589082638362397,\n",
    "  \"counts\": {\n",
    "    \"format=1 answer=1\": 232,\n",
    "    \"format=1 answer=0\": 433,\n",
    "    \"format=0 answer=0\": 654,\n",
    "    \"format=0 answer=1\": 0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "基准模型 `Qwen2.5-Math-1.5B` 的回答准确率是 `17.59%`，格式准确率为 `50.42%`。接着我们观察了 10 个格式奖励为 0 的案例，我们发现问题主要出在解析器的格式要求过于严格，而不是基础模型的输出质量问题。从 `r1_zero_reward_fn` 的代码看，格式检查的条件是：\n",
    "\n",
    "```python\n",
    "if \"</think> <answer>\" in response and \"</answer>\" in response:\n",
    "```\n",
    "\n",
    "这个条件要求：\n",
    "- 响应中必须包含确切的字符串 `</think> <answer>` （注意 think 结束标签和 answer 开始标签之间必须有空格）\n",
    "- 响应中必须包含 `</answer>`\n",
    "\n",
    "而在多个案例中我们发现模型输出通常为 `</think>\\n<answer>` 或 `</think><answer>`（没有空格），不符合解析器期望的 `</think> <answer>`（必须有空格）\n",
    "\n",
    "对于格式正确但答案错误的案例，问题主要出在基础模型的推理能力上，而不是解析器的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bb66a",
   "metadata": {},
   "source": [
    "# 第二部分：用于数学推理的监督微调 (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51159843",
   "metadata": {},
   "source": [
    "\n",
    "### 学习目标\n",
    "在本部分中，我们将学习如何通过监督微调来提升语言模型的数学推理能力。这包括：\n",
    "1. 理解 SFT 在数学推理中的作用和原理\n",
    "2. 掌握推理轨迹数据的准备和处理\n",
    "3. 实现完整的 SFT 训练流程\n",
    "4. 学习模型评估和性能分析技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971037cf",
   "metadata": {},
   "source": [
    "## 2.1 SFT 的动机和原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc9a90",
   "metadata": {},
   "source": [
    "### 为什么需要 SFT？\n",
    "\n",
    "尽管 Qwen 2.5 Math 1.5B 已经经过数学预训练，但在推理任务上仍然表现不佳：\n",
    "- **缺乏推理结构**：模型不知道如何进行逐步推理\n",
    "- **格式不一致**：不知道使用 `<think>` 和 `<answer>` 标签\n",
    "- **推理模式缺失**：没有学习到思维链（Chain-of-Thought）推理模式，推理能力较弱、深度不够\n",
    "\n",
    "### SFT 的核心思想\n",
    "\n",
    "SFT通过让模型学习包含`数学问题+推理过程+最终答案`的推理轨迹，使其能够生成完整的推理过程，从而提供明确的推理模式和步骤以提升性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c22a2d",
   "metadata": {},
   "source": [
    "## 2.2 数据准备和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001a265",
   "metadata": {},
   "source": [
    "### 推理轨迹数据\n",
    "\n",
    "由于原项目的带推理轨迹的 MATH 12K 数据没有公开，我们使用来自`garg-aayush`用户复现该项目时制作的[hiyouga/math12k](https://huggingface.co/datasets/hiyouga/math12k) 数据集。该数据集使用 gpt-oss 蒸馏的[带推理轨迹的数据](https://huggingface.co/datasets/garg-aayush/sft-cs336-assign5-datasets/tree/main/sft-reason) 的推理轨迹数据：\n",
    "\n",
    "```json\n",
    "# 数据格式示例\n",
    "{\n",
    "    \"problem\": \"Compute $\\\\sin 45^\\\\circ$.\",\n",
    "    \"reasoning_trace\": \"The angle 45\\u00b0 corresponds to \\u03c0/4 radians. The sine of \\u03c0/4 is known from the unit circle or a 45-45-90 right triangle, where the legs are equal and the hypotenuse is \\u221a2 times a leg. Thus, sin\\u202f45\\u00b0 = opposite/hypotenuse = 1/\\u221a2 = \\u221a2/2.</think> <answer>\\u221a2/2</answer>\",\n",
    "    \"extracted_answer\": \"\\u221a2/2\",\n",
    "    \"expected_answer\": \"\\\\frac{\\\\sqrt{2}}{2}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d7dba",
   "metadata": {},
   "source": [
    "### 数据加载逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不同格式的数据文件\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    first_char = f.read(1)\n",
    "    f.seek(0)\n",
    "\n",
    "    if first_char == '[':\n",
    "        # JSON 数组格式\n",
    "        sft_data = json.load(f)\n",
    "    else:\n",
    "        # JSONL 格式\n",
    "        sft_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# 数据子采样（用于不同大小实验）\n",
    "if max_examples > 0 and len(sft_data) > max_examples:\n",
    "    sft_data = random.sample(sft_data, max_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3fe19",
   "metadata": {},
   "source": [
    "## 2.3 SFT 训练的核心组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3620b",
   "metadata": {},
   "source": [
    "下面的所有辅助组件均定义在`sft_helper.py`，可以在该脚本中查看完整实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb7fc4",
   "metadata": {},
   "source": [
    "### 1. 提示和输出的分词 (`tokenize_prompt_and_output`)\n",
    "\n",
    "这是 SFT 的基础函数，用于准备训练数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt_and_output(\n",
    "        prompt_strs: List[str],\n",
    "        output_strs: List[str],\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    ") -> Dict[str, torch.Tensor]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cb773",
   "metadata": {},
   "source": [
    "1. **分别对提示和输出进行分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tok = tokenizer(prompt_strs, add_special_tokens=False, padding=False)\n",
    "output_tok = tokenizer(output_strs, add_special_tokens=False, padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bc76d",
   "metadata": {},
   "source": [
    "2. **拼接完整序列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1736597",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids_list = [p + o for p, o in zip(prompt_ids_list, output_ids_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80284f",
   "metadata": {},
   "source": [
    "3. **填充到相同长度**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96089e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_full_len = max(len(x) for x in full_ids_list)\n",
    "full_padded = torch.full((bs, max_full_len), pad_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3803c",
   "metadata": {},
   "source": [
    "4. **创建训练标签**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0954ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = full_padded[:, :-1]  # 去掉最后一个token\n",
    "labels = full_padded[:, 1:]      # 去掉第一个token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a7671",
   "metadata": {},
   "source": [
    "5. **构建响应掩码**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mask = torch.zeros_like(labels, dtype=torch.long)\n",
    "for i in range(bs):\n",
    "    p_len = prompt_lens[i]\n",
    "    o_len = output_lens[i]\n",
    "    start = max(p_len - 1, 0)\n",
    "    end = min(p_len + o_len - 1, labels.size(1))\n",
    "    response_mask[i, start:end] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b098b",
   "metadata": {},
   "source": [
    "\n",
    "**返回结构：**\n",
    "```python\n",
    "{\n",
    "    \"input_ids\": input_ids,        # (B, T) - 模型输入\n",
    "    \"labels\": labels,             # (B, T) - 目标标签（右移一位）\n",
    "    \"response_mask\": response_mask # (B, T) - 响应部分掩码\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aaf924",
   "metadata": {},
   "source": [
    "### 2. 对数概率计算 (`get_response_log_probs`)\n",
    "\n",
    "计算模型对目标序列的条件对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_log_probs(\n",
    "        model: torch.nn.Module,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        return_token_entropy: bool = False,\n",
    ") -> Dict[str, torch.Tensor]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e173d",
   "metadata": {},
   "source": [
    "**计算过程：**\n",
    "1. **前向传播**：`logits = model(input_ids).logits`\n",
    "2. **转换为对数概率**：`log_probs_vocab = F.log_softmax(logits, dim=-1)`\n",
    "3. **选择目标token的概率**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Forward: logits (B, T, V)\n",
    "logits = model(input_ids).logits\n",
    "\n",
    "# log-probs over vocab: (B, T, V)\n",
    "log_probs_vocab = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Select log-prob of the label token at each position.\n",
    "# labels: (B, T) -> (B, T, 1) for gather\n",
    "log_probs = torch.gather(log_probs_vocab, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a7744",
   "metadata": {},
   "source": [
    "### 3. 熵计算 (`compute_entropy`)\n",
    "\n",
    "用于监控模型预测的确定性。**高熵**意味着模型对下一个token不确定，预测分布平坦；**低熵**表示模型很确定下一个token，预测分布集中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(logits: torch.Tensor) -> torch.Tensor:\n",
    "    # 计算每个位置的预测熵\n",
    "    log_z = torch.logsumexp(logits, dim=-1)  # log Z\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    expected_logit = (probs * logits).sum(dim=-1)\n",
    "    entropy = log_z - expected_logit  # H = log Z - E_p[logit]\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2503b9",
   "metadata": {},
   "source": [
    "### 4. 掩码归一化 (`masked_normalize`)\n",
    "\n",
    "用于在响应token上进行归一化的工具函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4934736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_normalize(\n",
    "        tensor: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        normalize_constant: float,\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:\n",
    "    masked_tensor = tensor * mask\n",
    "    summed = masked_tensor.sum() if dim is None else masked_tensor.sum(dim=dim)\n",
    "    return summed / normalize_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa75b7",
   "metadata": {},
   "source": [
    "### 5. 微批次训练步骤 (`sft_microbatch_train_step`)\n",
    "\n",
    "实现单个训练步骤的核心逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9064630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sft_microbatch_train_step(\n",
    "        policy_log_probs: torch.Tensor,\n",
    "        response_mask: torch.Tensor,\n",
    "        gradient_accumulation_steps: int,\n",
    "        normalize_constant: float = 1.0,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de0ae9",
   "metadata": {},
   "source": [
    "**训练步骤：**\n",
    "1. **计算负对数似然**：`per_token_nll = -policy_log_probs`\n",
    "2. **掩码求和**：`per_example_nll = (per_token_nll * mask).sum(dim=1)`\n",
    "3. **归一化**：`per_example_nll = per_example_nll / normalize_constant`\n",
    "4. **批次平均**：`microbatch_loss = per_example_nll.mean()`\n",
    "5. **梯度累积缩放**：`loss = microbatch_loss / gradient_accumulation_steps`\n",
    "6. **反向传播**：`loss.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8ad7b",
   "metadata": {},
   "source": [
    "## 2.4 训练流程和超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b7f57",
   "metadata": {},
   "source": [
    "训练流程定义在`run_sft_experiment`函数中，该函数会在主函数中被调用，并通过传递不同参数进行不同的实验：\n",
    "\n",
    "- `train_data_path`: 训练数据所在路径\n",
    "- `max_examples`: 取值为`-1`表示使用所有数据，也可以采样 128, 256, 512, 1024, 2048 等子集\n",
    "- `dataset_tag`: 取值为`raw`和`filtered`，表示使用原数据还是过滤后的数据\n",
    "- `size_tag`: 输入为一个字符串，用来标记使用的样本大小，可以为\"full\"、\"1024\"等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff551ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sft_experiment(train_data_path: str, max_examples: int = -1,\n",
    "                      dataset_tag: str = \"raw\", size_tag: str = \"full\"):\n",
    "    \"\"\"Run a single SFT experiment.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61070ab",
   "metadata": {},
   "source": [
    "### 初始化 wandb\n",
    "\n",
    "wandb 是一个用来记录深度学习实验的工具，如果你是第一次使用它，请先参考我们的教程`docs/chapter0/Weights & Biases（W&B）使用介绍.md`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b9969",
   "metadata": {},
   "source": [
    "### 加载提示模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0770d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_PATH = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "\n",
    "template = load_prompt_template(PROMPT_TEMPLATE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46e44b",
   "metadata": {},
   "source": [
    "### 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading training data from {train_data_path}...\")\n",
    "sft_data = []\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    # Check first char to see if it's a list '[' or a dict '{'\n",
    "    first_char = f.read(1)\n",
    "    f.seek(0)\n",
    "\n",
    "    if first_char == '[':\n",
    "        # Case A: It is a JSON List (Your format)\n",
    "        print(\"Detected JSON List format.\")\n",
    "        sft_data = json.load(f)\n",
    "    else:\n",
    "        # Case B: It is JSONL (Line-by-line)\n",
    "        print(\"Detected JSONL format.\")\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                sft_data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "# Subsample if needed\n",
    "if max_examples > 0 and len(sft_data) > max_examples:\n",
    "    sft_data = random.sample(sft_data, max_examples)\n",
    "    print(f\"Subsampled training data to {len(sft_data)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87940bb",
   "metadata": {},
   "source": [
    "### 初始化策略模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Policy Model on cuda:0...\")\n",
    "device_policy = \"cuda:0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(device_policy)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db7de8",
   "metadata": {},
   "source": [
    "### 初始化 vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing vLLM on cuda:1...\")\n",
    "device_eval = \"cuda:1\"\n",
    "llm = init_vllm(BASE_MODEL, device_eval, SEED)\n",
    "\n",
    "# Evaluation sampling parameters\n",
    "eval_sampling_params = SamplingParams(\n",
    "    temperature=0.0, top_p=1.0, max_tokens=1024,\n",
    "    stop=[\"</answer>\"], include_stop_str_in_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1f3d7",
   "metadata": {},
   "source": [
    "### 训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4          # 微批次大小\n",
    "GRAD_ACCUM = 8          # 梯度累积步数\n",
    "LR = 5e-5              # 学习率\n",
    "EPOCHS = 1             # 训练轮数\n",
    "MAX_GRAD_NORM = 1.0    # 梯度裁剪\n",
    "SEED = 2026   # 随机种子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf079599",
   "metadata": {},
   "source": [
    "### 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb6088",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "model.train()\n",
    "\n",
    "# Calculate eval frequency (roughly every half epoch)\n",
    "steps_per_epoch = max(1, len(sft_data) // BATCH_SIZE)\n",
    "eval_every_steps = EVAL_EVERY_STEPS\n",
    "print(f\"Training on {len(sft_data)} examples, {steps_per_epoch} steps per epoch, eval every {eval_every_steps} steps\")\n",
    "\n",
    "eval_count = 0\n",
    "batch_loss = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    random.shuffle(sft_data)\n",
    "\n",
    "    for i in tqdm(range(0, len(sft_data), BATCH_SIZE), desc=f\"Epoch {epoch + 1}\"):\n",
    "        # 1. 准备批次数据\n",
    "        batch_data = sft_data[i : i + BATCH_SIZE]\n",
    "        if not batch_data:\n",
    "            continue\n",
    "\n",
    "        # 2. 格式化提示和响应\n",
    "        prompt_strs = []\n",
    "        output_strs = []\n",
    "\n",
    "        for x in batch_data:\n",
    "            question = x.get('problem', x.get('question', ''))\n",
    "            formatted_prompt = format_prompt(template, question)\n",
    "\n",
    "            # Handle different response keys\n",
    "            if 'reasoning_trace' in x:\n",
    "                response = x['reasoning_trace']\n",
    "            elif 'response' in x:\n",
    "                response = x['response']\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            prompt_strs.append(formatted_prompt)\n",
    "            output_strs.append(response)\n",
    "\n",
    "        if not prompt_strs:\n",
    "            continue\n",
    "\n",
    "        # 3. 分词\n",
    "        tokenized = tokenize_prompt_and_output(prompt_strs, output_strs, tokenizer)\n",
    "        input_ids = tokenized[\"input_ids\"].to(device_policy)\n",
    "        labels = tokenized[\"labels\"].to(device_policy)\n",
    "        response_mask = tokenized[\"response_mask\"].to(device_policy)\n",
    "        \n",
    "        # 4. 前向传播\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        per_token_log_probs = torch.gather(\n",
    "            log_probs,\n",
    "            dim=2,\n",
    "            index=labels.unsqueeze(2)\n",
    "        ).squeeze(2)\n",
    "\n",
    "        # 5. 计算损失并反向传播\n",
    "        loss, _ = sft_microbatch_train_step(\n",
    "            policy_log_probs=per_token_log_probs,\n",
    "            response_mask=response_mask,\n",
    "            gradient_accumulation_steps=GRAD_ACCUM\n",
    "        )\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "        # 6. 梯度累积和优化器步骤\n",
    "        if (global_step + 1) % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log training metrics\n",
    "            wandb.log({\n",
    "                \"train/loss\": batch_loss,\n",
    "                \"train_step\": global_step\n",
    "            })\n",
    "            batch_loss = 0.0\n",
    "\n",
    "        # 7. 定期评估\n",
    "        if (global_step + 1) % eval_every_steps == 0:\n",
    "            print(f\"Step {global_step}: Evaluating...\")\n",
    "            model.eval()\n",
    "            load_policy_into_vllm_instance(model, llm)\n",
    "\n",
    "            eval_dir = f\"results/sft_experiments_{dataset_tag}_{size_tag}/step_{global_step}\"\n",
    "            os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "            # Use GSM8K test set for evaluation\n",
    "            metrics = evaluate_vllm(\n",
    "                vllm_model=llm,\n",
    "                reward_fn=r1_zero_reward_fn,\n",
    "                dataset_path=VAL_PATH,\n",
    "                prompt_template=template,\n",
    "                eval_sampling_params=eval_sampling_params,\n",
    "                output_filepath=os.path.join(eval_dir, \"results.jsonl\")\n",
    "            )\n",
    "\n",
    "            wandb.log({\n",
    "                \"eval/acc\": metrics[\"answer_accuracy\"],\n",
    "                \"eval/format_rate\": metrics[\"format_rate\"],\n",
    "                \"eval_step\": eval_count,\n",
    "                \"global_step\": global_step\n",
    "            })\n",
    "            eval_count += 1\n",
    "\n",
    "            print(f\"Eval Acc: {metrics['answer_accuracy']:.4f}\")\n",
    "\n",
    "            # Log generations to wandb\n",
    "            log_generations_to_wandb(\n",
    "                results_path=os.path.join(eval_dir, \"results.jsonl\"),\n",
    "                step=global_step,\n",
    "                run_name=f\"sft_{dataset_tag}_{size_tag}\"\n",
    "            )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "# Save final model（是否保存最后的模型，取消注释即可保存模型）\n",
    "# print(\"Saving final model...\")\n",
    "# final_save_dir = os.path.join(f\"results/sft_experiments_{dataset_tag}_{size_tag}\", \"latest\")\n",
    "# model.save_pretrained(final_save_dir)\n",
    "# tokenizer.save_pretrained(final_save_dir)\n",
    "# print(f\"Final model saved to {final_save_dir}\")\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899de57",
   "metadata": {},
   "source": [
    "## 2.5 实验设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe692d",
   "metadata": {},
   "source": [
    "### 1. 数据集大小扫描实验\n",
    "\n",
    "**目的：** 研究训练数据量对性能的影响，找到数据效率的平衡点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11111fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZES = [128, 256, 512, 1024]\n",
    "\n",
    "for size in DATASET_SIZES:\n",
    "    run_sft_experiment(\n",
    "        train_data_path=RAW_TRAIN,\n",
    "        max_examples=size,\n",
    "        dataset_tag=\"raw\",\n",
    "        size_tag=str(size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da2bca",
   "metadata": {},
   "source": [
    "### 2. 过滤正确示例实验\n",
    "\n",
    "**动机：**移除低质量的推理轨迹，提高训练数据的质量，减少模型学习错误推理模式的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65263d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理：只保留产生正确答案的推理轨迹\n",
    "new_sft_data = []\n",
    "for x in sft_data:\n",
    "    score = r1_zero_reward_fn(x['reasoning_trace'], x['expected_answer'])\n",
    "    if score['answer_reward'] == 1.0:\n",
    "        new_sft_data.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f9c98",
   "metadata": {},
   "source": [
    "### 3. 性能指标监控\n",
    "\n",
    "**训练指标：**\n",
    "- `train/loss`：训练损失\n",
    "- `train/entropy`：响应token的平均熵\n",
    "\n",
    "**评估指标：**\n",
    "- `eval/acc`：答案准确率\n",
    "- `eval/format_rate`：格式正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448ca87",
   "metadata": {},
   "source": [
    "## 2.6 实验结果分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822913d",
   "metadata": {},
   "source": [
    "### 数据集大小的影响\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/sft_train_loss.png\" width=\"400\"/>\n",
    "    <img src=\"images/sft_eval_acc.png\" width=\"400\"/>\n",
    "    <img src=\"images/sft_eval_format.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "从图中可以清晰地看到，数据集越大，最终的训练损失越低。而在评测集上的精确度 512 条数据时表现最好，能达到 `76.20%`。我们的一篇研究 [Structure Trumps Size: Rethinking Data Quality for LLM Reasoning](https://aclanthology.org/2025.findings-emnlp.616/) 发现，大的趋势是数据越多越好，但是 1k 之前带来的收益最大，在超过 10k 后收益持续变小，并且会对模型的通用能力损失较大。\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"images/sft_volume.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "### 过滤数据的效果\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"images/sft_256_raw_vs_filter.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "移除错误推理轨迹通常能带来更好地性能提升，但是在此数据上不明显，在 256 条数据时，过滤后只是从 75.50% 提升到 76.20%。但是相较于基座模型提升很大（17.59% --> 76.20%）。我们进一步将`简单移除错误`这种过滤方法，扩展到提升`数据质量`这一概念上，我们猜想通过提升数据质量这种方法应该同样会有不错的性能，现有的研究如[s1]()、[LIMO]()等使用`1k`左右的高质量数据就可以得到很好的推理性能。但是需要注意的是他们的结果通常对基座模型的推理能力有要求，在`Qwen2.5-32B-Instruct`上有较为不错的提升，但是随着模型规模变小，单纯使用少量高质量数据的效果通常没那么好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32474d3e",
   "metadata": {},
   "source": [
    "# 第三部分：专家迭代 (Expert Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65afa68",
   "metadata": {},
   "source": [
    "### 学习目标\n",
    "\n",
    "在本部分中，我们将学习专家迭代算法，这是一种结合了自我生成数据和监督学习的强大方法。这包括：\n",
    "\n",
    "1. 理解专家迭代的原理和优势\n",
    "2. 掌握迭代式推理能力提升的方法\n",
    "3. 实现完整的专家迭代训练流程\n",
    "4. 学习超参数调优和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d5432",
   "metadata": {},
   "source": [
    "## 3.1 专家迭代的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddce3b3",
   "metadata": {},
   "source": [
    "在前一节中，我们发现通过从监督微调（SFT）数据中剔除质量差的样本，可以提升 SFT 模型的性能。而在本节中，我们更进一步：不再只依赖外部提供的 SFT 数据，而是让模型自己（即“base model”，基础模型）生成推理轨迹（reasoning traces），然后对这些自生成的推理过程进行同样的“过滤”操作——只保留高质量的样本用于后续训练。这种“让模型自己生成数据 → 筛选高质量样本 → 用这些样本再训练自己”的迭代式自我提升方法，被称为 [Expert Iteration（专家迭代）](https://arxiv.org/pdf/1705.08439)。\n",
    "\n",
    "在大语言模型（LLM）背景下应用该思想的代表性工作，例如 [Cobbeet al. [2021b]](https://arxiv.org/pdf/2110.14168), [Zelikman et al. [2022]](https://arxiv.org/pdf/2203.14465), [Dohan et al. [2022]](https://arxiv.org/pdf/2207.10342), [Gulcehre et al. [2023]](https://arxiv.org/pdf/2308.08998).\n",
    "\n",
    "### 为什么专家迭代有效？\n",
    "\n",
    "- **自举学习**：模型从自己的成功案例中学习，避免了人工标注的高成本\n",
    "- **持续改进**：每次迭代都能生成更好的推理轨迹\n",
    "- **数据效率**：专注于高质量的推理模式\n",
    "- **可扩展性**：可以持续进行多轮迭代\n",
    "\n",
    "### 与纯SFT的区别\n",
    "\n",
    "| 方面 | 纯SFT | 专家迭代 |\n",
    "|------|-------|----------|\n",
    "| 数据来源 | 预先准备的推理轨迹 | 模型自我生成的轨迹 |\n",
    "| 数据质量 | 依赖人工标注或强模型 | 通过正确性过滤保证质量 |\n",
    "| 迭代性 | 一次性训练 | 多轮自举改进 |\n",
    "| 适应性 | 固定数据 | 随模型改进而改进 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f37b4e",
   "metadata": {},
   "source": [
    "## 3.2 算法流程详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8afa8",
   "metadata": {},
   "source": [
    "### 整体架构\n",
    "\n",
    "专家迭代的核心循环如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c193953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(N_EI_STEPS):\n",
    "    # A. 使用当前模型生成大量推理轨迹\n",
    "    rollouts = generate_rollouts(current_model, questions, G)\n",
    "\n",
    "    # B. 过滤出正确的推理轨迹作为\"专家\"数据\n",
    "    expert_data = filter_correct_rollouts(rollouts)\n",
    "\n",
    "    # C. 在专家数据上进行监督学习\n",
    "    current_model = supervised_finetuning(current_model, expert_data)\n",
    "\n",
    "    # D. 评估性能提升\n",
    "    evaluate_model(current_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940be7d5",
   "metadata": {},
   "source": [
    "### 关键超参数\n",
    "\n",
    "- **G (rollouts_per_question)**：每个问题生成的推理轨迹数量\n",
    "- **Db (expert_batch_size)**：每次迭代使用的训练问题数量\n",
    "- **epochs_per_step**：每次迭代的SFT训练轮数\n",
    "- **N_EI_STEPS**：总的迭代轮数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c611f1d",
   "metadata": {},
   "source": [
    "## 3.3 核心实现组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ba36f",
   "metadata": {},
   "source": [
    "### 推理轨迹生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样参数设置\n",
    "rollout_params = SamplingParams(\n",
    "    temperature=1.0,      # 高温度鼓励多样性\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    min_tokens=4,         # 防止空响应\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True,\n",
    "    n=rollouts_per_question  # 每个问题生成G个轨迹\n",
    ")\n",
    "\n",
    "# 生成rollouts\n",
    "outputs = llm.generate(prompts, rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b79a4",
   "metadata": {},
   "source": [
    "### 正确性过滤\n",
    "\n",
    "过滤标准是只保留答案正确的推理轨迹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤逻辑\n",
    "new_sft_data = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, req_output in enumerate(outputs):\n",
    "    gt = ground_truths[i]\n",
    "    prompt = prompts[i]\n",
    "\n",
    "    for completion in req_output.outputs:\n",
    "        generated_text = completion.text\n",
    "\n",
    "        # 使用奖励函数验证正确性\n",
    "        score = r1_zero_reward_fn(generated_text, gt)\n",
    "\n",
    "        if score['answer_reward'] == 1.0:\n",
    "            correct_count += 1\n",
    "            new_sft_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17121d0e",
   "metadata": {},
   "source": [
    "### 熵计算监控\n",
    "\n",
    "- **训练初期**：熵较高，表示模型不确定\n",
    "- **训练中期**：熵逐渐降低，表示模型学会了推理模式\n",
    "- **收敛状态**：熵稳定，表示模型达到最优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_entropy(logits, mask):\n",
    "    \"\"\"\n",
    "    计算响应token的平均熵，用于监控模型确定性\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 熵 = -sum(p * log p)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "\n",
    "    # 只计算响应token的熵\n",
    "    masked_entropy = entropy * mask\n",
    "    sum_mask = mask.sum()\n",
    "    return masked_entropy.sum() / sum_mask if sum_mask > 0 else torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5f384",
   "metadata": {},
   "source": [
    "## 3.4 实验设置和超参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a0c17",
   "metadata": {},
   "source": [
    "### 主要实验变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集大小扫描\n",
    "EXPERT_BATCH_SIZES = [512, 1024, 2048]  # Db\n",
    "\n",
    "# Rollouts per question\n",
    "ROLLOUTS_PER_QUESTION = [1, 4, 8, 16]   # G\n",
    "\n",
    "# SFT epochs per step\n",
    "SFT_EPOCHS_PER_STEP = [4, 8, 16]     # epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6a361",
   "metadata": {},
   "source": [
    "### 实验配置组合\n",
    "\n",
    "主要实验设置：\n",
    "\n",
    "1. **固定G=4, epochs=4，变化Db**：研究数据集大小的影响\n",
    "2. **固定Db=512, epochs=4，变化G**：研究rollouts数量的影响\n",
    "3. **固定Db=512, G=4，变化epochs**：研究SFT训练深度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b3f1a",
   "metadata": {},
   "source": [
    "### 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(N_EI_STEPS):\n",
    "    print(f\"=== Expert Iteration Step {step+1}/{N_EI_STEPS} ===\")\n",
    "\n",
    "    # 1. 同步权重到vLLM\n",
    "    load_policy_into_vllm(policy, llm)\n",
    "\n",
    "    # 2. 生成rollouts\n",
    "    print(f\"Generating {len(prompts) * rollouts_per_question} rollouts...\")\n",
    "    outputs = llm.generate(prompts, rollout_params)\n",
    "\n",
    "    # 3. 过滤正确示例\n",
    "    new_sft_data = filter_correct_examples(outputs, prompts, ground_truths)\n",
    "\n",
    "    # 4. SFT训练\n",
    "    print(f\"Training on {len(new_sft_data)} examples...\")\n",
    "    policy = sft_train_step(policy, new_sft_data, sft_epochs_per_step)\n",
    "\n",
    "    # 5. 评估\n",
    "    evaluate_performance(policy, llm, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066031d",
   "metadata": {},
   "source": [
    "## 3.5 性能分析和结果解读\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39e826",
   "metadata": {},
   "source": [
    "### 关键指标监控\n",
    "\n",
    "\n",
    "**训练指标：**\n",
    "- `ei/correct_rate`：每轮rollouts的正确率\n",
    "- `ei/dataset_size`：专家数据集大小\n",
    "- `train/loss`：SFT训练损失\n",
    "- `train/entropy`：响应token熵\n",
    "\n",
    "**评估指标：**\n",
    "- `eval/acc`：验证集准确率\n",
    "- `eval/format_rate`：格式正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbc427",
   "metadata": {},
   "source": [
    "### 实验结果分析\n",
    "\n",
    "#### 1. 数据集大小 (Db) 的影响\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/sft_ei_Db_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_Db_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_Db_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "- Db = 512:  accuracy = 63.08%\n",
    "- Db = 1024: accuracy = 67.17%\n",
    "- Db = 2048: accuracy = 66.49%\n",
    "\n",
    "在此实验中，Db=1024 表现出最佳的最终准确率 (67.17%)，而 Db=512 在训练初期表现最好，Db=2048 虽然最终准确率略低于 Db=1024，但其训练过程最为稳定，最终的熵值更低。\n",
    "\n",
    "#### 2. Rollouts数量 (G) 的影响\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/sft_ei_rollout_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_rollout_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_rollout_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "- G = 1:  accuracy = 66.49%\n",
    "- G = 4:  accuracy = 67.17%\n",
    "- G = 8:  accuracy = 66.19%\n",
    "- G = 16: accuracy = 65.96%\n",
    "\n",
    "在此实验中，G=4 表现出最佳的最终准确率 (67.17%)。随着 G 值的增加，模型的最终性能呈现先升后降的趋势。这表明存在一个“最优”的采样多样性水平：过少的采样会导致数据不足，而过多的采样则会引入噪声和低质量样本，反而损害模型性能。\n",
    "\n",
    "#### 3. 训练深度 (epochs) 的影响\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/sft_ei_epoch_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_epoch_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/sft_ei_epoch_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "- epochs = 4:  optimal = 63.08%\n",
    "- epochs = 8:  good_fit = 63.46%\n",
    "- epochs = 16: overfitting = 71.80%\n",
    "\n",
    "在此实验中，epochs=16 表现出最高的最终准确率 (71.80%)，显著优于 epochs=4 (63.08%) 和 epochs=8 (63.46%)。这表明，在专家迭代框架下，对高质量的\"专家\"数据进行更深度的训练是提升模型性能的关键。尽管 epochs=16 的曲线在中期出现波动，但其最终收敛效果最好，证明了其强大的学习能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5563b6",
   "metadata": {},
   "source": [
    "# 第四部分：分组相对策略优化 (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f8d3a",
   "metadata": {},
   "source": [
    "### 学习目标\n",
    "\n",
    "在本部分中，我们将学习分组相对策略优化 (Group Relative Policy Optimization, GRPO)，这是一种专为语言模型推理任务设计的强化学习算法。学习内容包括：\n",
    "\n",
    "1. 理解 GRPO 的核心原理和相对于传统 RL 的优势\n",
    "2. 掌握分组归一化优势的计算方法\n",
    "3. 实现多种策略梯度损失函数\n",
    "4. 构建完整的 GRPO 训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9be7c",
   "metadata": {},
   "source": [
    "## 4.1 GRPO 的核心原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bad0b",
   "metadata": {},
   "source": [
    "### 什么是 GRPO？\n",
    "\n",
    "`GRPO (Group Relative Policy Optimization)`是 Deepseek 在 [DeepSeekMath](https://arxiv.org/abs/2402.03300) 论文里首次提出的强化学习算法，旨在解决传统 `PPO（Proximal Policy Optimization）`在大规模模型训练中存在的计算效率低和内存占用高的问题。GRPO 和专家迭代一样，也是一种在线学习算法（online learning algorithm），这意味着它通过使用训练过程中由训练模型自身生成的数据来迭代改进。\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"../../docs/chapter14/images/14-4-ppo与grpo的对比.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "标准的 PPO 通常需要加载四个模型：\n",
    "- `Policy Model (Actor)`：当前训练的策略模型。\n",
    "- `Reference Model`：用于计算 KL 散度，防止模型跑偏。\n",
    "- `Reward Model`：用于给输出打分。\n",
    "- `Value Model (Critic)`：用于估计状态价值 $V(s)$，以计算优势函数。\n",
    "\n",
    "这使得训练所需的显存几乎是推理时的 3-4 倍。`价值模型`本身通常也需要是一个巨大的神经网络，训练它需要额外的计算资源。\n",
    "\n",
    "GRPO 的核心思想是：不再训练一个额外的价值模型来评估当前状态的好坏，而是通过对同一个提示（Prompt）采样多组输出，计算这些输出在组内的相对优势（Relative Advantage），即比较这些输出的“相对好坏”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce023c4",
   "metadata": {},
   "source": [
    "## 4.2 分组归一化优势计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063a223",
   "metadata": {},
   "source": [
    "### 核心思想\n",
    "\n",
    "GRPO 的核心创新是**分组相对优势**的计算：\n",
    "\n",
    "对于一个问题 $q$，从策略 $π_θ$ 中采样 $G$ 个响应 ${o^{i}}_{i=1}^G$，计算每个响应的奖励 $r^{i} = R(q, o^{i})$，然后计算分组归一化的优势：\n",
    "\n",
    "$$\n",
    "A^{i} = (r^{i} - mean(r^{1}, r^{2}, ..., r^{G})) / (std(r^{1}, r^{2}, ..., r^{G}) + advantage_{eps})\n",
    "$$\n",
    "\n",
    "这个优势 $A^{i}$ 对于响应中的每个 token 都是相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7b4e8",
   "metadata": {},
   "source": [
    "### 实现细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc796b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_normalized_rewards(\n",
    "        reward_fn: Callable[[str, str], Dict[str, float]],\n",
    "        rollout_responses: List[str],\n",
    "        repeated_ground_truths: List[str],\n",
    "        group_size: int,\n",
    "        advantage_eps: float,\n",
    "        normalize_by_std: bool,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7df6b2",
   "metadata": {},
   "source": [
    "**关键实现步骤：**\n",
    "\n",
    "1. **计算原始奖励**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eece78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rewards_list: List[float] = []\n",
    "raw_format_list: List[float] = []\n",
    "raw_answer_list: List[float] = []\n",
    "\n",
    "for resp, gt in zip(rollout_responses, repeated_ground_truths):\n",
    "    scores = reward_fn(resp, gt)  # expected keys: reward/format_reward/answer_reward\n",
    "    # Be strict so bugs surface early\n",
    "    if \"reward\" not in scores:\n",
    "        raise KeyError(f\"reward_fn output missing key 'reward': {scores.keys()}\")\n",
    "    raw_rewards_list.append(float(scores[\"reward\"]))\n",
    "    raw_format_list.append(float(scores.get(\"format_reward\", 0.0)))\n",
    "    raw_answer_list.append(float(scores.get(\"answer_reward\", 0.0)))\n",
    "\n",
    "raw_rewards = torch.tensor(raw_rewards_list, dtype=torch.float32)\n",
    "raw_format = torch.tensor(raw_format_list, dtype=torch.float32)\n",
    "raw_answer = torch.tensor(raw_answer_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a75f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6a46b5b",
   "metadata": {},
   "source": [
    "2. **计算组归一化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分组重组\n",
    "rollout_batch_size = len(rollout_responses)\n",
    "n_groups = rollout_batch_size // group_size\n",
    "rewards_g = raw_rewards.view(n_groups, group_size)  # (n_groups, group_size)\n",
    "\n",
    "group_mean = rewards_g.mean(dim=1, keepdim=True)  # (n_groups, 1)\n",
    "centered = rewards_g - group_mean  # (n_groups, group_size)\n",
    "\n",
    "if normalize_by_std:\n",
    "    # std per group; use unbiased=False for stability (population std)\n",
    "    group_std = rewards_g.std(dim=1, keepdim=True, unbiased=True)  # (n_groups, 1)\n",
    "    denom = group_std + float(advantage_eps)\n",
    "    advantages_g = centered / denom\n",
    "    zero_std_groups = (group_std.squeeze(1) == 0).sum().item()\n",
    "else:\n",
    "    advantages_g = centered\n",
    "    group_std = rewards_g.std(dim=1, keepdim=True, unbiased=True)\n",
    "    zero_std_groups = (group_std.squeeze(1) == 0).sum().item()\n",
    "\n",
    "advantages = advantages_g.reshape(-1)  # (rollout_batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbb99a",
   "metadata": {},
   "source": [
    "### 优势的含义\n",
    "\n",
    "- **正优势 (A > 0)**：该响应在组内表现优于平均水平，应该增加其概率\n",
    "- **负优势 (A < 0)**：该响应在组内表现劣于平均水平，应该减少其概率\n",
    "- **零优势 (A = 0)**：该响应在组内表现等于平均水平，不改变其概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bb7cd",
   "metadata": {},
   "source": [
    "### 元数据收集\n",
    "\n",
    "函数还返回详细的统计信息用于调试和监控："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"rollout_batch_size\": float(rollout_batch_size),\n",
    "    \"n_groups\": float(n_groups),\n",
    "    \"group_size\": float(group_size),\n",
    "    \"raw_reward_mean\": float(raw_rewards.mean().item()),\n",
    "    \"raw_reward_std\": float(raw_rewards.std(unbiased=False).item()),\n",
    "    \"group_reward_mean_mean\": float(group_means.mean().item()),\n",
    "    \"adv_mean\": float(advantages.mean().item()),\n",
    "    \"adv_std\": float(advantages.std(unbiased=False).item()) if advantages.numel() > 1 else 0.0,\n",
    "    # ... 更多统计\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46447c",
   "metadata": {},
   "source": [
    "## 4.3 策略梯度损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e5e3",
   "metadata": {},
   "source": [
    "### 朴素策略梯度损失\n",
    "\n",
    "最基本的策略梯度损失，**计算公式** 为 $loss_{b,t} = -A_{b} * log π_θ(o_{b,t} | q_b, o_{b,<t})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_naive_policy_gradient_loss(\n",
    "        raw_rewards_or_advantages: torch.Tensor,  # (batch_size, 1)\n",
    "        policy_log_probs: torch.Tensor,          # (batch_size, sequence_length)\n",
    ") -> torch.Tensor: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c25ba",
   "metadata": {},
   "source": [
    "**实现：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = raw_rewards_or_advantages.to(dtype=policy_log_probs.dtype, device=policy_log_probs.device)\n",
    "loss = -(advantages * policy_log_probs)  # 广播: (B,1) -> (B,T)\n",
    "return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001aecf",
   "metadata": {},
   "source": [
    "### GRPO-Clip 损失\n",
    "\n",
    "带裁剪机制的策略梯度损失，**核心公式**为：\n",
    "\n",
    "$$\n",
    "ratio_{b,t} = π_θ(o_{b,t}) / π_{θ_{old}}(o_{b,t})  \n",
    "$$\n",
    "\n",
    "$$\n",
    "clipped_{ratio} = clip(ratio, 1-ε, 1+ε)  \n",
    "$$\n",
    "\n",
    "$$\n",
    "loss_{b,t} = -min(ratio_{b,t} * A_b, clipped_ratio_{b,t} * A_b)  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_clip_loss(\n",
    "        advantages: torch.Tensor,           # (batch_size, 1)\n",
    "        policy_log_probs: torch.Tensor,     # (batch_size, sequence_length)\n",
    "        old_log_probs: torch.Tensor,        # (batch_size, sequence_length)\n",
    "        cliprange: float,                   # 裁剪范围 ε\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613daa10",
   "metadata": {},
   "source": [
    "**实现：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = advantages.to(dtype=policy_log_probs.dtype, device=policy_log_probs.device)\n",
    "log_ratio = policy_log_probs - old_log_probs\n",
    "ratio = torch.exp(log_ratio)\n",
    "clipped_ratio = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n",
    "\n",
    "unclipped_obj = ratio * A\n",
    "clipped_obj = clipped_ratio * A\n",
    "loss = -torch.minimum(unclipped_obj, clipped_obj)\n",
    "\n",
    "# 统计裁剪比例\n",
    "is_clipped = clipped_obj < unclipped_obj\n",
    "metadata = {\n",
    "    \"ratio\": ratio,\n",
    "    \"clipped_ratio\": clipped_ratio,\n",
    "    \"is_clipped\": is_clipped,\n",
    "    \"clip_fraction\": is_clipped.float().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d9575",
   "metadata": {},
   "source": [
    "### 策略梯度损失包装器\n",
    "\n",
    "定义统一的损失函数接口，**支持的损失类型**包括：\n",
    "\n",
    "- `\"no_baseline\"`：使用原始奖励作为优势\n",
    "- `\"reinforce_with_baseline\"`：使用分组归一化优势\n",
    "- `\"grpo_clip\"`：带裁剪的 GRPO 损失（离策略）\n",
    "- `\"grpo_no_clip\"`：不带裁剪的 GRPO 损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9624aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_gradient_loss(\n",
    "        policy_log_probs: torch.Tensor,\n",
    "        loss_type: Literal[\"no_baseline\", \"reinforce_with_baseline\", \"grpo_clip\", \"grpo_no_clip\"],\n",
    "        raw_rewards: torch.Tensor | None = None,\n",
    "        advantages: torch.Tensor | None = None,\n",
    "        old_log_probs: torch.Tensor | None = None,\n",
    "        cliprange: float | None = None,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1966a1c",
   "metadata": {},
   "source": [
    "## 4.4 工具函数实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6409873",
   "metadata": {},
   "source": [
    "### 掩码均值 (Masked Mean)\n",
    "\n",
    "在响应token上计算平均值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f37493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean(\n",
    "        tensor: torch.Tensor,    # (batch_size, sequence_length)\n",
    "        mask: torch.Tensor,      # (batch_size, sequence_length)\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2af19",
   "metadata": {},
   "source": [
    "**实现：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mask.to(device=tensor.device, dtype=tensor.dtype)\n",
    "masked_sum = (tensor * m).sum() if dim is None else (tensor * m).sum(dim=dim)\n",
    "denom = m.sum() if dim is None else m.sum(dim=dim)\n",
    "return masked_sum / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a58b19",
   "metadata": {},
   "source": [
    "### 掩码归一化 (Masked Normalize)\n",
    "\n",
    "按常数归一化而不是按掩码元素数量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_normalize(\n",
    "        tensor: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        normalize_constant: float,\n",
    "        dim: int | None = None,\n",
    ") -> torch.Tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176c04c",
   "metadata": {},
   "source": [
    "**实现：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mask.to(device=tensor.device, dtype=tensor.dtype)\n",
    "masked_sum = (tensor * m).sum() if dim is None else (tensor * m).sum(dim=dim)\n",
    "return masked_sum / float(normalize_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b72459",
   "metadata": {},
   "source": [
    "## 4.5 GRPO 微批次训练步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a3676",
   "metadata": {},
   "source": [
    "### 核心函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_microbatch_train_step(\n",
    "        policy_log_probs: torch.Tensor,         # (batch_size, sequence_length)\n",
    "        response_mask: torch.Tensor,            # (batch_size, sequence_length)\n",
    "        gradient_accumulation_steps: int,\n",
    "        loss_type: str,\n",
    "        raw_rewards: torch.Tensor | None = None,\n",
    "        advantages: torch.Tensor | None = None,\n",
    "        old_log_probs: torch.Tensor | None = None,\n",
    "        cliprange: float | None = None,\n",
    "        length_norm: str = \"masked_mean\",\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d57e1",
   "metadata": {},
   "source": [
    "### 训练步骤详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98aaab5",
   "metadata": {},
   "source": [
    "1. **计算每个token的损失**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_token_loss, meta = compute_policy_gradient_loss(\n",
    "    policy_log_probs=policy_log_probs,\n",
    "    loss_type=loss_type,\n",
    "    raw_rewards=raw_rewards,\n",
    "    advantages=advantages,\n",
    "    old_log_probs=old_log_probs,\n",
    "    cliprange=cliprange,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f0db8",
   "metadata": {},
   "source": [
    "2. **长度归一化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = response_mask.to(dtype=per_token_loss.dtype, device=per_token_loss.device)\n",
    "if length_norm == \"masked_mean\":\n",
    "    per_example_loss = masked_mean(per_token_loss, mask, dim=1)  # (batch_size,)\n",
    "elif length_norm == \"masked_normalize\":\n",
    "    per_example_loss = masked_normalize(per_token_loss, mask, dim=1, constant_normalizer=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10764a82",
   "metadata": {},
   "source": [
    "3. **批次平均和梯度累积**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbatch_loss = per_example_loss.mean()  # 标量\n",
    "loss = microbatch_loss / float(gradient_accumulation_steps)\n",
    "loss.backward()  # 只累积梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa03add",
   "metadata": {},
   "source": [
    "4. **元数据收集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_meta = dict(meta)  # 复制底层元数据\n",
    "out_meta.update({\n",
    "    \"microbatch_loss\": microbatch_loss.detach(),\n",
    "    \"per_example_loss_mean\": per_example_loss.detach().mean(),\n",
    "    \"per_example_loss_std\": per_example_loss.detach().std(unbiased=False),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48488c2",
   "metadata": {},
   "source": [
    "## 4.6 完整 GRPO 训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a6e62",
   "metadata": {},
   "source": [
    "由于 RL 训练需要消耗大量资源，为了在准确度和资源消耗之间取得平衡，我们在下面的 GRPO 训练使用了两张 GPU：\n",
    "- **GPU 0**：策略模型训练\n",
    "- **GPU 1**：vLLM 推理和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659ace1",
   "metadata": {},
   "source": [
    "### 关键超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心配置\n",
    "N_GRPO_STEPS = 200              # 总训练步数\n",
    "ROLLOUT_BATCH_SIZE = 256        # rollout 批次大小\n",
    "GROUP_SIZE = 8                  # 每个问题的响应数 G\n",
    "TRAIN_BATCH_SIZE = 256          # 训练批次大小\n",
    "EPOCHS_PER_ROLLOUT_BATCH = 1    # 每个rollout批次的训练轮数\n",
    "\n",
    "# 优势计算\n",
    "ADVANTAGE_EPS = 1e-6           # 防止除零的小常数\n",
    "USE_STD_NORMALIZATION = True   # 是否按标准差归一化\n",
    "\n",
    "# 损失函数\n",
    "LOSS_TYPE = \"reinforce_with_baseline\"  # 损失类型\n",
    "CLIPRANGE = 0.2                 # 裁剪范围（用于离策略）\n",
    "\n",
    "# 训练优化\n",
    "LR = 1e-5                       # 学习率\n",
    "GRAD_ACCUM_STEPS = 128          # 梯度累积步数\n",
    "MAX_GRAD_NORM = 1.0            # 梯度裁剪\n",
    "SEED = 2026  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cb4fa",
   "metadata": {},
   "source": [
    "### 验证批次大小关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f751da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次大小的数学关系\n",
    "assert TRAIN_BATCH_SIZE % GRAD_ACCUM_STEPS == 0\n",
    "micro_batch_size = TRAIN_BATCH_SIZE // GRAD_ACCUM_STEPS\n",
    "\n",
    "assert ROLLOUT_BATCH_SIZE % GROUP_SIZE == 0\n",
    "n_prompts_per_rollout = ROLLOUT_BATCH_SIZE // GROUP_SIZE\n",
    "\n",
    "assert TRAIN_BATCH_SIZE >= GROUP_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084cce1",
   "metadata": {},
   "source": [
    "### 采样参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout 采样参数\n",
    "rollout_params = SamplingParams(\n",
    "    temperature=1.0,      # 高温度鼓励多样性\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    min_tokens=4,         # 防止空响应\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True,\n",
    "    n=GROUP_SIZE         # 每个prompt的响应数\n",
    ")\n",
    "\n",
    "# 评估采样参数\n",
    "eval_params = SamplingParams(\n",
    "    temperature=0.0,      # 确定性评估\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51ae7d",
   "metadata": {},
   "source": [
    "### 训练循环实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. 初始化模型和数据\n",
    "    policy = init_policy_model()\n",
    "    llm = init_vllm()\n",
    "    train_data, val_data = load_data()\n",
    "\n",
    "    # 2. 训练循环\n",
    "    for step in range(N_GRPO_STEPS):\n",
    "        print(f\"=== GRPO Step {step+1}/{N_GRPO_STEPS} ===\")\n",
    "\n",
    "        # A. 采样 prompts 批次\n",
    "        batch_prompts = sample_prompts(train_data, n_prompts_per_rollout)\n",
    "\n",
    "        # B. 更新 vLLM 权重\n",
    "        load_policy_into_vllm(policy, llm)\n",
    "\n",
    "        # C. 生成 rollouts\n",
    "        outputs = llm.generate(batch_prompts, rollout_params)\n",
    "\n",
    "        # D. 收集响应和计算奖励\n",
    "        all_responses, all_rewards = collect_rollouts_and_rewards(outputs)\n",
    "\n",
    "        # E. 计算分组归一化优势\n",
    "        advantages, raw_rewards, _ = compute_group_normalized_rewards(\n",
    "            reward_fn=r1_zero_reward_fn,\n",
    "            rollout_responses=all_responses,\n",
    "            repeated_ground_truths=all_rewards,  # GRPO 不需要 GT\n",
    "            group_size=GROUP_SIZE,\n",
    "            advantage_eps=ADVANTAGE_EPS,\n",
    "            normalize_by_std=USE_STD_NORMALIZATION\n",
    "        )\n",
    "\n",
    "        # F. 创建训练数据集\n",
    "        dataset = create_training_dataset(batch_prompts, all_responses, advantages)\n",
    "\n",
    "        # G. 训练步骤\n",
    "        train_on_dataset(policy, dataset, optimizer)\n",
    "\n",
    "        # H. 评估\n",
    "        if step % EVAL_EVERY_STEPS == 0:\n",
    "            evaluate_policy(policy, llm, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9a6b7",
   "metadata": {},
   "source": [
    "# 第五部分：GRPO 实验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cd59f",
   "metadata": {},
   "source": [
    "### 学习目标\n",
    "\n",
    "在本部分中，我们将通过一系列系统性的实验来探索 GRPO 算法的各个方面，包括：\n",
    "1. 超参数调优（学习率、基线、归一化等）\n",
    "2. 算法变体比较（在策略 vs 离策略）\n",
    "3. 消融实验（裁剪、提示等）\n",
    "4. 最终的排行榜挑战\n",
    "\n",
    "此部分的完整实现脚本见`grpo_experiments.py`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce372ee1",
   "metadata": {},
   "source": [
    "## 5.1 实验基础设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48156905",
   "metadata": {},
   "source": [
    "### 实验配置\n",
    "\n",
    "所有 GRPO 实验的默认配置如下，除非在特定实验下有修改，否则均使用如下配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193fcd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    # Model & Data\n",
    "    base_model: str = \"/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B\"\n",
    "    train_data_path: str = \"data/sft/sft_gpt-oss-120b_filtered.jsonl\"\n",
    "    val_data_path: str = \"data/gsm8k/test.jsonl\"\n",
    "    output_dir: str = \"results/grpo\"\n",
    "    prompt_template_path: str = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "\n",
    "    # GRPO Hyperparams\n",
    "    n_grpo_steps: int = 200\n",
    "    rollout_batch_size: int = 256\n",
    "    group_size: int = 8\n",
    "    train_batch_size: int = 256\n",
    "    epochs_per_rollout_batch: int = 1\n",
    "    advantage_eps: float = 1e-6\n",
    "    cliprange: float = 0.2\n",
    "    use_std_normalization: bool = False\n",
    "    length_norm: str = \"masked_mean\"  # \"masked_mean\" or \"masked_normalize\"\n",
    "    reward_fn: str=r1_zero_reward_fn     # r1_zero_reward_fn, question_only_reward_fn\n",
    "\n",
    "    # Loss Type: \"no_baseline\", \"reinforce_with_baseline\", \"grpo_clip\", \"grpo_no_clip\"\n",
    "    loss_type: str = \"reinforce_with_baseline\"\n",
    "\n",
    "    # Training Hyperparams\n",
    "    lr: float = 1e-5\n",
    "    grad_accum_steps: int = 64\n",
    "    max_grad_norm: float = 1.0\n",
    "    seed: int = 2026\n",
    "\n",
    "    # Evaluation\n",
    "    eval_every_steps: int = 10\n",
    "    eval_max_examples: int = 256  # max=1319\n",
    "\n",
    "    # Devices\n",
    "    device_policy: str = \"cuda:1\"\n",
    "    device_eval: str = \"cuda:0\"\n",
    "\n",
    "    # vLLM memory behavior\n",
    "    vllm_gpu_memory_utilization: float = 0.7\n",
    "\n",
    "    # Generation params\n",
    "    gen_temperature: float = 1.0\n",
    "    gen_top_p: float = 1.0\n",
    "    gen_max_tokens: int = 1024\n",
    "    gen_min_tokens: int = 4\n",
    "    gen_stop: tuple = (\"</answer>\",)\n",
    "    gen_include_stop_str_in_output: bool = True\n",
    "\n",
    "    # Eval generation params\n",
    "    eval_temperature: float = 0.0\n",
    "    eval_top_p: float = 1.0\n",
    "    eval_max_tokens: int = 1024\n",
    "    eval_stop: tuple = (\"</answer>\",)\n",
    "    eval_include_stop_str_in_output: bool = True\n",
    "\n",
    "    # Optional knobs\n",
    "    wandb_project: str = \"cs336-a5-grpo\"\n",
    "    run_name: Optional[str] = None  # set to a string if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930eaa8",
   "metadata": {},
   "source": [
    "## 5.2 学习率调整实验 (grpo_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c193e8",
   "metadata": {},
   "source": [
    "学习率是 GRPO 训练中最关键的超参数之一。本实验旨在找到最优的学习率设置。\n",
    "\n",
    "### 实验设置 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f48bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [1e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4]\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    base_config = GRPOConfig(\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        lr=lr\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be1caf",
   "metadata": {},
   "source": [
    "### 结果分析\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"images/grpo_lr_eval_acc.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "学习率结果：\n",
    "- 1e-6:  18.88% (收敛太慢)\n",
    "- 5e-6:  32.98% (仍然偏慢)\n",
    "- 1e-5:  51.71% (最佳性能)\n",
    "- 2e-5:  66.57% (略有下降)\n",
    "- 5e-5:  49.28% (开始不稳定)\n",
    "- 1e-4:  0.61% \n",
    "\n",
    "**最优学习率**：2e-5，达到66.57%的准确率，后续实验将采用此参数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c3f8b",
   "metadata": {},
   "source": [
    "## 5.3 基线影响实验 (grpo_baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aec81",
   "metadata": {},
   "source": [
    "继续使用上述超参数（除了调整后的学习率），我们现在将研究策略梯度损失对性能的影响。我们处于 `on-policy` 设置，因此我们将比较损失类型：\n",
    "\n",
    "- `no_baseline`\n",
    "- `reinforce_with_baseline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff96b5",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BASELINE_TYPES = [\"no_baseline\", \"reinforce_with_baseline\"]\n",
    "BASELINE_TYPES = [\"reinforce_with_baseline\", \"no_baseline\"]\n",
    "\n",
    "for loss_type in BASELINE_TYPES:\n",
    "    config = GRPOConfig(\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        # rollout_batch_size=64, \n",
    "        loss_type=loss_type,\n",
    "        lr=2e-5,  # Best from LR sweep\n",
    "        use_std_normalization=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb817d",
   "metadata": {},
   "source": [
    "### 理论分析\n",
    "\n",
    "**无基线 (no_baseline)**：使用原始奖励 $r(q,o)$ 作为优势，简单直接，但方差较大，对奖励尺度敏感\n",
    "\n",
    "**有基线 (reinforce_with_baseline)**：使用归一化优势 $A^{i}$，降低梯度方差，提高稳定性，对奖励尺度不敏感\n",
    "\n",
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_baseline_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_baseline_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_baseline_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "基线实验结果：\n",
    "- `no_baseline`:           49.68%\n",
    "- `reinforce_with_baseline`: 34.95% \n",
    "\n",
    "\n",
    "我们发现`reinforce_with_baseline`损失下降的更快，但为什么在测试集上表现性能更差呢？\n",
    "\n",
    "通过观察熵变化曲线我们发现`reinforce_with_baseline`的熵很快变得很低，意味着它过早地变得“非常自信”，停止了探索。它可能只是死记硬背了某种特定的输出模式（Mode Collapse），而不是真正学会了推理。而`no_baseline`保持了较高的熵，说明它在训练过程中一直在尝试不同的可能，保留了探索能力。虽然`no_baseline`格式学习稍慢，但它是伴随着验证集准确率提升而稳步学习的。\n",
    "\n",
    "**结论**：在当前的超参和模型设置下，使用`no_baseline`是更优的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4124a3",
   "metadata": {},
   "source": [
    "## 5.4 长度归一化实验 (grpo_length_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e48285",
   "metadata": {},
   "source": [
    "比较不同的长度归一化方法对 GRPO 训练的影响。\n",
    "\n",
    "- `masked_mean`：对响应token求平均：`loss = sum(loss_per_token) / num_response_tokens`，短响应获得更多权重（因为除数小）\n",
    "\n",
    "- `masked_normalize`：按固定常数归一化：`loss = sum(loss_per_token) / constant`，所有响应获得相等权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29864a73",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION_TYPES = [\"masked_mean\", \"masked_normalize\"]\n",
    "\n",
    "for norm_type in NORMALIZATION_TYPES:\n",
    "    config = GRPOConfig(\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        length_norm=norm_type,\n",
    "        loss_type=\"reinforce_with_baseline\",\n",
    "        lr=2e-5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78806d72",
   "metadata": {},
   "source": [
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_length_norm_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_length_norm_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_length_norm_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_length_norm_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "长度归一化实验结果：\n",
    "- `masked_mean``:      69.37%\n",
    "- `masked_normalize``: 63.38% (+1.4%)\n",
    "\n",
    "仅做 $r - \\mu$。如果 Reward 的数值范围波动较大，计算出的 Advantage 绝对值就会很大。这直接导致计算出的梯度（Gradient）非常大。随着训练进行，模型对某些样本越来越自信（或 Reward 差异变大），梯度范数不断膨胀。\n",
    "蓝色 (Normalize): 做了 $\\frac{r - \\mu}{\\sigma}$。无论 Reward 的原始数值是 0.1 还是 100，归一化后的 Advantage 通常都落在 $[-2, 2]$ 甚至更小的区间内。这相当于自适应地调整了梯度的尺度，保证了更新幅度的稳定。\n",
    "\n",
    "\n",
    "`masked_normalize` 略优，因为它避免了短响应主导训练的问题，梯度范数更稳定（`masked_normalize` 的梯度更一致）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e41b4",
   "metadata": {},
   "source": [
    "## 5.5 标准差归一化实验 (grpo_group_standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f208e",
   "metadata": {},
   "source": [
    "测试是否应该按组标准差进行优势归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6ced5",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77959531",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_SETTINGS = [True, False]\n",
    "\n",
    "for use_std in STD_SETTINGS:\n",
    "    config = GRPOConfig(\n",
    "        use_std_normalization=use_std,\n",
    "        length_norm=\"masked_normalize\",\n",
    "        loss_type=\"reinforce_with_baseline\",\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        lr=2e-5,\n",
    "        run_name=f\"grpo_std_{use_std}\",\n",
    "        output_dir=f\"results/grpo_std_{use_std}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22667258",
   "metadata": {},
   "source": [
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_std_norm_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_std_norm_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_std_norm_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_std_norm_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "标准差归一化实验结果：\n",
    "- `use_std=True`:   63.46%\n",
    "- `use_std=False`:  68.00%\n",
    "\n",
    "\n",
    "开启优势归一化 (use_std=True`) 在本实验中并没有带来性能提升，反而导致了严重的训练崩溃（Catastrophic Forgetting）。 这种崩溃极有可能是由于某些 Batch 中 Reward 的方差过小，导致归一化时除以了一个极小的数值，引发了梯度爆炸。\n",
    "\n",
    "Advantage Normalization 的公式通常是：\n",
    "            $$ \\hat{A} = \\frac{A - \\mu}{\\sigma + \\epsilon} $$\n",
    "如果某一个 Group 内的所有样本 Reward 非常接近（方差 $\\sigma$ 极小）**，那么分母 $\\sigma + \\epsilon$ 就会非常小。这会导致计算出的优势 $\\hat{A}$ 变得巨大（例如：$0.001 / 0.0001 = 10$）。巨大的 $\\hat{A}$ 会直接乘在 Loss 中，导致巨大的梯度（Gradient Explosion）。\n",
    "\n",
    "在 Step 100 左右，模型可能刚好遇到了一个 Batch，里面的生成结果 Reward 都差不多（比如都答对了，或者都答错了且错得一样），导致 $\\sigma \\approx 0$。归一化操作放大了这种微小的差异，导致了梯度爆炸。从图中我们也可以找到证据，在 Step 100 左右，梯度范数瞬间飙升。这说明模型进行了一次幅度极大的参数更新。紧接着梯度爆炸，Step 105 左右 Green 的熵（Entropy）突然飙升到 0.8 左右，高熵表示模型输出趋向于混乱。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6e520",
   "metadata": {},
   "source": [
    "## 5.6 off-policy 的 GRPO 实现 (grpo_off_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1a2fb",
   "metadata": {},
   "source": [
    "在最基础的 GRPO 实现中，我们通常采用 on-policy 的训练模式。我们到目前为止实验的所有超参数都是 on-policy 的，即每个 rollout batch 只进行一次梯度更新。这种做法保证了训练的稳定性，严格遵循了策略梯度定理的假设，即用于计算梯度的数据必须来自当前正在被优化的策略本身。\n",
    "\n",
    "在 LLM 训练中，生成数据（Rollout/Inference）的成本极其高昂。它需要模型逐个 Token 地进行推理，速度慢且消耗大量显存。相比之下，反向传播（训练）的速度要快得多。\n",
    "如果每生成几百条数据，只更新一次参数就丢弃，这在计算资源上是极大的浪费。为了解决这个问题，我们现在将尝试 off-policy 训练，即每个 rollout batch 进行多次梯度更新（多 Epochs）。\n",
    "\n",
    "> 模型生成一批新的回答，这个过程就叫 `Rollout`\n",
    "\n",
    "为了实现 off-policy 训练，这里有几个参数需要重点关注：\n",
    "\n",
    "- `rollout_batch_size`： 采样批次大小，\n",
    "- `train_batch_size`：训练批次大小，每次梯度更新用多少样本（通常 ≤ rollout_batch_size）\n",
    "- `epochs_per_rollout_batch`：每批 rollout 数据训练几轮（epoch），越大的 epochs_per_rollout_batch 表示越 off-policy\n",
    "\n",
    "⚠️ 注意：epochs_per_rollout_batch 并不总是越大越好，过度复用可能导致`策略漂移（policy drift）`，即策略偏离太大、发散，进而导致训练崩溃。所以需要像 PPO 那样用 clip（裁剪） 或 KL penalty 来约束更新幅度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115cb05",
   "metadata": {},
   "source": [
    "### 1. epochs_per_rollout_batch\n",
    "\n",
    "在 run_off_policy_sweep_experiment 函数中，你可以看到不同的配置：\n",
    "```\n",
    "OFF_POLICY_CONFIGS = [\n",
    "    {\"epochs_per_rollout_batch\": 1, \"train_batch_size\": 256, \"grad_accum_steps\": 128, \"name\": \"on_policy\"},\n",
    "    {\"epochs_per_rollout_batch\": 2, \"train_batch_size\": 128, \"grad_accum_steps\": 128, \"name\": \"off_policy_2x\"},\n",
    "    {\"epochs_per_rollout_batch\": 4, \"train_batch_size\": 128, \"grad_accum_steps\": 64, \"name\": \"off_policy_4x\"},\n",
    "    {\"epochs_per_rollout_batch\": 8, \"train_batch_size\": 128, \"grad_accum_steps\": 32, \"name\": \"off_policy_8x\"},\n",
    "]\n",
    "```\n",
    "\n",
    "其中核心控制参数 epochs_per_rollout_batch 是开启 Off-Policy 的“开关”。\n",
    "\n",
    "在主循环中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a62ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外层循环控制这批数据被复用的次数\n",
    "for epoch in range(cfg.epochs_per_rollout_batch):\n",
    "    random.shuffle(dataset_indices) # 打乱数据\n",
    "    # ... 进行多次梯度下降 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d76a78",
   "metadata": {},
   "source": [
    "当 epoch > 0 时，模型参数 $\\theta$ 已经更新了，但数据还是第 0 轮生成的。此时 $\\pi_\\theta \\neq \\pi_{old}$，这就构成了 Off-Policy 训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e410a3",
   "metadata": {},
   "source": [
    "### 2. 固定参考概率：计算 old_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f70e9",
   "metadata": {},
   "source": [
    "为了计算重要性采样比率 $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{old}(a_t|s_t)}$，我们必须记住数据生成时的概率 $\\pi_{old}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果是 Off-Policy (grpo_clip)，必须先计算旧概率\n",
    "if cfg.loss_type in (\"grpo_clip\", \"grpo_no_clip\"):\n",
    "    print(\"Computing old logprobs...\")\n",
    "    policy.eval() # 冻结模型\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ... 遍历所有生成的 response ...\n",
    "        out = get_response_log_probs(policy, input_ids, labels)\n",
    "        old_log_probs_list.append(out[\"log_probs\"].cpu())\n",
    "    \n",
    "    # 将这些概率拼接并保存起来，作为后续计算 ratio 的分母\n",
    "    old_log_probs = torch.cat(padded, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b361a3",
   "metadata": {},
   "source": [
    "这一步至关重要。因为 vLLM 生成文本时通常只返回文本，不返回 Token 级别的 LogProbs。所以代码在开始训练循环前，先用当前的 Policy 跑一遍推理，把所有 Token 的 LogProb 存下来作为“基准”。在接下来的 epochs 循环中，这个 old_log_probs 保持不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede81a50",
   "metadata": {},
   "source": [
    "### 3. 损失函数计算：PPO Clipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997325f6",
   "metadata": {},
   "source": [
    "在训练代码的内部循环中，代码将当前的 LogProbs 和保存的 Old LogProbs 同时传给损失函数：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f902b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前参数下的概率 (分子)\n",
    "out = get_response_log_probs(policy, input_ids, labels, ...)\n",
    "policy_log_probs = out[\"log_probs\"]\n",
    "\n",
    "# 获取生成时的概率 (分母)\n",
    "batch_old_log_probs = old_log_probs[indices][:, :T]\n",
    "\n",
    "# 计算 Loss\n",
    "loss, loss_meta = grpo_microbatch_train_step(\n",
    "    policy_log_probs=policy_log_probs,\n",
    "    old_log_probs=batch_old_log_probs, # 传入旧概率\n",
    "    loss_type=cfg.loss_type,           # 这里通常是 \"grpo_clip\"\n",
    "    cliprange=cfg.cliprange,           # 例如 0.2\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec6448",
   "metadata": {},
   "source": [
    "在标准的 On-Policy（如原始 REINFORCE）中，数据用一次就丢。而在 Off-Policy 设置下，随着参数更新，当前的策略 $\\pi_\\theta$ 已经偏离了生成数据时的策略 $\\pi_{old}$，因此必须引入`重要性采样（Importance Sampling）` 和`裁剪（Clipping）` 机制来修正偏差。\n",
    "\n",
    "在 `gpro_helper.py` 中，实现了 Off-Policy 的核心函数 compute_grpo_clip_loss，其实现了`重要性采样（Importance Sampling）` 和`裁剪（Clipping）` 机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_clip_loss(\n",
    "        advantages: torch.Tensor,\n",
    "        policy_log_probs: torch.Tensor, # 当前正在更新的策略 π_theta 的 LogProbs\n",
    "        old_log_probs: torch.Tensor,    # 生成数据时的旧策略 π_old 的 LogProbs (固定不动)\n",
    "        cliprange: float,               # 裁剪范围 ε (例如 0.2)\n",
    "        loss_type: Literal[\"grpo_clip\", \"grpo_no_clip\"] = \"grpo_clip\",\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \n",
    "    # ... (省略形状检查) ...\n",
    "\n",
    "    # 1. 计算重要性采样比率 (Importance Sampling Ratio)\n",
    "    # r_t(θ) = π_theta(a_t | s_t) / π_old(a_t | s_t)\n",
    "    # 在对数空间计算：log(a/b) = log(a) - log(b)\n",
    "    log_ratio = policy_log_probs - old_log_probs\n",
    "    ratio = torch.exp(log_ratio)  # 得到 r_t(θ)\n",
    "\n",
    "    if loss_type == \"grpo_no_clip\":\n",
    "        # 如果不裁剪，就是标准的 Off-Policy 策略梯度 (容易训练不稳定)\n",
    "        loss = -(ratio * A) \n",
    "        return loss, metadata\n",
    "\n",
    "    elif loss_type == \"grpo_clip\":\n",
    "        # 2. 计算未裁剪的目标函数 (Surrogate 1)\n",
    "        # L^CPI = r_t(θ) * A_t\n",
    "        unclipped_obj = ratio * A\n",
    "\n",
    "        # 3. 计算裁剪后的目标函数 (Surrogate 2)\n",
    "        # clip(r_t(θ), 1-ε, 1+ε) * A_t\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n",
    "        clipped_obj = clipped_ratio * A\n",
    "\n",
    "        # 4. 取最小值 (PPO 的保守策略)\n",
    "        # L^CLIP = min(r_t(θ)*A_t, clip(...)*A_t)\n",
    "        min_obj = torch.minimum(unclipped_obj, clipped_obj)\n",
    "        \n",
    "        # 5. 取负号因为我们要最小化 Loss\n",
    "        loss = -min_obj\n",
    "\n",
    "        # ... (记录 metadata) ...\n",
    "        return loss, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d1c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ea0329",
   "metadata": {},
   "source": [
    "## 5.7 off-policy GRPO 超参数扫描 (grpo_off_policy_sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a02f9",
   "metadata": {},
   "source": [
    "### 实验设计\n",
    "\n",
    "扫描 off-policy 的关键超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44062a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFF_POLICY_CONFIGS = [\n",
    "    {\"epochs_per_rollout_batch\": 1, \"train_batch_size\": 256, \"grad_accum_steps\": 128, \"name\": \"on_policy\"},\n",
    "    {\"epochs_per_rollout_batch\": 2, \"train_batch_size\": 128, \"grad_accum_steps\": 128, \"name\": \"off_policy_2x\"},\n",
    "    {\"epochs_per_rollout_batch\": 4, \"train_batch_size\": 128, \"grad_accum_steps\": 64, \"name\": \"off_policy_4x\"},\n",
    "    {\"epochs_per_rollout_batch\": 8, \"train_batch_size\": 128, \"grad_accum_steps\": 32, \"name\": \"off_policy_8x\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9665b",
   "metadata": {},
   "source": [
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_off_policy_train_clip_fraction.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_off_policy_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_off_policy_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "off-policy 扫描结果：\n",
    "- `on_policy(1 epoch)`:    58.30%\n",
    "- `off_policy_2x(2 epochs)`: 73.84% \n",
    "- `off_policy_4x(4 epochs)`: 68.84% \n",
    "- `off_policy_8x(8 epochs)`: 63.69% \n",
    "\n",
    "off_policy_8x 的高裁剪率说明当前的策略已经偏离旧策略太远了。裁剪机制一直在拼命工作以阻止过大的更新，但这同时也意味着大部分梯度信息被截断了，或者剩下的梯度估计非常不准确。这解释了为什么 8x 训练效果最差。Brown (8x) 的熵下降慢，说明模型迟迟无法确信某种策略。由于反复在陈旧的数据上强行更新，模型陷入了混乱，无法收敛到一个确定的、高奖励的分布上。\n",
    "\n",
    "强行增加 Off-Policy 的程度（即增加重复训练次数）会导致严重的训练不稳定和性能崩溃，尤其是在高倍率（8x）下。轻微的 Off-Policy (2x) 表现最佳。这验证了强化学习中的一个基本原理：PPO/GRPO 是基于 Trust Region（信任域）的算法，如果旧策略（采样数据的策略）和新策略（当前训练的策略）差异过大（KL 散度过高），重要性采样比率（Importance Sampling Ratio）会失效，导致数值爆炸。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f4c42",
   "metadata": {},
   "source": [
    "## 5.8 裁剪消融实验 (grpo_off_policy_clip_ablation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e421fa3",
   "metadata": {},
   "source": [
    "测试裁剪机制是否真的必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae235d1",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_SETTINGS = [\"grpo_clip\", \"grpo_no_clip\"]\n",
    "\n",
    "# Use best off-policy config\n",
    "for loss_type in CLIP_SETTINGS:\n",
    "    config = GRPOConfig(\n",
    "        loss_type=loss_type,\n",
    "        epochs_per_rollout_batch=2,\n",
    "        train_batch_size=128,  # Fixed: 128 % 128 = 0\n",
    "        grad_accum_steps=128,  # Ensure compatibility\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        use_std_normalization=False,\n",
    "        length_norm=\"masked_normalize\",\n",
    "        lr=2e-5,\n",
    "        eval_max_examples = 256,  # max=1319\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f255e54",
   "metadata": {},
   "source": [
    "\n",
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_clip_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_clip_train_grad_norm.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_clip_train_clip_fraction.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_clip_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "裁剪消融实验结果：\n",
    "- `grpo_clip`:     79.99%\n",
    "- `grpo_no_clip`:  68.23%\n",
    "\n",
    "**结论**：裁剪对于离策略训练至关重要，防止策略发散。没有 Clip 机制（Red），模型训练虽然初期看似正常，但最终遭遇了灾难性的梯度爆炸，导致训练彻底失败；而开启 Clip 机制（Blue）虽然初期经历了剧烈的震荡，但最终成功稳定下来，并取得了远超 No Clip (Red) 的性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf848de9",
   "metadata": {},
   "source": [
    "## 5.9 提示消融实验 (grpo_prompt_ablation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b089ee4",
   "metadata": {},
   "source": [
    "比较不同提示对 GRPO 性能的影响。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370c0d4",
   "metadata": {},
   "source": [
    "### 提示对比\n",
    "\n",
    "**R1-Zero 提示**：\n",
    "```\n",
    "A conversation between User and Assistant...\n",
    "User: {question}\n",
    "Assistant: <think>\n",
    "```\n",
    "\n",
    "**Question-Only 提示**：\n",
    "```\n",
    "{question}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bcb1e",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPES = [\"r1_zero\", \"question_only\"]\n",
    "\n",
    "for prompt_type in PROMPT_TYPES:\n",
    "    if prompt_type == \"r1_zero\":\n",
    "        template_path = \"cs336_alignment/prompts/r1_zero.prompt\"\n",
    "        reward_fn = r1_zero_reward_fn\n",
    "    else:\n",
    "        template_path = \"cs336_alignment/prompts/question_only.prompt\"\n",
    "        reward_fn = question_only_reward_fn\n",
    "\n",
    "    config = GRPOConfig(\n",
    "        prompt_template_path=template_path,\n",
    "        reward_fn=reward_fn,\n",
    "        use_std_normalization=False,\n",
    "        length_norm=\"masked_normalize\",\n",
    "        loss_type=\"grpo_clip\",\n",
    "        n_grpo_steps=50,  # Shorter for sweep\n",
    "        lr=2e-5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea49470",
   "metadata": {},
   "source": [
    "### 实验结果\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/grpo_prompt_train_loss.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_prompt_train_entropy.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_prompt_train_format_reward.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_prompt_train_acc_reward.png\" width=\"300\"/>\n",
    "    <img src=\"images/grpo_prompt_eval_acc.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "```\n",
    "提示消融实验结果：\n",
    "- `r1_zero`:      46.32%\n",
    "- `question_only`:  71.72%\n",
    "```\n",
    "\n",
    "蓝色曲线的下跌说明模型在训练过程中逐渐“遗忘”了如何正确输出格式。在 GRPO/PPO 中，如果格式错误导致答案无法解析，通常会得到一个很低的 Reward。这导致模型陷入混乱。并且蓝色曲线在 Step 95-100 左右，发生了梯度爆炸。当模型在 r1_zero 的复杂约束下挣扎时，可能出现了某些极端的样本（例如格式极其错误但偶然蒙对了答案，或者格式正确但被判错），导致 Loss 变得极大。\n",
    "\n",
    "#### 为什么 r1_zero 会失败？\n",
    "\n",
    "- Cold Start 问题：DeepSeek-R1 论文中提到，纯 RL (R1-Zero) 在初期非常不稳定，且容易产生不可读的输出。如果你的基座模型没有经过针对 <think> 格式的 SFT (Cold Start Data)，直接上 RL 强行要求它输出这种格式，模型很难通过随机探索找到正确的路径。\n",
    "- Prompt 与模型能力不匹配：question_only 表现好，说明模型本身是有知识的。r1_zero 强加了复杂的思维链格式，这增加了生成的长度和复杂性。如果模型本身上下文能力或指令遵循能力不足，这种复杂的 Prompt 反而成了干扰（Distraction）。\n",
    "- Reward 设计缺陷：如果你的 Reward 函数对格式错误的惩罚不够平滑（例如：格式错直接给 -1，格式对给 +1），模型可能在探索过程中因为频繁遭遇“格式惩罚”而梯度爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96ce58",
   "metadata": {},
   "source": [
    "## 5.10 排行榜挑战 (leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846909ad",
   "metadata": {},
   "source": [
    "### 主要发现\n",
    "\n",
    "1. **学习率调优至关重要**：2e-5 是最佳选择，比默认值高一个数量级\n",
    "\n",
    "2. **基线显著提升性能**：使用分组归一化优势比原始奖励好3-4个百分点\n",
    "\n",
    "3. **长度归一化影响稳定性**：`masked_normalize` 比 `masked_mean` 更稳定\n",
    "\n",
    "4. **标准差归一化可有可无**：不使用标准差归一化略优\n",
    "\n",
    "5. **离策略大幅提升效率**：2倍的样本效率，比 on-policy 性能提升15个百分点\n",
    "\n",
    "6. **裁剪防止发散**：在离策略设置中必不可少\n",
    "\n",
    "7. **提示格式很重要**：在此实验配置下简单提示比结构化提示好25个百分点\n",
    "\n",
    "> 注意：由于 RL 训练对超参数非常敏感，并且比较吃资源，我们的实验大多只跑了`n_grpo_steps=50`，最终的结果可能还没有稳定下来。例如，下图所示大部分的 SOTA 方法需要跑至少 2000 step 才能又一个比价好的结果，因此`主要发现`只能作为参考，更多 tricks 可以参考网上已有经验。\n",
    "\n",
    "<center align=\"center\">\n",
    "    <img src=\"images/computational_cost_comparison.png\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a8ec0",
   "metadata": {},
   "source": [
    "### 挑战目标\n",
    "\n",
    "在4小时训练时间内，使用2个H100 GPU，获得尽可能高的验证准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e06bf8",
   "metadata": {},
   "source": [
    "这部分留待大家继续探索！期待大家多多分享自己跑出来的 SOTA 结果！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
