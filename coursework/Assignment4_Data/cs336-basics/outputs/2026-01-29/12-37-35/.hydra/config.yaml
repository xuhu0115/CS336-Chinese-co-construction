paths:
  train_bin: ???
  valid_bin: ???
  model_output: ???
model:
  vocab_size: 50257
  context_length: 512
  d_model: 768
  d_ff: 2048
  num_layers: 12
  num_heads: 12
  rope_theta: 10000.0
training:
  seed: 0
  dtype: bfloat16
  train_batch_size: 128
  eval_batch_size: ${training.train_batch_size}
  train_steps: 100000
  gradient_accumulation_steps: 1
  compile: true
  eval_iterations: 1000
  eval_interval: 2000
  max_grad_norm: 1.0
  device: cuda
  lr: 0.001
  warmup_ratio: 0.01
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_eps: 1.0e-09
  wandb_project: null
  wandb_entity: null
  log_interval: 20
  save_checkpoints: false
