{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c04b0b",
   "metadata": {},
   "source": [
    "# 作业八\n",
    "\n",
    "一种用于去除**完全重复内容**的简单方法是：只保留在整个语料库中**唯一出现**的行。事实证明，这足以消除很大一部分冗余内容，例如前面提到的页眉和菜单选项。在更简单的情况下，当我们删除在其他地方被**完全重复**的行时，通常可以得到每个页面的**唯一主体内容**（例如 StackOverflow 上的问题和回答）。\n",
    "\n",
    "为此，我们可以对语料库进行**一次遍历**，统计每一行出现的次数。然后，在**第二次遍历**中，通过只保留其唯一行来重写每个文档。\n",
    "\n",
    "一种朴素的做法是，使用一个数据结构来保存计数器，但这样会占用与存储语料库中所有唯一行相同规模的内存。一个简单的**内存优化技巧**是：不直接使用整行文本作为键，而是使用该行的**哈希值**作为键，从而使键的大小固定（而不是依赖于行的长度）。现在你将实现这种简单的去重方法。\n",
    "\n",
    "\n",
    "\n",
    "### 问题（exact_deduplication）：3 分\n",
    "\n",
    "编写一个函数，接收一组输入文件路径，并对这些文件执行**精确行去重**。该函数应首先统计语料库中每一行的出现频率，并使用**哈希**来降低内存使用。\n",
    "\n",
    "然后，通过**只保留唯一行**来重写每个文件。\n",
    "\n",
    "**交付物（Deliverable）：**\n",
    "一个执行**精确行去重**的函数。你的函数应接收两个参数：\n",
    "(a) 输入文件路径列表；\n",
    "(b) 一个输出目录。\n",
    "\n",
    "该函数应将每个输入文件重写到输出目录中，**文件名保持不变**，但通过删除在输入文件集合中**出现超过一次**的行来实现内容去重。\n",
    "\n",
    "例如，如果输入路径是 `a/1.txt` 和 `a/2.txt`，输出目录是 `b/`，那么你的函数应生成文件 `b/1.txt` 和 `b/2.txt`。\n",
    "\n",
    "请实现适配器 **[run_exact_line_deduplication]**，并确保它能够通过以下测试：\n",
    "\n",
    "```\n",
    "uv run pytest -k test_exact_line_deduplication\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6103496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def _line_hash(line: str) -> str:\n",
    "    \"\"\"\n",
    "    对一行文本计算稳定哈希值\n",
    "    \"\"\"\n",
    "    return hashlib.md5(line.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def exact_deduplication(file_paths: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    对给定文件列表执行精确行去重：\n",
    "    - 第一次遍历：统计每一行（基于哈希）的全局出现次数\n",
    "    - 第二次遍历：仅保留全语料中唯一出现的行，重写文件\n",
    "\n",
    "    参数：\n",
    "        file_paths: 输入文件路径列表\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 第一次遍历：统计 ----------\n",
    "    hash_counter = defaultdict(int)\n",
    "\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                h = _line_hash(line)\n",
    "                hash_counter[h] += 1\n",
    "\n",
    "    # ---------- 第二次遍历：重写 ----------\n",
    "    for path in file_paths:\n",
    "        unique_lines = []\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                h = _line_hash(line)\n",
    "                if hash_counter[h] == 1:\n",
    "                    unique_lines.append(line)\n",
    "\n",
    "        # 覆盖写回文件\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(unique_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033e18d",
   "metadata": {},
   "source": [
    "# 作业九\n",
    "\n",
    "### 3.2 MinHash + LSH 文档去重\n",
    "\n",
    "**精确去重（Exact deduplication）** 对于移除在多个网页中逐字重复的内容非常有用，但它无法处理文档内容仅有轻微差异的情况。\n",
    "例如，考虑软件许可证文档：许可证通常由一个模板生成，其中只需要填写年份和软件作者的名字。因此，一个 MIT 许可证项目中的 license 文件与另一个 MIT 许可证项目中的 license 文件在内容上高度相似，但它们并不是完全重复的文档。为了去除这类“高度重复但不完全相同”的模板化内容，我们需要**模糊去重（fuzzy deduplication）**。\n",
    "为了高效地执行文档级别的模糊去重，我们将使用 **MinHash 结合局部敏感哈希（LSH）**。\n",
    "\n",
    "为了执行模糊去重，我们采用一种特定的文档相似性度量方式：**文档 n-gram 集合之间的 Jaccard 相似度**。集合 $S$ 与 $T$ 的 Jaccard 相似度定义为\n",
    "\n",
    "$$\n",
    "\\frac{|S \\cap T|}{|S \\cup T|}\n",
    "$$\n",
    "\n",
    "一种朴素的模糊去重方法是：将每个文档表示为一个 n-gram 集合，计算所有文档对之间的 Jaccard 相似度，如果相似度超过某个阈值，就将该文档对标记为重复。然而，这种方法在大规模文档集合（例如 Common Crawl）上是不可行的。此外，直接存储 n-gram 集合在内存上也会比存储原始文档占用更多空间。\n",
    "\n",
    "---\n",
    "\n",
    "### MinHash\n",
    "\n",
    "为了解决内存问题，我们用**签名（signature）**来替代基于 n-gram 集合的文档表示。具体来说，我们希望构造这样一种签名：当我们比较两个文档的签名时，可以得到对原始 n-gram 集合 Jaccard 相似度的近似值。MinHash 签名正好满足这一性质。\n",
    "\n",
    "为了计算一个文档 n-gram 集合\n",
    "\n",
    "$$\n",
    "S = {s_1, \\ldots, s_n}\n",
    "$$\n",
    "\n",
    "的 MinHash 签名，我们需要 $k$ 个不同的哈希函数 $h_1, \\ldots, h_k$。每个哈希函数将一个 n-gram 映射为一个整数。\n",
    "给定哈希函数 $h_i$，文档 n-gram 集合 $S$ 的 MinHash 值定义为：\n",
    "\n",
    "$$\n",
    "\\text{minhash}[h_i, S] = \\min[h_i]s_1$, h_i$s_2$, \\ldots, h_i$s_n$\n",
    "$$\n",
    "\n",
    "文档 n-gram 集合 $S$ 的签名是一个位于 $\\mathbb{R}^k$ 中的向量，其中每一维对应一个随机哈希函数下的 MinHash 值：\n",
    "$$\n",
    "$$\\text{minhash}$h_1, S$, \\text{minhash}$h_2, S$, \\ldots, \\text{minhash}$h_k, S$$$\n",
    "$$\n",
    "\n",
    "可以证明，对于两个文档的 n-gram 集合 $S_1$ 和 $S_2$，它们的 Jaccard 相似度可以通过 **签名中相同 MinHash 值所占的比例** 来近似（证明见第 3.3 节，参考 *Leskovec et al., 2014*）。\n",
    "例如，给定两个文档的签名分别为 `[1, 2, 3, 2]` 和 `[5, 2, 3, 4]\n",
    "`，由于第二列和第三列的 MinHash 值相同，Jaccard 相似度被近似为 $2/4$。\n",
    "\n",
    "---\n",
    "\n",
    "### 局部敏感哈希（LSH）\n",
    "\n",
    "尽管 MinHash 为我们提供了一种**内存高效**且能保持期望相似度的文档表示方式，但我们仍然需要在所有文档对之间进行比较，以找到相似度最高的文档对。\n",
    "**LSH（Locality-Sensitive Hashing）** 提供了一种高效的方法，可以将**可能具有较高相似度的文档分到同一个桶中**，从而避免全量两两比较。\n",
    "\n",
    "在将 LSH 应用于文档签名（即 $\\mathbb{R}^k$ 中的向量）时，我们会将签名划分为 $b$ 个 band，每个 band 包含 $r$ 个 MinHash 值，其中 $k = b r$。\n",
    "例如，如果我们有长度为 100 的文档签名（由 100 个随机哈希函数生成），我们可以划分为 **2 个 band，每个 band 含 50 个 minhash**，或者 **4 个 band，每个 band 含 25 个 minhash**，或者 **50 个 band，每个 band 含 2 个 minhash**，等等。\n",
    "如果两个文档在某一个 band 上具有**完全相同的哈希值**，那么它们就会被聚到同一个桶（bucket）中，并被视为**候选重复文档（candidate duplicates）**。\n",
    "因此，在签名长度固定的情况下，**增加 band 的数量会提高召回率（recall），但会降低精确率（precision）**。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b429ab",
   "metadata": {},
   "source": [
    "\n",
    "### 具体示例\n",
    "\n",
    "假设我们有一个文档 $D_1$，其 minhash 签名为\n",
    "$[1, 2, 3, 4, 5, 6]$，\n",
    "另一个文档 $D_2$ 的 minhash 签名为\n",
    "$[1, 2, 3, 5, 1, 2]$。\n",
    "\n",
    "如果我们使用 **3 个 band，每个 band 含 2 个 minhash**，那么：\n",
    "\n",
    "* $D_1$ 的第一个 band 是 $[1, 2]$，第二个 band 是 $[3, 4]$，第三个 band 是 $[5, 6]$\n",
    "* $D_2$ 的第一个 band 是 $[1, 2]$，第二个 band 是 $[3, 5]$，第三个 band 是 $[1, 2]$\n",
    "\n",
    "由于在**第一个 band** 中的哈希值完全一致（对两个文档都是 $[1, 2]$），$D_1$ 和 $D_2$ 会在该 band 下被聚到同一个桶中。\n",
    "而在其他 band 中，由于哈希值不匹配，它们不会被聚到同一个桶。\n",
    "\n",
    "不过需要注意的是：**只要文档在至少一个 band 中被聚到同一个桶里，它们就会被视为候选重复文档**，而不需要其他 band 也匹配。\n",
    "\n",
    "---\n",
    "\n",
    "一旦我们识别出了候选重复文档，就可以用多种方式对它们进行后处理。例如，可以计算所有候选文档对之间的 **真实 n-gram Jaccard 相似度**，并将超过某个阈值的文档对标记为重复。\n",
    "\n",
    "---\n",
    "\n",
    "最后，我们还需要**跨桶聚类重复文档**。\n",
    "例如，假设文档 A 和 B 在某一个桶中匹配，并且它们的真实 Jaccard 相似度高于阈值；同时，文档 B 和 C 在另一个桶中匹配，且它们的真实 Jaccard 相似度也高于阈值。\n",
    "那么，我们会将文档 A、B、C 视为**同一个聚类（cluster）**，并在每个聚类中**随机保留一个文档，其余删除**。\n",
    "\n",
    "---\n",
    "\n",
    "## 问题（MinHash 去重）：8 分\n",
    "\n",
    "编写一个函数，该函数接收一组输入文件路径，并使用 **MinHash + LSH** 执行**模糊文档去重**。具体要求如下：\n",
    "\n",
    "* 对给定路径列表中的每个文档计算 **MinHash 签名**\n",
    "* 使用给定数量的 **band** 通过 **LSH** 识别候选重复文档\n",
    "* 对候选重复文档计算 **真实的 n-gram Jaccard 相似度**\n",
    "* 删除相似度超过给定阈值的文档\n",
    "\n",
    "为提升效果（参考 *Penedo et al., 2023*），在计算 MinHash 签名和 / 或比较 Jaccard 相似度之前，应对文本进行如下规范化处理：\n",
    "\n",
    "* 全部转换为小写\n",
    "* 移除标点符号\n",
    "* 规范化空白符\n",
    "* 移除重音符号（accent）\n",
    "* 应用 **Unicode NFD 规范化**\n",
    "\n",
    "---\n",
    "\n",
    "### 交付物（Deliverable）\n",
    "\n",
    "实现一个执行模糊文档去重的函数。该函数至少应接收以下参数：\n",
    "\n",
    "1. 输入文件路径列表\n",
    "2. 用于计算 MinHash 签名的哈希函数数量\n",
    "3. LSH 中使用的 band 数量\n",
    "4. 用于计算 MinHash 签名的 n-gram 长度（以“词”为单位）\n",
    "5. 输出目录路径\n",
    "\n",
    "你可以假设：用于计算 MinHash 签名的哈希函数数量可以被 band 数量整除。\n",
    "\n",
    "---\n",
    "\n",
    "### 输出要求\n",
    "\n",
    "你的函数应将每个输入文件写入输出目录，文件名保持不变，但**只写出以下两类文档**：\n",
    "\n",
    "* $a$ 不是候选重复文档的文档\n",
    "* $b$ 在聚类后的 bucket 中被**随机选中保留**的文档\n",
    "\n",
    "例如，如果输入路径是 `a/1.txt` 和 `a/2.txt`，输出目录是 `b/`，那么你的函数应输出 `b/1.txt` 和 `b/2.txt`。\n",
    "\n",
    "---\n",
    "\n",
    "请实现适配器函数 **`run_minhash_deduplication`**，并确保它能够通过以下测试：\n",
    "\n",
    "```bash\n",
    "pytest -k test_minhash_deduplication\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**注释：**\n",
    "6. 关于 LSH 和 MinHash 的更深入讨论，请参考 *Leskovec et al., 2014* 第 3 章，在线版本可在 `http://infolab.stanford.edu/~ullman/mmds/ch3.pdf` 获取。\n",
    "7. 这些哈希函数可以来自同一个哈希函数族，但使用不同的随机种子。例如，MurmurHash3 是一个哈希函数族，使用不同的种子可以实例化出不同的具体哈希函数。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c995f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import shutil  # 用于复制文件（保留元数据）\n",
    "from unicodedata import normalize\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    预备工作 1：文本标准化（Normalization）\n",
    "\n",
    "    目的：\n",
    "    - 消除与语义无关的差异（大小写、标点、重音、空白）\n",
    "    - 让“本质相同”的文本在后续 shingle 层面尽可能一致\n",
    "    - 这是近似去重中“降低噪声”的关键步骤\n",
    "\n",
    "    如果跳过此步骤：\n",
    "    - 同一句话可能因为大小写或标点不同而被认为完全不相似\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 全部转为小写\n",
    "    # WHY：避免 \"Apple\" 和 \"apple\" 被当作不同词\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2 删除标点符号\n",
    "    # 正则解释：\n",
    "    # \\w  → 字母、数字、下划线\n",
    "    # \\s  → 空白字符\n",
    "    # [^\\w\\s] → 所有“非字母、非数字、非空白”的字符，即标点\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 3 规范化空白字符\n",
    "    # \\s+ 表示一个或多个连续空白\n",
    "    # 统一替换为一个空格，并去掉首尾空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4 Unicode NFD 规范化\n",
    "    # 例如：é → e + ´（把重音符号拆出来）\n",
    "    text = normalize('NFD', text)\n",
    "\n",
    "    # 5 删除所有“非间距重音符号”\n",
    "    # unicodedata.category(ch) == \"Mn\"\n",
    "    # Mn = Mark, Nonspacing（重音、变音符）\n",
    "    text = \"\".join(\n",
    "        ch for ch in text\n",
    "        if unicodedata.category(ch) != \"Mn\"\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_shingles(text: str, n: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    预备工作 2：生成 n-gram（shingles）\n",
    "\n",
    "    理论背景：\n",
    "    - MinHash 的输入不是原始文本，而是“集合”\n",
    "    - 我们用 n-gram 字符子串来近似表示文本内容\n",
    "\n",
    "    举例：\n",
    "    text = \"hello\", n = 3\n",
    "    → {\"hel\", \"ell\", \"llo\"}\n",
    "\n",
    "    为什么用 set：\n",
    "    - Jaccard 相似度基于“集合”，不关心频次\n",
    "    \"\"\"\n",
    "\n",
    "    shingles = set()\n",
    "\n",
    "    # 滑动窗口提取长度为 n 的子串\n",
    "    # len(text) - n + 1 确保不会越界\n",
    "    for i in range(len(text) - n + 1):\n",
    "        shingles.add(text[i:i+n])\n",
    "\n",
    "    return shingles\n",
    "\n",
    "\n",
    "def estimate_jaccard(sig1: list, sig2: list) -> float:\n",
    "    \"\"\"\n",
    "    使用 MinHash 签名估算 Jaccard 相似度\n",
    "\n",
    "    理论公式：\n",
    "    J(A, B) ≈ 相等的 MinHash 行数 / 总 hash 数 K\n",
    "\n",
    "    重要说明：\n",
    "    - 这是“估算值”，不是精确 Jaccard\n",
    "    - 但在 K 足够大时，期望值等于真实 Jaccard\n",
    "    \"\"\"\n",
    "\n",
    "    if len(sig1) != len(sig2):\n",
    "        raise ValueError(\"Signatures must have the same length.\")\n",
    "\n",
    "    # zip(sig1, sig2)：\n",
    "    # - 同时遍历两个签名的第 i 行\n",
    "    # - 判断 hash 是否相等\n",
    "    matching_hashes = sum(\n",
    "        1 for h1, h2 in zip(sig1, sig2) if h1 == h2\n",
    "    )\n",
    "\n",
    "    return matching_hashes / len(sig1)\n",
    "\n",
    "def run_minhash_deduplication(\n",
    "    input_files: list[os.PathLike],\n",
    "    num_hashes: int,\n",
    "    num_bands: int,\n",
    "    ngrams: int,\n",
    "    jaccard_threshold: float,\n",
    "    output_directory: os.PathLike,\n",
    "):\n",
    "    \"\"\"\n",
    "    主函数：使用 MinHash + LSH 完成近似文本去重\n",
    "\n",
    "    输入：\n",
    "    - input_files         : 文档路径列表\n",
    "    - num_hashes (K)      : MinHash 签名长度\n",
    "    - num_bands (B)       : LSH 分段数量\n",
    "    - ngrams              : shingle 的 n\n",
    "    - jaccard_threshold   : 相似度阈值\n",
    "    - output_directory    : 输出目录\n",
    "    \"\"\"\n",
    "    # 第一步，先读取文件，并进行预备工作处理。\n",
    "    doc_shingles = defaultdict(set)\n",
    "    all_shingles = set()# 所有出现过的shingle,也就是所有文档的并集\n",
    "    for file_path in input_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            normalized_text = normalize_text(text)\n",
    "            shingles = get_shingles(normalized_text, ngrams)\n",
    "            doc_shingles[file_path] = shingles\n",
    "            all_shingles.update(shingles) # 收集所有出现过的shingle\n",
    "            # print(shingles)\n",
    "    \n",
    "    #第二步，生成MinHash签名\n",
    "    # 初始化签名矩阵 M，所有值为无穷大\n",
    "    # signatures[doc_id] = [inf, inf, ..., inf]，相当于按列来创建矩阵\n",
    "    signatures = {doc_id: [float('inf')] * num_hashes for doc_id in doc_shingles}\n",
    "    # 对每一个唯一的 shingle 计算它的 K 个哈希值\n",
    "    # 这是按“行r”来计算哈希值 val_1, val_2, ...\n",
    "    for shingle in all_shingles:\n",
    "        # 使用不同的 salt (盐)来模拟 k 个独立的哈希函数\n",
    "        hash_values = [hash(shingle + str(i)) for i in range(num_hashes)]\n",
    "        # 遍历所有文档，如果文档包含这个 shingle，则更新它的签名\n",
    "        for doc_id, shingles_set in doc_shingles.items():\n",
    "            if shingle in shingles_set:\n",
    "                # 规则：M(i, c) = min(M(i, c), val_i)\n",
    "                for i in range(num_hashes):\n",
    "                    signatures[doc_id][i] = min(signatures[doc_id][i], hash_values[i])\n",
    "\n",
    "    # --- 第三步: LSH分段与分桶 ---\n",
    "    r = num_hashes // num_bands # 每个band包含的hash签名数量 (rows per band)\n",
    "\n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    # 1. 对每一个band进行处理\n",
    "    for band_index in range(num_bands):\n",
    "        # buckets是一个哈希表，用于存放当前band的“哈希桶”\n",
    "        # key是band的哈希值，value是落入这个桶的文档列表\n",
    "        buckets = defaultdict(list)\n",
    "        \n",
    "        # 2. 遍历所有文档\n",
    "        for doc_id, sig in signatures.items():\n",
    "            # 提取当前band对应的签名部分\n",
    "            start_index = band_index * r\n",
    "            end_index = start_index + r\n",
    "            band = tuple(sig[start_index:end_index]) #转为tuple才能作为dict的key\n",
    "            # 3. 将 (band -> doc_id) 存入桶中\n",
    "            buckets[band].append(doc_id)\n",
    "            \n",
    "        # 4. 生成候选对\n",
    "        # 只要一个桶里的文档数 > 1，它们就是候选对\n",
    "        for bucket_docs in buckets.values():\n",
    "            if len(bucket_docs) > 1:\n",
    "                # 使用itertools.combinations来生成桶内所有可能的配对\n",
    "                for pair in combinations(bucket_docs, 2):\n",
    "                    candidate_pairs.add(tuple(sorted(pair))) #排序后加入set，避免(a,b)和(b,a)重复\n",
    "\n",
    "    # --- 第四步: 验证候选对并输出结果 ---    \n",
    "    duplicate_pairs = []\n",
    "    for doc1, doc2 in candidate_pairs:\n",
    "        # 从签名矩阵中直接估算Jaccard相似度\n",
    "        j_estimate = estimate_jaccard(signatures[doc1], signatures[doc2])\n",
    "        \n",
    "        if j_estimate >= jaccard_threshold:\n",
    "            duplicate_pairs.append((doc1, doc2))\n",
    "\n",
    "    \n",
    "    # --- 第五步: 根据新要求，构建重复集群并选择要保留的文件 ---\n",
    "    adj = defaultdict(list)\n",
    "    for doc1, doc2 in duplicate_pairs:\n",
    "        adj[doc1].append(doc2)\n",
    "        adj[doc2].append(doc1)\n",
    "        \n",
    "    # 2. 寻找连通分量 (即重复的集群)\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "    for doc in input_files:\n",
    "        if doc not in visited:\n",
    "            cluster = []\n",
    "            q = [doc]\n",
    "            visited.add(doc)\n",
    "            head = 0\n",
    "            while head < len(q):\n",
    "                current = q[head]\n",
    "                head += 1\n",
    "                cluster.append(current)\n",
    "                # 仅当节点在adj中时才遍历邻居\n",
    "                if current in adj:\n",
    "                    for neighbor in adj[current]:\n",
    "                        if neighbor not in visited:\n",
    "                            visited.add(neighbor)\n",
    "                            q.append(neighbor)\n",
    "            clusters.append(cluster)\n",
    "\n",
    "    # 3. 决定要保留的文件\n",
    "    files_to_keep = set()\n",
    "    for cluster in clusters:\n",
    "        # 如果一个集群只有一个文件，说明它是唯一的\n",
    "        if len(cluster) == 1:\n",
    "            files_to_keep.add(cluster[0])\n",
    "        else:\n",
    "            # 如果是重复集群，按字母顺序排序并选择第一个作为代表\n",
    "            cluster.sort()\n",
    "            representative = cluster[0]\n",
    "            files_to_keep.add(representative)\n",
    "\n",
    "\n",
    "    # --- 第六步: 将选定的文件写入输出目录 ---\n",
    "    os.makedirs(output_directory, exist_ok=True) # 创建输出目录\n",
    "    \n",
    "    copied_count = 0\n",
    "    for file_path in files_to_keep:\n",
    "        # 构建目标路径，保持文件名不变\n",
    "        destination_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        shutil.copy2(file_path, destination_path) # copy2会同时复制元数据\n",
    "        copied_count += 1    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61d77d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
