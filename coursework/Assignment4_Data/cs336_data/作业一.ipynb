{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c4ec4c",
   "metadata": {},
   "source": [
    "## 创建环境，启动 ：\n",
    "\n",
    "在终端中运行：\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    ". venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224fbb3",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "**问题（look_at_cc）：4 分**\n",
    "\n",
    "(a) 下载上面的 WARC 文件，或者找到我们在集群上提供的副本。让我们看看这个文件中的第一页。这是一个 gzipped 文件，你可以使用以下命令浏览其内容：\n",
    "\n",
    "```\n",
    "$ zcat /data/CC/example.warc.gz | less\n",
    "```\n",
    "\n",
    "`less` 让你可以使用键盘箭头、Page Up、Page Down 来浏览文件。要退出，按“q”。\n",
    "查看非常第一个网页。它的 URL 是什么？它仍然可以访问吗？你能通过查看原始 HTML 来告诉我们这个页面似乎是什么吗？\n",
    "交付物：2-3 句回应。\n",
    "\n",
    "(b) 现在让我们看看对应的 WET 文件：\n",
    "\n",
    "```\n",
    "$ zcat /data/CC/example.warc.wet.gz | less\n",
    "```\n",
    "\n",
    "注意 WET 文件包含 HTTP 头（例如，Content-Length），这些不是提取的文本内容的一部分。如果你查看第一个例子，你会看到它包含从你刚刚看到的原始 HTML 中提取的文本。\n",
    "\n",
    "注意提取的文本中有多少是 HTML 结构的遗迹，而不是页面的主要内容。你认为哪些部分应该被提取器过滤掉？将此文本作为训练数据时，可能会出错的地方是什么：在训练看起来像这样的文本的模型时，可能会出错的地方是什么？相反，模型可能从这个页面中提取哪些有用的信息？\n",
    "交付物：3-4 句回应。\n",
    "\n",
    "(c) 什么使一个好的训练示例具有高度的上下文性。描述一个应用领域，在这个领域中，这个示例可能对训练数据有用，以及一个它可能没有用的地方。\n",
    "交付物：1-2 句回应。\n",
    "\n",
    "(d) 让我们看更多的例子，以便更好地了解 Common Crawl 中的内容。浏览另外 25 个 WET 记录。对于每条记录，非常简要地评论文档的语言（如果可以识别），域名，页面类型等。你需要看到多少个例子才能确定你认为的“高质量”网页？\n",
    "交付物：对 25 个文档进行简短注释，包括文档的语言、域名、页面类型和其他任何关于文档的杂项注释。直到你看到一个高质量的示例。示例的数量取决于你看到高质量示例的时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b3b4f",
   "metadata": {},
   "source": [
    "运行以下命令来下载数据集\n",
    "```bash\n",
    "wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
    "\n",
    "wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a234cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:23:52--  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
      "Resolving data.commoncrawl.org (data.commoncrawl.org)... 18.238.217.12, 18.238.217.90, 18.238.217.75, ...\n",
      "Connecting to data.commoncrawl.org (data.commoncrawl.org)|18.238.217.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1121135968 (1.0G) [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’\n",
      "\n",
      "CC-MAIN-20250417135 100%[===================>]   1.04G  6.10MB/s    in 2m 58s  \n",
      "\n",
      "2026-01-26 13:27:04 (6.00 MB/s) - ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’ saved [1121135968/1121135968]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-proxy https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175e618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:27:04--  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\n",
      "Resolving data.commoncrawl.org (data.commoncrawl.org)... 3.163.189.35, 3.163.189.100, 3.163.189.118, ...\n",
      "Connecting to data.commoncrawl.org (data.commoncrawl.org)|3.163.189.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80904812 (77M) [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz’\n",
      "\n",
      "CC-MAIN-20250417135 100%[===================>]  77.16M  6.09MB/s    in 14s     \n",
      "\n",
      "2026-01-26 13:27:21 (5.47 MB/s) - ‘CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz’ saved [80904812/80904812]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget --no-proxy  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da7b10",
   "metadata": {},
   "source": [
    "我们可以使用\n",
    "```bash\n",
    "zcat CC-MAIN-20250417135010-20250417165010-00065.warc.gz | less\n",
    "\n",
    "zcat CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz | less\n",
    "```\n",
    "在命令行中进行预览，less表示可以使用键盘方向键、Page Up 和 Page Down 来浏览文件。要退出，请按“q“。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03d7bc",
   "metadata": {},
   "source": [
    "`.warc.gz` 和 `.warc.wet.gz` 都是 **WARC（Web ARChive）格式** 的压缩文件，广泛用于大规模网页抓取数据（如 Common Crawl）。\n",
    "\n",
    "## 一、`.warc.gz` 与 `.warc.wet.gz` 的区别\n",
    "\n",
    "### 1. `.warc.gz`\n",
    "\n",
    "**原始网页归档文件**，包含完整抓取结果：HTTP 请求 / 响应头、HTML 原文、二进制资源（图片、PDF 等）、元数据（抓取时间、URL、状态码等）。\n",
    "\n",
    "### 2. `.warc.wet.gz`\n",
    "\n",
    "**WET = Web Extracted Text**\n",
    "\n",
    "从 `.warc.gz` 中**抽取后的纯文本**，仅保留网页的可读文本内容，已去除 HTML 标签、脚本、样式等\n",
    "\n",
    "适合NLP 训练（LM、Embedding、Topic），文本挖掘，语料构建。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a85e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://13.usnccm.org/\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\n",
      "  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" version=\"XHTML+RDFa 1.0\" dir=\"ltr\"\n",
      "  xmlns:content=\"http://purl.org/rss/1.0/modules/content/\"\n",
      "  xmlns:dc=\"http://purl.org/dc/terms/\"\n",
      "  xmlns:foaf=\"http://xmlns.com/foaf/0.1/\"\n",
      "  xmlns:og=\"http://ogp.me/ns#\"\n",
      "  xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\n",
      "  xmlns:sioc=\"http://rdfs.org/sioc/ns#\"\n",
      "  xmlns:sioct=\"http://rdfs.org/sioc/ty\n"
     ]
    }
   ],
   "source": [
    "# 从 warcio 库中导入 ArchiveIterator\n",
    "# ArchiveIterator 用于按“记录（record）”顺序流式读取 WARC 文件\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# 导入 gzip，用于读取 .gz 压缩格式的 WARC 文件\n",
    "import gzip\n",
    "\n",
    "# 导入 html 模块，用于将 HTML 实体（如 &#x4EBA;）解码为正常字符\n",
    "import html \n",
    "\n",
    "# WARC 文件路径（Common Crawl 的一个分片）\n",
    "# 注意：变量名 flie_path 拼写不影响运行，但语义上建议改为 file_path\n",
    "flie_path  = 'CC-MAIN-20250417135010-20250417165010-00065.warc.gz'\n",
    "\n",
    "# 以二进制只读模式打开 gzip 压缩的 WARC 文件\n",
    "# 必须使用 \"rb\"，因为 WARC 是二进制格式\n",
    "with gzip.open(flie_path, \"rb\") as f:\n",
    "\n",
    "    # 使用 ArchiveIterator 对 WARC 文件进行流式遍历\n",
    "    # 每次循环返回一个 WARC record（请求、响应、元数据等）\n",
    "    for step,record in enumerate(ArchiveIterator(f)):\n",
    "       \n",
    "        # 只处理类型为 \"response\" 的记录\n",
    "        # response 表示真实的 HTTP 响应内容（网页、图片等）\n",
    "        if record.rec_type == \"response\" and step > 5:\n",
    "\n",
    "            # 从 WARC 头中获取该网页对应的原始 URL\n",
    "            url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "\n",
    "            # 读取 HTTP 响应体内容（网页原始字节流）\n",
    "            # decode 为 utf-8，忽略非法字符，防止程序崩溃\n",
    "            html1 = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # 简单判断该响应是否为 HTML 页面\n",
    "            # 这是一个经验性过滤，避免处理图片、PDF 等非 HTML 内容\n",
    "            if \"<html\" in html1.lower():\n",
    "\n",
    "                # 输出网页 URL\n",
    "                print(url)\n",
    "\n",
    "                # 对 HTML 内容前 500 个字符进行：\n",
    "                # 1. 截断（避免输出过多内容）\n",
    "                # 2. HTML 实体解码（&#x4EBA; → 人）\n",
    "                print(html.unescape(html1[:500]))\n",
    "\n",
    "                # 找到第一个 HTML 页面后立即退出循环\n",
    "            \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecffc438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0371rykj.com/ipfhsb/34.html\n",
      "> ip防護(hù)系列 >\n",
      "產(chǎn)品詳情/ products details\n",
      "恒溫恒濕試驗(yàn)箱\n",
      "產(chǎn)品用途\n",
      "恒溫恒濕試驗(yàn)箱是航空、汽車(chē)、家電、科研等領(\n"
     ]
    }
   ],
   "source": [
    "# 从 warcio 库中导入 ArchiveIterator\n",
    "# ArchiveIterator 用于按顺序流式遍历 WARC / WET 文件中的每一条记录\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# 导入 gzip 模块，用于读取 .gz 压缩格式的文件\n",
    "import gzip\n",
    "\n",
    "# 指定 Common Crawl 的 WET 文件路径\n",
    "# .warc.wet.gz 文件中存放的是已抽取好的“纯文本内容”\n",
    "flie_path  = 'CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz'\n",
    "\n",
    "# 以二进制只读模式打开 gzip 压缩的 WET 文件\n",
    "# 必须使用 \"rb\"，因为 WARC/WET 属于二进制流格式\n",
    "with gzip.open(flie_path, \"rb\") as f:\n",
    "\n",
    "    # 使用 ArchiveIterator 对 WET 文件进行流式迭代\n",
    "    # 每次循环返回一个 WARC record（一条网页对应一条 record）\n",
    "    for record in ArchiveIterator(f):\n",
    "\n",
    "        # 只处理类型为 \"conversion\" 的记录\n",
    "        # conversion 表示从 HTML 中抽取出的“可读纯文本”\n",
    "        if record.rec_type == \"conversion\":\n",
    "\n",
    "            # 从 WARC 头信息中获取该文本对应的原始网页 URL\n",
    "            url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "\n",
    "            # 读取当前 record 的正文内容（纯文本字节流）\n",
    "            # 解码为 UTF-8 字符串，忽略非法字符，防止解码异常\n",
    "            text = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # 判断文本在去除首尾空白字符后是否仍然有内容\n",
    "            # 用于过滤空页面或无有效文本的网页\n",
    "            if len(text.strip()) > 0:\n",
    "\n",
    "                # 打印网页对应的 URL\n",
    "                print(url)\n",
    "\n",
    "                # 打印正文内容的前 400 - 500 个字符\n",
    "                # 用于快速查看文本样本，避免一次性输出过多内容\n",
    "                print(text[400:500])\n",
    "\n",
    "                # 找到第一条有效文本记录后立即退出循环\n",
    "                # 适用于“快速验证 WET 文件内容是否正常”的场景\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ab1e0",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "**问题（提取文本）：3分**\n",
    "\n",
    "（a）编写一个函数，用于从包含原始 HTML 的字节字符串中提取文本。使用 `resiliparse.extract.html2text.extract_plain_text` 执行提取操作。该函数需要一个字符串作为输入，因此您先需要将字节字符串解码为 Unicode 字符串。请注意，输入的字节字符串可能并未采用 UTF-8 编码，因此您的函数应具备检测编码的能力，以防 UTF-8 解码失败。此外，Resiliparse 还提供 `resiliparse.parse.encoding.detect_encoding()`，这可能会有用。\n",
    "\n",
    "**交付物**：一个函数，接收包含 HTML 的字节字符串，并返回提取出的文本字符串。实现适配器 `[run_extract_text_from_html_bytes]`，并确保其通过 uv 运行\n",
    "`pytest -k test_extract_text_from_html_bytes`\n",
    "\n",
    "（b）对单个 WARC 文件运行你的文本提取功能。将其输出与提取的内容进行比较，对应 WET 文件中的文本。你注意到哪些差异和/或相似之处？哪种提取方式似乎更好？\n",
    "\n",
    "**可交付成果**：用您自定义函数提取的文本与 WET 文件中的提取文本进行比较和对比，撰写 2–3 句话的回应。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6879d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "\n",
    "\n",
    "def run_extract_text_from_html_bytes(html_bytes: bytes) -> str:\n",
    "    \"\"\"\n",
    "    从包含 HTML 的字节字符串中提取纯文本。\n",
    "\n",
    "    参数:\n",
    "        html_bytes (bytes): 原始 HTML 字节串\n",
    "\n",
    "    返回:\n",
    "        str: 提取出的纯文本\n",
    "    \"\"\"\n",
    "    # 1. 尝试直接用 UTF-8 解码（最快路径）\n",
    "    try:\n",
    "        html_text = html_bytes.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        # 2. UTF-8 失败时，使用 resiliparse 的编码检测\n",
    "        encoding = detect_encoding(html_bytes)\n",
    "        # 使用检测到的编码进行解码，忽略无法解码的字符\n",
    "        html_text = html_bytes.decode(encoding, errors=\"ignore\")\n",
    "\n",
    "    # 3. 使用 Resiliparse 从 HTML 字符串中提取纯文本\n",
    "    text = extract_plain_text(html_text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cf0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0371rykj.com/ipfhsb/34.html\n",
      "久久久久女人精品毛片,99久久精品无码一区二区毛片,被老外的又粗又大日出了水,一边吃奶一边哭乱抻又乱扭\n",
      "\n",
      "        • <th id=\"gckmo\"></th>\n",
      "        • <ul id=\"gckmo\"><center id=\"gckmo\"></center></ul>\n",
      "      •  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "         \n",
      "   \n",
      "         \n",
      "    \n",
      "         \n",
      "    \n",
      "        恒溫恒濕試驗(yàn)箱\n",
      "        \n",
      "\t在線(xiàn)咨詢(xún)\n",
      "    \n",
      "         \n",
      "    \n",
      "         \n",
      "      淋雨試驗(yàn)箱 \n",
      "     \n",
      "        上海林頻儀器股份有限公司Shanghai Linpin Instrument Stock Co Ltd\n",
      "         \n",
      "     \n",
      "        服務(wù)熱線(xiàn)：4000 662 888 \n",
      "         手機(jī)咨詢(xún)：13818467052\n",
      "        \n",
      "    \n",
      "         \n",
      "    \n",
      "         \n",
      "     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "file_path = \"CC-MAIN-20250417135010-20250417165010-00065.warc.gz\"\n",
    "with gzip.open(file_path, \"rb\") as f:\n",
    "    for record in ArchiveIterator(f):\n",
    "        if record.rec_type == \"response\":\n",
    "            # 只处理 HTML 页面\n",
    "            content_type = record.http_headers.get_header(\"Content-Type\", \"\")\n",
    "            if \"text/html\" not in content_type:\n",
    "                continue\n",
    "\n",
    "            # 1. 读取网页的原始 HTML（bytes）\n",
    "            html_bytes = record.content_stream().read()\n",
    "\n",
    "            # 2. 使用函数提取文本\n",
    "            text = run_extract_text_from_html_bytes(html_bytes)\n",
    "\n",
    "            # 3. 查看结果\n",
    "            if text.strip():\n",
    "                print(record.rec_headers.get_header(\"WARC-Target-URI\"))\n",
    "                print(text[:500])\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817d14f",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "问题（语言识别）：6 分\n",
    "\n",
    "(a) 编写一个函数，该函数将接收一个 Unicode 字符串，并识别其中出现的主要语言。你的函数应返回一个元组，包含该语言的标识符以及一个介于 0 和 1 之间的分数，表示对该预测的置信度。\n",
    "\n",
    "**交付物：** 一个执行语言识别的函数，返回其最可能的语言预测及相应得分。实现适配器 `[run_identify_language]`，并确保它能通过 uv 中运行 `pytest -k test_identify_language` 的两项测试。请注意，这些测试假定英文使用特定的字符串标识符（“en”），中文使用 “zh”，因此你的测试适配器应根据需要执行任何必要的重新映射操作。\n",
    "\n",
    "---\n",
    "\n",
    "(b) 语言模型在推理阶段的行为在很大程度上取决于其训练所用的数据。因此，数据过滤管道中的问题可能会导致下游出现一系列难题。你认为语言识别环节出现问题可能引发哪些具体问题？在风险更高的场景中（例如部署面向用户的产品时），你将如何有效应对并缓解这些问题？\n",
    "\n",
    "**可交付成果：** 2 至 5 句话的回复。\n",
    "\n",
    "---\n",
    "\n",
    "(c) 将您的语言识别系统应用于从 WARC 文件中提取的文本（通过您先前实现的文本提取功能）。在 20 个随机示例中手动识别语言，并将您的标注与分类器的预测结果进行对比。报告任何分类错误。有多少比例的文档是英语？根据您的观察，使用何种合适的分类器置信度阈值进行过滤会比较有效？\n",
    "\n",
    "**交付物：** 2 至 5 句话的回复。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b894213",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "网络上包含着用数千种语言编写的页面。然而，在大多数计算预算下，训练一种能够有效利用如此多样化数据并实现大规模应用的多语言模型，仍是一项极具挑战性的任务。因此，许多源自Common Crawl的数据集所涵盖的语言种类相对有限。\n",
    "一个适用于此目的的实用库是 fastText https://fasttext.cc，它提供了高效的文本分类器。该库既支持基于您自己的数据训练分类器的基础设施，也内置了一系列预训练模型，其中包括用于语言识别的模型。它提供了高效的文本分类器。该库既支持基于您自己的数据训练分类器的基础设施，也内置了一系列预训练模型，其中包括用于语言识别的模型。您可以从https://fasttext.cc/docs/en/language-identification.html 下载 fastText 语言识别模型 \n",
    "\n",
    "通常，语言过滤器会利用分类器给出的得分来决定是否保留某页面。请使用fastText语言识别分类器实现一个语言识别过滤器，该过滤器应能输出一个非负分数，以表示其预测的置信度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d02e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:27:21--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.185.19, 13.35.185.64, 13.35.185.53, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.185.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘lid.176.bin’\n",
      "\n",
      "lid.176.bin         100%[===================>] 125.18M  9.93MB/s    in 14s     \n",
      "\n",
      "2026-01-26 13:27:35 (8.88 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载模型 速度更快，准确度也略高，但文件大小为 126MB\n",
    "\n",
    "!wget --no-proxy https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b4b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:27:36--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.51, 3.163.189.108, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 938013 (916K) [binary/octet-stream]\n",
      "Saving to: ‘lid.176.ftz’\n",
      "\n",
      "lid.176.ftz         100%[===================>] 916.03K   777KB/s    in 1.2s    \n",
      "\n",
      "2026-01-26 13:27:38 (777 KB/s) - ‘lid.176.ftz’ saved [938013/938013]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载模型 是该模型的压缩版本，文件大小为 917kB。\n",
    "\n",
    "!wget --no-proxy https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9c0346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126MB版本：\n",
      "文本: 'This is a simple English sentence.'\n",
      "预测语言: en, 置信度: 0.9385\n",
      "--------------------------------------------------\n",
      "126MB版本：\n",
      "文本: '这是一个用于测试的中文句子。'\n",
      "预测语言: zh, 置信度: 1.0001\n",
      "--------------------------------------------------\n",
      "126MB版本：\n",
      "文本: 'Bonjour, comment allez-vous ?'\n",
      "预测语言: fr, 置信度: 0.9883\n",
      "--------------------------------------------------\n",
      "126MB版本：\n",
      "文本: 'こんにちは、元気ですか？'\n",
      "预测语言: ja, 置信度: 1.0000\n",
      "--------------------------------------------------\n",
      "126MB版本：\n",
      "文本: ''\n",
      "预测语言: unknown, 置信度: 0.0000\n",
      "--------------------------------------------------\n",
      "126MB版本：\n",
      "文本: '    '\n",
      "预测语言: unknown, 置信度: 0.0000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fasttext\n",
    "from functools import lru_cache\n",
    "\n",
    "# fastText 语言识别模型路径\n",
    "\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _load_model():\n",
    "    \"\"\"\n",
    "    加载 fastText 语言识别模型（单例模式）\n",
    "\n",
    "    使用 lru_cache 的原因：\n",
    "    - fastText 模型体积较大（~126MB）\n",
    "    - 避免在每次函数调用时重复加载模型\n",
    "    - 提高整体运行效率，尤其在批量文本处理时\n",
    "    \"\"\"\n",
    "    return fasttext.load_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "def run_identify_language(text: str):\n",
    "    \"\"\"\n",
    "    识别输入 Unicode 字符串中的主要语言\n",
    "\n",
    "    参数：\n",
    "        text (str): 任意 Unicode 文本字符串\n",
    "\n",
    "    返回：\n",
    "        (language_id, confidence_score)\n",
    "        - language_id: 语言标识符（如 \"en\", \"zh\"）\n",
    "        - confidence_score: 介于 [0, 1] 的置信度分数\n",
    "\n",
    "    注意：\n",
    "    - 该函数是测试要求的“适配器函数”\n",
    "    - 必须保证英文返回 \"en\"，中文返回 \"zh\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 对空字符串或只包含空白符的情况进行兜底处理\n",
    "    # fastText 在这种情况下预测结果不可靠\n",
    "    if not text or not text.strip():\n",
    "        return (\"unknown\", 0.0)\n",
    "\n",
    "    # 加载（或从缓存中获取）语言识别模型\n",
    "    model = _load_model()\n",
    "\n",
    "    # fastText 的 predict 接口：\n",
    "    # - 返回 labels 和 scores 两个列表\n",
    "    # - labels 形如 \"__label__en\"\n",
    "    # - scores 为预测置信度\n",
    "    labels, scores = model.predict(\n",
    "        text.replace(\"\\n\", \" \"),  # 去除换行，避免影响模型判断\n",
    "        k=1                        # 只取概率最高的一个语言\n",
    "    )\n",
    "\n",
    "    # 取出预测结果\n",
    "    raw_label = labels[0]                 # \"__label__en\"\n",
    "    confidence = float(scores[0])         # 置信度（0~1）\n",
    "\n",
    "    # 去掉 fastText 特有的 \"__label__\" 前缀\n",
    "    language = raw_label.replace(\"__label__\", \"\")\n",
    "\n",
    "    # 根据作业测试要求，对语言标签进行统一映射\n",
    "    # fastText 可能返回 zh, zh-cn, zh-tw 等\n",
    "    if language.startswith(\"zh\"):\n",
    "        language = \"zh\"\n",
    "    elif language == \"en\":\n",
    "        language = \"en\"\n",
    "    # 其他语言保持原样（如 \"fr\", \"de\", \"ja\" 等）\n",
    "\n",
    "    return (language, confidence)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    examples = [\n",
    "        \"This is a simple English sentence.\",\n",
    "        \"这是一个用于测试的中文句子。\",\n",
    "        \"Bonjour, comment allez-vous ?\",\n",
    "        \"こんにちは、元気ですか？\",\n",
    "        \"\",\n",
    "        \"    \"\n",
    "    ]\n",
    "\n",
    "    for text in examples:\n",
    "        MODEL_PATH = \"lid.176.bin\"\n",
    "        lang, score = run_identify_language(text)\n",
    "        print('126MB版本：')\n",
    "        print(f\"文本: {repr(text)}\")\n",
    "        print(f\"预测语言: {lang}, 置信度: {score:.4f}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af69db0",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "## 问题（屏蔽 PII）：3 分\n",
    "\n",
    "### 1. 屏蔽电子邮件地址\n",
    "\n",
    "编写一个函数来屏蔽电子邮件。该函数将接收一个字符串作为输入，并用字符串\n",
    "**`\"|||EMAIL_ADDRESS|||\"`**\n",
    "替换所有出现的电子邮件地址。要检测电子邮件地址，你可以查阅能够可靠完成此任务的正则表达式。\n",
    "\n",
    "**可交付成果：**\n",
    "一个函数，用于将给定字符串中的所有电子邮件地址替换为字符串 `\"|||EMAIL_ADDRESS|||\"`，并返回一个元组，其中包含新字符串以及被屏蔽的实例数量。实现适配器 **`[run_mask_emails]`**，并确保其通过\n",
    "`uv run pytest -k test_mask_emails` 中的所有测试。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 屏蔽电话号码\n",
    "\n",
    "编写一个函数来屏蔽电话号码。该函数将接收一个字符串作为输入，并用字符串\n",
    "**`\"|||PHONE_NUMBER|||\"`**\n",
    "替换所有出现的电话号码。要可靠地实现这一点可能极具挑战性，因为电话号码的书写形式可能极其多样。\n",
    "\n",
    "你应尽量捕获**美国最常用的电话号码格式**，并能有效应对轻微的语法偏差。\n",
    "\n",
    "**可交付成果：**\n",
    "一个函数，用于将给定字符串中的电话号码替换为字符串 `\"|||PHONE_NUMBER|||\"`，并返回包含新字符串及被屏蔽的电话号码数量的元组。实现适配器 **`[run_mask_phone_numbers]`**，并确保其通过\n",
    "`uv run pytest -k test_mask_phones`。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 屏蔽 IP 地址\n",
    "\n",
    "编写一个函数来屏蔽 IP 地址。对于这个问题，只需关注 **IPv4 地址**（由四个介于 0 到 255 之间的数字组成，各数字之间用点分隔）。\n",
    "\n",
    "你的函数将接收一个字符串作为输入，并将所有出现的 IP 地址替换为字符串\n",
    "**`\"|||IP_ADDRESS|||\"`**。\n",
    "\n",
    "**可交付成果：**\n",
    "一个函数，用于将给定字符串中的 IPv4 地址替换为字符串 `\"|||IP_ADDRESS|||\"`，并返回一个元组，其中包含新字符串以及被掩码替换的实例数量。实现适配器 **`[run_mask_ips]`**，并确保其通过\n",
    "`uv run pytest -k test_mask_ips`。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 分析题（2–5 句）\n",
    "\n",
    "你认为在训练集中如果**天真地**应用这些过滤器，可能会在语言模型的下游环节引发哪些问题？你又该如何缓解这些问题？\n",
    "\n",
    "**交付物：** 2 至 5 句话的回复。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 实验分析（2–5 句）\n",
    "\n",
    "在从 WARC 文件中提取的文本上运行你的 PII 脱敏函数（通过你先前实现的文本提取功能）。随机选取 20 个已进行替换的示例进行检查，并列举一些**假阳性（false positives）**和**假阴性（false negatives）**的具体示例。\n",
    "\n",
    "**交付物：** 2 至 5 句话的回复。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c795a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "def run_mask_emails(text: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    屏蔽字符串中的电子邮件地址。\n",
    "\n",
    "    参数:\n",
    "        text (str): 输入文本\n",
    "\n",
    "    返回:\n",
    "        (masked_text, count)\n",
    "    \"\"\"\n",
    "\n",
    "    # 常见电子邮件地址的正则表达式\n",
    "    email_pattern = re.compile(\n",
    "        r'\\b[a-zA-Z0-9._%+-]+'\n",
    "        r'@'\n",
    "        r'[a-zA-Z0-9.-]+'\n",
    "        r'\\.[a-zA-Z]{2,}\\b'\n",
    "    )\n",
    "\n",
    "    # findall 用于统计匹配数量\n",
    "    matches = email_pattern.findall(text)\n",
    "\n",
    "    # sub 用于统一替换\n",
    "    masked_text = email_pattern.sub(\"|||EMAIL_ADDRESS|||\", text)\n",
    "\n",
    "    return masked_text, len(matches)\n",
    "\n",
    "\n",
    "def run_mask_phone_numbers(text: str) -> Tuple[str, int]:\n",
    "\n",
    "\n",
    "    phone_pattern = re.compile(\n",
    "        r'''\n",
    "        \\b\n",
    "        (?:\\+?1[\\s.-]?)?          # 可选国家码\n",
    "        (?:\\(?\\d{3}\\)?[\\s.-]?)    # 区号\n",
    "        \\d{3}[\\s.-]?\\d{4}         # 主号码\n",
    "        \\b\n",
    "        ''',\n",
    "        re.VERBOSE\n",
    "    )\n",
    "\n",
    "    matches = phone_pattern.findall(text)\n",
    "    masked_text = phone_pattern.sub(\"|||PHONE_NUMBER|||\", text)\n",
    "\n",
    "    return masked_text, len(matches)\n",
    "\n",
    "def run_mask_ips(text: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    屏蔽 IPv4 地址\n",
    "    \"\"\"\n",
    "\n",
    "    ip_pattern = re.compile(\n",
    "        r'\\b'\n",
    "        r'(?:25[0-5]|2[0-4]\\d|1?\\d{1,2})'\n",
    "        r'(?:\\.'\n",
    "        r'(?:25[0-5]|2[0-4]\\d|1?\\d{1,2})){3}'\n",
    "        r'\\b'\n",
    "    )\n",
    "\n",
    "    matches = ip_pattern.findall(text)\n",
    "    masked_text = ip_pattern.sub(\"|||IP_ADDRESS|||\", text)\n",
    "\n",
    "    return masked_text, len(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1688a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: 请联系 admin@example.com 获取信息。\n",
      "屏蔽后: 请联系 |||EMAIL_ADDRESS||| 获取信息。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 发送邮件到 first.user@test.com 或 second_user@domain.co.uk。\n",
      "屏蔽后: 发送邮件到 |||EMAIL_ADDRESS||| 或 |||EMAIL_ADDRESS|||。\n",
      "匹配数量: 2\n",
      "\n",
      "原文: 联系 support+sales@my-domain.org 了解详情。\n",
      "屏蔽后: 联系 |||EMAIL_ADDRESS||| 了解详情。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 我的邮箱是 abc.def@example.com, 请发送文件。\n",
      "屏蔽后: 我的邮箱是 |||EMAIL_ADDRESS|||, 请发送文件。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 这段文字中没有邮箱地址。\n",
      "屏蔽后: 这段文字中没有邮箱地址。\n",
      "匹配数量: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_tests = [\n",
    "    # 普通邮箱\n",
    "    \"请联系 admin@example.com 获取信息。\",\n",
    "    # 多个邮箱\n",
    "    \"发送邮件到 first.user@test.com 或 second_user@domain.co.uk。\",\n",
    "    # 邮箱带特殊符号\n",
    "    \"联系 support+sales@my-domain.org 了解详情。\",\n",
    "    # 邮箱在句子中\n",
    "    \"我的邮箱是 abc.def@example.com, 请发送文件。\",\n",
    "    # 没有邮箱\n",
    "    \"这段文字中没有邮箱地址。\"\n",
    "]\n",
    "\n",
    "for text in email_tests:\n",
    "    masked_text, count = run_mask_emails(text)\n",
    "    print(f\"原文: {text}\\n屏蔽后: {masked_text}\\n匹配数量: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1618c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: 请拨打 123-456-7890 联系客服。\n",
      "屏蔽后: 请拨打 |||PHONE_NUMBER||| 联系客服。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 办公室电话是 (987) 654-3210。\n",
      "屏蔽后: 办公室电话是 (|||PHONE_NUMBER|||。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 紧急电话 +1 800 555 1212。\n",
      "屏蔽后: 紧急电话 +|||PHONE_NUMBER|||。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 备用号码 555.123.4567。\n",
      "屏蔽后: 备用号码 |||PHONE_NUMBER|||。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 这里没有电话号码。\n",
      "屏蔽后: 这里没有电话号码。\n",
      "匹配数量: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phone_tests = [\n",
    "    # 普通美国手机号\n",
    "    \"请拨打 123-456-7890 联系客服。\",\n",
    "    # 带区号括号\n",
    "    \"办公室电话是 (987) 654-3210。\",\n",
    "    # 带国家码\n",
    "    \"紧急电话 +1 800 555 1212。\",\n",
    "    # 点号分隔\n",
    "    \"备用号码 555.123.4567。\",\n",
    "    # 没有手机号\n",
    "    \"这里没有电话号码。\"\n",
    "]\n",
    "\n",
    "for text in phone_tests:\n",
    "    masked_text, count = run_mask_phone_numbers(text)\n",
    "    print(f\"原文: {text}\\n屏蔽后: {masked_text}\\n匹配数量: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8880e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: 服务器 IP 地址是 192.168.0.1。\n",
      "屏蔽后: 服务器 IP 地址是 |||IP_ADDRESS|||。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 网关 IP 为 255.255.255.255，子网 IP 10.0.0.0。\n",
      "屏蔽后: 网关 IP 为 |||IP_ADDRESS|||，子网 IP |||IP_ADDRESS|||。\n",
      "匹配数量: 2\n",
      "\n",
      "原文: 访问 172.16.0.1 或 8.8.8.8 获取信息。\n",
      "屏蔽后: 访问 |||IP_ADDRESS||| 或 |||IP_ADDRESS||| 获取信息。\n",
      "匹配数量: 2\n",
      "\n",
      "原文: 错误码 404，用户 IP 127.0.0.1。\n",
      "屏蔽后: 错误码 404，用户 IP |||IP_ADDRESS|||。\n",
      "匹配数量: 1\n",
      "\n",
      "原文: 这段文字没有 IP 地址。\n",
      "屏蔽后: 这段文字没有 IP 地址。\n",
      "匹配数量: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ip_tests = [\n",
    "    # 普通 IP\n",
    "    \"服务器 IP 地址是 192.168.0.1。\",\n",
    "    # 边界 IP\n",
    "    \"网关 IP 为 255.255.255.255，子网 IP 10.0.0.0。\",\n",
    "    # 多个 IP\n",
    "    \"访问 172.16.0.1 或 8.8.8.8 获取信息。\",\n",
    "    # 文本中夹杂数字\n",
    "    \"错误码 404，用户 IP 127.0.0.1。\",\n",
    "    # 没有 IP\n",
    "    \"这段文字没有 IP 地址。\"\n",
    "]\n",
    "\n",
    "for text in ip_tests:\n",
    "    masked_text, count = run_mask_ips(text)\n",
    "    print(f\"原文: {text}\\n屏蔽后: {masked_text}\\n匹配数量: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc67dcc",
   "metadata": {},
   "source": [
    "# 作业五\n",
    "\n",
    "从网络上直接获取的未过滤转储文件包含大量文本，我们不希望语言模型在推理过程中重复这些内容。令人意外的是，这些训练样本本甚至可能来自看似无害的网站，比如维基百科——例如，用户在多个页面留下的评论竟颇具毒性。尽管要明确界定何为有害内容几乎不可能，但许多数据过滤流程仍会对那些主要包含有害内容的页面进行一定程度的筛选。\n",
    "\n",
    "识别此类内容的方法有很多，包括统计黑名单中的词汇数量，或基于人工标注员给出的标签构建简单的分类器。在本次作业的这一部分，我们将重点聚焦于识别两类广泛存在的有害内容：“不适合工作场合”（NSFW）——涵盖色情、脏话或其他可能令人不适的内容——以及有毒言论（“指那些粗鲁、不尊重或无理取闹，极有可能导致他人退出讨论的语言”）。我们将利用Dolma项目提供的fasttext快速预训练模型，判断输入文本是否属于上述任一类别。这些分类器是在Jigsaw有毒评论数据集上训练而成的，该数据集包含被归类为多种不同标签的维基百科评论。\n",
    "\n",
    "NSFW分类器可在以下网址下载：\n",
    "dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_nsfw_final.bin\n",
    "\n",
    "仇恨言论分类器可在以下位置获取：\n",
    "dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_hatespeech_final.bin\n",
    "\n",
    "---\n",
    "\n",
    "使用这些模型实现一个函数，该函数接收包含页面内容的Unicode字符串，并返回一个标签（例如“有毒”、“无毒”），以及相应的置信度分数。\n",
    "\n",
    "**问题（有害内容）：6分**\n",
    "\n",
    "1. 编写一个函数来检测不适宜内容。\n",
    "   **可交付成果**：一个函数，用于将给定字符串标记为是否包含不适宜内容，并返回包含标签和置信度分数的元组。实现适配器 `[运行NSFW分类]`，并确保其通过 `uv` 运行 `pytest -k test_classify_nsfw`。请注意，此测试仅作为基本的健全性检查，数据来源于Jigsaw语料库，但绝非用于验证您的分类器是否准确，您仍需自行进行准确性验证。\n",
    "\n",
    "2. 编写一个函数以检测有毒言论。\n",
    "   **交付物**：一个函数，用于将给定字符串标记为是否包含毒性言论，并返回一个包含标签和置信度分数的元组。实现该适配器 `[运行分类有毒言论]`，并确保其通过 `uv` 运行 `pytest -k test_classify_toxic_speech`。同样，此测试仅作为基本的正确性检查，也源自Jigsaw。\n",
    "\n",
    "3. 你认为在将这些过滤器应用于构建训练集时，语言模型下游可能会出现哪些问题？你又该如何缓解这些问题？\n",
    "   **交付物**：2至5句的回应。\n",
    "\n",
    "4. 请对从WARC文件中提取的文本（通过您先前实现的文本提取功能）运行有害内容过滤器。随机选取20个示例进行检查，并将分类器的预测结果与您的个人判断进行对比。同时，报告任何分类器的误判情况。此外，统计有多少比例的文档被判定为有害。根据您的观察，确定在过滤过程中应采用哪些合适的分类器置信度阈值？\n",
    "   **交付物**：2至5句话的回应。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f4d1a",
   "metadata": {},
   "source": [
    "下载训练好的fasttext模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325a4591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:27:39--  http://dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_nsfw_final.bin\n",
      "Resolving dolma-artifacts.org (dolma-artifacts.org)... 104.21.90.110, 172.67.156.91, 2606:4700:3035::ac43:9c5b, ...\n",
      "Connecting to dolma-artifacts.org (dolma-artifacts.org)|104.21.90.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 991753734 (946M) [application/octet-stream]\n",
      "Saving to: ‘jigsaw_fasttext_bigrams_nsfw_final.bin’\n",
      "\n",
      "jigsaw_fasttext_big 100%[===================>] 945.81M  6.32MB/s    in 2m 45s  \n",
      "\n",
      "2026-01-26 13:30:38 (5.72 MB/s) - ‘jigsaw_fasttext_bigrams_nsfw_final.bin’ saved [991753734/991753734]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NSFW分类器\n",
    "!wget --no-proxy dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_nsfw_final.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d778ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 13:30:38--  http://dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_hatespeech_final.bin\n",
      "Resolving dolma-artifacts.org (dolma-artifacts.org)... 104.21.90.110, 172.67.156.91, 2606:4700:3037::6815:5a6e, ...\n",
      "Connecting to dolma-artifacts.org (dolma-artifacts.org)|104.21.90.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 992245824 (946M) [application/octet-stream]\n",
      "Saving to: ‘jigsaw_fasttext_bigrams_hatespeech_final.bin’\n",
      "\n",
      "jigsaw_fasttext_big 100%[===================>] 946.28M  5.54MB/s    in 2m 42s  \n",
      "\n",
      "2026-01-26 13:33:33 (5.84 MB/s) - ‘jigsaw_fasttext_bigrams_hatespeech_final.bin’ saved [992245824/992245824]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 仇恨言论分类器\n",
    "!wget --no-proxy dolma-artifacts.org/fasttext_models/jigsaw_fasttext_bigrams_20230515/jigsaw_fasttext_bigrams_hatespeech_final.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "780849e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from typing import Tuple\n",
    "\n",
    "# ==========================\n",
    "# 1. 加载预训练模型\n",
    "# ==========================\n",
    "\n",
    "# NSFW 模型路径（本地路径）\n",
    "NSFW_MODEL_PATH = \"jigsaw_fasttext_bigrams_nsfw_final.bin\"\n",
    "# 有毒言论模型路径（本地路径）\n",
    "HATE_MODEL_PATH = \"jigsaw_fasttext_bigrams_hatespeech_final.bin\"\n",
    "\n",
    "# 加载 fastText 模型，注意：这一步会占用一定内存\n",
    "nsfw_model = fasttext.load_model(NSFW_MODEL_PATH)\n",
    "hate_model = fasttext.load_model(HATE_MODEL_PATH)\n",
    "\n",
    "# ==========================\n",
    "# 2. 定义 NSFW 分类函数\n",
    "# ==========================\n",
    "\n",
    "def classify_nsfw(text: str) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    检测输入文本是否包含不适宜内容（NSFW）。\n",
    "\n",
    "    参数:\n",
    "        text (str): 待检测的 Unicode 文本字符串。\n",
    "\n",
    "    返回:\n",
    "        tuple: (标签, 置信度)\n",
    "            标签: \"有害\" 或 \"无害\"\n",
    "            置信度: 模型对预测标签的概率分数（0~1之间的浮点数）\n",
    "    \n",
    "    使用说明:\n",
    "        >>> classify_nsfw(\"示例文本\")\n",
    "        ('无害', 0.95)\n",
    "    \"\"\"\n",
    "    # fastText 的 predict 函数返回一个元组: (标签列表, 概率列表)\n",
    "    labels, probs = nsfw_model.predict(text, k=1)  # 只取最可能的一个类别\n",
    "    label = labels[0].replace(\"__label__\", \"\")    # 去掉 fastText 特殊前缀\n",
    "    confidence = probs[0]\n",
    "\n",
    "    # 将模型标签映射为可读标签\n",
    "    if label.lower() in [\"nsfw\", \"1\"]:  # 根据训练集标签可能是 \"nsfw\" 或 \"1\"\n",
    "        return \"有害\", float(confidence)\n",
    "    else:\n",
    "        return \"无害\", float(confidence)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 3. 定义有毒言论分类函数\n",
    "# ==========================\n",
    "\n",
    "def classify_toxic_speech(text: str) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    检测输入文本是否包含有毒言论。\n",
    "\n",
    "    参数:\n",
    "        text (str): 待检测的 Unicode 文本字符串。\n",
    "\n",
    "    返回:\n",
    "        tuple: (标签, 置信度)\n",
    "            标签: \"有毒\" 或 \"无毒\"\n",
    "            置信度: 模型对预测标签的概率分数（0~1之间的浮点数）\n",
    "    \n",
    "    使用说明:\n",
    "        >>> classify_toxic_speech(\"示例文本\")\n",
    "        ('无毒', 0.98)\n",
    "    \"\"\"\n",
    "    # fastText 的 predict 函数返回一个元组: (标签列表, 概率列表)\n",
    "    labels, probs = hate_model.predict(text, k=1)  # 只取最可能的一个类别\n",
    "    label = labels[0].replace(\"__label__\", \"\")     # 去掉 fastText 特殊前缀\n",
    "    confidence = probs[0]\n",
    "\n",
    "    # 将模型标签映射为可读标签\n",
    "    if label.lower() in [\"toxic\", \"1\"]:  # 根据训练集标签可能是 \"toxic\" 或 \"1\"\n",
    "        return \"有毒\", float(confidence)\n",
    "    else:\n",
    "        return \"无毒\", float(confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db265bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('有害', 1.0000100135803223)\n",
      "('无毒', 1.0000100135803223)\n"
     ]
    }
   ],
   "source": [
    "text1 = \"fuck\"\n",
    "text2 = 'hellp'\n",
    "print(classify_nsfw(text1))          # ('有害', 0.87)\n",
    "print(classify_toxic_speech(text2))  # ('无毒', 0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca58cec",
   "metadata": {},
   "source": [
    "# 作业六\n",
    "\n",
    "\n",
    "即使在按语言过滤页面并移除有害内容后，仍会有相当一部分剩余页面对语言模型训练而言质量较低。同样地，“质量”本身并不容易定义，但通过浏览Common Crawl中的示例，有助于识别低质量内容的典型情况，例如：\n",
    "- 带有付费内容的页面\n",
    "- 用于修复链接的占位页面\n",
    "- 登录、注册或联系表单\n",
    "- 主要包含非文本内容的页面，这些内容在文本提取过程中会丢失（例如：照片、视频）\n",
    "\n",
    "Gopher论文[Rae等，2021]描述了一组简单的质量过滤器，用于从网络抓取的数据中移除低质量文本的相似简单案例。这些过滤器由易于理解的简单启发式规则组成，通常能够覆盖大量明显不适用的示例。Gopher质量过滤器包括基于文档长度、词长、符号与词的比例，以及特定英语停用词是否存在等多个标准。在本次作业中，你将实现Gopher论文[Rae等，2021]中所描述的部分过滤器。具体而言，你需要移除以下类型的文档：\n",
    "\n",
    "- 字数少于50字或超过10万字。\n",
    "- 单词平均长度超出3到10个字符的范围。\n",
    "- 超过30%的行以省略号（“...”）结尾。\n",
    "- 包含少于80%的至少含有一个字母字符的单词。\n",
    "\n",
    "有关Gopher论文所使用的所有质量过滤器的完整描述，请参阅其附录A。\n",
    "\n",
    "---\n",
    "\n",
    "问题（gopher_quality_filters）：3分\n",
    "\n",
    "(a) 实现（至少）上述描述的Gopher质量过滤器的一个子集。对于将文本标记化为单词，您可能会发现NLTK包很有用（特别是nltk.word_tokenize），尽管您不需要使用它。\n",
    "交付物：一个函数，它接受一个字符串作为其唯一参数，并返回一个布尔值，指示文本是否通过Gopher质量过滤器。实现适配器 [run_gopher_quality_filter]。然后，确保您的过滤器通过在 uv run pytest -k test_gopher 中运行的测试。\n",
    "\n",
    "(b) 在从WARC文件中提取的文本上运行您的基于规则的质量过滤器（通过您先前实现的文本提取函数）。查看20个随机示例，并将过滤器预测与您自己的判断进行比较。评论任何质量过滤器与您的判断不同的案例。\n",
    "交付物：2-5句的回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68223005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kangkang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# 如果你还没下载 punkt 分词器，需要先运行下面一行\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00fc6447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/kangkang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad31ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gopher_quality_filter(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Gopher 质量过滤器子集实现\n",
    "    \n",
    "    接受一个字符串 text，返回 True 如果文本通过过滤器，False 如果文本被过滤掉。\n",
    "    \n",
    "    实现的过滤规则：\n",
    "    1. 字数少于50字或超过10万字 -> 不通过\n",
    "    2. 单词平均长度超出 3 到 10 个字符 -> 不通过\n",
    "    3. 超过30%的行以省略号结尾 -> 不通过\n",
    "    4. 少于80%的单词至少含有一个字母 -> 不通过\n",
    "    \n",
    "    Args:\n",
    "        text (str): 输入文本\n",
    "    \n",
    "    Returns:\n",
    "        bool: True 表示通过质量过滤，False 表示不通过\n",
    "    \"\"\"\n",
    "    \n",
    "    # 规则1：字数过滤\n",
    "    num_chars = len(text)\n",
    "    if num_chars < 50 or num_chars > 100000:\n",
    "        return False  # 字数过少或过多\n",
    "    \n",
    "    # 将文本按空白字符分词\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 规则2：单词平均长度过滤\n",
    "    if len(words) > 0:\n",
    "        avg_word_len = sum(len(w) for w in words) / len(words)\n",
    "        if avg_word_len < 3 or avg_word_len > 10:\n",
    "            return False  # 单词平均长度不符合要求\n",
    "    \n",
    "    # 规则3：省略号行比例过滤\n",
    "    lines = text.splitlines()\n",
    "    if lines:\n",
    "        ellipsis_lines = sum(1 for line in lines if line.strip().endswith(\"...\"))\n",
    "        if ellipsis_lines / len(lines) > 0.3:\n",
    "            return False  # 省略号行过多\n",
    "    \n",
    "    # 规则4：至少含有一个字母的单词比例过滤\n",
    "    if words:\n",
    "        letter_word_count = sum(1 for w in words if re.search(r\"[A-Za-z]\", w))\n",
    "        if letter_word_count / len(words) < 0.8:\n",
    "            return False  # 含字母单词比例过低\n",
    "    \n",
    "    # 所有规则都通过\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_gopher_quality_filter(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    适配器函数，用于测试框架调用\n",
    "    \"\"\"\n",
    "    return gopher_quality_filter(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a7e7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试案例 1 (字数过少): 不通过, 期望: 不通过\n",
      "测试案例 2 (正常文本): 通过, 期望: 通过\n",
      "测试案例 3 (省略号行过多): 不通过, 期望: 不通过\n",
      "测试案例 4 (大部分单词无字母): 不通过, 期望: 不通过\n",
      "测试案例 5 (单词平均长度过长): 通过, 期望: 不通过\n"
     ]
    }
   ],
   "source": [
    "# 测试 gopher_quality_filter 的简单脚本\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"text\": \"这是一个非常短的文本。\",\n",
    "        \"expected\": False,  # 字数 < 50\n",
    "        \"desc\": \"字数过少\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \" \".join([\"word\"]*1000),  # 1000个单词\n",
    "        \"expected\": True,  # 字数足够，单词长度正常\n",
    "        \"desc\": \"正常文本\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"word \" * 10 + \"...\"*50 + \"\\n\" + \"word \" * 10 + \"...\\n\"*5, \n",
    "        \"expected\": False,  # 超过30%的行以省略号结尾\n",
    "        \"desc\": \"省略号行过多\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"1234 5678 91011 $$$ %%%\", \n",
    "        \"expected\": False,  # 含字母单词比例 < 80%\n",
    "        \"desc\": \"大部分单词无字母\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"a \" * 20 + \"supercalifragilisticexpialidocious \" * 5, \n",
    "        \"expected\": False,  # 平均单词长度 > 10\n",
    "        \"desc\": \"单词平均长度过长\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    result = gopher_quality_filter(case[\"text\"])\n",
    "    print(f\"测试案例 {i+1} ({case['desc']}): {'通过' if result else '不通过'}, 期望: {'通过' if case['expected'] else '不通过'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a992308",
   "metadata": {},
   "source": [
    "# 作业七 \n",
    "\n",
    "质量分类器\n",
    "\n",
    "现在让我们超越Gopher规则所捕获的简单句法标准，考虑质量评估。语言模型训练并不是根据内容质量进行排名的首选方法。特别是，文本质量也是信息检索中的一个基本挑战。搜索引擎利用网页链接结构的经典信号之一是：高质量页面倾向于链接到其他高质量页面 [Page et al., 1998]。OpenAI在构建WebText时使用了类似的见解，GPT-2 [Radford et al., 2019] 训练的数据集包括那些在Reddit评论中获得高于最低“karma”阈值的页面。另一个选择是使用Wikipedia作为高质量链接的来源，因为从Wikipedia页面链接的外部来源往往是可信的 [Touvron et al., 2023]。\n",
    "\n",
    "虽然使用受控来源通常会导致高质量内容，但结果数据集最终对于今天的标准来说太小了（OpenWebText有40GB的文本，而Pile比它大20倍）。一种方法是使用这些参考页面作为正面例子，从Common Crawl中抽取的（随机）页面作为负面例子，并训练一个fastText分类器。这个分类器会给出质量分数，您可以使用它来过滤掉整个Common Crawl中的页面。设置质量阈值会在精确度和召回率之间进行权衡。\n",
    "\n",
    "在本次作业中，您将构建一个质量分类器。为了方便，我们已经从最近的Wikipedia转储中提取了参考页面的URL，并将其放置在Together集群的 /data/wiki/enwiki-20240420-extracted_urls.txt.gz。该文件包含截至2024年4月在英语维基百科页面上找到的43,500个外部链接的列表，但我们希望您订阅这些URL以获得用于训练您分类器的“高质量”文本的正面例子。请注意，这些正面例子可能仍包含不理想的内容，因此应用您构建的其他原语可能是有用的。\n",
    "\n",
    "（例如，语言识别、过滤规则等）以进一步提高其质量。给定一个URL文件，您可以使用wget以WARC格式抓取其内容：\n",
    "\n",
    "```\n",
    "wget --timeout=5 \\\n",
    "  -i subsampled_positive_urls.txt \\\n",
    "  --warc-file=subsampled_positive_urls.warc \\\n",
    "  -O /dev/null\n",
    "```\n",
    "\n",
    "问题（quality_classifier）：15分\n",
    "\n",
    "(a) 训练一个质量分类器，给定文本，返回一个数值质量分数。\n",
    "交付物：一个用于下一个子问题的可用质量分类器。\n",
    "\n",
    "(b) 编写一个函数，将页面标记为高质量或低质量，并在标签中提供置信度分数。\n",
    "交付物：一个函数，它接受一个字符串作为其唯一参数，并返回一个带有标签（高质量或非高质量）和置信度分数的对。实现适配器 [run_classify_quality]。作为基本检查，确保它通过运行 uv run pytest -k test_classify_quality 正确分类我们提供的两个示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44f80548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 16:33:15--  https://downloads.cs.stanford.edu/nlp/data/nfliu/cs336-spring-2024/assignment4/enwiki-20240420-extracted_urls.txt.gz\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1055989383 (1007M) [application/octet-stream]\n",
      "Saving to: ‘enwiki-20240420-extracted_urls.txt.gz.1’\n",
      "\n",
      "enwiki-20240420-ext 100%[===================>]   1007M  3.19MB/s    in 5m 48s  \n",
      "\n",
      "2026-01-26 16:39:23 (2.90 MB/s) - ‘enwiki-20240420-extracted_urls.txt.gz.1’ saved [1055989383/1055989383]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.cs.stanford.edu/nlp/data/nfliu/cs336-spring-2024/assignment4/enwiki-20240420-extracted_urls.txt.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c0cad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip enwiki-20240420-extracted_urls.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d685ee",
   "metadata": {},
   "source": [
    "使用以下官方给的命令，并发太大，消耗内存导致wsl崩溃！几乎所有的崩溃都和这个原因有关\n",
    "所以我使用以下代码下载\n",
    "```bash\n",
    "wget --timeout=5 \\\n",
    "  -i enwiki-20240420-extracted_urls.txt \\\n",
    "  --warc-file=subsampled_positive_urls.warc \\\n",
    "  -O /dev/null\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6388cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 191/43579680 [08:03<31399:58:11,  2.59s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) >= MAX_WORKERS*\u001b[32m4\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    112\u001b[39m         pbar.close()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     77\u001b[39m urls = \u001b[38;5;28mlist\u001b[39m(url_stream(URL_FILE))\n\u001b[32m     78\u001b[39m total = \u001b[38;5;28mlen\u001b[39m(urls)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m     \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDONE_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m     \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing URLs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/thread.py:238\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1147\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1167\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1168\u001b[39m         lock.release()\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "import time\n",
    "import os\n",
    "\n",
    "# =====================\n",
    "# 配置参数\n",
    "# =====================\n",
    "MAX_WORKERS = 12       # 并发线程数，WSL 推荐 4–6，太高容易崩\n",
    "TIMEOUT = 20           # 网络请求超时时间（秒）\n",
    "MIN_TEXT_LEN = 100     # 文本最小长度，小于该长度的网页会被丢弃\n",
    "OUTPUT = \"wiki_train.jsonl\"  # 训练数据输出文件，每行 JSON\n",
    "DONE_FILE = \"done_urls.txt\"  # 已处理 URL 文件，用于断点续跑\n",
    "URL_FILE = \"enwiki-20240420-extracted_urls.txt\"  # 待抓取 URL 列表\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"WikiDataCollector/1.0 (training)\"  # 避免被网站封禁\n",
    "}\n",
    "\n",
    "# =====================\n",
    "# 读取已完成 URL（断点续跑）\n",
    "# =====================\n",
    "done_urls = set()\n",
    "if os.path.exists(DONE_FILE):\n",
    "    # 如果 done 文件存在，就把所有 URL 加入 set\n",
    "    # 避免重复抓取\n",
    "    with open(DONE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            done_urls.add(line.strip())\n",
    "\n",
    "# =====================\n",
    "# URL 流生成器（跳过已完成 URL）\n",
    "# =====================\n",
    "def url_stream(path):\n",
    "    \"\"\"\n",
    "    按行读取 URL 文件，并跳过已经完成的 URL\n",
    "    使用生成器可以避免一次性加载大文件，节约内存\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            url = line.strip()\n",
    "            if url and url not in done_urls:\n",
    "                yield url\n",
    "\n",
    "# =====================\n",
    "# 配置 requests Session（复用连接 + 重试）\n",
    "# =====================\n",
    "session = requests.Session()\n",
    "# 设置重试策略，遇到指定 HTTP 错误码自动重试\n",
    "retries = Retry(total=2, backoff_factor=1, status_forcelist=[429,500,502,503,504])\n",
    "# 配置连接池大小，避免线程阻塞\n",
    "adapter = HTTPAdapter(pool_connections=MAX_WORKERS*2, pool_maxsize=MAX_WORKERS*2, max_retries=retries)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# =====================\n",
    "# 网页抓取与文本清洗函数\n",
    "# =====================\n",
    "def fetch_and_process(url):\n",
    "    \"\"\"\n",
    "    对单个 URL 抓取网页、提取纯文本\n",
    "    返回一个字典 {\"url\", \"length\", \"text\"}，否则返回 None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 发起 GET 请求\n",
    "        r = session.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "\n",
    "        # 非 200 响应直接丢弃\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        # 非 HTML 内容丢弃\n",
    "        if \"text/html\" not in r.headers.get(\"Content-Type\", \"\"):\n",
    "            return None\n",
    "\n",
    "        # 使用 BeautifulSoup 清洗 HTML\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")  # ⚠ 用 html.parser 更稳，不依赖 lxml\n",
    "\n",
    "        # 移除脚本、样式、noscript 标签\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        # 提取纯文本，并压缩空格\n",
    "        text = \" \".join(soup.get_text(separator=\" \").split())\n",
    "\n",
    "        # 文本过短丢弃\n",
    "        if len(text) < MIN_TEXT_LEN:\n",
    "            return None\n",
    "\n",
    "        # 返回字典，用于写入 JSONL\n",
    "        return {\"url\": url, \"length\": len(text), \"text\": text}\n",
    "\n",
    "    except Exception:\n",
    "        # 抓取或解析异常直接返回 None，不中断整个流程\n",
    "        return None\n",
    "\n",
    "# =====================\n",
    "# 主流程\n",
    "# =====================\n",
    "def main():\n",
    "    # 生成待抓取 URL 列表\n",
    "    urls = list(url_stream(URL_FILE))\n",
    "    total = len(urls)  # 总 URL 数量，用于进度条\n",
    "\n",
    "    # 打开输出文件和断点续跑文件\n",
    "    with open(OUTPUT, \"a\", encoding=\"utf-8\") as out, \\\n",
    "         open(DONE_FILE, \"a\", encoding=\"utf-8\") as done_out, \\\n",
    "         ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "        futures = []  # 存储线程任务\n",
    "        pbar = tqdm(total=total, desc=\"Processing URLs\")  # 进度条\n",
    "\n",
    "        for url in urls:\n",
    "            # 提交线程池任务\n",
    "            futures.append(executor.submit(fetch_and_process, url))\n",
    "\n",
    "            # 控制 futures 批量处理，防止无限增长占用内存\n",
    "            if len(futures) >= MAX_WORKERS*4:\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        # 写入 JSONL 输出文件\n",
    "                        out.write(json.dumps(result) + \"\\n\")\n",
    "                        out.flush()  # 确保写入磁盘\n",
    "\n",
    "                        # 记录已完成 URL\n",
    "                        done_out.write(result[\"url\"] + \"\\n\")\n",
    "                        done_out.flush()\n",
    "\n",
    "                    # 更新进度条\n",
    "                    pbar.update(1)\n",
    "                futures.clear()  # 清空批量 futures\n",
    "\n",
    "        # 处理剩余未完成的 futures\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                out.write(json.dumps(result) + \"\\n\")\n",
    "                out.flush()\n",
    "                done_out.write(result[\"url\"] + \"\\n\")\n",
    "                done_out.flush()\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()  # 关闭进度条\n",
    "\n",
    "# =====================\n",
    "# 脚本入口\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47d98b",
   "metadata": {},
   "source": [
    "但是下载还是太慢了，最后直接下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b241a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 21:06:40--  https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 24796216934 (23G) [application/octet-stream]\n",
      "Saving to: ‘enwiki-latest-pages-articles.xml.bz2’\n",
      "\n",
      "test-pages-articles   6%[>                   ]   1.43G  4.03MB/s    eta 97m 41s^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476266d",
   "metadata": {},
   "source": [
    "上面是24G版本，所占内存太大，如果只是体验流程可以下载以下小版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33f4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 21:14:08--  https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 294537752 (281M) [application/octet-stream]\n",
      "Saving to: ‘enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2’\n",
      "\n",
      "enwiki-latest-pages 100%[===================>] 280.89M  3.62MB/s    in 75s     \n",
      "\n",
      "2026-01-26 21:15:26 (3.76 MB/s) - ‘enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2’ saved [294537752/294537752]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f95e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-26 22:46:21--  https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream2.xml-p41243p151573.bz2\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 395601027 (377M) [application/octet-stream]\n",
      "Saving to: ‘enwiki-latest-pages-articles-multistream2.xml-p41243p151573.bz2’\n",
      "\n",
      "enwiki-latest-pages 100%[===================>] 377.27M  3.56MB/s    in 1m 56s  \n",
      "\n",
      "2026-01-26 22:48:27 (3.25 MB/s) - ‘enwiki-latest-pages-articles-multistream2.xml-p41243p151573.bz2’ saved [395601027/395601027]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream2.xml-p41243p151573.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90566b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取了 1000 篇文章\n",
      "提取了 1000 篇文章\n"
     ]
    }
   ],
   "source": [
    "import mwxml\n",
    "import json\n",
    "import bz2\n",
    "\n",
    "def extract_to_json(bz2_file, output_json):\n",
    "    \"\"\"将维基百科 XML 转为 JSON\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    # 直接读取 bz2 文件\n",
    "    with bz2.open(bz2_file, 'rb') as f:\n",
    "        dump = mwxml.Dump.from_file(f)\n",
    "        \n",
    "        for page in dump:\n",
    "            # 跳过重定向和特殊页面\n",
    "            if page.namespace != 0 or page.redirect:\n",
    "                continue\n",
    "                \n",
    "            # 获取最新版本\n",
    "            revision = next(page)\n",
    "            if revision.text:\n",
    "                articles.append({\n",
    "                    'id': str(page.id),\n",
    "                    'title': page.title,\n",
    "                    'text': revision.text,\n",
    "                    'url': f'https://en.wikipedia.org/wiki?curid={page.id}'\n",
    "                })\n",
    "                \n",
    "            # 可选：限制数量用于测试\n",
    "            if len(articles) >= 1000:\n",
    "                break\n",
    "    \n",
    "    # 保存为 JSON\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"提取了 {len(articles)} 篇文章\")\n",
    "\n",
    "# 使用\n",
    "extract_to_json(\n",
    "    'enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2',\n",
    "    'wiki_articles.json'\n",
    ")\n",
    "extract_to_json(\n",
    "    'enwiki-latest-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
    "    'wiki_articles1.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cd501",
   "metadata": {},
   "source": [
    "使用之前写的函数进行清洗"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
