{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3224fbb3",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "**问题（look_at_cc）：4 分**\n",
    "\n",
    "(a) 下载上面的 WARC 文件，或者找到我们在集群上提供的副本。让我们看看这个文件中的第一页。这是一个 gzipped 文件，你可以使用以下命令浏览其内容：\n",
    "\n",
    "```\n",
    "$ zcat /data/CC/example.warc.gz | less\n",
    "```\n",
    "\n",
    "`less` 让你可以使用键盘箭头、Page Up、Page Down 来浏览文件。要退出，按“q”。\n",
    "查看非常第一个网页。它的 URL 是什么？它仍然可以访问吗？你能通过查看原始 HTML 来告诉我们这个页面似乎是什么吗？\n",
    "交付物：2-3 句回应。\n",
    "\n",
    "(b) 现在让我们看看对应的 WET 文件：\n",
    "\n",
    "```\n",
    "$ zcat /data/CC/example.warc.wet.gz | less\n",
    "```\n",
    "\n",
    "注意 WET 文件包含 HTTP 头（例如，Content-Length），这些不是提取的文本内容的一部分。如果你查看第一个例子，你会看到它包含从你刚刚看到的原始 HTML 中提取的文本。\n",
    "\n",
    "注意提取的文本中有多少是 HTML 结构的遗迹，而不是页面的主要内容。你认为哪些部分应该被提取器过滤掉？将此文本作为训练数据时，可能会出错的地方是什么：在训练看起来像这样的文本的模型时，可能会出错的地方是什么？相反，模型可能从这个页面中提取哪些有用的信息？\n",
    "交付物：3-4 句回应。\n",
    "\n",
    "(c) 什么使一个好的训练示例具有高度的上下文性。描述一个应用领域，在这个领域中，这个示例可能对训练数据有用，以及一个它可能没有用的地方。\n",
    "交付物：1-2 句回应。\n",
    "\n",
    "(d) 让我们看更多的例子，以便更好地了解 Common Crawl 中的内容。浏览另外 25 个 WET 记录。对于每条记录，非常简要地评论文档的语言（如果可以识别），域名，页面类型等。你需要看到多少个例子才能确定你认为的“高质量”网页？\n",
    "交付物：对 25 个文档进行简短注释，包括文档的语言、域名、页面类型和其他任何关于文档的杂项注释。直到你看到一个高质量的示例。示例的数量取决于你看到高质量示例的时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b3b4f",
   "metadata": {},
   "source": [
    "运行以下命令来下载数据集\n",
    "```bash\n",
    "wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
    "\n",
    "wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a234cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-22 22:59:03--  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1121135968 (1.0G) [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’\n",
      "\n",
      "CC-MAIN-20250417135   0%[                    ]  16.53K  --.-KB/s    in 29s     \n",
      "\n",
      "2026-01-22 22:59:38 (593 B/s) - Connection closed at byte 16923. Retrying.\n",
      "\n",
      "--2026-01-22 22:59:39--  (try: 2)  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 206 Partial Content\n",
      "Length: 1121135968 (1.0G), 1121119045 (1.0G) remaining [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’\n",
      "\n",
      "CC-MAIN-20250417135  13%[=>                  ] 142.71M  --.-KB/s    in 5m 53s  \n",
      "\n",
      "2026-01-22 23:05:59 (414 KB/s) - Connection closed at byte 149640251. Retrying.\n",
      "\n",
      "--2026-01-22 23:06:01--  (try: 3)  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 206 Partial Content\n",
      "Length: 1121135968 (1.0G), 971495717 (926M) remaining [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’\n",
      "\n",
      "CC-MAIN-20250417135 100%[++=================>]   1.04G  4.27MB/s    in 7m 31s  \n",
      "\n",
      "2026-01-22 23:14:08 (2.05 MB/s) - ‘CC-MAIN-20250417135010-20250417165010-00065.warc.gz’ saved [1121135968/1121135968]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/warc/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175e618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-22 23:16:17--  https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\n",
      "Connecting to 127.0.0.1:7890... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 80904812 (77M) [application/octet-stream]\n",
      "Saving to: ‘CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz’\n",
      "\n",
      "CC-MAIN-20250417135 100%[===================>]  77.16M  1.77MB/s    in 52s     \n",
      "\n",
      "2026-01-22 23:17:14 (1.48 MB/s) - ‘CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz’ saved [80904812/80904812]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/segments/1744889135610.12/wet/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da7b10",
   "metadata": {},
   "source": [
    "我们可以使用\n",
    "```bash\n",
    "zcat CC-MAIN-20250417135010-20250417165010-00065.warc.gz | less\n",
    "\n",
    "zcat CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz | less\n",
    "```\n",
    "在命令行中进行预览，less表示可以使用键盘方向键、Page Up 和 Page Down 来浏览文件。要退出，请按“q“。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03d7bc",
   "metadata": {},
   "source": [
    "`.warc.gz` 和 `.warc.wet.gz` 都是 **WARC（Web ARChive）格式** 的压缩文件，广泛用于大规模网页抓取数据（如 Common Crawl）。\n",
    "\n",
    "## 一、`.warc.gz` 与 `.warc.wet.gz` 的区别\n",
    "\n",
    "### 1. `.warc.gz`\n",
    "\n",
    "**原始网页归档文件**，包含完整抓取结果：HTTP 请求 / 响应头、HTML 原文、二进制资源（图片、PDF 等）、元数据（抓取时间、URL、状态码等）。\n",
    "\n",
    "### 2. `.warc.wet.gz`\n",
    "\n",
    "**WET = Web Extracted Text**\n",
    "\n",
    "从 `.warc.gz` 中**抽取后的纯文本**，仅保留网页的可读文本内容，已去除 HTML 标签、脚本、样式等\n",
    "\n",
    "适合NLP 训练（LM、Embedding、Topic），文本挖掘，语料构建。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae8855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c CC-MAIN-20250417135010-20250417165010-00065.warc.gz > data/CC_data.warc\n",
    "!gunzip -c CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz > data/CC_data.warc.wet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a85e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://13.usnccm.org/\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\n",
      "  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" version=\"XHTML+RDFa 1.0\" dir=\"ltr\"\n",
      "  xmlns:content=\"http://purl.org/rss/1.0/modules/content/\"\n",
      "  xmlns:dc=\"http://purl.org/dc/terms/\"\n",
      "  xmlns:foaf=\"http://xmlns.com/foaf/0.1/\"\n",
      "  xmlns:og=\"http://ogp.me/ns#\"\n",
      "  xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\n",
      "  xmlns:sioc=\"http://rdfs.org/sioc/ns#\"\n",
      "  xmlns:sioct=\"http://rdfs.org/sioc/ty\n"
     ]
    }
   ],
   "source": [
    "# 从 warcio 库中导入 ArchiveIterator\n",
    "# ArchiveIterator 用于按“记录（record）”顺序流式读取 WARC 文件\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# 导入 gzip，用于读取 .gz 压缩格式的 WARC 文件\n",
    "import gzip\n",
    "\n",
    "# 导入 html 模块，用于将 HTML 实体（如 &#x4EBA;）解码为正常字符\n",
    "import html \n",
    "\n",
    "# WARC 文件路径（Common Crawl 的一个分片）\n",
    "# 注意：变量名 flie_path 拼写不影响运行，但语义上建议改为 file_path\n",
    "flie_path  = 'CC-MAIN-20250417135010-20250417165010-00065.warc.gz'\n",
    "\n",
    "# 以二进制只读模式打开 gzip 压缩的 WARC 文件\n",
    "# 必须使用 \"rb\"，因为 WARC 是二进制格式\n",
    "with gzip.open(flie_path, \"rb\") as f:\n",
    "\n",
    "    # 使用 ArchiveIterator 对 WARC 文件进行流式遍历\n",
    "    # 每次循环返回一个 WARC record（请求、响应、元数据等）\n",
    "    for step,record in enumerate(ArchiveIterator(f)):\n",
    "       \n",
    "        # 只处理类型为 \"response\" 的记录\n",
    "        # response 表示真实的 HTTP 响应内容（网页、图片等）\n",
    "        if record.rec_type == \"response\" and step > 5:\n",
    "\n",
    "            # 从 WARC 头中获取该网页对应的原始 URL\n",
    "            url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "\n",
    "            # 读取 HTTP 响应体内容（网页原始字节流）\n",
    "            # decode 为 utf-8，忽略非法字符，防止程序崩溃\n",
    "            html1 = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # 简单判断该响应是否为 HTML 页面\n",
    "            # 这是一个经验性过滤，避免处理图片、PDF 等非 HTML 内容\n",
    "            if \"<html\" in html1.lower():\n",
    "\n",
    "                # 输出网页 URL\n",
    "                print(url)\n",
    "\n",
    "                # 对 HTML 内容前 500 个字符进行：\n",
    "                # 1. 截断（避免输出过多内容）\n",
    "                # 2. HTML 实体解码（&#x4EBA; → 人）\n",
    "                print(html.unescape(html1[:500]))\n",
    "\n",
    "                # 找到第一个 HTML 页面后立即退出循环\n",
    "            \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffc438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0371rykj.com/ipfhsb/34.html\n",
      "> ip防護(hù)系列 >\n",
      "產(chǎn)品詳情/ products details\n",
      "恒溫恒濕試驗(yàn)箱\n",
      "產(chǎn)品用途\n",
      "恒溫恒濕試驗(yàn)箱是航空、汽車(chē)、家電、科研等領(\n"
     ]
    }
   ],
   "source": [
    "# 从 warcio 库中导入 ArchiveIterator\n",
    "# ArchiveIterator 用于按顺序流式遍历 WARC / WET 文件中的每一条记录\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# 导入 gzip 模块，用于读取 .gz 压缩格式的文件\n",
    "import gzip\n",
    "\n",
    "# 指定 Common Crawl 的 WET 文件路径\n",
    "# .warc.wet.gz 文件中存放的是已抽取好的“纯文本内容”\n",
    "flie_path  = 'CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz'\n",
    "\n",
    "# 以二进制只读模式打开 gzip 压缩的 WET 文件\n",
    "# 必须使用 \"rb\"，因为 WARC/WET 属于二进制流格式\n",
    "with gzip.open(flie_path, \"rb\") as f:\n",
    "\n",
    "    # 使用 ArchiveIterator 对 WET 文件进行流式迭代\n",
    "    # 每次循环返回一个 WARC record（一条网页对应一条 record）\n",
    "    for record in ArchiveIterator(f):\n",
    "\n",
    "        # 只处理类型为 \"conversion\" 的记录\n",
    "        # conversion 表示从 HTML 中抽取出的“可读纯文本”\n",
    "        if record.rec_type == \"conversion\":\n",
    "\n",
    "            # 从 WARC 头信息中获取该文本对应的原始网页 URL\n",
    "            url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "\n",
    "            # 读取当前 record 的正文内容（纯文本字节流）\n",
    "            # 解码为 UTF-8 字符串，忽略非法字符，防止解码异常\n",
    "            text = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # 判断文本在去除首尾空白字符后是否仍然有内容\n",
    "            # 用于过滤空页面或无有效文本的网页\n",
    "            if len(text.strip()) > 0:\n",
    "\n",
    "                # 打印网页对应的 URL\n",
    "                print(url)\n",
    "\n",
    "                # 打印正文内容的前 400 - 500 个字符\n",
    "                # 用于快速查看文本样本，避免一次性输出过多内容\n",
    "                print(text[400:500])\n",
    "\n",
    "                # 找到第一条有效文本记录后立即退出循环\n",
    "                # 适用于“快速验证 WET 文件内容是否正常”的场景\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment4_Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
