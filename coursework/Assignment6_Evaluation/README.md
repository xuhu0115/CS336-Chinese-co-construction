# CS336 Assignment 6: å¤§å‹è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶ä»‹ç»

æœ¬æ–‡ä»¶å¤¹åŒ…å«CS336è¯¾ç¨‹ç¬¬å…­æ¬¡ä½œä¸šçš„å†…å®¹ï¼Œä¸»è¦ä»‹ç»å¸¸ç”¨çš„å‡ ç§å¤§å‹è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶åŠå…¶ä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ“ æ–‡ä»¶å¤¹ç»“æ„

```
assignment6_evaluation/
â”œâ”€â”€ demo.ipynb                    # ä¸»è¦çš„æ¼”ç¤ºnotebook
â”œâ”€â”€ lm_eval_demo.py              # lm-evaluation-harness æç®€å®ç°è„šæœ¬
â”œâ”€â”€ evalscope_demo.py            # evalscope æç®€å®ç°è„šæœ¬
â”œâ”€â”€ data/                        # æ•°æ®æ–‡ä»¶å¤¹
â”‚   â””â”€â”€ index_testset.jsonl      # evalscopeç”Ÿæˆçš„è¯„æµ‹æ•°æ®é›†
â”œâ”€â”€ outputs/                     # è¯„æµ‹è¾“å‡ºç»“æœ
â”‚   â”œâ”€â”€ 20260119_232050/         
â”‚   â””â”€â”€ 20260120_000654/         
â”œâ”€â”€ images/                      # å›¾ç‰‡
â”‚   â””â”€â”€ evalscope_panel.png      
â”œâ”€â”€ lm-evaluation-harness/       # lm-evaluation-harness æ¡†æ¶æºç 
â””â”€â”€ README.md                    # æœ¬æ–‡ä»¶
```

## ğŸ¯ ä½œä¸šç›®æ ‡

ä»‹ç»å¸¸ç”¨çš„å‡ ç§å¤§å‹è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼š
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - å­¦æœ¯ç•Œæ ‡å‡†è¯„æµ‹æ¡†æ¶
- [evalscope](https://github.com/modelscope/evalscope) - æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ç»„åˆå’Œå¯è§†åŒ–åˆ†æ
- [Evalchemy](https://github.com/mlfoundations/evalchemy) - è½»é‡çº§è¯„æµ‹æ¡†æ¶
- [lighteval](https://github.com/huggingface/lighteval) - Hugging Faceç”Ÿæ€é›†æˆ

é‡ç‚¹æ¼”ç¤º**lm-evaluation-harness**å’Œ**evalscope**çš„ä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ“Š è¯„æµ‹æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶åç§° | å¼€å‘æœºæ„ | ä¸»è¦ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| lm-evaluation-harness | EleutherAI | åŠŸèƒ½ä¸°å¯Œï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œä»»åŠ¡ï¼Œå­¦æœ¯ç•Œæ ‡å‡† | å­¦æœ¯ç ”ç©¶ã€åŸºå‡†æµ‹è¯• |
| evalscope | ModelScope | æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ç»„åˆï¼Œå¯è§†åŒ–åˆ†æï¼Œä¸­æ–‡å‹å¥½ | äº§ä¸šåº”ç”¨ã€æ¨¡å‹è¯„ä¼° |
| Evalchemy | ML Foundations | è½»é‡çº§ï¼Œæ³¨é‡å¯å¤ç°æ€§å’Œæ‰©å±•æ€§ | ç ”ç©¶å®éªŒã€å¿«é€ŸåŸå‹ |
| lighteval | Hugging Face | é›†æˆTransformersç”Ÿæ€ï¼Œæ˜“äºä½¿ç”¨ | Hugging Faceç”¨æˆ· |

## ğŸ”§ ä¸»è¦å†…å®¹

### lm-evaluation-harness

- **é›¶æ ·æœ¬è¯„æµ‹**ï¼šarc_easy, piqa, lambada, triviaqa
- **å°‘æ ·æœ¬è¯„æµ‹**ï¼šhumaneval, mbpp, gsm8k, minerva_math
- **å¤šç»´åº¦èƒ½åŠ›è¯„æµ‹**ï¼šé€šç”¨è¯­è¨€ç†è§£ã€å¸¸è¯†æ¨ç†ã€ä»£ç ã€æ•°å­¦æ¨ç†

### evalscope

- **è‡ªå®šä¹‰æ•°æ®é›†ç»„åˆ**ï¼šé€šè¿‡CollectionSchemaå®šä¹‰è¯„æµ‹ç´¢å¼•
- **åŠ æƒé‡‡æ ·**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´æ•°æ®é›†æƒé‡
- **å¯è§†åŒ–åˆ†æ**ï¼šé€šè¿‡Webç•Œé¢åˆ†æè¯„æµ‹ç»“æœè¯¦æƒ…


## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. åˆ›å»ºå¹¶æ¿€æ´»condaç¯å¢ƒ
conda create -n eval_env python=3.10
conda activate eval_env

# 2. å®‰è£…lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
pip install -e .[math]

# 3. å®‰è£…evalscope
pip install evalscope
pip install 'evalscope[app]' -U  # å¯è§†åŒ–ä¾èµ–
```

### å¼€å§‹å­¦ä¹ 

è¿›å…¥ `demo.ipynb` è·Ÿç€å­¦ä¹ ä¸¤ç§æ¡†æ¶çš„ç®€å•ä½¿ç”¨ï¼Œæ›´è¯¦ç»†å†…å®¹å¯å‚è€ƒæ¡†æ¶çš„æŒ‡å¯¼æ‰‹å†Œã€‚

å¦‚æœä½ æƒ³ç›´æ¥è¿è¡Œï¼š

#### 1. lm-evaluation-harnessæ¼”ç¤º

```bash
# Pythonè„šæœ¬æ–¹å¼
python lm_eval_demo.py
```

#### 2. evalscopeæ¼”ç¤º

```python
# è¿è¡Œevalscopeæ¼”ç¤º
python evalscope_demo.py
```

## ğŸ“ ä½¿ç”¨å»ºè®®

- **å­¦æœ¯ç ”ç©¶**ï¼šæ¨èä½¿ç”¨ `lm-evaluation-harness`
- **äº§ä¸šåº”ç”¨**ï¼šæ¨èä½¿ç”¨ `evalscope`
- **å¿«é€ŸåŸå‹**ï¼šæ¨èä½¿ç”¨ `Evalchemy`
- **Hugging Faceç”¨æˆ·**ï¼šæ¨èä½¿ç”¨ `lighteval`

## ğŸ“ æ›´å¤šå†…å®¹

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- [lm-evaluation-harnessæ–‡æ¡£](https://github.com/EleutherAI/lm-evaluation-harness)
- [evalscopeæ–‡æ¡£](https://github.com/modelscope/evalscope)
