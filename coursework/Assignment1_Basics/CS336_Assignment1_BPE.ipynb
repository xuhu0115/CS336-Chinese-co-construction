{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7450c68",
   "metadata": {},
   "source": [
    "ä¸€ã€ä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97776718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     
     ]
    }
   ],
   "source": [
    "!pip install tokenizers tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c968fe5",
   "metadata": {},
   "source": [
    "äºŒã€åˆ†å—è¯»å–å·¥å…·å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45b248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    å°†æ–‡ä»¶åˆ‡åˆ†ä¸ºå¯ä»¥ç‹¬ç«‹è®¡æ•°çš„å—ï¼ˆChunkï¼‰ã€‚\n",
    "    å¦‚æœè¾¹ç•Œå‘ç”Ÿé‡å ï¼ˆä¾‹å¦‚æ–‡ä»¶å¤ªå°æˆ–åˆ†éš”ç¬¦å¤ªç¨€ç–ï¼‰ï¼Œè¿”å›çš„å—æ•°é‡å¯èƒ½ä¼šå°‘äºé¢„æœŸã€‚\n",
    "    \"\"\"\n",
    "    # ç¡®ä¿ä¼ å…¥çš„åˆ†éš”ç¬¦æ˜¯å­—èŠ‚ä¸²ç±»å‹ï¼Œå› ä¸ºæ–‡ä»¶æ˜¯ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–çš„\n",
    "    assert isinstance(split_special_token, bytes), \"å¿…é¡»ä½¿ç”¨å­—èŠ‚ä¸²ï¼ˆbytesï¼‰è¡¨ç¤ºç‰¹æ®Š Token\"\n",
    "\n",
    "    # è·å–æ–‡ä»¶çš„æ€»å­—èŠ‚å¤§å°\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0) # å›åˆ°æ–‡ä»¶å¼€å¤´\n",
    "\n",
    "    # è®¡ç®—åˆæ­¥çš„ç†æƒ³å—å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # åˆå§‹çš„è¾¹ç•ŒçŒœæµ‹ï¼šæ ¹æ®å—å¤§å°è¿›è¡Œå‡åŒ€åˆ†å¸ƒ\n",
    "    # è¾¹ç•Œæ•°ç»„åŒ…å«èµ·å§‹ä½ç½® 0 å’Œç»“æŸä½ç½® file_size\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size # ç¡®ä¿æœ€åä¸€ä¸ªè¾¹ç•Œç²¾ç¡®æŒ‡å‘æ–‡ä»¶æœ«å°¾\n",
    "\n",
    "    mini_chunk_size = 4096  # æ¯æ¬¡å‘åæœç´¢çš„ç¼“å†²åŒºå¤§å°4kå­—èŠ‚\n",
    "\n",
    "    # éå†é™¤äº†å¼€å¤´å’Œç»“å°¾ä¹‹å¤–çš„æ‰€æœ‰ä¸­é—´è¾¹ç•Œç‚¹\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # è·³è½¬åˆ°åˆæ­¥çŒœæµ‹çš„è¾¹ç•Œä½ç½®\n",
    "        \n",
    "        while True:\n",
    "            # è¯»å–ä¸€å°å—æ•°æ®è¿›è¡Œæ‰«æ\n",
    "            mini_chunk = file.read(mini_chunk_size)\n",
    "\n",
    "            # å¦‚æœè¯»åˆ°äº†æ–‡ä»¶æœ«å°¾ï¼ˆEOFï¼‰ï¼Œè¯´æ˜åé¢æ²¡æœ‰åˆ†éš”ç¬¦äº†ï¼Œç›´æ¥è®¾ä¸ºæ–‡ä»¶æœ«å°¾\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # åœ¨å½“å‰å°å—æ•°æ®ä¸­æŸ¥æ‰¾æŒ‡å®šçš„åˆ†éš”ç¬¦\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            \n",
    "            if found_at != -1:\n",
    "                # å¦‚æœæ‰¾åˆ°äº†åˆ†éš”ç¬¦ï¼Œå°†è¾¹ç•Œè°ƒæ•´åˆ°è¯¥åˆ†éš”ç¬¦çš„ç¡®åˆ‡ä½ç½®\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            \n",
    "            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œç»§ç»­å‘åç§»åŠ¨æŒ‡é’ˆè¿›è¡Œä¸‹ä¸€è½®æœç´¢\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # set()ï¼šå»é™¤é‡å¤çš„è¾¹ç•Œï¼Œé˜²æ­¢å¤šä¸ªçŒœæµ‹ç‚¹æŒ‡å‘åŒä¸€ä¸ªåˆ†éš”ç¬¦\n",
    "    # sorted()ï¼šç¡®ä¿è¾¹ç•ŒæŒ‰ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d5fb7",
   "metadata": {},
   "source": [
    "ä¸‰ã€æŒ‰å—è¯»å–æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213ff1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def iter_text_chunks_with_monitor(\n",
    "    file_path: str,\n",
    "    chunk_size: int = 1_000_000,  # 1MB\n",
    "    log_every: int = 5,          # æ¯Nä¸ªchunkæ‰“å°ä¸€æ¬¡\n",
    "):\n",
    "    start_time = time.time()\n",
    "    bytes_processed = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        buffer = []\n",
    "        buffer_size = 0\n",
    "\n",
    "        for line in f:\n",
    "            buffer.append(line)\n",
    "            buffer_size += len(line)\n",
    "            bytes_processed += len(line)\n",
    "\n",
    "            if buffer_size >= chunk_size:\n",
    "                yield \"\".join(buffer)\n",
    "                buffer = []\n",
    "                buffer_size = 0\n",
    "                chunk_count += 1\n",
    "\n",
    "                if chunk_count % log_every == 0:\n",
    "                    log_status(\n",
    "                        prefix=\"ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç†\",\n",
    "                        bytes_processed=bytes_processed,\n",
    "                        start_time=start_time,\n",
    "                    )\n",
    "\n",
    "        if buffer:\n",
    "            yield \"\".join(buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf16ae7",
   "metadata": {},
   "source": [
    "å››ã€è®­ç»ƒBPEåˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d844d1",
   "metadata": {},
   "source": [
    "Step1:åœ¨ä¸ä¿®æ”¹tokenizerå†…éƒ¨å®ç°çš„å‰æä¸‹ï¼Œå®æ—¶ç›‘æ§å†…å­˜å ç”¨ä¸æ•°æ®ååé‡ï¼Œç†è§£tokenizerè®­ç»ƒçš„çœŸå®ç³»ç»Ÿè¡Œä¸ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdc0824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\l1337\\.conda\\envs\\llm\\lib\\site-packages (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…ç›‘æ§éœ€è¦çš„ä¾èµ–\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ef6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½“å‰è¿›ç¨‹æ‰€å å†…å­˜\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_mb():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73897d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥å¿—çŠ¶æ€å‡½æ•°è¾“å‡ºï¼ˆå†…å­˜å ç”¨ã€å¤„ç†æ•°æ®é‡ã€å¤„ç†é€Ÿåº¦ï¼‰\n",
    "import time\n",
    "def log_status(prefix, bytes_processed, start_time):\n",
    "    elapsed = time.time() - start_time  \n",
    "    mb = bytes_processed / 1024 / 1024\n",
    "    throughput = mb / elapsed if elapsed > 0 else 0.0 # è®¡ç®—ååé‡å³æ¯ç§’å¤„ç†çš„æ•°æ®é‡\n",
    "    mem = get_memory_mb() \n",
    "\n",
    "    print(\n",
    "        f\"{prefix} | \"\n",
    "        f\"mem={mem:7.1f} MB | \"\n",
    "        f\"data={mb:8.1f} MB | \"\n",
    "        f\"speed={throughput:6.2f} MB/s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd40a5",
   "metadata": {},
   "source": [
    "Step2:è®­ç»ƒBPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38747fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9f8a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒBPE Tokenizer...\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=   89.2 MB | data=    19.1 MB | speed=217.19 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  108.4 MB | data=    38.2 MB | speed=226.17 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  126.9 MB | data=    57.2 MB | speed=228.38 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  146.1 MB | data=    76.3 MB | speed=228.28 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  165.3 MB | data=    95.4 MB | speed=229.61 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  184.3 MB | data=   114.5 MB | speed=230.22 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  204.4 MB | data=   133.5 MB | speed=218.34 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  223.6 MB | data=   152.6 MB | speed=223.29 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  242.7 MB | data=   171.7 MB | speed=218.71 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  261.9 MB | data=   190.8 MB | speed=208.73 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  281.0 MB | data=   209.8 MB | speed=212.70 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  300.1 MB | data=   228.9 MB | speed=215.92 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1382.3 MB | data=   248.0 MB | speed= 18.86 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1105.1 MB | data=   267.1 MB | speed= 20.00 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  146.9 MB | data=   286.1 MB | speed= 20.44 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  165.9 MB | data=   305.2 MB | speed= 21.69 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  185.0 MB | data=   324.3 MB | speed= 22.92 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  204.2 MB | data=   343.4 MB | speed= 24.09 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  223.3 MB | data=   362.4 MB | speed= 25.22 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  242.3 MB | data=   381.5 MB | speed= 26.41 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  261.5 MB | data=   400.6 MB | speed= 27.59 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  280.6 MB | data=   419.7 MB | speed= 28.72 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  299.7 MB | data=   438.7 MB | speed= 29.88 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  318.8 MB | data=   457.8 MB | speed= 31.02 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  337.9 MB | data=   476.9 MB | speed= 32.14 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1364.9 MB | data=   496.0 MB | speed= 18.76 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1217.4 MB | data=   515.0 MB | speed= 19.34 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  169.3 MB | data=   534.1 MB | speed= 19.55 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  170.4 MB | data=   553.2 MB | speed= 20.19 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  189.5 MB | data=   572.3 MB | speed= 20.83 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  208.6 MB | data=   591.3 MB | speed= 21.46 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  227.7 MB | data=   610.4 MB | speed= 22.09 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  246.8 MB | data=   629.5 MB | speed= 22.71 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  265.9 MB | data=   648.6 MB | speed= 23.34 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  285.1 MB | data=   667.6 MB | speed= 23.95 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  304.2 MB | data=   686.7 MB | speed= 24.57 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  323.3 MB | data=   705.8 MB | speed= 25.19 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  342.5 MB | data=   724.9 MB | speed= 25.79 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1354.1 MB | data=   743.9 MB | speed= 18.59 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1033.0 MB | data=   763.0 MB | speed= 18.97 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  176.6 MB | data=   782.1 MB | speed= 19.15 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  195.7 MB | data=   801.2 MB | speed= 19.58 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  214.8 MB | data=   820.3 MB | speed= 20.01 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  233.9 MB | data=   839.3 MB | speed= 20.44 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  252.8 MB | data=   858.4 MB | speed= 20.86 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  272.0 MB | data=   877.5 MB | speed= 21.28 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  291.1 MB | data=   896.6 MB | speed= 21.70 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  310.2 MB | data=   915.6 MB | speed= 22.12 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  329.3 MB | data=   934.7 MB | speed= 22.54 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  348.4 MB | data=   953.8 MB | speed= 22.96 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  367.6 MB | data=   972.9 MB | speed= 23.38 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1199.3 MB | data=   991.9 MB | speed= 18.69 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1027.4 MB | data=  1011.0 MB | speed= 18.99 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  183.8 MB | data=  1030.1 MB | speed= 19.13 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  202.8 MB | data=  1049.2 MB | speed= 19.46 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  221.8 MB | data=  1068.2 MB | speed= 19.79 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  240.8 MB | data=  1087.3 MB | speed= 20.10 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  259.9 MB | data=  1106.4 MB | speed= 20.42 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  279.0 MB | data=  1125.5 MB | speed= 20.75 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  298.1 MB | data=  1144.5 MB | speed= 21.07 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  317.2 MB | data=  1163.6 MB | speed= 21.39 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  336.4 MB | data=  1182.7 MB | speed= 21.72 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem=  355.5 MB | data=  1201.8 MB | speed= 22.04 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1384.4 MB | data=  1220.8 MB | speed= 18.61 MB/s\n",
      "ğŸ“˜ åˆ†è¯å™¨æµå¼å¤„ç† | mem= 1316.3 MB | data=  1239.9 MB | speed= 18.81 MB/s\n",
      "âœ… BPE Tokenizerè®­ç»ƒå®Œæˆ\n",
      "ğŸ’¾ åˆ†è¯å™¨å·²ä¿å­˜è‡³./bpe_tokenizer/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "def train_bpe_tokenizer(\n",
    "    train_file: str,\n",
    "    val_file: str | None = None,\n",
    "    vocab_size: int = 50257,\n",
    "    num_chunks: int = 8,\n",
    "    output_dir: str = \"./bpe_tokenizer\",\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    special_tokens = [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|unk|>\",\n",
    "        \"<|pad|>\",\n",
    "        \"<|bos|>\",\n",
    "        \"<|eos|>\",\n",
    "    ]\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<|unk|>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "\n",
    "    # è®¾è®¡GPT-2é£æ ¼çš„BPE Tokenizer\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True, \n",
    "    )\n",
    "\n",
    "    def text_iterator():\n",
    "        # è®­ç»ƒé›†\n",
    "        for chunk in iter_text_chunks_with_monitor(\n",
    "            train_file,\n",
    "            chunk_size=1_000_000,    # æ¯æ¬¡å¤„ç†å¤§å°çº¦ä¸º1MBçš„åŸå§‹æ•°æ®ï¼Œå¤„ç†å®Œå°±ä¼šæ¸…é™¤æ•°æ®æ–¹ä¾¿ç»§ç»­å¤„ç†\n",
    "            log_every=20,\n",
    "        ):\n",
    "            yield chunk\n",
    "\n",
    "        # éªŒè¯é›†ï¼ˆå¯é€‰ï¼‰\n",
    "        if val_file is not None:\n",
    "            for chunk in iter_text_chunks_with_monitor(\n",
    "                val_file,\n",
    "                chunk_size=1_000_000,\n",
    "                log_every=10,\n",
    "            ):\n",
    "                yield chunk\n",
    "\n",
    "\n",
    "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒBPE Tokenizer...\")\n",
    "    tokenizer.train_from_iterator(text_iterator(), trainer=trainer)\n",
    "    print(\"âœ… BPE Tokenizerè®­ç»ƒå®Œæˆ\")\n",
    "\n",
    "    tokenizer.save(os.path.join(output_dir, \"tokenizer.json\"))\n",
    "    print(f\"ğŸ’¾ åˆ†è¯å™¨å·²ä¿å­˜è‡³{output_dir}/tokenizer.json\")\n",
    "\n",
    "    return tokenizer\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"./TinyStoriesV2-GPt4-train.txt\"\n",
    "    val_path = \"./TinyStoriesV2-GPt4-valid.txt\"\n",
    "    tokenizer = train_bpe_tokenizer(\n",
    "        train_file=train_path,\n",
    "        val_file=val_path,\n",
    "        vocab_size=50257,     # è¯è¡¨å¤§å°ï¼ˆé€šå¸¸è®¾ä¸º32000æˆ–50257ç­‰ï¼‰\n",
    "        num_chunks=16,        # è¯»å–æ–‡ä»¶æ—¶çš„åˆ†å—æ•°é‡ï¼ˆå»ºè®®æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´ï¼‰\n",
    "        output_dir=\"./bpe_tokenizer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffb6d4",
   "metadata": {},
   "source": [
    "äº”ã€éªŒè¯è®­ç»ƒçš„BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c883422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä Hello', ',', 'Ä world', '!', 'Ä ', '<|endoftext|>']\n",
      "[8431, 16, 1501, 5, 149, 0]\n",
      "[' Hello', ',', ' world', '!', ' ', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\" Hello, world! <|endoftext|>\")\n",
    "print(encoded.tokens)    # æ‰“å°ç¼–ç åçš„tokenåºåˆ—\n",
    "print(encoded.ids)       # æ‰“å°ç¼–ç åçš„IDåºåˆ—\n",
    "# print(tokenizer.decode([1501])) # è¾“å‡ºåº”è¯¥æ˜¯\" world\"ï¼ˆå‰é¢å¸¦ä¸ªç©ºæ ¼ï¼‰\n",
    "\n",
    "# å°†Ä æ›¿æ¢å›ç©ºæ ¼\n",
    "clean_tokens = [t.replace('Ä ', ' ') for t in encoded.tokens]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58010263",
   "metadata": {},
   "source": [
    "Q: ä¸ºä»€ä¹ˆåœ¨GTP-2æ­£åˆ™åŒ–é£æ ¼ä¸­è®­ç»ƒå‡ºæ¥çš„tokenizerä¼šåœ¨å•è¯å‰æ·»åŠ ä¸€ä¸ª'Ä 'ï¼Ÿ\n",

    "A: å› ä¸ºå¯¹äºæ¨¡å‹è€Œè¨€ï¼Œåœ¨åŒä¸€ä¸ªå•è¯ä¸­ï¼Œæ²¡æœ‰å¸¦ç©ºæ ¼å’Œå¸¦æœ‰ç©ºæ ¼è¡¨ç¤ºçš„Tokenæ˜¯ä¸åŒçš„ï¼Œå› æ­¤ä¸ºäº†åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µå°±ä¼šæ·»åŠ äº†ä¸€ä¸ª'Ä 'æ¥è¡¨ç¤ºå¸¦ç©ºæ ¼çš„Tokenï¼ˆæ¯”å¦‚\"hello\"å’Œ\"Ä hello\"ï¼Œå‰è€…è¡¨ç¤º\"hello\"è¿™ä¸ªå•è¯ï¼Œåè€…è¡¨ç¤º\" hello\"è¿™ä¸ªå•è¯ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d202b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenç»Ÿè®¡å‡½æ•°\n",
    "def analyze_tokenizer(tokenizer, texts):\n",
    "    lengths = [len(tokenizer.encode(t).ids) for t in texts]\n",
    "    return {\n",
    "        \"avg_tokens\": sum(lengths) / len(lengths), # å¹³å‡å¤„ç†tokenæ•°\n",
    "        \"max_tokens\": max(lengths),                # æœ€å¤§tokenæ•°ï¼Œç”¨äºè®¾ç½®æœ€å¤§å¤„ç†åºåˆ—é•¿åº¦ï¼ˆå†³å®šæ˜¯å¦æˆªæ–­åºåˆ—å¤„ç†ï¼‰\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d246c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éšæœºæŠ½å–çš„è®­ç»ƒæ ·æœ¬æ•°: 20\n",
      "éšæœºæŠ½å–çš„éªŒè¯æ ·æœ¬æ•°: 10\n",
      "è®­ç»ƒé›†ç»Ÿè®¡: {'avg_tokens': 197.0, 'max_tokens': 308}\n",
      "éªŒè¯é›†ç»Ÿè®¡: {'avg_tokens': 208.2, 'max_tokens': 434}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def load_stories(file_path, num_samples=None):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    stories = [s.strip() for s in content.split(\"<|endoftext|>\") if s.strip()]\n",
    "    \n",
    "    if num_samples is not None:\n",
    "        n = min(num_samples, len(stories))\n",
    "        stories = random.sample(stories, k=n)\n",
    "    return stories\n",
    "\n",
    "train_path = \"./TinyStoriesV2-GPT4-train.txt\"\n",
    "val_path = \"./TinyStoriesV2-GPT4-valid.txt\"\n",
    "test_texts = load_stories(val_path, num_samples=10)\n",
    "train_texts = load_stories(train_path, num_samples=20)\n",
    "print(f\"éšæœºæŠ½å–çš„è®­ç»ƒæ ·æœ¬æ•°: {len(train_texts)}\")\n",
    "print(f\"éšæœºæŠ½å–çš„éªŒè¯æ ·æœ¬æ•°: {len(test_texts)}\")\n",
    "\n",
    "# åˆ†æè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„tokenç»Ÿè®¡\n",
    "train_stats = analyze_tokenizer(tokenizer, train_texts)\n",
    "val_stats = analyze_tokenizer(tokenizer, test_texts)\n",
    "print(\"è®­ç»ƒé›†ç»Ÿè®¡:\", train_stats)\n",
    "print(\"éªŒè¯é›†ç»Ÿè®¡:\", val_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
