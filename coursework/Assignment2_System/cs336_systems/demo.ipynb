{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daa8d39",
   "metadata": {},
   "source": [
    "# 我们将完成\n",
    "1. 基准测试和性能分析框架\n",
    "2. Flash Attention \n",
    "2. Triton 内核\n",
    "3. 分布式数据并行训练\n",
    "4. 优化器状态分片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d935",
   "metadata": {},
   "source": [
    "# 1. 基准测试和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9210e8",
   "metadata": {},
   "source": [
    "## 首先检查GPU是否可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea2358",
   "metadata": {},
   "source": [
    "这边通过pyproject.toml下载的torch是cpu版本\n",
    "如果要暗转GPU版本：\n",
    "```bash\n",
    "uv pip unstall torch \n",
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "cu121是我的cuda版本为12.1，可以改为自己的cuda版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765a294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "import math\n",
    "import time\n",
    "from cs336_basics.model import BasicsTransformerLM \n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "from cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.data import get_batch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA GPU detected\")\n",
    "\n",
    "print(\"MPS（苹果的MPS） available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2625f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b8414",
   "metadata": {},
   "source": [
    "现在来测试一下sleep函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7986b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\"sleep\", lambda : time.sleep(50 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6048",
   "metadata": {},
   "source": [
    "我们来实测一下transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[339, 340, 341, 342, 343, 344, 345, 346],\n",
      "        [114, 115, 116, 117, 118, 119, 120, 121],\n",
      "        [430, 431, 432, 433, 434, 435, 436, 437],\n",
      "        [579, 580, 581, 582, 583, 584, 585, 586]])\n",
      "tensor([[340, 341, 342, 343, 344, 345, 346, 347],\n",
      "        [115, 116, 117, 118, 119, 120, 121, 122],\n",
      "        [431, 432, 433, 434, 435, 436, 437, 438],\n",
      "        [580, 581, 582, 583, 584, 585, 586, 587]])\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "context_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "rope_theta = 10000.0\n",
    "\n",
    "max_lr = 3e-4\n",
    "min_lr = 3e-5\n",
    "warmup_iters = 200\n",
    "cosine_cycle_iters = 10_000\n",
    "num_steps = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BasicsTransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    rope_theta=rope_theta,\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=max_lr,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    # logits: (B, T, V)\n",
    "    # targets: (B, T)\n",
    "    B, T, V = logits.shape\n",
    "    return F.cross_entropy(\n",
    "        logits.view(B * T, V),\n",
    "        targets.view(B * T),\n",
    "    )\n",
    "import numpy as np\n",
    "\n",
    "# 假设是 token 序列\n",
    "dataset = np.arange(1000, dtype=np.int64)\n",
    "x, y = get_batch(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    context_length=8,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(x.shape)  # (4, 8)\n",
    "print(y.shape)  # (4, 8)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "\n",
    "model.train()\n",
    "print(f'设备：{device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c8295b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(num_steps = 1):\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        # 1️⃣ 更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # 2️⃣ 取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # 3️⃣ 前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4️⃣ loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5️⃣ 反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6️⃣ 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7️⃣ 日志\n",
    "    if it % 100 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fadb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f16aabb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.5185 | lr 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "run_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51e3f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.5236 | lr 0.00e+00\n",
      "现在真正计时!\n",
      "0/1\n",
      "step      0 | loss 10.5171 | lr 0.00e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3180.018663406372"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dcaf6",
   "metadata": {},
   "source": [
    "## 推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bb75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor([[ 6338, 39429, 49762,  2890, 38374]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 用 get_batch 拿一个 batch\n",
    "    x, _ = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=1,\n",
    "        context_length= 1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 取第一个样本作为 prompt\n",
    "    prompt = x # shape: (context_length,)\n",
    "    # 调用模型自带的 generate\n",
    "    generated = model.generate(\n",
    "        x=prompt,\n",
    "        max_new_tokens=5,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        eos_token_id=None,\n",
    "    )\n",
    "\n",
    "    print(generated.shape)  # (<=50,)\n",
    "    print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ad903",
   "metadata": {},
   "source": [
    "### 性能分析工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19c38995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c5ba23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    \n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(sort_by=\"cuda_time_total\",\n",
    "                                      max_name_column_width=80,\n",
    "                                      row_limit=10)\n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.5191 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.5105 | lr 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "tabel = profile(description ='transformer' , run = run_model, num_warmups = 1, with_stack = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca9462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                               aten::bmm         1.98%       7.168ms         1.98%       7.168ms      43.440us     161.452ms        33.52%     161.452ms     978.497us           165  \n",
      "                       autograd::engine::evaluate_function: BmmBackward0         0.36%       1.322ms         7.39%      26.801ms     487.295us     361.000us         0.07%     113.495ms       2.064ms            55  \n",
      "                                                            BmmBackward0         5.07%      18.394ms         7.02%      25.480ms     463.265us       3.601ms         0.75%     113.134ms       2.057ms            55  \n",
      "                                               Optimizer.step#AdamW.step         5.55%      20.136ms        22.71%      82.393ms      82.393ms       9.634ms         2.00%      91.673ms      91.673ms             1  \n",
      "                                                               aten::mul         2.70%       9.790ms         2.70%       9.790ms      15.841us      72.990ms        15.15%      72.990ms     118.107us           618  \n",
      "                                                            aten::einsum         3.23%      11.728ms         7.00%      25.415ms     462.085us       4.252ms         0.88%      68.555ms       1.246ms            55  \n",
      "                       autograd::engine::evaluate_function: MulBackward0         1.06%       3.847ms         2.80%      10.156ms     118.098us       1.842ms         0.38%      33.928ms     394.512us            86  \n",
      "                                                            MulBackward0         0.77%       2.778ms         1.39%       5.046ms      58.671us     797.000us         0.17%      30.539ms     355.105us            86  \n",
      "                                                               aten::add         0.83%       3.005ms         0.83%       3.005ms       8.610us      20.945ms         4.35%      20.945ms      60.014us           349  \n",
      "                autograd::engine::evaluate_function: LogSoftmaxBackward0         0.00%      12.900us         0.03%      95.800us      95.800us       3.000us         0.00%      17.768ms      17.768ms             1  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 362.854ms\n",
      "Self CUDA time total: 481.717ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dbd6",
   "metadata": {},
   "source": [
    "# Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cbd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2_System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
