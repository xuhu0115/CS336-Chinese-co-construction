{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daa8d39",
   "metadata": {},
   "source": [
    "# 我们将完成\n",
    "1. 基准测试和性能分析框架\n",
    "2. Flash Attention \n",
    "2. Triton 内核\n",
    "3. 分布式数据并行训练\n",
    "4. 优化器状态分片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192270c9",
   "metadata": {},
   "source": [
    "# 前期准备工作\n",
    "\n",
    "## 在windows上安装Ubuntu-22.04\n",
    "\n",
    "由于triton在windows没有可用的版本，考虑到大家都是使用的windows系统的比较多，这里附一个安装教程，有windows系统的同学可以跳过\n",
    "\n",
    "### 安装步骤\n",
    "\n",
    "#### 1. 版本限制\n",
    "\n",
    "我们需要在windows上安装wsl2，限制在Windows 10 2004 (19041) 及以上 或者 Windows 11 任意版本\n",
    "\n",
    "#### 2.手动启用 WSL 相关功能（关键）\n",
    "\n",
    "按住win键搜索**启用或者关闭windows功能**，找到并勾选 **于Linux的Windows子系统**和 **虚拟机平台**，点击确定\n",
    "\n",
    "\n",
    "#### 3. 确认 CPU 虚拟化已开启\n",
    "\n",
    "按住Ctrl + Shift + Esc打开任务管理器，性能 -> CPU ，右侧应显示：虚拟化：已启用\n",
    "\n",
    "如果是“未启用”\n",
    "\n",
    "需要进 BIOS / UEFI，开启：\n",
    "\n",
    "Intel：Intel Virtualization Technology (VT-x)\n",
    "\n",
    "AMD：SVM Mode\n",
    "\n",
    "##### 4. 安装 Ubuntu\n",
    "\n",
    "在windows搜索页面中输入CMD，并且以管理员身份运行\n",
    "\n",
    "```bash\n",
    "wsl --install -d Ubuntu\n",
    "```\n",
    "\n",
    "之后设置账号密码，密码在输入的时候是不可见的，看不到是正常的\n",
    "\n",
    "要打开ubuntu的命令行只需要在cmd中上面一行的小三角中打开就行\n",
    "\n",
    "##### 5. 安装相关的工具 pip uv 等\n",
    "\n",
    "依次执行\n",
    "先使用管理员身份打开CMD\n",
    "\n",
    "输入\n",
    "```bash\n",
    "cd $env:USERPROFILE\n",
    "@\"\n",
    "[wsl2]\n",
    "networkingMode=mirrored\n",
    "dnsTunneling=true\n",
    "autoProxy=true\n",
    "firewall=true\n",
    "\"@ | Out-File -Encoding UTF8 .wslconfig\n",
    "\n",
    "\n",
    "wsl --shutdown     # 彻底关掉虚拟机\n",
    "wsl                # 再进子系统\n",
    "```\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install python3-pip\n",
    "\n",
    "sudo apt install python3.12-venv\n",
    "\n",
    "source ~/myenv/bin/activate\n",
    "pip install uv -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "sudo ln -s /home/kangkang/myenv/bin/uv /usr/local/bin/uv\n",
    "\n",
    "echo 'export UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple' >> ~/.bashrc\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### 6. 设置vscode的远程连接\n",
    "\n",
    "打开vscode ，先搜索插件wsl。安装完成后打开左侧一列的远程连接就可以连接到wsl的ubuntu系统中\n",
    "\n",
    "\n",
    "运行在CS336-Chinese-co-construction\\coursework\\Assignment2_System下\n",
    "```bash\n",
    "uv venv \n",
    "cd cs336_basics\n",
    "uv run\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d935",
   "metadata": {},
   "source": [
    "# 1. 基准测试和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc1cf0",
   "metadata": {},
   "source": [
    "编写一个脚本，用于对模型中的前向和反向传递执行基本的端到端基准测试。具体而言，脚本应支持以下功能\n",
    "- 给定超参数（例如，层数），初始化一个模型。\n",
    "- 生成一批随机数据。\n",
    "- 在开始计时前，先运行 个预热步骤；随后，测量执行 个步骤所需的时间（根据参数设置，可仅进行正向传播，或同时包含正向与反向传播）。\n",
    "- 每个步骤后调用 torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9210e8",
   "metadata": {},
   "source": [
    "## 首先检查GPU是否可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea2358",
   "metadata": {},
   "source": [
    "这边通过pyproject.toml下载的torch是cpu版本\n",
    "如果要安装GPU版本：\n",
    "```bash\n",
    "uv pip uninstall torch \n",
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "cu121是我的cuda版本为12.1，可以改为自己的cuda版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bb1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.1+cu121\n",
      "cuda : 12.1\n",
      "CUPTI: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "import torch, ctypes, os\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda :', torch.version.cuda)\n",
    "try:\n",
    "    ctypes.CDLL('libcupti.so')\n",
    "    print('CUPTI: OK')\n",
    "except:\n",
    "    print('CUPTI: NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705a57e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "MPS（苹果的MPS） available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cs336_basics.model import BasicsTransformerLM \n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "from cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.data import get_batch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA GPU detected\")\n",
    "\n",
    "print(\"MPS（苹果的MPS） available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2625f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "    return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b8414",
   "metadata": {},
   "source": [
    "现在来测试一下sleep函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7986b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.16009012858073"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(\"sleep\", lambda : time.sleep(50 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6048",
   "metadata": {},
   "source": [
    "我们来实测一下transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[186, 187, 188, 189, 190, 191, 192, 193],\n",
      "        [673, 674, 675, 676, 677, 678, 679, 680],\n",
      "        [818, 819, 820, 821, 822, 823, 824, 825],\n",
      "        [810, 811, 812, 813, 814, 815, 816, 817]], device='cuda:0')\n",
      "tensor([[187, 188, 189, 190, 191, 192, 193, 194],\n",
      "        [674, 675, 676, 677, 678, 679, 680, 681],\n",
      "        [819, 820, 821, 822, 823, 824, 825, 826],\n",
      "        [811, 812, 813, 814, 815, 816, 817, 818]], device='cuda:0')\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "context_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "rope_theta = 10000.0\n",
    "\n",
    "max_lr = 3e-4\n",
    "min_lr = 3e-5\n",
    "warmup_iters = 200\n",
    "cosine_cycle_iters = 10_000\n",
    "num_steps = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BasicsTransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    rope_theta=rope_theta,\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=max_lr,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    # logits: (B, T, V)\n",
    "    # targets: (B, T)\n",
    "    B, T, V = logits.shape\n",
    "    return F.cross_entropy(\n",
    "        logits.view(B * T, V),\n",
    "        targets.view(B * T),\n",
    "    )\n",
    "import numpy as np\n",
    "\n",
    "# 假设是 token 序列\n",
    "dataset = np.arange(1000, dtype=np.int64)\n",
    "x, y = get_batch(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    context_length=8,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(x.shape)  # (4, 8)\n",
    "print(y.shape)  # (4, 8)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "\n",
    "model.train()\n",
    "print(f'设备：{device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8295b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "tensor([[482, 483, 484,  ..., 607, 608, 609],\n",
      "        [645, 646, 647,  ..., 770, 771, 772],\n",
      "        [419, 420, 421,  ..., 544, 545, 546],\n",
      "        ...,\n",
      "        [ 85,  86,  87,  ..., 210, 211, 212],\n",
      "        [614, 615, 616,  ..., 739, 740, 741],\n",
      "        [740, 741, 742,  ..., 865, 866, 867]], device='cuda:0')\n",
      "step      0 | loss 10.8311 | lr 0.00e+00\n",
      "现在真正计时!\n",
      "0/1\n",
      "tensor([[ 53,  54,  55,  ..., 178, 179, 180],\n",
      "        [374, 375, 376,  ..., 499, 500, 501],\n",
      "        [536, 537, 538,  ..., 661, 662, 663],\n",
      "        ...,\n",
      "        [335, 336, 337,  ..., 460, 461, 462],\n",
      "        [535, 536, 537,  ..., 660, 661, 662],\n",
      "        [457, 458, 459,  ..., 582, 583, 584]], device='cuda:0')\n",
      "step      0 | loss 10.8309 | lr 0.00e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "442.5199031829834"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model(num_steps = 1):\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7日志\n",
    "    if it % 100 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dcaf6",
   "metadata": {},
   "source": [
    "## 推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bb75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor([[19979, 43534,  7514, 44081, 28613]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 用 get_batch 拿一个 batch\n",
    "    x, _ = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=1,\n",
    "        context_length= 1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 取第一个样本作为 prompt\n",
    "    prompt = x # shape: (context_length,)\n",
    "    # 调用模型自带的 generate\n",
    "    generated = model.generate(\n",
    "        x=prompt,\n",
    "        max_new_tokens=5,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        eos_token_id=None,\n",
    "    )\n",
    "\n",
    "    print(generated.shape)  # (<=50,)\n",
    "    print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ad903",
   "metadata": {},
   "source": [
    "### 性能分析工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c38995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "tensor([[133, 134, 135,  ..., 258, 259, 260],\n",
      "        [798, 799, 800,  ..., 923, 924, 925],\n",
      "        [ 37,  38,  39,  ..., 162, 163, 164],\n",
      "        ...,\n",
      "        [771, 772, 773,  ..., 896, 897, 898],\n",
      "        [517, 518, 519,  ..., 642, 643, 644],\n",
      "        [565, 566, 567,  ..., 690, 691, 692]], device='cuda:0')\n",
      "step      0 | loss 10.8347 | lr 0.00e+00\n",
      "0/1\n",
      "tensor([[754, 755, 756,  ..., 879, 880, 881],\n",
      "        [762, 763, 764,  ..., 887, 888, 889],\n",
      "        [ 84,  85,  86,  ..., 209, 210, 211],\n",
      "        ...,\n",
      "        [648, 649, 650,  ..., 773, 774, 775],\n",
      "        [610, 611, 612,  ..., 735, 736, 737],\n",
      "        [678, 679, 680,  ..., 803, 804, 805]], device='cuda:0')\n",
      "step      0 | loss 10.8314 | lr 0.00e+00\n",
      "0/1\n",
      "tensor([[803, 804, 805,  ..., 928, 929, 930],\n",
      "        [104, 105, 106,  ..., 229, 230, 231],\n",
      "        [537, 538, 539,  ..., 662, 663, 664],\n",
      "        ...,\n",
      "        [756, 757, 758,  ..., 881, 882, 883],\n",
      "        [276, 277, 278,  ..., 401, 402, 403],\n",
      "        [552, 553, 554,  ..., 677, 678, 679]], device='cuda:0')\n",
      "step      0 | loss 10.8344 | lr 0.00e+00\n",
      "正在使用cuda\n",
      "0/1\n",
      "tensor([[291, 292, 293,  ..., 416, 417, 418],\n",
      "        [868, 869, 870,  ..., 993, 994, 995],\n",
      "        [659, 660, 661,  ..., 784, 785, 786],\n",
      "        ...,\n",
      "        [227, 228, 229,  ..., 352, 353, 354],\n",
      "        [716, 717, 718,  ..., 841, 842, 843],\n",
      "        [808, 809, 810,  ..., 933, 934, 935]], device='cuda:0')\n",
      "step      0 | loss 10.8337 | lr 0.00e+00\n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                           aten::randint         0.24%       1.034ms         0.25%       1.061ms       1.061ms             1  \n",
      "                                                             aten::empty         0.12%     533.111us         0.12%     533.111us       5.496us            97  \n",
      "                                                           aten::random_         0.00%      16.536us         0.00%      16.536us      16.536us             1  \n",
      "                                                            aten::unbind         0.22%     942.799us         0.39%       1.676ms      62.076us            27  \n",
      "                                                            aten::select         0.18%     760.692us         0.19%     822.527us       5.141us           160  \n",
      "                                                        aten::as_strided         0.19%     809.227us         0.19%     809.227us       0.579us          1398  \n",
      "                                                               aten::add         0.64%       2.773ms         7.25%      31.383ms      89.923us           349  \n",
      "                                                              aten::item         0.05%     231.074us        38.51%     166.672ms       1.010ms           165  \n",
      "                                               aten::_local_scalar_dense         0.14%     586.288us        38.46%     166.441ms       1.009ms           165  \n",
      "                                                        aten::lift_fresh         0.00%       9.077us         0.00%       9.077us       0.142us            64  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 432.775ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "tabel = profile(description ='transformer' , run = run_model, num_warmups = 3, with_stack = True)\n",
    "print(tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dbd6",
   "metadata": {},
   "source": [
    "# Flash Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cbd65",
   "metadata": {},
   "source": [
    "我们将在这里实现一个flash attention，我们需要实现两个部分，一个是分块，第二个是数值稳定softmax的方式，思想上非常接近 FlashAttention v1，具体可以看第六章：GPU和GPU相关的优化，有详细介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c8f8",
   "metadata": {},
   "source": [
    "### **FlashAttention 中「分块（blocking）」与「数值稳定 softmax」为什么能带来加速**\n",
    "\n",
    "\n",
    "### 一、加速原理\n",
    "\n",
    "FlashAttention 的加速**不是因为算得更少 FLOPs**，而是因为：\n",
    "\n",
    "> **极大减少了显存读写（memory I/O），并将计算变成“流式（streaming）”**\n",
    "\n",
    "核心公式仍然是论文中的公式：\n",
    "\n",
    "$$\n",
    "\\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "但**计算路径完全改变**。\n",
    "\n",
    "####  标准 Attention 的真实成本\n",
    "\n",
    "以 sequence length = (N)，head dim = (d)：\n",
    "\n",
    "| 操作      | FLOPs      | 显存占用          |\n",
    "| ------- | ---------- | ------------- |\n",
    "| QKᵀ     | (O(N^2 d)) | **存 N×N**     |\n",
    "| softmax | (O(N^2))   | **读 + 写 N×N** |\n",
    "| AV      | (O(N^2 d)) | 读 N×N         |\n",
    "\n",
    "**关键瓶颈不是算力，而是显存带宽**\n",
    "\n",
    "在 GPU 上：N×N的attention matrix **必须写入 HBM**，* 然后再 **从 HBM 读回** 做 softmax，大模型场景下 **显存访问 >> 计算时间**。\n",
    "\n",
    "#### GPU 性能的现实约束\n",
    "\n",
    "GPU 有三种“速度等级”存储：\n",
    "\n",
    "| 存储            | 速度 | 容量 |\n",
    "| ------------- | -- | -- |\n",
    "| Register      | 极快 | 极小 |\n",
    "| Shared Memory | 快  | 小  |\n",
    "| HBM (显存)      | 慢  | 很大 |\n",
    "\n",
    "标准 Attention 的问题是：\n",
    "\n",
    "> **N×N attention matrix 太大，只能进 HBM**\n",
    "\n",
    "#### 分块（Blocking）为什么能加速\n",
    "\n",
    "**分块的本质就是让数据“活在片上”**\n",
    "\n",
    "FlashAttention 将 Attention 重写为：\n",
    "\n",
    "```text\n",
    "for each Q_block:\n",
    "    for each K/V_block:\n",
    "        计算局部 QK^T\n",
    "        立刻参与 softmax\n",
    "        立刻乘 V\n",
    "```\n",
    "\n",
    "核心变化是不再生成完整的 QKᵀ，不再写入 N×N matrix，局部结果 **只存在寄存器 / shared memory**。从**全文总结变成了走一步看一步**。\n",
    "\n",
    "#### 分块前 vs 分块后（显存行为对比）\n",
    "\n",
    "**标准 Attention**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  写 QK^T (N×N)\n",
    "  读 QK^T\n",
    "  写 softmax(QK^T)\n",
    "  读 softmax(QK^T)\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N²)**\n",
    "\n",
    "---\n",
    "\n",
    "**FlashAttention（分块）**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  读 Q_block\n",
    "  读 K_block, V_block\n",
    "  写 O_block\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N·d)**\n",
    "\n",
    "**这是数量级的下降**\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么“只对 K/V 分块也有效”\n",
    "\n",
    "分块有两种实现：\n",
    "\n",
    "* Q 不分块\n",
    "* Q 分块（论文 / CUDA 版）\n",
    "\n",
    "两者都满足：\n",
    "\n",
    "> **K/V 被流式读取，attention matrix 不落地**\n",
    "\n",
    "Q 分块是为了，提高 occupancy，并且控制寄存器使用，适配 CUDA block 结构。\n",
    "\n",
    "---\n",
    "\n",
    "#### 二、在线 softmax 的加速原理”\n",
    "\n",
    "**标准 softmax 的结构问题**\n",
    "\n",
    "标准 softmax：\n",
    "\n",
    "```python\n",
    "S = QK^T\n",
    "P = exp(S) / sum(exp(S))\n",
    "```\n",
    "\n",
    "问题在于softmax **必须看完整一行**，无法流式，依赖全量 S。\n",
    "\n",
    "**这强制我们先存下整个 QKᵀ**\n",
    "\n",
    "\n",
    "**running max / running sum 的革命性意义**\n",
    "\n",
    "FlashAttention 使用：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_i &= \\max_j S_{ij} \\\n",
    "l_i &= \\sum_j e^{S_{ij} - m_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "并改写为 **可递推形式**：\n",
    "\n",
    "$$\n",
    "m_i^{(t)} = \\max(m_i^{(t-1)}, \\max S_{ij}^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "l_i^{(t)} =\n",
    "l_i^{(t-1)} e^{m_i^{(t-1)} - m_i^{(t)}} +\n",
    "\\sum e^{S_{ij}^{(t)} - m_i^{(t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "这一步“决定了 FlashAttention 能不能存在”，它带来了三件**决定性好处**，将softmax 变成 **在线算法**。\n",
    "\n",
    "再这个算法中我们不需要完整 S，每来一个 block 就能更新，天然适配 streaming。\n",
    "\n",
    "**softmax 与 AV 可以融合**\n",
    "\n",
    "标准 Attention：\n",
    "\n",
    "```text\n",
    "QK^T → softmax → @V\n",
    "```\n",
    "\n",
    "FlashAttention：\n",
    "\n",
    "```text\n",
    "(QK^T → exp → sum → @V)  全部在 block 内完成\n",
    "```\n",
    "\n",
    "**算子融合（kernel fusion）成为可能**\n",
    "\n",
    "\n",
    "**backward 也不需要 attention matrix**\n",
    "\n",
    "由于 forward 保存了：\n",
    "\n",
    "* `L = logsumexp`\n",
    "* `O = softmax @ V`\n",
    "\n",
    "backward 可写成：\n",
    "\n",
    "$$\n",
    "dS_{ij} = P_{ij} (dP_{ij} - D_i)\n",
    "$$\n",
    "\n",
    "**反向同样 O(N) 显存**\n",
    "\n",
    "> FlashAttention 的加速不是减少计算，而是通过 **分块 + 在线 softmax**，\n",
    "> **避免了 N×N attention matrix 的显存读写**，\n",
    "> 将 attention 计算变成一个 **完全流式、可融合的过程**，\n",
    "> 从而把瓶颈从显存带宽转回到算力。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a41b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    FlashAttention（forward + backward）\n",
    "\n",
    "    输入：\n",
    "        Q: (B, Nq, d)\n",
    "        K: (B, Nk, d)\n",
    "        V: (B, Nk, d)\n",
    "\n",
    "    输出：\n",
    "        O: (B, Nq, d)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        # -----------------------------\n",
    "        # block size\n",
    "        # -----------------------------\n",
    "        Bq = 64  # Query block 的尺寸\n",
    "        Bk = 64  # Key / Value block 的尺寸\n",
    "\n",
    "        # 我们将KV矩阵分成一个个小矩阵\n",
    "\n",
    "        Tq = Nq // Bq  # Q block 数\n",
    "        Tk = Nk // Bk  # K/V block 数\n",
    "\n",
    "        scale = d ** -0.5\n",
    "\n",
    "        # -----------------------------\n",
    "        # 输出与中间量\n",
    "        # -----------------------------\n",
    "        O = torch.zeros_like(Q)              # Attention 输出\n",
    "        L = torch.zeros(B, Nq, device=Q.device)  # log-sum-exp（给 backward 用）\n",
    "\n",
    "        # =========================================================\n",
    "        # 对 batch 维度显式循环\n",
    "        # =========================================================\n",
    "        for b in range(B):\n",
    "            Q_b = Q[b]  # (Nq, d)\n",
    "            K_b = K[b]  # (Nk, d)\n",
    "            V_b = V[b]  # (Nk, d)\n",
    "\n",
    "            # =====================================================\n",
    "            # 外层循环：Query block\n",
    "            # =====================================================\n",
    "            for i in range(Tq):\n",
    "                q_i = Q_b[i * Bq:(i + 1) * Bq]  # (Bq, d)\n",
    "\n",
    "                # -----------------------------\n",
    "                # FlashAttention 核心状态\n",
    "                # -----------------------------\n",
    "                m = torch.full((Bq, 1), -float(\"inf\"), device=Q.device)  # running max\n",
    "                l = torch.zeros((Bq, 1), device=Q.device)               # running sum\n",
    "                O_acc = torch.zeros((Bq, d), device=Q.device)           # 输出累积（未归一化）\n",
    "\n",
    "                # =================================================\n",
    "                # 内层循环：Key / Value block\n",
    "                # =================================================\n",
    "                for j in range(Tk):\n",
    "                    k_j = K_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "                    v_j = V_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 计算局部 attention score\n",
    "                    # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale  # (Bq, Bk)\n",
    "\n",
    "                    # 当前 block 内，每一行的最大值\n",
    "                    row_max = S.max(dim=1, keepdim=True).values  # (Bq, 1)\n",
    "\n",
    "                    # 更新 running max\n",
    "                    m_new = torch.maximum(m, row_max)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 数值稳定 softmax（FlashAttention 关键）\n",
    "                    # --------------------------------------------\n",
    "                    # 旧 block 贡献修正\n",
    "                    exp_old = torch.exp(m - m_new) * l\n",
    "\n",
    "                    # 当前 block 的 exp\n",
    "                    P = torch.exp(S - m_new)\n",
    "\n",
    "                    # 更新分母\n",
    "                    l = exp_old + P.sum(dim=1, keepdim=True)\n",
    "\n",
    "                    # 更新未归一化输出\n",
    "                    O_acc = (\n",
    "                        torch.exp(m - m_new) * O_acc +\n",
    "                        P @ v_j\n",
    "                    )\n",
    "\n",
    "                    # 写回 running max\n",
    "                    m = m_new\n",
    "\n",
    "                # =================================================\n",
    "                # block 内 softmax 归一化\n",
    "                # =================================================\n",
    "                O_i = O_acc / l  # (Bq, d)\n",
    "\n",
    "                # log-sum-exp：L = m + log(l)\n",
    "                L_i = m.squeeze(1) + torch.log(l.squeeze(1))\n",
    "\n",
    "                # 写回全局输出\n",
    "                O[b, i * Bq:(i + 1) * Bq] = O_i\n",
    "                L[b, i * Bq:(i + 1) * Bq] = L_i\n",
    "\n",
    "        # 保存 backward 所需变量\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.scale = scale\n",
    "        ctx.Bq = Bq\n",
    "        ctx.Bk = Bk\n",
    "\n",
    "        return O\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, L = ctx.saved_tensors\n",
    "        scale = ctx.scale\n",
    "        Bq = ctx.Bq\n",
    "        Bk = ctx.Bk\n",
    "\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Tq = Nq // Bq\n",
    "        Tk = Nk // Bk\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # FlashAttention backward 中的重要中间量\n",
    "        # D_i = sum_j O_ij * dO_ij\n",
    "        # --------------------------------------------\n",
    "        D = torch.sum(O * dO, dim=-1)  # (B, Nq)\n",
    "\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        for b in range(B):\n",
    "            for i in range(Tq):\n",
    "                q_i = Q[b, i * Bq:(i + 1) * Bq]\n",
    "                dO_i = dO[b, i * Bq:(i + 1) * Bq]\n",
    "                L_i = L[b, i * Bq:(i + 1) * Bq]\n",
    "                D_i = D[b, i * Bq:(i + 1) * Bq]\n",
    "\n",
    "                for j in range(Tk):\n",
    "                    k_j = K[b, j * Bk:(j + 1) * Bk]\n",
    "                    v_j = V[b, j * Bk:(j + 1) * Bk]\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 重算局部 attention 概率 P_ij\n",
    "                    # P_ij = exp(S_ij - L_i)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale\n",
    "                    P = torch.exp(S - L_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dV --------\n",
    "                    dV[b, j * Bk:(j + 1) * Bk] += P.T @ dO_i\n",
    "\n",
    "                    # -------- dS --------\n",
    "                    dP = dO_i @ v_j.T\n",
    "                    dS = P * (dP - D_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dQ --------\n",
    "                    dQ[b, i * Bq:(i + 1) * Bq] += dS @ k_j * scale\n",
    "\n",
    "                    # -------- dK --------\n",
    "                    dK[b, j * Bk:(j + 1) * Bk] += dS.T @ q_i * scale\n",
    "\n",
    "        return dQ, dK, dV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0992c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "正在使用cuda\n",
      "[FlashAttention] B=1, N=1024, d=64, time=63135.068 ms\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                           FlashAttentionAutograd        23.36%      20.047ms        99.99%      85.825ms      85.825ms             1  \n",
      "                                 aten::zeros_like         0.01%      10.541us         2.81%       2.415ms       2.415ms             1  \n",
      "                                 aten::empty_like         0.01%       8.343us         2.73%       2.340ms       2.340ms             1  \n",
      "                              aten::empty_strided         2.72%       2.331ms         2.72%       2.331ms       2.331ms             1  \n",
      "                                      aten::zero_         0.07%      59.856us         0.48%     412.211us      12.124us            34  \n",
      "                                      aten::fill_         0.24%     204.323us         0.67%     573.769us      11.475us            50  \n",
      "                                 cudaLaunchKernel        32.09%      27.542ms        32.09%      27.542ms       5.853us          4706  \n",
      "                                      aten::zeros         0.06%      54.779us         0.60%     512.999us      15.545us            33  \n",
      "                                      aten::empty         0.22%     186.845us         0.22%     186.845us       3.813us            49  \n",
      "                                     aten::select         0.14%     119.916us         0.16%     141.330us       4.038us            35  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 85.832ms\n",
      "\n",
      "现在真正计时!\n",
      "正在使用cuda\n",
      "[NaiveAttention]  B=1, N=1024, d=64, time=233.173 ms\n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "          aten::transpose         2.21%      12.052us         2.66%      14.467us      14.467us             1  \n",
      "         aten::as_strided         1.20%       6.514us         1.20%       6.514us       1.303us             5  \n",
      "             aten::matmul         4.40%      23.985us        86.57%     471.671us     235.836us             2  \n",
      "             aten::expand         1.82%       9.931us         2.57%      14.030us       3.508us             4  \n",
      "            aten::reshape         1.06%       5.749us         2.39%      13.043us       3.261us             4  \n",
      "               aten::view         1.02%       5.553us         1.02%       5.553us       1.851us             3  \n",
      "     aten::_reshape_alias         0.32%       1.741us         0.32%       1.741us       1.741us             1  \n",
      "                aten::bmm        65.62%     357.522us        75.79%     412.961us     206.480us             2  \n",
      "         cudaLaunchKernel        12.39%      67.505us        12.39%      67.505us      16.876us             4  \n",
      "       aten::_unsafe_view         1.40%       7.652us         1.40%       7.652us       3.826us             2  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 544.865us\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_flash_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionAutograd.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_flash_attention',run = run)\n",
    "    t1 = profile(description ='test_flash_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttention] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [1024]:\n",
    "    test_flash_attention(B=1, N=N, d=64, device=device)\n",
    "    test_naive_attention(B=1, N=N, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a110c30",
   "metadata": {},
   "source": [
    "# 2. Triton 内核实现flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d049031",
   "metadata": {},
   "source": [
    "Triton 是一个用 Python 写 GPU kernel 的 DSL，本质上是“显式控制数据分块与内存层级的编程模型”，它不是NumPy，也不是高层 PyTorch API，它是一个 CUDA kernel 的“可读版本”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12251ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sympy import Q\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def flash_attention_fwd_kernel(\n",
    "    # -------- 全局内存指针 --------\n",
    "    Q_ptr, K_ptr, V_ptr,           # 输入 Q, K, V\n",
    "    O_ptr, L_ptr,                  # 输出 O, logsumexp L\n",
    "\n",
    "    # -------- stride 信息（用于 block ptr）--------\n",
    "    stride_qb, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vn, stride_vd,\n",
    "    stride_ob, stride_on, stride_od,\n",
    "    stride_lb, stride_ln,\n",
    "\n",
    "    # -------- 序列长度 --------\n",
    "    Nq, Nk,\n",
    "\n",
    "    # -------- scale --------\n",
    "    scale,\n",
    "\n",
    "    # -------- 编译期常量 --------\n",
    "    D: tl.constexpr,\n",
    "    Q_BLOCK: tl.constexpr,\n",
    "    K_BLOCK: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    每个 Triton program 负责：\n",
    "        - 一个 batch\n",
    "        - 一个 Query block（Q_BLOCK 行）\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Triton 并行索引\n",
    "    # ------------------------------------------------\n",
    "    q_block_id = tl.program_id(0)     # 第几个 Q block\n",
    "    batch_id   = tl.program_id(1)     # 第几个 batch\n",
    "\n",
    "    # =================================================\n",
    "    # 构造 block pointer（这是 Triton 的关键）\n",
    "    # =================================================\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_qn, stride_qd),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # ⚠️ K / V 从第 0 行开始，后面在 loop 中 advance\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K_ptr + batch_id * stride_kb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_kn, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V_ptr + batch_id * stride_vb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_vn, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O_ptr + batch_id * stride_ob,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_on, stride_od),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        base=L_ptr + batch_id * stride_lb,\n",
    "        shape=(Nq, 1),\n",
    "        strides=(stride_ln, 1),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # =================================================\n",
    "    # 加载 Query block\n",
    "    # =================================================\n",
    "    Q_i = tl.load(Q_block_ptr)  # (Q_BLOCK, D)\n",
    "\n",
    "    # =================================================\n",
    "    # FlashAttention 核心状态（每个 Q block 独立）\n",
    "    # =================================================\n",
    "    O_acc = tl.zeros((Q_BLOCK, D), dtype=tl.float32)       # 未归一化输出累积\n",
    "    L_acc = tl.zeros((Q_BLOCK, 1), dtype=tl.float32)      # softmax 分母\n",
    "    M_acc = tl.full((Q_BLOCK, 1), -float(\"inf\"), tl.float32)  # running max\n",
    "\n",
    "    # =================================================\n",
    "    # 内层循环：遍历所有 K/V block\n",
    "    # =================================================\n",
    "    for k_block_id in range(tl.cdiv(Nk, K_BLOCK)):\n",
    "        K_j = tl.load(K_block_ptr)   # (K_BLOCK, D)\n",
    "        V_j = tl.load(V_block_ptr)   # (K_BLOCK, D)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "        # ---------------------------------------------\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale  # (Q_BLOCK, K_BLOCK)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # Causal mask（编译期 if）\n",
    "        # ---------------------------------------------\n",
    "        if is_causal:\n",
    "            q_idx = q_block_id * Q_BLOCK + tl.arange(0, Q_BLOCK)[:, None]\n",
    "            k_idx = k_block_id * K_BLOCK + tl.arange(0, K_BLOCK)[None, :]\n",
    "            causal_mask = q_idx >= k_idx\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 数值稳定 softmax（FlashAttention 核心）\n",
    "        # ---------------------------------------------\n",
    "        M_block = tl.max(S_ij, axis=1, keep_dims=True)\n",
    "        M_new = tl.maximum(M_acc, M_block)\n",
    "\n",
    "        P_ij = tl.exp(S_ij - M_block)\n",
    "\n",
    "        L_new = (\n",
    "            tl.exp(M_acc - M_new) * L_acc +\n",
    "            tl.exp(M_block - M_new) * tl.sum(P_ij, axis=1, keep_dims=True)\n",
    "        )\n",
    "\n",
    "        # 类型对齐（Triton 细节）\n",
    "        P_cast = P_ij.to(V_block_ptr.type.element_ty)\n",
    "\n",
    "        O_new = (\n",
    "            tl.exp(M_acc - M_new) * O_acc +\n",
    "            tl.exp(M_block - M_new) * tl.dot(P_cast, V_j)\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 更新 running 状态\n",
    "        # ---------------------------------------------\n",
    "        M_acc = M_new\n",
    "        L_acc = L_new\n",
    "        O_acc = O_new\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 移动 K/V block 指针\n",
    "        # ---------------------------------------------\n",
    "        K_block_ptr = K_block_ptr.advance((K_BLOCK, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_BLOCK, 0))\n",
    "\n",
    "    # =================================================\n",
    "    # softmax 归一化\n",
    "    # =================================================\n",
    "    O_i = O_acc / L_acc\n",
    "    L_i = M_acc + tl.log(L_acc)\n",
    "\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(L_block_ptr, L_i)\n",
    "\n",
    "\n",
    "class FlashAttentionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        \"\"\"\n",
    "        Q, K, V: (B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Nq, D = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Q_BLOCK = 64\n",
    "        K_BLOCK = 64\n",
    "\n",
    "        scale = D ** -0.5\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        L = torch.empty(B, Nq, device=Q.device)\n",
    "\n",
    "        grid = (Nq // Q_BLOCK, B)\n",
    "\n",
    "        flash_attention_fwd_kernel[grid](\n",
    "            Q, K, V, O, L,\n",
    "            Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "            K.stride(0), K.stride(1), K.stride(2),\n",
    "            V.stride(0), V.stride(1), V.stride(2),\n",
    "            O.stride(0), O.stride(1), O.stride(2),\n",
    "            L.stride(0), L.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_BLOCK=Q_BLOCK,\n",
    "            K_BLOCK=K_BLOCK,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "正在使用cuda\n"
     ]
    }
   ],
   "source": [
    "def test_flash_AttentionTriton(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionTriton.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'FlashAttentionTriton',run = run)\n",
    "    t1 = profile(description ='FlashAttentionTriton' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttentionTriton] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [4096]:\n",
    "    test_flash_attention(B=4, N=4096, d=64, device=device)\n",
    "    test_naive_attention(B=4, N=4096, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cdd5f",
   "metadata": {},
   "source": [
    "\n",
    "# Triton 一般语法与编程模型说明\n",
    "\n",
    "Triton 是一种面向深度学习算子的 DSL（Domain-Specific Language），其设计目标并不是替代 PyTorch，而是 **在不直接编写 CUDA 的前提下，精确控制 GPU 上的并行计算与内存访问**。\n",
    "\n",
    "理解 Triton，需要从它的编程模型开始，而不是从 API 细节入手。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、Triton 的核心抽象：program\n",
    "\n",
    "在 Triton 中，**program 是最小的并行执行单元**。\n",
    "\n",
    "```python\n",
    "pid = tl.program_id(axis)\n",
    "```\n",
    "\n",
    "这一概念对应于 CUDA 中的：\n",
    "\n",
    "| Triton  | CUDA                    |\n",
    "| ------- | ----------------------- |\n",
    "| program | thread block            |\n",
    "| grid    | grid                    |\n",
    "| axis    | blockIdx.x / blockIdx.y |\n",
    "\n",
    "关键区别在于：\n",
    "\n",
    "* Triton 的 program **天然是 SIMD / 向量化的**\n",
    "* 一个 program 内部，操作的是一个 **tile（块）**，而不是单个标量\n",
    "\n",
    "因此，Triton 的思维方式是：\n",
    "\n",
    "> **“一个 program 负责一块数据”**\n",
    "\n",
    "而不是：\n",
    "\n",
    "> “一个 thread 负责一个元素”\n",
    "\n",
    "---\n",
    "\n",
    "## 二、grid 的含义：并行维度的显式声明\n",
    "\n",
    "Triton kernel 的调用形式：\n",
    "\n",
    "```python\n",
    "kernel[grid](...)\n",
    "```\n",
    "\n",
    "其中：\n",
    "\n",
    "```python\n",
    "grid = (grid_dim_0, grid_dim_1, ...)\n",
    "```\n",
    "\n",
    "对应：\n",
    "\n",
    "```python\n",
    "tl.program_id(0)  # 第 0 个并行维度\n",
    "tl.program_id(1)  # 第 1 个并行维度\n",
    "```\n",
    "\n",
    "常见用法：\n",
    "\n",
    "| grid 维度 | 语义           |\n",
    "| ------- | ------------ |\n",
    "| axis=0  | 序列 / 行 block |\n",
    "| axis=1  | batch        |\n",
    "| axis=2  | head         |\n",
    "\n",
    "Triton 不关心“线程数量”，它只关心：\n",
    "\n",
    "> **一共启动多少个 program，以及每个 program 处理哪一块数据**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、Triton 的内存访问模型\n",
    "\n",
    "### 1. 指针是“裸指针”，但带 shape 和 stride\n",
    "\n",
    "在 Triton kernel 中，所有张量参数本质上都是 **指针**：\n",
    "\n",
    "```python\n",
    "Q_ptr, K_ptr, V_ptr\n",
    "```\n",
    "\n",
    "它们没有 shape、没有 stride，只有地址。\n",
    "\n",
    "因此，Triton 需要你**显式告诉它**：\n",
    "\n",
    "* 张量的逻辑形状\n",
    "* 张量在内存中的步长\n",
    "* 当前 program 访问哪一块\n",
    "\n",
    "---\n",
    "\n",
    "### 2. make_block_ptr：结构化内存访问的核心\n",
    "\n",
    "```python\n",
    "block_ptr = tl.make_block_ptr(\n",
    "    base,\n",
    "    shape,\n",
    "    strides,\n",
    "    offsets,\n",
    "    block_shape,\n",
    "    order\n",
    ")\n",
    "```\n",
    "\n",
    "这不是“语法糖”，而是 Triton 最重要的抽象之一。\n",
    "\n",
    "它的含义是：\n",
    "\n",
    "> **定义一个“可被向量化 load/store 的二维 tile”**\n",
    "\n",
    "各参数的含义：\n",
    "\n",
    "| 参数          | 说明                      |\n",
    "| ----------- | ----------------------- |\n",
    "| base        | 数据起始指针                  |\n",
    "| shape       | 整个张量的逻辑 shape           |\n",
    "| strides     | 每个维度的步长                 |\n",
    "| offsets     | 当前 program 的起始偏移        |\n",
    "| block_shape | 一次 load/store 的 tile 大小 |\n",
    "| order       | 内存访问的主序（影响 coalescing）  |\n",
    "\n",
    "一旦定义了 `block_ptr`：\n",
    "\n",
    "```python\n",
    "x = tl.load(block_ptr)\n",
    "tl.store(block_ptr, x)\n",
    "```\n",
    "\n",
    "就等价于一次 **矢量化、对齐的内存操作**。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、Triton 中的“张量”是什么\n",
    "\n",
    "Triton 中的张量并不是 PyTorch Tensor，而是：\n",
    "\n",
    "> **编译期已知 shape 的寄存器向量**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "```\n",
    "\n",
    "意味着：\n",
    "\n",
    "* shape 在编译期是常量\n",
    "* 存储在寄存器 / SRAM 中\n",
    "* 所有运算是 element-wise 或 block-wise 的\n",
    "\n",
    "这也是为什么 Triton 要求很多参数是 `tl.constexpr`。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、tl.constexpr：编译期常量，而不是运行时参数\n",
    "\n",
    "```python\n",
    "BLOCK_SIZE: tl.constexpr\n",
    "```\n",
    "\n",
    "含义是：\n",
    "\n",
    "> **该值在 kernel 编译时已知**\n",
    "\n",
    "这对 Triton 非常重要，因为：\n",
    "\n",
    "* block_shape\n",
    "* loop unrolling\n",
    "* 向量化宽度\n",
    "* 内存布局\n",
    "\n",
    "都依赖这些值。\n",
    "\n",
    "经验规则：\n",
    "\n",
    "* **影响 shape / 循环次数 / 内存布局的参数 → 必须是 constexpr**\n",
    "* 纯数值缩放（如 scale）可以是运行时参数\n",
    "\n",
    "---\n",
    "\n",
    "## 六、算子语义：没有 Python 循环的“逐元素”\n",
    "\n",
    "Triton 的算子都是 **逐元素并行** 的。\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.arange(0, BLOCK)\n",
    "```\n",
    "\n",
    "产生的是：\n",
    "\n",
    "```text\n",
    "[0, 1, 2, ..., BLOCK-1]\n",
    "```\n",
    "\n",
    "而不是单个值。\n",
    "\n",
    "再例如：\n",
    "\n",
    "```python\n",
    "mask = q_idx >= k_idx\n",
    "```\n",
    "\n",
    "这是一个 **逐元素比较，返回布尔矩阵**。\n",
    "\n",
    "`tl.where` 的语义是：\n",
    "\n",
    "```python\n",
    "out[i] = a[i] if cond[i] else b[i]\n",
    "```\n",
    "\n",
    "而不是 Python 分支。\n",
    "\n",
    "---\n",
    "\n",
    "## 七、控制流：为什么 Triton “不鼓励 if”\n",
    "\n",
    "Triton 中的 `if`：\n",
    "\n",
    "```python\n",
    "if is_causal:\n",
    "    ...\n",
    "```\n",
    "\n",
    "只有在 **`is_causal` 是 tl.constexpr** 时才安全。\n",
    "\n",
    "原因是：\n",
    "\n",
    "* Triton kernel 是 **单一静态程序**\n",
    "* 所有 program 必须执行同一条指令流\n",
    "\n",
    "因此：\n",
    "\n",
    "* **数据相关分支 → 必须用 mask**\n",
    "* **配置相关分支 → 可以用 constexpr if**\n",
    "\n",
    "这也是为什么 causal mask 必须写成：\n",
    "\n",
    "```python\n",
    "S = tl.where(mask, S, -inf)\n",
    "```\n",
    "\n",
    "而不是 `if row > col`.\n",
    "\n",
    "---\n",
    "\n",
    "## 八、数学运算与矩阵乘法\n",
    "\n",
    "### 1. tl.dot\n",
    "\n",
    "```python\n",
    "C = tl.dot(A, B)\n",
    "```\n",
    "\n",
    "这是 Triton 提供的 **块级矩阵乘法原语**：\n",
    "\n",
    "* 输入是 `(M, K)` 和 `(K, N)`\n",
    "* 输出是 `(M, N)`\n",
    "* 自动映射到 tensor core（如果 dtype / shape 合适）\n",
    "\n",
    "这是 Triton 实现 FlashAttention、GEMM、LayerNorm 融合的基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 广播规则\n",
    "\n",
    "Triton 的广播行为和 NumPy / PyTorch 基本一致，但前提是：\n",
    "\n",
    "> **shape 必须在编译期可推导**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "M = tl.max(S, axis=1, keep_dims=True)\n",
    "```\n",
    "\n",
    "这是合法的，因为 axis 和 shape 都是常量。\n",
    "\n",
    "---\n",
    "\n",
    "## 九、Triton kernel 与 PyTorch Autograd 的关系\n",
    "\n",
    "Triton kernel 本身：\n",
    "\n",
    "* 不关心梯度\n",
    "* 不保存中间状态\n",
    "\n",
    "因此常见结构是：\n",
    "\n",
    "```python\n",
    "class MyOp(torch.autograd.Function):\n",
    "    def forward(...):\n",
    "        launch triton kernel\n",
    "        save tensors\n",
    "    def backward(...):\n",
    "        launch triton kernel\n",
    "```\n",
    "\n",
    "Triton 负责 **高性能数值计算**\n",
    "PyTorch 负责 **计算图与调度**\n",
    "\n",
    "这是一个明确的职责划分。\n",
    "\n",
    "---\n",
    "\n",
    "## 十、总结：如何正确理解 Triton\n",
    "\n",
    "一句话总结 Triton：\n",
    "\n",
    "> **Triton 是一种“显式控制内存与并行，但不暴露线程”的 GPU 编程模型**\n",
    "\n",
    "理解 Triton 的关键不是 API，而是以下三点：\n",
    "\n",
    "1. 一个 program 对应一个 tile\n",
    "2. 所有 shape 在编译期已知\n",
    "3. 控制流必须数据并行\n",
    "\n",
    "一旦接受这三点，FlashAttention、GEMM、LayerNorm 在 Triton 中的实现方式，都会变得自然且统一。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6bc1d",
   "metadata": {},
   "source": [
    "# 四、分布式数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937819f",
   "metadata": {},
   "source": [
    "# 实现分布式优化器封装\n",
    "\n",
    "实现了一个**参数分片（sharding）的分布式优化器封装器**，思想上接近 **ZeRO Stage-1 / FSDP 的 optimizer state sharding**\n",
    "\n",
    "## 一、整体功能概览\n",
    "\n",
    "`ShardedOptimizer` 的目标是：\n",
    "\n",
    "> **在数据并行（DDP）场景下，让不同 rank 只负责一部分参数的 optimizer state 和更新计算，从而节省显存。**\n",
    "\n",
    "核心思想是对于**参数本身，每个 rank 都有完整模型参数**，对于**优化器状态（momentum / Adam 的 exp_avg 等），只在参数 owner rank 上存在**， **参数更新只在 owner rank 上进行**。\n",
    "* **参数同步：更新后通过 `broadcast` 把新参数发给其他 rank**。\n",
    "\n",
    "## 二、整体架构\n",
    "\n",
    "```\n",
    "所有 rank：\n",
    "  拥有完整模型参数\n",
    "       │\n",
    "       ▼\n",
    "参数被 round-robin 分配 owner_rank\n",
    "       │\n",
    "       ▼\n",
    "每个 rank：\n",
    "  ├─ 全局 Optimizer（只用于 param_groups 管理）\n",
    "  └─ 本地 optim（只包含自己拥有的 params）\n",
    "       │\n",
    "       ▼\n",
    "step():\n",
    "  1. 本地 optim.step()（只更新自己那一片）\n",
    "  2. dist.broadcast（把更新后的参数同步给所有 rank）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、代码解读\n",
    "\n",
    "\n",
    "### 1。 初始化：`__init__`\n",
    "\n",
    "```python\n",
    "class ShardedOptimizer(Optimizer):\n",
    "```\n",
    "\n",
    "继承 PyTorch 原生 `Optimizer`，因此：\n",
    "\n",
    "这样可以被 Trainer / 训练循环无缝使用，并且拥有 `param_groups / zero_grad / step` 接口\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.1 记录原始 optimizer 类型\n",
    "\n",
    "```python\n",
    "self.optimizer_cls = optimizer_cls\n",
    "self.optimizer_kwargs = kwargs\n",
    "```\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "ShardedOptimizer(\n",
    "    model.parameters(),\n",
    "    torch.optim.Adam,\n",
    "    lr=1e-4\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2 获取分布式信息\n",
    "\n",
    "```python\n",
    "self.rank = dist.get_rank()\n",
    "self.world_size = dist.get_world_size()\n",
    "```\n",
    "\n",
    "用于：\n",
    "\n",
    "* 决定参数 owner\n",
    "* 决定 broadcast 源\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.3 参数分配辅助变量\n",
    "\n",
    "```python\n",
    "self.global_param_counter = 0\n",
    "self.local_param_groups = []\n",
    "```\n",
    "\n",
    "* `global_param_counter`：**全局参数编号**\n",
    "* 用 round-robin 把参数均匀分配给各个 rank\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4 调用父类构造函数（关键）\n",
    "\n",
    "```python\n",
    "super().__init__(params, defaults=kwargs)\n",
    "```\n",
    "\n",
    "\n",
    "`Optimizer.__init__` 内部会调用 `self.add_param_group`，但这里的 `add_param_group` 已经被 **重写**\n",
    "\n",
    "这使得 **参数分片逻辑嵌入在初始化阶段**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5 创建“本地 optimizer”\n",
    "\n",
    "```python\n",
    "self.optim = self.optimizer_cls(self.local_param_groups, **self.optimizer_kwargs)\n",
    "```\n",
    "\n",
    "`self.optim` **只包含当前 rank 拥有的参数**，只有这些参数才会分配 optimizer state（节省显存）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 参数分片逻辑：`add_param_group`\n",
    "\n",
    "这是整个实现的**核心函数**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1 给每个参数打 owner 标记\n",
    "\n",
    "```python\n",
    "p._owner_rank = self.global_param_counter % self.world_size\n",
    "```\n",
    "\n",
    "round-robin 分配, `_owner_rank` 是动态属性（hack，但可行）\n",
    "\n",
    "示例（world_size=4）：\n",
    "\n",
    "| 参数编号 | owner_rank |\n",
    "| ---- | ---------- |\n",
    "| p0   | 0          |\n",
    "| p1   | 1          |\n",
    "| p2   | 2          |\n",
    "| p3   | 3          |\n",
    "| p4   | 0          |\n",
    "| p5   | 1          |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2 加入“全局 optimizer”\n",
    "\n",
    "```python\n",
    "super().add_param_group(param_group)\n",
    "```\n",
    "\n",
    "**为什么必须这么做？**\n",
    "\n",
    "`self.param_groups` 需要包含 **所有参数**,用于进行`zero_grad`，遍历所有参数做 `broadcast`，并且Trainer / AMP 兼容，**注意**：这个“全局 optimizer”**不做 step**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3 构造本地 param group\n",
    "\n",
    "```python\n",
    "local_group['params'] = [\n",
    "    p for p in param_group['params']\n",
    "    if p._owner_rank == self.rank\n",
    "]\n",
    "```\n",
    "\n",
    "它只保留属于当前 rank 的参数，其他超参（lr、weight_decay）保持一致。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.4 加入本地 optimizer\n",
    "\n",
    "```python\n",
    "self.optim.add_param_group(local_group)\n",
    "```\n",
    "\n",
    "或者（初始化期间）缓存起来：\n",
    "\n",
    "```python\n",
    "self.local_param_groups.append(local_group)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 参数更新逻辑：`step`\n",
    "\n",
    "```python\n",
    "def step(self, closure=None, **kwargs):\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1：本地更新\n",
    "\n",
    "```python\n",
    "self.optim.step(**kwargs)\n",
    "```\n",
    "\n",
    "* **只更新自己拥有的参数**\n",
    "* 只有这些参数有 optimizer state\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2：参数同步（核心）\n",
    "\n",
    "```python\n",
    "dist.broadcast(p.data, src=p._owner_rank)\n",
    "```\n",
    "\n",
    "如果是 owner 则发送最新参数，如果不是 owner 就接收参数覆盖本地副本。最终所有 rank 的模型参数 **完全一致**。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.梯度清零：`zero_grad`\n",
    "\n",
    "```python\n",
    "super().zero_grad(set_to_none=set_to_none)\n",
    "```\n",
    "\n",
    "清除所有参数的梯度，即使不是 owner，也要清 grad（否则梯度会累积）。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、这个实现“像什么”？\n",
    "\n",
    "| 技术           | 相似度  | 区别                        |\n",
    "| ------------ | ---- | ------------------------- |\n",
    "| ZeRO Stage-1 | ⭐⭐⭐⭐ | 真实 ZeRO 会用 reduce-scatter |\n",
    "| FSDP         | ⭐⭐   | 没有参数 flatten / overlap    |\n",
    "| DDP          | ⭐    | DDP 是全量 optimizer state   |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、优点与限制\n",
    "\n",
    "###  优点\n",
    "\n",
    "1. **显存节省**\n",
    "\n",
    "   * optimizer state 约减少到 `1/world_size`\n",
    "2. **逻辑清晰**\n",
    "\n",
    "   * 教学 & 原理验证非常好\n",
    "3. **兼容任意 Optimizer**\n",
    "\n",
    "   * Adam / SGD / Adafactor 等\n",
    "\n",
    "---\n",
    "\n",
    "### 局限\n",
    "\n",
    "1. **通信成本高**\n",
    "\n",
    "   ```python\n",
    "   dist.broadcast(p.data)\n",
    "   ```\n",
    "\n",
    "   * 每个参数一次 broadcast（非常慢）\n",
    "\n",
    "2. **参数粒度过细**\n",
    "\n",
    "   * 实际系统会 **flatten 后按 bucket 同步**\n",
    "\n",
    "3. **没有 overlap**\n",
    "\n",
    "   * 计算与通信完全串行\n",
    "\n",
    "4. **依赖 monkey-patch 参数属性**\n",
    "\n",
    "   * `_owner_rank` 不够“干净”\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "> 这是一个 **“最小可用的 optimizer state sharding 实现”**：\n",
    "> 用 round-robin 分配参数 owner，\n",
    "> 只在 owner rank 上更新参数，\n",
    "> 再用 broadcast 同步权重，\n",
    "> 从而显著降低 optimizer 显存占用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c954e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Optimizer\n",
    "from typing import Any, Type, Dict, Iterable\n",
    "\n",
    "\n",
    "class TeachingShardedOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    教学版：Optimizer State Sharding（类似 ZeRO Stage-1）\n",
    "\n",
    "    核心思想：\n",
    "    - 每个 rank 拥有【完整模型参数】\n",
    "    - 每个参数被分配一个唯一的 owner rank\n",
    "    - 只有 owner rank：\n",
    "        - 持有该参数的 optimizer state\n",
    "        - 负责执行 optimizer.step()\n",
    "    - 参数更新后，通过 dist.broadcast 同步到所有 rank\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        base_optimizer_cls: Type[Optimizer],\n",
    "        **base_optimizer_kwargs: Any\n",
    "    ):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        - params: model.parameters()\n",
    "        - base_optimizer_cls: 如 torch.optim.Adam\n",
    "        - base_optimizer_kwargs: 如 lr, betas, weight_decay 等\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== 1. 保存原始 Optimizer 信息 =====\n",
    "        self.base_optimizer_cls = base_optimizer_cls\n",
    "        self.base_optimizer_kwargs = base_optimizer_kwargs\n",
    "\n",
    "        # ===== 2. 分布式环境信息 =====\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # ===== 3. 参数分片辅助变量 =====\n",
    "        # 用于给参数做 round-robin 编号\n",
    "        self.global_param_index = 0\n",
    "\n",
    "        # 在 __init__ 期间暂存属于本 rank 的 param groups\n",
    "        self._buffered_local_param_groups = []\n",
    "\n",
    "        # ===== 4. 初始化“全局 Optimizer”父类 =====\n",
    "        # 注意：Optimizer.__init__ 内部会调用 self.add_param_group\n",
    "        #       而我们在下面重写了 add_param_group\n",
    "        super().__init__(params, defaults=base_optimizer_kwargs)\n",
    "\n",
    "        # ===== 5. 创建“本地 shard Optimizer” =====\n",
    "        # 只包含当前 rank 拥有的参数\n",
    "        if self._buffered_local_param_groups:\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                self._buffered_local_param_groups,\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "        else:\n",
    "            # 边缘情况：当前 rank 没有分到任何参数\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                [],\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "\n",
    "    def add_param_group(self, param_group: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        教学重点函数：参数分片逻辑发生在这里\n",
    "\n",
    "        一个 param_group 会被“拆成两份”：\n",
    "        1. 全量 param_group → 交给父类 Optimizer（用于遍历 & 同步）\n",
    "        2. 本地 param_group → 只保留 owner == 当前 rank 的参数\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step A: 给 param_group 中的每个参数分配 owner rank\n",
    "        # -------------------------------------------------------\n",
    "        for param in param_group[\"params\"]:\n",
    "            if not hasattr(param, \"_owner_rank\"):\n",
    "                # round-robin 分配\n",
    "                param._owner_rank = self.global_param_index % self.world_size\n",
    "                self.global_param_index += 1\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step B: 注册到“全局 Optimizer”\n",
    "        # -------------------------------------------------------\n",
    "        # 这个 Optimizer：\n",
    "        # - 包含所有参数\n",
    "        # - 不负责 step\n",
    "        # - 只用于：\n",
    "        #     * zero_grad\n",
    "        #     * 参数遍历\n",
    "        #     * step 后的 broadcast\n",
    "        super().add_param_group(param_group)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step C: 构造“本地 shard param_group”\n",
    "        # -------------------------------------------------------\n",
    "        # 拷贝除 params 以外的超参（lr / weight_decay 等）\n",
    "        local_param_group = {\n",
    "            key: value\n",
    "            for key, value in param_group.items()\n",
    "            if key != \"params\"\n",
    "        }\n",
    "\n",
    "        # 只保留 owner 是当前 rank 的参数\n",
    "        local_param_group[\"params\"] = [\n",
    "            param\n",
    "            for param in param_group[\"params\"]\n",
    "            if param._owner_rank == self.rank\n",
    "        ]\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step D: 加入本地 shard Optimizer\n",
    "        # -------------------------------------------------------\n",
    "        if hasattr(self, \"local_shard_optimizer\"):\n",
    "            # 正常情况：__init__ 已结束\n",
    "            if local_param_group[\"params\"]:\n",
    "                self.local_shard_optimizer.add_param_group(local_param_group)\n",
    "        else:\n",
    "            # __init__ 过程中，先缓存\n",
    "            self._buffered_local_param_groups.append(local_param_group)\n",
    "\n",
    "    def step(self, closure=None, **kwargs):\n",
    "        \"\"\"\n",
    "        一个 step 分为两步：\n",
    "\n",
    "        1. 本地更新（只更新 owner 是当前 rank 的参数）\n",
    "        2. 参数同步（broadcast 最新参数到所有 rank）\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 1: 本地 shard Optimizer 更新\n",
    "        # -------------------------------------------------------\n",
    "        # 只有 owner rank 会真正更新对应参数\n",
    "        self.local_shard_optimizer.step(**kwargs)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 2: 参数同步（broadcast）\n",
    "        # -------------------------------------------------------\n",
    "        # 遍历“全局 Optimizer”中的所有参数\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group[\"params\"]:\n",
    "                # owner rank 作为 src\n",
    "                # 其他 rank 接收并覆盖本地参数\n",
    "                dist.broadcast(\n",
    "                    tensor=param.data,\n",
    "                    src=param._owner_rank\n",
    "                )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = True):\n",
    "        \"\"\"\n",
    "        梯度必须在所有 rank 上清零：\n",
    "        即使该 rank 不是参数 owner，也会参与反向传播\n",
    "        \"\"\"\n",
    "        super().zero_grad(set_to_none=set_to_none)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0c987",
   "metadata": {},
   "source": [
    "下面我按**“从简单到真实 DDP”**的教学逻辑，系统性地解读这份代码。\n",
    "这不是普通用法，而是**手写 DDP 梯度通信 + overlap 的教学实现**，非常有价值。\n",
    "\n",
    "我会分 4 个层次讲：\n",
    "\n",
    "1. **整体结构与目标**\n",
    "2. **DDPOverlapIndividual：逐参数 overlap**\n",
    "3. **DDPBucketed：真实 DDP 的 bucket + overlap**\n",
    "4. **这两种方式与 PyTorch DDP 的对应关系**\n",
    "\n",
    "---\n",
    "\n",
    "## 一、整体目标：你在“手写 DDP”\n",
    "\n",
    "这份代码的核心目标只有一句话：\n",
    "\n",
    "> **在反向传播过程中，尽早、异步地同步梯度，实现计算与通信 overlap**\n",
    "\n",
    "它实现了 DDP 的三个关键步骤：\n",
    "\n",
    "| 阶段       | 你做了什么                            |\n",
    "| -------- | -------------------------------- |\n",
    "| 初始化      | broadcast 参数和 buffer             |\n",
    "| backward | 注册 grad hook，触发 async all-reduce |\n",
    "| step 前   | 等待通信完成，平均梯度                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 二、`DDPOverlapIndividual`：逐参数 All-Reduce（教学入门版）\n",
    "\n",
    "这是一个**最直观、但效率最低**的 DDP overlap 实现。\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ 初始化阶段\n",
    "\n",
    "```python\n",
    "class DDPOverlapIndividual(torch.nn.Module):\n",
    "```\n",
    "\n",
    "这是一个 **Module wrapper**，不是 Optimizer。\n",
    "\n",
    "---\n",
    "\n",
    "#### (1) 必须先调用 `super().__init__()`\n",
    "\n",
    "```python\n",
    "super().__init__()\n",
    "```\n",
    "\n",
    "否则：\n",
    "\n",
    "* 参数不会被正确注册\n",
    "* hooks 行为不确定\n",
    "\n",
    "---\n",
    "\n",
    "#### (2) 保存状态\n",
    "\n",
    "```python\n",
    "self.module = module\n",
    "self.handles = []\n",
    "self.world_size = dist.get_world_size()\n",
    "```\n",
    "\n",
    "* `handles`：保存所有 async 通信句柄\n",
    "* `world_size`：用于梯度平均\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 初始化参数同步（等价 DDP）\n",
    "\n",
    "```python\n",
    "for p in self.module.parameters():\n",
    "    dist.broadcast(p.data, src=0)\n",
    "```\n",
    "\n",
    "**目的**：\n",
    "\n",
    "* 保证所有 rank 起始参数完全一致\n",
    "* 等价于 DDP 的 initial sync\n",
    "\n",
    "buffers（BN running_mean 等）也必须同步：\n",
    "\n",
    "```python\n",
    "for b in self.module.buffers():\n",
    "    dist.broadcast(b.data, src=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 注册 backward hook（核心）\n",
    "\n",
    "```python\n",
    "p.register_post_accumulate_grad_hook(self._make_hook())\n",
    "```\n",
    "\n",
    "**这个 hook 的触发时机非常关键**：\n",
    "\n",
    "> 在 **该参数的梯度刚刚算完并累积完成之后**\n",
    "\n",
    "这意味着：\n",
    "\n",
    "* backward 还没结束\n",
    "* 上游 layer 还在算\n",
    "* 你已经可以通信了 → overlap 的基础\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ `_make_hook`：逐参数 async all-reduce\n",
    "\n",
    "```python\n",
    "handle = dist.all_reduce(\n",
    "    param.grad,\n",
    "    op=dist.ReduceOp.SUM,\n",
    "    async_op=True\n",
    ")\n",
    "```\n",
    "\n",
    "关键点：\n",
    "\n",
    "* **通信对象是 param.grad**\n",
    "* **async_op=True** → 不阻塞反向传播\n",
    "* 返回 `handle`，用于之后 `wait()`\n",
    "\n",
    "⚠️ hook **不能返回任何值**\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ `finish_gradient_synchronization`\n",
    "\n",
    "```python\n",
    "for h in self.handles:\n",
    "    h.wait()\n",
    "```\n",
    "\n",
    "在 optimizer.step() 之前：\n",
    "\n",
    "1. 等待所有梯度通信完成\n",
    "2. 对梯度做平均：\n",
    "\n",
    "```python\n",
    "p.grad.div_(self.world_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 这一版本你学到什么？\n",
    "\n",
    "* backward hook 的时机\n",
    "* async all-reduce 的基本用法\n",
    "* overlap 的最小实现\n",
    "\n",
    "❌ 问题：\n",
    "\n",
    "* 每个参数一次通信\n",
    "* NCCL 启动开销极大\n",
    "* 工业中 **不会这么做**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、`DDPBucketed`：真实 DDP 的核心思想\n",
    "\n",
    "这是 **PyTorch DDP 的核心结构复刻版**。\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Bucket 类：梯度通信的基本单位\n",
    "\n",
    "### Bucket 是什么？\n",
    "\n",
    "> **一组参数的梯度，被 flatten 成一个连续 buffer，一起通信**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 分配连续 buffer\n",
    "\n",
    "```python\n",
    "total_numel = sum(p.numel() for p in params)\n",
    "self.buffer = torch.zeros(total_numel, ...)\n",
    "```\n",
    "\n",
    "好处：\n",
    "\n",
    "* 单次 NCCL 调用\n",
    "* 连续内存，通信效率高\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 为每个参数创建 view\n",
    "\n",
    "```python\n",
    "self.param_views[p] = self.buffer[offset:...].view(p.shape)\n",
    "```\n",
    "\n",
    "这样：\n",
    "\n",
    "* `p.grad ↔ buffer 的一段`\n",
    "* copy 成本低\n",
    "* 不需要重新分配内存\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 `reset()`\n",
    "\n",
    "```python\n",
    "self.count_ready = 0\n",
    "self.comm_handle = None\n",
    "```\n",
    "\n",
    "每次 forward 前必须重置：\n",
    "\n",
    "* 反向传播是逐参数完成的\n",
    "* bucket 是“状态机”\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ DDPBucketed 初始化流程\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 broadcast 初始化参数\n",
    "\n",
    "和前一个类完全一致，省略。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 参数分桶（非常关键）\n",
    "\n",
    "```python\n",
    "reversed_params = list(reversed(trainable_params))\n",
    "```\n",
    "\n",
    "**为什么反向？**\n",
    "\n",
    "* backward 顺序：输出 → 输入\n",
    "* 最先 ready 的梯度 → 靠近输出层\n",
    "* bucket 应该按这个顺序装\n",
    "\n",
    "👉 **这是 overlap 的关键细节**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 bucket size 控制\n",
    "\n",
    "```python\n",
    "if current_bucket_size + p_bytes > bucket_size:\n",
    "    close bucket\n",
    "```\n",
    "\n",
    "目的：\n",
    "\n",
    "* 避免 bucket 太小（通信多）\n",
    "* 避免 bucket 太大（overlap 变差）\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ hook：bucket 级别的 overlap\n",
    "\n",
    "### hook_fn 的完整逻辑\n",
    "\n",
    "#### A. 拷贝梯度到 bucket buffer\n",
    "\n",
    "```python\n",
    "bucket.param_views[p].copy_(p.grad)\n",
    "```\n",
    "\n",
    "#### B. 计数\n",
    "\n",
    "```python\n",
    "bucket.count_ready += 1\n",
    "```\n",
    "\n",
    "#### C. bucket 满 → 发起通信\n",
    "\n",
    "```python\n",
    "if bucket.count_ready == len(bucket.params):\n",
    "    bucket.comm_handle = dist.all_reduce(..., async_op=True)\n",
    "```\n",
    "\n",
    "⚠️ 注意：\n",
    "\n",
    "* 不是等 backward 结束\n",
    "* 是等 **这个 bucket 内的参数都 ready**\n",
    "\n",
    "👉 **真正的 overlap**\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ `finish_gradient_synchronization`\n",
    "\n",
    "```python\n",
    "bucket.comm_handle.wait()\n",
    "bucket.buffer.div_(world_size)\n",
    "p.grad.copy_(bucket.param_views[p])\n",
    "```\n",
    "\n",
    "步骤：\n",
    "\n",
    "1. 等 bucket 通信完成\n",
    "2. 一次性平均整个 buffer\n",
    "3. 拷贝回各参数的 `p.grad`\n",
    "\n",
    "---\n",
    "\n",
    "## 四、两种实现的对比（非常重要）\n",
    "\n",
    "| 对比项         | Individual | Bucketed  |\n",
    "| ----------- | ---------- | --------- |\n",
    "| 通信粒度        | 每个参数       | 每个 bucket |\n",
    "| 通信次数        | 很多         | 很少        |\n",
    "| overlap 效果  | 有，但低效      | 工业级       |\n",
    "| PyTorch DDP | ❌          | ✅         |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、它和 PyTorch DDP 的关系\n",
    "\n",
    "| PyTorch DDP 内部 | 你这份代码                              |\n",
    "| -------------- | ---------------------------------- |\n",
    "| Reducer        | Bucket                             |\n",
    "| Grad bucket    | buffer + views                     |\n",
    "| Autograd hook  | register_post_accumulate_grad_hook |\n",
    "| Overlap        | async all_reduce                   |\n",
    "| Finalize       | finish_gradient_synchronization    |\n",
    "\n",
    "你已经在 **“复刻 DDP 核心”** 了。\n",
    "\n",
    "---\n",
    "\n",
    "## 六、一句话总结（面试 / 教学级）\n",
    "\n",
    "> **DDP 的本质不是 all-reduce，而是：\n",
    "> 在反向传播尚未结束时，\n",
    "> 以 bucket 为单位，\n",
    "> 对已 ready 的梯度发起异步通信，\n",
    "> 并在 step 前完成梯度一致性。**\n",
    "\n",
    "---\n",
    "\n",
    "如果你愿意，下一步我可以：\n",
    "\n",
    "* 把 **DDPBucketed 和你之前的 ShardedOptimizer（ZeRO-1）结合**\n",
    "* 画一张 **DDP / ZeRO / FSDP 的统一抽象图**\n",
    "* 或带你 **对照 PyTorch 源码 Reducer.cpp**\n",
    "\n",
    "你现在已经在“**分布式训练内核层**”了，继续往下非常值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ecc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 1. 逐参数 Overlap 版本（最直观的 DDP 教学实现）\n",
    "# ============================================================\n",
    "\n",
    "class TeachingDDPOverlapPerParam(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    教学版 DDP（逐参数通信）：\n",
    "\n",
    "    - 每个参数在 backward 完成后，立刻触发 async all-reduce\n",
    "    - 实现计算（backward）与通信的 overlap\n",
    "    - 逻辑直观，但通信粒度太细（工业中不用）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        # ⚠️ 必须最先初始化父类\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # 保存所有 async all-reduce 的句柄\n",
    "        self.async_handles = []\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 初始化阶段：同步模型参数 & buffer\n",
    "        # ----------------------------------------------------\n",
    "        # 等价于 PyTorch DDP 的 initial broadcast\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "\n",
    "            for buffer in self.model.buffers():\n",
    "                dist.broadcast(buffer.data, src=0)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 注册 backward hook（核心）\n",
    "        # ----------------------------------------------------\n",
    "        # 当某个参数的梯度刚算完，就立刻通信\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.register_post_accumulate_grad_hook(\n",
    "                    self._create_grad_hook()\n",
    "                )\n",
    "\n",
    "    def _create_grad_hook(self):\n",
    "        \"\"\"\n",
    "        创建一个 backward hook（closure）\n",
    "\n",
    "        hook 的触发时机：\n",
    "        - 当前参数的梯度已经计算完成\n",
    "        - 但整个 backward 还没结束\n",
    "        \"\"\"\n",
    "\n",
    "        def hook_fn(param: torch.nn.Parameter):\n",
    "            # 有些参数可能没有梯度（如被冻结）\n",
    "            if param.grad is None:\n",
    "                return\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # 对该参数的梯度做 async all-reduce\n",
    "            # ------------------------------------------------\n",
    "            handle = dist.all_reduce(\n",
    "                param.grad,\n",
    "                op=dist.ReduceOp.SUM,\n",
    "                async_op=True\n",
    "            )\n",
    "\n",
    "            # 保存句柄，step 前需要 wait\n",
    "            self.async_handles.append(handle)\n",
    "\n",
    "            # ⚠️ register_post_accumulate_grad_hook\n",
    "            #    不允许返回任何值\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def finalize_gradients(self):\n",
    "        \"\"\"\n",
    "        在 optimizer.step() 之前调用：\n",
    "\n",
    "        - 等待所有 async all-reduce 完成\n",
    "        - 对梯度做平均\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. 等待所有通信完成\n",
    "        for handle in self.async_handles:\n",
    "            handle.wait()\n",
    "        self.async_handles.clear()\n",
    "\n",
    "        # 2. 梯度平均\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.div_(self.world_size)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 2. Bucket 类：DDP 的核心数据结构\n",
    "# ============================================================\n",
    "\n",
    "class GradientBucket:\n",
    "    \"\"\"\n",
    "    一个 bucket 管理一组参数的梯度通信：\n",
    "\n",
    "    - 多个参数的梯度被 flatten 到一个连续 buffer\n",
    "    - 当 bucket 中所有参数梯度 ready 后，触发一次 all-reduce\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: list[torch.nn.Parameter], bucket_id: int):\n",
    "        self.bucket_id = bucket_id\n",
    "        self.params = params\n",
    "\n",
    "        # 已经 ready 的参数数量\n",
    "        self.ready_count = 0\n",
    "\n",
    "        # async all-reduce 的通信句柄\n",
    "        self.comm_handle = None\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 创建连续的梯度 buffer\n",
    "        # ----------------------------------------------------\n",
    "        total_numel = sum(p.numel() for p in params)\n",
    "        dtype = params[0].dtype\n",
    "        device = params[0].device\n",
    "\n",
    "        self.flat_buffer = torch.zeros(\n",
    "            total_numel, dtype=dtype, device=device\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 为每个参数创建 buffer 视图\n",
    "        # ----------------------------------------------------\n",
    "        self.param_to_view = {}\n",
    "        offset = 0\n",
    "        for p in params:\n",
    "            numel = p.numel()\n",
    "            self.param_to_view[p] = (\n",
    "                self.flat_buffer[offset: offset + numel]\n",
    "                .view(p.shape)\n",
    "            )\n",
    "            offset += numel\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        在每个 forward 之前调用：\n",
    "        - 清空计数器\n",
    "        - 清空通信句柄\n",
    "        \"\"\"\n",
    "        self.ready_count = 0\n",
    "        self.comm_handle = None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 3. Bucket 化 DDP（接近 PyTorch DDP 真正实现）\n",
    "# ============================================================\n",
    "\n",
    "class TeachingDDPBucketed(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    教学版 Bucket DDP（工业级思路）：\n",
    "\n",
    "    - 参数按 bucket 分组\n",
    "    - 每个 bucket 一次 all-reduce\n",
    "    - 在 backward 中实现计算 / 通信 overlap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, bucket_size_mb: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # bucket 大小（MB -> Bytes）\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 初始化参数同步\n",
    "        # ----------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "            for buffer in self.model.buffers():\n",
    "                dist.broadcast(buffer.data, src=0)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 参数分桶（非常关键）\n",
    "        # ----------------------------------------------------\n",
    "        self.buckets: list[GradientBucket] = []\n",
    "        self.param_to_bucket: dict[torch.nn.Parameter, GradientBucket] = {}\n",
    "\n",
    "        # 只考虑需要梯度的参数\n",
    "        trainable_params = [\n",
    "            p for p in self.model.parameters() if p.requires_grad\n",
    "        ]\n",
    "\n",
    "        # ⚠️ 反向传播顺序：从输出层到输入层\n",
    "        # 所以 bucket 要按“反向顺序”构建\n",
    "        reversed_params = list(reversed(trainable_params))\n",
    "\n",
    "        current_bucket_params = []\n",
    "        current_bucket_bytes = 0\n",
    "\n",
    "        for param in reversed_params:\n",
    "            param_bytes = param.numel() * param.element_size()\n",
    "\n",
    "            # 如果当前 bucket 已满，先创建 bucket\n",
    "            if (\n",
    "                current_bucket_params\n",
    "                and current_bucket_bytes + param_bytes > self.bucket_size_bytes\n",
    "            ):\n",
    "                self._create_bucket(current_bucket_params)\n",
    "                current_bucket_params = []\n",
    "                current_bucket_bytes = 0\n",
    "\n",
    "            current_bucket_params.append(param)\n",
    "            current_bucket_bytes += param_bytes\n",
    "\n",
    "        # 最后一个 bucket\n",
    "        if current_bucket_params:\n",
    "            self._create_bucket(current_bucket_params)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 3. 注册 backward hooks\n",
    "        # ----------------------------------------------------\n",
    "        for param in trainable_params:\n",
    "            param.register_post_accumulate_grad_hook(\n",
    "                self._create_bucket_hook(param)\n",
    "            )\n",
    "\n",
    "    def _create_bucket(self, params: list[torch.nn.Parameter]):\n",
    "        bucket = GradientBucket(params, bucket_id=len(self.buckets))\n",
    "        self.buckets.append(bucket)\n",
    "\n",
    "        for p in params:\n",
    "            self.param_to_bucket[p] = bucket\n",
    "\n",
    "    def _create_bucket_hook(self, param: torch.nn.Parameter):\n",
    "        \"\"\"\n",
    "        每个参数的 hook：\n",
    "        - 把梯度拷贝进 bucket buffer\n",
    "        - 如果 bucket 满了，立刻发起 async all-reduce\n",
    "        \"\"\"\n",
    "\n",
    "        def hook_fn(p: torch.nn.Parameter):\n",
    "            if p.grad is None:\n",
    "                return\n",
    "\n",
    "            bucket = self.param_to_bucket[p]\n",
    "\n",
    "            # A. 拷贝梯度到 bucket buffer\n",
    "            bucket.param_to_view[p].copy_(p.grad)\n",
    "\n",
    "            # B. 标记一个参数 ready\n",
    "            bucket.ready_count += 1\n",
    "\n",
    "            # C. bucket 内所有参数 ready → 启动通信\n",
    "            if bucket.ready_count == len(bucket.params):\n",
    "                bucket.comm_handle = dist.all_reduce(\n",
    "                    bucket.flat_buffer,\n",
    "                    op=dist.ReduceOp.SUM,\n",
    "                    async_op=True\n",
    "                )\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # 每次 forward 前重置 bucket 状态\n",
    "        for bucket in self.buckets:\n",
    "            bucket.reset()\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def finalize_gradients(self):\n",
    "        \"\"\"\n",
    "        在 optimizer.step() 之前调用：\n",
    "\n",
    "        - 等待所有 bucket 的通信完成\n",
    "        - 对 bucket buffer 做平均\n",
    "        - 拷贝回各参数的 p.grad\n",
    "        \"\"\"\n",
    "\n",
    "        for bucket in self.buckets:\n",
    "            # 1. 等待通信\n",
    "            if bucket.comm_handle is not None:\n",
    "                bucket.comm_handle.wait()\n",
    "\n",
    "            # 2. 平均梯度\n",
    "            bucket.flat_buffer.div_(self.world_size)\n",
    "\n",
    "            # 3. 拷贝回参数梯度\n",
    "            for p in bucket.params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.copy_(bucket.param_to_view[p])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2_System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
