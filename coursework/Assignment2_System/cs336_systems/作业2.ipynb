{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8639c8be",
   "metadata": {},
   "source": [
    "# 四、分布式数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0219cf",
   "metadata": {},
   "source": [
    "# 实现分布式优化器封装\n",
    "\n",
    "实现了一个**参数分片（sharding）的分布式优化器封装器**，思想上接近 **ZeRO Stage-1 / FSDP 的 optimizer state sharding**\n",
    "\n",
    "## 一、整体功能概览\n",
    "\n",
    "`ShardedOptimizer` 的目标是：\n",
    "\n",
    "> **在数据并行（DDP）场景下，让不同 rank 只负责一部分参数的 optimizer state 和更新计算，从而节省显存。**\n",
    "\n",
    "核心思想是对于**参数本身，每个 rank 都有完整模型参数**，对于**优化器状态（momentum / Adam 的 exp_avg 等），只在参数 owner rank 上存在**， **参数更新只在 owner rank 上进行**。\n",
    "* **参数同步：更新后通过 `broadcast` 把新参数发给其他 rank**。\n",
    "\n",
    "## 二、整体架构\n",
    "\n",
    "```\n",
    "所有 rank：\n",
    "  拥有完整模型参数\n",
    "       │\n",
    "       ▼\n",
    "参数被 round-robin 分配 owner_rank\n",
    "       │\n",
    "       ▼\n",
    "每个 rank：\n",
    "  ├─ 全局 Optimizer（只用于 param_groups 管理）\n",
    "  └─ 本地 optim（只包含自己拥有的 params）\n",
    "       │\n",
    "       ▼\n",
    "step():\n",
    "  1. 本地 optim.step()（只更新自己那一片）\n",
    "  2. dist.broadcast（把更新后的参数同步给所有 rank）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、代码解读\n",
    "\n",
    "\n",
    "### 1。 初始化：`__init__`\n",
    "\n",
    "```python\n",
    "class ShardedOptimizer(Optimizer):\n",
    "```\n",
    "\n",
    "继承 PyTorch 原生 `Optimizer`，因此：\n",
    "\n",
    "这样可以被 Trainer / 训练循环无缝使用，并且拥有 `param_groups / zero_grad / step` 接口\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.1 记录原始 optimizer 类型\n",
    "\n",
    "```python\n",
    "self.optimizer_cls = optimizer_cls\n",
    "self.optimizer_kwargs = kwargs\n",
    "```\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "ShardedOptimizer(\n",
    "    model.parameters(),\n",
    "    torch.optim.Adam,\n",
    "    lr=1e-4\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2 获取分布式信息\n",
    "\n",
    "```python\n",
    "self.rank = dist.get_rank()\n",
    "self.world_size = dist.get_world_size()\n",
    "```\n",
    "\n",
    "用于：\n",
    "\n",
    "* 决定参数 owner\n",
    "* 决定 broadcast 源\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.3 参数分配辅助变量\n",
    "\n",
    "```python\n",
    "self.global_param_counter = 0\n",
    "self.local_param_groups = []\n",
    "```\n",
    "\n",
    "* `global_param_counter`：**全局参数编号**\n",
    "* 用 round-robin 把参数均匀分配给各个 rank\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4 调用父类构造函数（关键）\n",
    "\n",
    "```python\n",
    "super().__init__(params, defaults=kwargs)\n",
    "```\n",
    "\n",
    "\n",
    "`Optimizer.__init__` 内部会调用 `self.add_param_group`，但这里的 `add_param_group` 已经被 **重写**\n",
    "\n",
    "这使得 **参数分片逻辑嵌入在初始化阶段**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5 创建“本地 optimizer”\n",
    "\n",
    "```python\n",
    "self.optim = self.optimizer_cls(self.local_param_groups, **self.optimizer_kwargs)\n",
    "```\n",
    "\n",
    "`self.optim` **只包含当前 rank 拥有的参数**，只有这些参数才会分配 optimizer state（节省显存）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 参数分片逻辑：`add_param_group`\n",
    "\n",
    "这是整个实现的**核心函数**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1 给每个参数打 owner 标记\n",
    "\n",
    "```python\n",
    "p._owner_rank = self.global_param_counter % self.world_size\n",
    "```\n",
    "\n",
    "round-robin 分配, `_owner_rank` 是动态属性（hack，但可行）\n",
    "\n",
    "示例（world_size=4）：\n",
    "\n",
    "| 参数编号 | owner_rank |\n",
    "| ---- | ---------- |\n",
    "| p0   | 0          |\n",
    "| p1   | 1          |\n",
    "| p2   | 2          |\n",
    "| p3   | 3          |\n",
    "| p4   | 0          |\n",
    "| p5   | 1          |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2 加入“全局 optimizer”\n",
    "\n",
    "```python\n",
    "super().add_param_group(param_group)\n",
    "```\n",
    "\n",
    "**为什么必须这么做？**\n",
    "\n",
    "`self.param_groups` 需要包含 **所有参数**,用于进行`zero_grad`，遍历所有参数做 `broadcast`，并且Trainer / AMP 兼容，**注意**：这个“全局 optimizer”**不做 step**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3 构造本地 param group\n",
    "\n",
    "```python\n",
    "local_group['params'] = [\n",
    "    p for p in param_group['params']\n",
    "    if p._owner_rank == self.rank\n",
    "]\n",
    "```\n",
    "\n",
    "它只保留属于当前 rank 的参数，其他超参（lr、weight_decay）保持一致。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.4 加入本地 optimizer\n",
    "\n",
    "```python\n",
    "self.optim.add_param_group(local_group)\n",
    "```\n",
    "\n",
    "或者（初始化期间）缓存起来：\n",
    "\n",
    "```python\n",
    "self.local_param_groups.append(local_group)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 参数更新逻辑：`step`\n",
    "\n",
    "```python\n",
    "def step(self, closure=None, **kwargs):\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1：本地更新\n",
    "\n",
    "```python\n",
    "self.optim.step(**kwargs)\n",
    "```\n",
    "\n",
    "* **只更新自己拥有的参数**\n",
    "* 只有这些参数有 optimizer state\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2：参数同步（核心）\n",
    "\n",
    "```python\n",
    "dist.broadcast(p.data, src=p._owner_rank)\n",
    "```\n",
    "\n",
    "如果是 owner 则发送最新参数，如果不是 owner 就接收参数覆盖本地副本。最终所有 rank 的模型参数 **完全一致**。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.梯度清零：`zero_grad`\n",
    "\n",
    "```python\n",
    "super().zero_grad(set_to_none=set_to_none)\n",
    "```\n",
    "\n",
    "清除所有参数的梯度，即使不是 owner，也要清 grad（否则梯度会累积）。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、这个实现“像什么”？\n",
    "\n",
    "| 技术           | 相似度  | 区别                        |\n",
    "| ------------ | ---- | ------------------------- |\n",
    "| ZeRO Stage-1 | ⭐⭐⭐⭐ | 真实 ZeRO 会用 reduce-scatter |\n",
    "| FSDP         | ⭐⭐   | 没有参数 flatten / overlap    |\n",
    "| DDP          | ⭐    | DDP 是全量 optimizer state   |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、优点与限制\n",
    "\n",
    "###  优点\n",
    "\n",
    "1. **显存节省**\n",
    "\n",
    "   * optimizer state 约减少到 `1/world_size`\n",
    "2. **逻辑清晰**\n",
    "\n",
    "   * 教学 & 原理验证非常好\n",
    "3. **兼容任意 Optimizer**\n",
    "\n",
    "   * Adam / SGD / Adafactor 等\n",
    "\n",
    "---\n",
    "\n",
    "### 局限\n",
    "\n",
    "1. **通信成本高**\n",
    "\n",
    "   ```python\n",
    "   dist.broadcast(p.data)\n",
    "   ```\n",
    "\n",
    "   * 每个参数一次 broadcast（非常慢）\n",
    "\n",
    "2. **参数粒度过细**\n",
    "\n",
    "   * 实际系统会 **flatten 后按 bucket 同步**\n",
    "\n",
    "3. **没有 overlap**\n",
    "\n",
    "   * 计算与通信完全串行\n",
    "\n",
    "4. **依赖 monkey-patch 参数属性**\n",
    "\n",
    "   * `_owner_rank` 不够“干净”\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "\n",
    "> 用 round-robin 分配参数 owner，\n",
    "> 只在 owner rank 上更新参数，\n",
    "> 再用 broadcast 同步权重，\n",
    "> 从而显著降低 optimizer 显存占用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Optimizer\n",
    "from typing import Any, Type, Dict, Iterable\n",
    "\n",
    "\n",
    "class TeachingShardedOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    教学版：Optimizer State Sharding（类似 ZeRO Stage-1）\n",
    "\n",
    "    核心思想：\n",
    "    - 每个 rank 拥有【完整模型参数】\n",
    "    - 每个参数被分配一个唯一的 owner rank\n",
    "    - 只有 owner rank：\n",
    "        - 持有该参数的 optimizer state\n",
    "        - 负责执行 optimizer.step()\n",
    "    - 参数更新后，通过 dist.broadcast 同步到所有 rank\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        base_optimizer_cls: Type[Optimizer],\n",
    "        **base_optimizer_kwargs: Any\n",
    "    ):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        - params: model.parameters()\n",
    "        - base_optimizer_cls: 如 torch.optim.Adam\n",
    "        - base_optimizer_kwargs: 如 lr, betas, weight_decay 等\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== 1. 保存原始 Optimizer 信息 =====\n",
    "        self.base_optimizer_cls = base_optimizer_cls\n",
    "        self.base_optimizer_kwargs = base_optimizer_kwargs\n",
    "\n",
    "        # ===== 2. 分布式环境信息 =====\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # ===== 3. 参数分片辅助变量 =====\n",
    "        # 用于给参数做 round-robin 编号\n",
    "        self.global_param_index = 0\n",
    "\n",
    "        # 在 __init__ 期间暂存属于本 rank 的 param groups\n",
    "        self._buffered_local_param_groups = []\n",
    "\n",
    "        # ===== 4. 初始化“全局 Optimizer”父类 =====\n",
    "        # 注意：Optimizer.__init__ 内部会调用 self.add_param_group\n",
    "        #       而我们在下面重写了 add_param_group\n",
    "        super().__init__(params, defaults=base_optimizer_kwargs)\n",
    "\n",
    "        # ===== 5. 创建“本地 shard Optimizer” =====\n",
    "        # 只包含当前 rank 拥有的参数\n",
    "        if self._buffered_local_param_groups:\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                self._buffered_local_param_groups,\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "        else:\n",
    "            # 边缘情况：当前 rank 没有分到任何参数\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                [],\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "\n",
    "    def add_param_group(self, param_group: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        教学重点函数：参数分片逻辑发生在这里\n",
    "\n",
    "        一个 param_group 会被“拆成两份”：\n",
    "        1. 全量 param_group → 交给父类 Optimizer（用于遍历 & 同步）\n",
    "        2. 本地 param_group → 只保留 owner == 当前 rank 的参数\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step A: 给 param_group 中的每个参数分配 owner rank\n",
    "        # -------------------------------------------------------\n",
    "        for param in param_group[\"params\"]:\n",
    "            if not hasattr(param, \"_owner_rank\"):\n",
    "                # round-robin 分配\n",
    "                param._owner_rank = self.global_param_index % self.world_size\n",
    "                self.global_param_index += 1\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step B: 注册到“全局 Optimizer”\n",
    "        # -------------------------------------------------------\n",
    "        # 这个 Optimizer：\n",
    "        # - 包含所有参数\n",
    "        # - 不负责 step\n",
    "        # - 只用于：\n",
    "        #     * zero_grad\n",
    "        #     * 参数遍历\n",
    "        #     * step 后的 broadcast\n",
    "        super().add_param_group(param_group)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step C: 构造“本地 shard param_group”\n",
    "        # -------------------------------------------------------\n",
    "        # 拷贝除 params 以外的超参（lr / weight_decay 等）\n",
    "        local_param_group = {\n",
    "            key: value\n",
    "            for key, value in param_group.items()\n",
    "            if key != \"params\"\n",
    "        }\n",
    "\n",
    "        # 只保留 owner 是当前 rank 的参数\n",
    "        local_param_group[\"params\"] = [\n",
    "            param\n",
    "            for param in param_group[\"params\"]\n",
    "            if param._owner_rank == self.rank\n",
    "        ]\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step D: 加入本地 shard Optimizer\n",
    "        # -------------------------------------------------------\n",
    "        if hasattr(self, \"local_shard_optimizer\"):\n",
    "            # 正常情况：__init__ 已结束\n",
    "            if local_param_group[\"params\"]:\n",
    "                self.local_shard_optimizer.add_param_group(local_param_group)\n",
    "        else:\n",
    "            # __init__ 过程中，先缓存\n",
    "            self._buffered_local_param_groups.append(local_param_group)\n",
    "\n",
    "    def step(self, closure=None, **kwargs):\n",
    "        \"\"\"\n",
    "        一个 step 分为两步：\n",
    "\n",
    "        1. 本地更新（只更新 owner 是当前 rank 的参数）\n",
    "        2. 参数同步（broadcast 最新参数到所有 rank）\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 1: 本地 shard Optimizer 更新\n",
    "        # -------------------------------------------------------\n",
    "        # 只有 owner rank 会真正更新对应参数\n",
    "        self.local_shard_optimizer.step(**kwargs)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 2: 参数同步（broadcast）\n",
    "        # -------------------------------------------------------\n",
    "        # 遍历“全局 Optimizer”中的所有参数\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group[\"params\"]:\n",
    "                # owner rank 作为 src\n",
    "                # 其他 rank 接收并覆盖本地参数\n",
    "                dist.broadcast(\n",
    "                    tensor=param.data,\n",
    "                    src=param._owner_rank\n",
    "                )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = True):\n",
    "        \"\"\"\n",
    "        梯度必须在所有 rank 上清零：\n",
    "        即使该 rank 不是参数 owner，也会参与反向传播\n",
    "        \"\"\"\n",
    "        super().zero_grad(set_to_none=set_to_none)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
