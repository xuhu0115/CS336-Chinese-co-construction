{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8639c8be",
   "metadata": {},
   "source": [
    "# 四、分布式数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd826a49",
   "metadata": {},
   "source": [
    "### 一 分布式数据并行训练（Distributed Data Parallel Training）\n",
    "\n",
    "在作业的这一部分中，我们将探索使用多张 GPU 训练语言模型的方法，重点关注**数据并行（data parallelism）**。\n",
    "我们将首先对 PyTorch 中的分布式通信进行一个入门介绍。接着，我们会学习一种朴素（naive）的分布式数据并行训练实现方式，并在此基础上实现和评测多种用于提升通信效率的改进方案。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 PyTorch 中的单机分布式通信（Single-Node Distributed Communication in PyTorch）\n",
    "\n",
    "我们先来看一个 PyTorch 中的简单分布式应用，其目标是：\n",
    "**生成四个随机整数张量，并计算它们的和。**\n",
    "\n",
    "在下面的分布式示例中，我们会启动 **4 个 worker 进程**，每个进程都会生成一个随机整数张量。\n",
    "为了对这些进程中的张量求和，我们将调用 **all-reduce** 这一集合通信（collective communication）操作。\n",
    "\n",
    "`all-reduce` 的作用是：\n",
    "\n",
    "> 将所有进程中的张量进行规约（这里是求和），并用规约后的结果 **原地替换** 每个进程中的原始张量。\n",
    "\n",
    "也就是说，在 all-reduce 完成后，每个进程中的张量都会变成相同的“全局求和结果”。\n",
    "\n",
    "下面我们来看代码。\n",
    "\n",
    "---\n",
    "\n",
    "在运行下面的脚本后，我们会得到如下输出。\n",
    "可以看到：在 all-reduce 之前，每个 worker 进程持有的张量都不同；而在 all-reduce 操作之后，由于所有进程中的张量被求和并同步，每个进程中的 `data` 都被**原地修改**为相同的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "如果你多次运行这个脚本，你会发现打印输出的顺序并不是确定的。这是因为该应用运行在分布式环境中，我们无法控制各个进程执行打印语句的精确顺序。\n",
    "**唯一可以保证的是**：在 all-reduce 操作完成之后，所有进程都会持有**逐位（bitwise）完全一致**的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba55c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distributed_demo.py\n",
    "# =========================\n",
    "# 标准库 & PyTorch 相关导入\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 分布式环境初始化函数\n",
    "# =========================\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"\n",
    "    初始化分布式通信环境（Process Group）\n",
    "\n",
    "    参数：\n",
    "    - rank: 当前进程的编号（0 ~ world_size-1）\n",
    "    - world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # 指定“主进程”的地址\n",
    "    # 所有进程都会通过这个地址进行 rendezvous（集合）\n",
    "    # 单机多进程时使用 localhost\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "\n",
    "    # 指定主进程监听的端口号\n",
    "    # 所有进程必须保持一致\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组（非常关键的一步）\n",
    "    # backend=\"gloo\"：使用 gloo 通信后端（CPU 通用，支持多平台）\n",
    "    # rank：当前进程在所有进程中的唯一编号\n",
    "    # world_size：参与通信的总进程数\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\",\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 每个进程实际执行的函数\n",
    "# =========================\n",
    "def distributed_demo(rank, world_size):\n",
    "    \"\"\"\n",
    "    每个进程都会执行这个函数一次\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化分布式通信环境\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # 每个进程各自生成一个本地张量\n",
    "    # torch.randint 是“本地操作”，不同 rank 得到的值不同\n",
    "    # 这里生成一个 shape 为 (3,) 的一维张量\n",
    "    data = torch.randint(0, 10, (3,))\n",
    "\n",
    "    # 打印 all-reduce 之前的数据\n",
    "    # 注意：由于是多进程并行执行，打印顺序不确定\n",
    "    print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "\n",
    "    # 核心操作：all-reduce\n",
    "    # - 对所有 rank 上的 data 做规约（默认是 SUM）\n",
    "    # - 结果会“原地”写回到每个 rank 的 data 中\n",
    "    # - async_op=False 表示同步执行（阻塞，直到完成）\n",
    "    dist.all_reduce(data, async_op=False)\n",
    "\n",
    "    # 打印 all-reduce 之后的数据\n",
    "    # 此时每个 rank 的 data 都是完全相同的\n",
    "    print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 程序主入口\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # world_size 表示总进程数\n",
    "    # 在 DDP 中通常等于 GPU 数量\n",
    "    world_size = 4\n",
    "\n",
    "    # 使用 torch.multiprocessing.spawn 启动多进程\n",
    "    mp.spawn(\n",
    "        fn=distributed_demo,   # 每个进程要执行的函数\n",
    "        args=(world_size,),    # 传给函数的额外参数（rank 会自动作为第一个参数）\n",
    "        nprocs=world_size,     # 启动的进程数量\n",
    "        join=True              # 主进程是否等待所有子进程结束\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee989a51",
   "metadata": {},
   "source": [
    "由于分布式环境不支持ipynb的notebook格式，只能先写入.py文件再在notebook中运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11497694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank [Gloo] Rank [Gloo] Rank [Gloo] Rank 021 is connected to 3 is connected to  is connected to 3 is connected to 33 peer ranks. 3 peer ranks.  peer ranks. Expected number of connected peer ranks is :  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 3Expected number of connected peer ranks is : 33\n",
      "\n",
      "3\n",
      "\n",
      "rank 2 data (before all-reduce): tensor([1, 8, 8])rank 3 data (before all-reduce): tensor([2, 6, 8])rank 0 data (before all-reduce): tensor([0, 3, 4])\n",
      "\n",
      "\n",
      "rank 1 data (before all-reduce): tensor([1, 9, 3])\n",
      "rank 0 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 3 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 2 data (after all-reduce): tensor([ 4, 26, 23])rank 1 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_demo.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d60b11",
   "metadata": {},
   "source": [
    "# 代码解释\n",
    "\n",
    "**这段代码在一台机器上启动了 4 个相互独立的进程。每个进程都会生成一个随机张量，随后通过 `all_reduce` 操作将所有进程中的张量求和，并把最终结果同步回每一个进程。**\n",
    "它实际上构成了**数据并行训练中“梯度同步”机制的最小可运行原型**。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、整体执行流程\n",
    "\n",
    "当你执行：\n",
    "\n",
    "```bash\n",
    "python distributed_hello_world.py\n",
    "```\n",
    "\n",
    "程序并不是只跑了一个进程。首先会启动一个主进程，随后主进程通过 `mp.spawn` 再拉起 4 个子进程。每个子进程都会被分配一个唯一的 `rank`（从 0 到 3），并加入同一个分布式 `process group`。\n",
    "\n",
    "在这个过程中，每个进程都会各自生成数据，然后通过 `all_reduce` 进行通信与同步。需要特别强调的是，这里的每个 `rank` 都是**独立的 Python 进程，而不是线程**，它们之间并不共享内存，只能通过通信原语交换数据。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、setup：初始化分布式通信环境\n",
    "\n",
    "```python\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "```\n",
    "\n",
    "`setup` 函数承担的角色，可以理解为分布式程序的“入群仪式”。在这里，每个进程会被正确地注册到同一个通信世界中。\n",
    "\n",
    "首先需要指定 `MASTER_ADDR` 和 `MASTER_PORT`。它们用于告诉所有进程，谁是整个通信过程中的“总协调者”（Rendezvous Server）。在单机多进程的场景下，直接使用 `localhost` 即可；如果是多机分布式训练，这里则需要填写主节点的 IP 地址。\n",
    "\n",
    "完成地址配置后，真正关键的一步是调用 `dist.init_process_group`：\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend=\"gloo\",\n",
    "    rank=rank,\n",
    "    world_size=world_size\n",
    ")\n",
    "```\n",
    "\n",
    "这行代码定义了当前进程在分布式系统中的身份和规模。`backend=\"gloo\"` 指定了通信后端，适合 CPU 或通用场景；`rank` 是当前进程的唯一编号；`world_size` 则表示整个进程组一共有多少个成员。\n",
    "\n",
    "从概念上看，`rank` 回答的是“我是谁”，而 `world_size` 回答的是“我们一共有多少人”。在 DDP 场景中，通常可以直接把它理解为：**一个 GPU 对应一个 rank**。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、distributed_demo：每个进程实际执行的逻辑\n",
    "\n",
    "```python\n",
    "def distributed_demo(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "```\n",
    "\n",
    "`distributed_demo` 是通过 `mp.spawn` 启动后，每一个子进程都会独立执行一次的函数。它的第一步，就是调用前面介绍的 `setup`，完成分布式通信的初始化。\n",
    "\n",
    "### 4.1 生成本地数据\n",
    "\n",
    "```python\n",
    "data = torch.randint(0, 10, (3,))\n",
    "```\n",
    "\n",
    "在这一步，每个进程都会生成一个属于自己的随机张量。由于各个进程的随机状态彼此独立，因此不同 rank 上得到的结果也不同。例如，某一次运行中，可能会出现：\n",
    "\n",
    "* rank 0 得到 `[4, 4, 7]`\n",
    "* rank 1 得到 `[9, 5, 3]`\n",
    "* rank 2 得到 `[6, 0, 7]`\n",
    "* rank 3 得到 `[3, 7, 8]`\n",
    "\n",
    "这些数据在 `all_reduce` 之前是完全不一致的。\n",
    "\n",
    "### 4.2 all-reduce 之前的输出\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "```\n",
    "\n",
    "这里的打印顺序是不可预测的。由于 4 个进程是并行执行的，谁先输出完全取决于调度时机，这本身也是多进程程序的一个直观特征。\n",
    "\n",
    "### 4.3 核心操作：all_reduce\n",
    "\n",
    "```python\n",
    "dist.all_reduce(data, async_op=False)\n",
    "```\n",
    "\n",
    "`all_reduce` 会对所有 rank 上的 `data` 做一次集体通信：先把各个进程中的数据收集起来，然后按照指定的规则进行规约，默认是求和，最后再把结果写回到每一个进程中。整个过程是原地完成的，`data` 本身会被直接覆盖。\n",
    "\n",
    "从数学角度看，它等价于：\n",
    "\n",
    "```python\n",
    "data = data_rank0 + data_rank1 + data_rank2 + data_rank3\n",
    "```\n",
    "\n",
    "执行完成后，每个 rank 中的 `data` 都会变成完全一致的结果。这正是分布式训练中梯度同步的核心机制。\n",
    "\n",
    "### 4.4 all-reduce 之后的输出\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "```\n",
    "\n",
    "此时会看到，不论是哪个 rank，打印出来的张量内容都已经相同，例如：\n",
    "\n",
    "\n",
    "## 五、主入口：启动多进程\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "```\n",
    "\n",
    "在主入口中，首先定义了 `world_size`，也就是要启动的进程数量。在 DDP 的实际训练中，这个值通常与 GPU 的数量保持一致。\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "## 六、代码与真实 DDP 训练的对应关系\n",
    "\n",
    "代码几乎完整复刻了 DDP 中最核心的一步，只是把复杂的模型计算简化成了随机张量的生成。\n",
    "\n",
    "在这里，`data` 可以类比为模型参数的梯度，`torch.randint` 对应真实训练中的前向与反向计算，而 `all_reduce(data)` 则等价于梯度同步操作。示例中只执行了一次 `all_reduce`，而在真实训练中，这个过程会在每一次 iteration 的反向传播之后反复发生。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc9cf6",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1.1 分布式应用基准测试的最佳实践\n",
    "\n",
    "（Best Practices for Benchmarking Distributed Applications）\n",
    "\n",
    "在本作业的这一部分中，你将对**分布式应用进行基准测试**，以更好地理解**通信带来的开销**。以下是一些推荐的最佳实践：\n",
    "\n",
    "* **尽可能在同一台机器上运行基准测试**，以便进行可控、公平的对比。\n",
    "\n",
    "* **在正式计时之前进行多次 warm-up（预热）**。\n",
    "  这一点对 **NCCL 通信调用**尤为重要。通常 **5 次预热迭代** 就足够了。\n",
    "\n",
    "* **在 GPU 上进行基准测试时，务必调用 `torch.cuda.synchronize()`**，以等待 CUDA 操作真正完成。\n",
    "  注意：即使在通信操作中设置了 `async_op=False`，这一步仍然是必须的，因为该调用只保证操作被**加入 GPU 队列**，而不是通信实际完成。³\n",
    "\n",
    "* **不同 rank 的测量时间可能略有差异**，因此通常会在所有 rank 之间**聚合测量结果**以提高估计的稳定性。\n",
    "  可以使用 **all-gather 集合通信操作**（特别是 `dist.all_gather_object`）来收集所有 rank 的结果。\n",
    "\n",
    "* **一般建议先在 CPU 上使用 Gloo 后端进行本地调试**，然后在实际问题中根据需要使用 GPU + NCCL 进行基准测试。\n",
    "  在两种后端之间切换通常只需要修改：\n",
    "\n",
    "  * `init_process_group` 中的 backend 参数\n",
    "  * 张量的设备（CPU / GPU）转换\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01bea2",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "## 问题（distributed_communication_single_node）：5 分\n",
    "\n",
    "请编写一个脚本，用于在**单节点多进程（single-node multi-process）**设置下，**基准测试 all-reduce 操作的运行时间**。\n",
    "\n",
    "你可以参考前面给出的示例代码作为起点，并尝试改变以下配置参数：\n",
    "\n",
    "### 实验设置要求\n",
    "\n",
    "* **后端 + 设备类型**\n",
    "\n",
    "  * Gloo + CPU\n",
    "  * NCCL + GPU\n",
    "\n",
    "* **all-reduce 数据规模**\n",
    "\n",
    "  * 数据类型：`float32`\n",
    "  * 张量大小：\n",
    "\n",
    "    * 1MB\n",
    "    * 10MB\n",
    "    * 100MB\n",
    "    * 1GB\n",
    "\n",
    "* **进程数量**\n",
    "\n",
    "  * 2 个进程\n",
    "  * 4 个进程\n",
    "  * 6 个进程\n",
    "\n",
    "### 资源限制\n",
    "\n",
    "* 最多可使用 **6 张 GPU**\n",
    "* 每一次基准测试运行时间 **不得超过 5 分钟**\n",
    "\n",
    "---\n",
    "\n",
    "## 交付内容（Deliverable）\n",
    "\n",
    "* 提供 **图表（plot）和/或表格（table）**，用于比较不同实验设置下的性能表现\n",
    "* 并附上 **2–3 句话的分析说明**，总结你的实验结果，以及你对各因素（后端、数据规模、进程数等）如何相互影响的理解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6259ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:# 有多少个GPU就增加测试用例，当然一个也能跑\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bec746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2600 ms\n",
      "  Bandwidth      : 4.0336 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0162 ms\n",
      "  Bandwidth      : 64.5348 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 1.9385 ms\n",
      "  Bandwidth      : 5.4092 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0178 ms\n",
      "  Bandwidth      : 589.5505 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 18.0035 ms\n",
      "  Bandwidth      : 5.8243 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0177 ms\n",
      "  Bandwidth      : 5927.2864 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 169.5600 ms\n",
      "  Bandwidth      : 6.1841 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0180 ms\n",
      "  Bandwidth      : 58252.2717 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32464a",
   "metadata": {},
   "source": [
    "## 一、Gloo和NCCL的基本概念\n",
    "\n",
    "在 PyTorch 中，多进程/多 GPU 之间进行通信（比如 `all_reduce`, `broadcast`, `all_gather`）需要 **通信后端（backend）**。\n",
    "常用的两种是：\n",
    "\n",
    "| 名称       | 适用设备            | 特点                                                 |\n",
    "| -------- | --------------- | -------------------------------------------------- |\n",
    "| **Gloo** | CPU / GPU（有限支持） | 跨平台，支持 CPU，多节点可以用 TCP/IP 通信，适合 CPU 张量，简单可靠         |\n",
    "| **NCCL** | GPU（NVIDIA GPU） | NVIDIA 官方库，专门优化 GPU 间通信，支持高速 PCIe、NVLink、网络通信，性能最好 |\n",
    "\n",
    "---\n",
    "\n",
    "## 二、Gloo\n",
    "\n",
    "Gloo 在 CPU 和 GPU 都可以用，但 GPU 支持有限，通常用在 CPU 张量上，它支持跨节点网络通信（TCP/IP）。并且使用方便，启动和调试比 NCCL 容易。在 CPU 环境下是**默认且可靠的选择**。\n",
    "\n",
    "它通常使用在单机 CPU 多进程训练，或者小规模实验和调试，没有 GPU 的服务器环境。\n",
    "\n",
    "在CPU 通信效率中等，GPU 上比 NCCL 慢，不适合大规模深度学习训练。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、NCCL（NVIDIA Collective Communication Library）\n",
    "\n",
    "\n",
    "它是专门为 NVIDIA GPU 设计的，支持多 GPU、多节点高速通信。并且优化了**PCIe 直连**，**NVLink（GPU 内部高速总线）**，**网络互联（Ethernet / InfiniBand）**，PyTorch 会自动使用 NCCL 的 ring / tree 通信算法做 `all_reduce`、`all_gather` 等操作。\n",
    "\n",
    "通常在单机多 GPU 训练（如 2~8 GPU）上使用。分布式训练深度学习大模型，高性能 GPU 集群使用较多。\n",
    "\n",
    "能将GPU 间带宽最大化，并且延迟极低（特别是大张量）， NCCL + CUDA 几乎是训练大模型的默认选择。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、总结对比\n",
    "\n",
    "| 特性           | Gloo            | NCCL               |\n",
    "| ------------ | --------------- | ------------------ |\n",
    "| 设备           | CPU（主要），GPU（有限） | GPU（NVIDIA 专用）     |\n",
    "| 性能           | 中等              | 很高                 |\n",
    "| 跨节点支持        | TCP/IP          | NVLink / PCIe / 网络 |\n",
    "| 用途           | CPU 分布式 / 小实验   | GPU 多卡训练 / 大模型     |\n",
    "| PyTorch 默认推荐 | CPU → Gloo      | GPU → NCCL         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00027",
   "metadata": {},
   "source": [
    "\n",
    "## 核心模块的作用划分\n",
    "\n",
    "代码依赖的核心模块主要有两个。`torch.distributed` 是 PyTorch 的分布式通信基础设施，负责进程组的初始化和管理，并提供诸如 `all_reduce`、`broadcast`、`all_gather` 等集体通信原语。所有跨进程的数据同步，最终都会落在这个模块上。\n",
    "\n",
    "另一个关键组件是 `torch.multiprocessing.spawn`。它用于启动多个 Python 进程，并为每个进程分配一个唯一的 `rank`。在分布式训练或通信 benchmark 中，通常会让 `rank` 与 GPU 编号一一对应。需要强调的是，这里采用的是多进程模型，而不是多线程模型，每个 `rank` 都是一个完全独立的 Python 进程，进程之间只能通过通信原语交换数据。\n",
    "\n",
    "---\n",
    "\n",
    "## 分布式环境的初始化过程\n",
    "\n",
    "分布式环境的初始化集中在 `setup` 函数中，该函数需要指定主节点地址、端口号、当前进程的 `rank`、进程总数以及通信后端。\n",
    "\n",
    "首先需要设置 `MASTER_ADDR` 和 `MASTER_PORT`。这两个环境变量定义了主节点（通常是 `rank 0`）的地址，所有进程都会通过这个地址完成 rendezvous。即使是在单机多卡的场景下，这一步也是必不可少的，因为 PyTorch 的分布式初始化依然需要一个统一的入口。\n",
    "\n",
    "随后调用 `dist.init_process_group` 来真正建立进程组。这里需要明确指定通信后端、当前进程的全局编号以及总进程数。不同的 backend 对应不同的使用场景：`gloo` 更适合 CPU 或小规模通信，`nccl` 是 GPU 高性能通信的事实标准，而 `mpi` 则更多用于传统 HPC 环境。\n",
    "\n",
    "---\n",
    "\n",
    "## 资源清理\n",
    "\n",
    "在 benchmark 结束后，代码会显式调用 `dist.destroy_process_group()` 来销毁进程组。这一步的目的并不仅仅是“干净退出”，更重要的是避免程序异常终止时遗留通信状态，从而导致后续运行出现死锁或挂起。\n",
    "\n",
    "如果程序运行在 GPU 环境下，还会额外调用 `torch.cuda.empty_cache()`。需要注意的是，这并不是释放显存给系统，而只是清空 PyTorch 自己维护的 CUDA 缓存。在 benchmark 场景中，这样做可以减少显存碎片对测试结果的干扰。\n",
    "\n",
    "---\n",
    "\n",
    "## `benchmark_all_reduce` 的执行流程\n",
    "\n",
    "`benchmark_all_reduce` 是整个测试流程的核心函数。进入函数后，首先会完成分布式环境的初始化，并在 GPU 场景下显式绑定当前进程使用的设备。通常的约定是让 `rank` 与 GPU 编号一一对应，即 `rank 0` 使用 GPU 0，`rank 1` 使用 GPU 1，这样可以避免多进程竞争同一张卡。\n",
    "\n",
    "接下来构造用于测试的张量。代码通过指定张量大小（以 MB 为单位）来计算元素个数，并创建一个 `float32` 类型的随机张量。这里使用 `randn` 只是为了填充数据，具体的数值分布对通信性能并没有实质影响。\n",
    "\n",
    "在正式计时之前，代码会先进行多次 warm-up。这一步在 GPU 通信 benchmark 中非常关键，因为它可以触发 CUDA kernel 的惰性初始化、建立 NCCL communicator、让 GPU 频率进入稳定状态，同时填充相关的缓存和流水线。如果省略 warm-up，测到的时间往往会明显偏大且不稳定。由于 CUDA 执行是异步的，warm-up 之后还需要显式调用 `torch.cuda.synchronize()`，确保所有操作真正完成。\n",
    "\n",
    "在正式 benchmark 开始前，代码通过 `dist.barrier()` 让所有进程对齐，保证每个 `rank` 都在同一时刻进入计时阶段，避免由于调度差异导致的偏差。\n",
    "\n",
    "计时阶段会重复执行多次 `all_reduce`，并记录总耗时。通过多次迭代取平均，可以有效降低单次抖动带来的噪声。在 GPU 场景下，同样需要在计时结束后进行一次同步，否则测到的只是 kernel launch 时间，而不是完整的通信时间。\n",
    "\n",
    "---\n",
    "\n",
    "## 性能指标的计算方式\n",
    "\n",
    "测试结束后，代码会根据总耗时计算单次 `all_reduce` 的平均耗时，并进一步估算通信带宽。带宽的计算方式非常直接，即用张量大小除以平均耗时，并转换为 GB/s。\n",
    "\n",
    "需要注意的是，这只是一个简化模型，并没有区分具体的通信拓扑（例如 ring 或 tree）。因此，这个数值更适合用来做横向对比，而不应被理解为理论峰值带宽。\n",
    "\n",
    "---\n",
    "\n",
    "## 七、结果汇总\n",
    "\n",
    "为了避免多进程同时打印日志，代码只允许 `rank 0` 输出最终结果。同时，通过 `dist.all_gather_object` 将每个进程的本地结果汇总到一起。相比基于 Tensor 的 `all_gather`，这种方式可以直接传输 Python 对象，在 benchmark 和 profiling 场景中更加灵活。\n",
    "\n",
    "最后，所有进程都会执行清理逻辑，子进程在完成任务后直接退出，而主进程则返回汇总后的结果，整个 benchmark 流程至此结束。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d57918",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "## 问题（naive_ddp）：5 分\n",
    "\n",
    "**交付物**：编写一个脚本来通过在反向传播后对各个参数梯度进行全规约，以简单方式执行分布式数据并行训练。为了验证你的DDP实现的正确性，使用它在随机生成的数据上训练一个小玩具模型，并验证其权重与单进程训练的结果相匹配。\n",
    "\n",
    "---\n",
    "\n",
    "## 问题（naive_ddp_benchmarking）：3 分\n",
    "\n",
    "在这种简单的DDP实现中，参数在每个反向传播后在各个rank之间单独进行全规约。为了更好地理解数据并行训练的开销，创建一个脚本来对之前实现的语言模型进行基准测试，该模型使用这种简单的DDP实现进行训练。测量每个训练步骤的总时间以及用于通信梯度的时间比例。收集在单节点设置（1个节点 x 2个GPU）下对XL模型的测量结果，如§1.1.2中所述。\n",
    "\n",
    "**交付物**：描述你的基准测试设置，以及每个训练迭代的测量时间以及用于通信梯度的时间。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a310004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_model_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_model_demo.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 一、定义一个简单的全连接神经网络\n",
    "# ============================\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    一个最简单的三层全连接网络，用于 MNIST 分类\n",
    "    输入：28×28 展平后的 784 维向量\n",
    "    输出：10 类（0~9）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一层全连接：输入层 -> 隐藏层\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # 第二层全连接：隐藏层 -> 隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 输出层：隐藏层 -> 类别数\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播逻辑\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # 使用 log_softmax，方便后续配合 NLLLoss\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 二、分布式环境初始化与清理\n",
    "# ============================\n",
    "def init_distributed_env(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    初始化分布式进程组（所有进程都必须调用）\n",
    "    \"\"\"\n",
    "    # 主进程地址和端口\n",
    "    # 在单机多卡场景中，localhost 即可\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_env():\n",
    "    \"\"\"\n",
    "    销毁进程组并清理资源\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、DDP 训练主逻辑（教学重点）\n",
    "# ============================\n",
    "def ddp_training_worker(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    每一个进程都会执行这个函数\n",
    "    rank      : 当前进程编号\n",
    "    world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. 固定随机种子（确保所有进程行为一致）\n",
    "    # ------------------------------------------------\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 确保 cudnn 的确定性行为（教学更容易对齐结果）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 初始化分布式环境\n",
    "    # ------------------------------------------------\n",
    "    init_distributed_env(rank, world_size, backend)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 为当前进程分配设备\n",
    "    # ------------------------------------------------\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(rank)\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "        print(f\"[进程 {rank}] 使用 GPU：{device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"[进程 {rank}] CUDA 不可用，使用 CPU\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 数据预处理与加载\n",
    "    # ------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        # 将 28×28 展平为 784\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.MNIST(\n",
    "        root=\"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. 手动划分数据（教学版 DDP）\n",
    "    #    每个进程只处理数据集的一部分\n",
    "    # ------------------------------------------------\n",
    "    total_samples = len(full_dataset)\n",
    "    samples_per_rank = total_samples // world_size\n",
    "\n",
    "    start_index = rank * samples_per_rank\n",
    "    end_index = start_index + samples_per_rank\n",
    "\n",
    "    subset_dataset = torch.utils.data.Subset(\n",
    "        full_dataset,\n",
    "        range(start_index, end_index)\n",
    "    )\n",
    "\n",
    "    # DataLoader 使用相同随机种子，确保 shuffle 行为可复现\n",
    "    data_generator = torch.Generator()\n",
    "    data_generator.manual_seed(seed)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        subset_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        generator=data_generator\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[进程 {rank}] 负责样本区间：\"\n",
    "        f\"{start_index} ~ {end_index - 1}，\"\n",
    "        f\"共 {samples_per_rank} 条数据\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 6. 创建模型与优化器\n",
    "    # ------------------------------------------------\n",
    "    model = SimpleNet(\n",
    "        input_dim=784,\n",
    "        hidden_dim=50,\n",
    "        num_classes=10\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 7. 步骤一：广播模型参数（关键教学点）\n",
    "    #    确保所有进程从“完全相同”的初始模型开始\n",
    "    # ------------------------------------------------\n",
    "    for param in model.parameters():\n",
    "        dist.broadcast(param.data, src=0)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 8. 正式开始训练\n",
    "    # ------------------------------------------------\n",
    "    for epoch in range(1, 3):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤二：前向 + 反向传播\n",
    "            # 每个进程只计算自己那一部分数据的梯度\n",
    "            # ----------------------------\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            step_start_time = time.time()\n",
    "            loss.backward()\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤三：梯度 All-Reduce\n",
    "            # 将所有进程的梯度相加，再取平均\n",
    "            # ----------------------------\n",
    "            comm_start_time = time.time()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    dist.all_reduce(\n",
    "                        param.grad.data,\n",
    "                        op=dist.ReduceOp.SUM\n",
    "                    )\n",
    "                    param.grad.data /= world_size\n",
    "\n",
    "            comm_time = time.time() - comm_start_time\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤四：参数更新\n",
    "            # 所有进程使用“完全相同”的平均梯度\n",
    "            # ----------------------------\n",
    "            optimizer.step()\n",
    "\n",
    "            step_time = time.time() - step_start_time\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[进程 {rank}] \"\n",
    "                    f\"Epoch {epoch} | \"\n",
    "                    f\"Batch {batch_idx} | \"\n",
    "                    f\"Loss = {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[主进程] 单步耗时：{step_time * 1000:.2f} ms，\"\n",
    "                    f\"通信耗时：{comm_time * 1000:.2f} ms\"\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 9. 仅在 rank 0 保存模型\n",
    "    # ------------------------------------------------\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"[主进程] 模型已保存：mnist_simple_ddp.pt\")\n",
    "\n",
    "    destroy_distributed_env()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 四、程序入口\n",
    "# ============================\n",
    "def main():\n",
    "    # world_size = 使用的进程（GPU）数量\n",
    "    # 如果只有一张 GPU，就设为 1（等价于普通训练）\n",
    "    world_size = 1\n",
    "\n",
    "    # NCCL：GPU 通信首选后端\n",
    "    backend = \"nccl\"\n",
    "\n",
    "    # 检查数据是否能被平均分配\n",
    "    dataset_size = len(\n",
    "        datasets.MNIST(\"../data\", train=True, download=True)\n",
    "    )\n",
    "\n",
    "    if dataset_size % world_size != 0:\n",
    "        print(\n",
    "            f\"警告：数据量 {dataset_size} 不能被 world_size={world_size} 整除，\"\n",
    "            f\"将丢弃部分样本以保证均分\"\n",
    "        )\n",
    "\n",
    "    print(f\"启动分布式训练，进程数：{world_size}\")\n",
    "    print(f\"每个进程处理样本数：{dataset_size // world_size}\")\n",
    "\n",
    "    # 启动多进程\n",
    "    spawn(\n",
    "        ddp_training_worker,\n",
    "        args=(world_size, backend),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "\n",
    "    print(\"分布式训练结束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32970985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动分布式训练，进程数：1\n",
      "每个进程处理样本数：60000\n",
      "[进程 0] 使用 GPU：cuda:0\n",
      "[进程 0] 负责样本区间：0 ~ 59999，共 60000 条数据\n",
      "[进程 0] Epoch 1 | Batch 0 | Loss = 2.294834\n",
      "[主进程] 单步耗时：178.22 ms，通信耗时：14.50 ms\n",
      "[进程 0] Epoch 1 | Batch 50 | Loss = 0.515532\n",
      "[主进程] 单步耗时：1.15 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 100 | Loss = 0.286108\n",
      "[主进程] 单步耗时：1.21 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 150 | Loss = 0.235657\n",
      "[主进程] 单步耗时：1.63 ms，通信耗时：0.49 ms\n",
      "[进程 0] Epoch 1 | Batch 200 | Loss = 0.305218\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 1 | Batch 250 | Loss = 0.421952\n",
      "[主进程] 单步耗时：1.36 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 300 | Loss = 0.376283\n",
      "[主进程] 单步耗时：1.17 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 350 | Loss = 0.159866\n",
      "[主进程] 单步耗时：1.60 ms，通信耗时：0.39 ms\n",
      "[进程 0] Epoch 1 | Batch 400 | Loss = 0.211988\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.65 ms\n",
      "[进程 0] Epoch 1 | Batch 450 | Loss = 0.401293\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 500 | Loss = 0.201109\n",
      "[主进程] 单步耗时：1.12 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 1 | Batch 550 | Loss = 0.257683\n",
      "[主进程] 单步耗时：1.62 ms，通信耗时：0.59 ms\n",
      "[进程 0] Epoch 1 | Batch 600 | Loss = 0.241983\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.67 ms\n",
      "[进程 0] Epoch 1 | Batch 650 | Loss = 0.171643\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 700 | Loss = 0.333229\n",
      "[主进程] 单步耗时：2.02 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 750 | Loss = 0.091621\n",
      "[主进程] 单步耗时：1.23 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 800 | Loss = 0.125746\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 850 | Loss = 0.194321\n",
      "[主进程] 单步耗时：1.01 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 900 | Loss = 0.435536\n",
      "[主进程] 单步耗时：1.33 ms，通信耗时：0.34 ms\n",
      "[进程 0] Epoch 2 | Batch 0 | Loss = 0.216730\n",
      "[主进程] 单步耗时：2.21 ms，通信耗时：0.42 ms\n",
      "[进程 0] Epoch 2 | Batch 50 | Loss = 0.168439\n",
      "[主进程] 单步耗时：1.09 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 100 | Loss = 0.067365\n",
      "[主进程] 单步耗时：1.25 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 2 | Batch 150 | Loss = 0.089863\n",
      "[主进程] 单步耗时：1.28 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 200 | Loss = 0.234549\n",
      "[主进程] 单步耗时：1.11 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 250 | Loss = 0.070795\n",
      "[主进程] 单步耗时：1.32 ms，通信耗时：0.41 ms\n",
      "[进程 0] Epoch 2 | Batch 300 | Loss = 0.088156\n",
      "[主进程] 单步耗时：1.68 ms，通信耗时：0.57 ms\n",
      "[进程 0] Epoch 2 | Batch 350 | Loss = 0.120999\n",
      "[主进程] 单步耗时：1.19 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 2 | Batch 400 | Loss = 0.121158\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.60 ms\n",
      "[进程 0] Epoch 2 | Batch 450 | Loss = 0.042282\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 500 | Loss = 0.265627\n",
      "[主进程] 单步耗时：1.38 ms，通信耗时：0.50 ms\n",
      "[进程 0] Epoch 2 | Batch 550 | Loss = 0.091650\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 600 | Loss = 0.142866\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.56 ms\n",
      "[进程 0] Epoch 2 | Batch 650 | Loss = 0.178375\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.68 ms\n",
      "[进程 0] Epoch 2 | Batch 700 | Loss = 0.181190\n",
      "[主进程] 单步耗时：1.10 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 750 | Loss = 0.107817\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 800 | Loss = 0.056186\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.63 ms\n",
      "[进程 0] Epoch 2 | Batch 850 | Loss = 0.103768\n",
      "[主进程] 单步耗时：1.04 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 900 | Loss = 0.167343\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.37 ms\n",
      "[主进程] 模型已保存：mnist_simple_ddp.pt\n",
      "分布式训练结束\n"
     ]
    }
   ],
   "source": [
    "!python ddp_model_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebc786",
   "metadata": {},
   "source": [
    "# 问题（minimal_ddp_flat_benchmarking）：2分\n",
    "\n",
    "修改您的最小DDP实现，以从所有参数中传递一个扁平化的梯度张量。将其性能与在先前使用条件下（1个节点 x 2个GPU，XL模型大小如§1.1.3中所述）为每个参数张量发出一个all-reduce的最小DDP实现进行比较。\n",
    "\n",
    "交付物：在单次批量all-reduce调用下，未分布式数据并行训练中每个训练迭代的测量时间以及梯度通信所花费的时间。1-2句话比较批量与单独通信梯度的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790a370",
   "metadata": {},
   "source": [
    "**验证：把“很多小的梯度通信”合并成“一次大的梯度通信”，能否显著降低 DDP 的通信开销**\n",
    "\n",
    "这在真实系统里是一个**非常核心的工程问题**。\n",
    "\n",
    "\n",
    "\n",
    "## naive DDP （对照基线）\n",
    "\n",
    "之前的最小 DDP 实现是这样的逻辑：\n",
    "\n",
    "```text\n",
    "for param in model.parameters():\n",
    "    all_reduce(param.grad)\n",
    "```\n",
    "\n",
    "也就是：**每个参数张量**，**单独一次 all-reduce**，通信调用次数 = 参数个数，XL 模型来说：参数张量数量几百～上千个，NCCL / GPU 通信，每一次 all-reduce 都有 **启动开销（latency）**，小 tensor → 极其低效，这就是作业的**优化的瓶颈**\n",
    "\n",
    "---\n",
    "\n",
    "## 改什么\n",
    "\n",
    "###  核心改动：**梯度展平 + 单次通信**\n",
    "\n",
    "把：\n",
    "\n",
    "```python\n",
    "grad_1, grad_2, grad_3, ..., grad_n\n",
    "```\n",
    "\n",
    "变成：\n",
    "\n",
    "```python\n",
    "flat_grad = concat([grad_1, grad_2, ..., grad_n])\n",
    "```\n",
    "\n",
    "然后只做：\n",
    "\n",
    "```python\n",
    "dist.all_reduce(flat_grad)\n",
    "```\n",
    "\n",
    "再把结果 **切回每个参数的 grad**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9739eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minimal_ddp_flat_benchmarking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minimal_ddp_flat_benchmarking.py\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # 定义第一个隐藏层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # 定义第二个隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 定义输出层\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  \n",
    "# 示例：创建一个SimpleNet实例\n",
    "\n",
    "def setup(rank, world_size, backend):\n",
    "    \"\"\" 初始化分布式环境 \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost' # 设置主节点IP地址以及端口，其他节点需要通过这个地址连接到主节点\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    # 根据后端初始化进程组\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size) # 初始化进程组，rank是当前进程的rank，world_size是总进程数，backend: 这是指定通信后端的参数。常见的后端有：gloo(CPU), nccl(GPU), mpi等。\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\" 清理分布式环境 \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    # 清理GPU缓存\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def ddp_train(rank, world_size, backend):\n",
    "    \"\"\" Naive DDP训练函数，实现图片中描述的4个步骤 \"\"\"\n",
    "    # 设置随机种子确保可重现性（所有进程使用相同种子）\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 设置确定性行为\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 初始化分布式环境\n",
    "    setup(rank, world_size, backend)\n",
    "    \n",
    "    # 为每个进程分配不同的GPU设备\n",
    "    # 使用GPU 0和1，它们当前是空闲的\n",
    "    gpu_id = rank  # rank 0 -> GPU 0, rank 1 -> GPU 1\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        print(f\"Rank {rank} using GPU: {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"CUDA not available, using CPU for rank {rank}\")\n",
    "    \n",
    "    # 数据加载和预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # 为了验证DDP正确性，我们让每个进程处理不同的数据子集\n",
    "    # 这样总的有效batch size = 64 * world_size\n",
    "    total_samples = len(dataset)\n",
    "    samples_per_process = total_samples // world_size\n",
    "    start_idx = rank * samples_per_process\n",
    "    end_idx = start_idx + samples_per_process\n",
    "    \n",
    "    # 创建当前进程的数据子集\n",
    "    subset = torch.utils.data.Subset(dataset, range(start_idx, end_idx))\n",
    "    \n",
    "    # 设置确定性的数据加载器\n",
    "    # 重要：为了与单进程训练比较，我们需要确保数据顺序一致\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)  # 所有进程使用相同的种子确保一致性\n",
    "    train_loader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=True, generator=generator)\n",
    "    \n",
    "    print(f\"Rank {rank} processing samples {start_idx} to {end_idx-1} ({samples_per_process} samples)\")\n",
    "    print(f\"Rank {rank} effective batch size: 64, total distributed batch size: {64 * world_size}\")\n",
    "    \n",
    "    # 创建模型和优化器（每个设备都创建相同的模型）\n",
    "    model = SimpleNet(input_size=784, hidden_size=50, num_classes=10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # ===== 新增：记录梯度展平所需的元信息 =====\n",
    "    grad_shapes = []\n",
    "    grad_numels = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "        grad_shapes.append(p.shape)\n",
    "        grad_numels.append(p.numel())\n",
    "\n",
    "    total_grad_numel = sum(grad_numels)\n",
    "\n",
    "    # 步骤1: Broadcast模型参数从rank 0到所有其他ranks\n",
    "    # 确保所有设备从相同的初始模型和优化器状态开始\n",
    "    for param in model.parameters():\n",
    "        # 确保参数在正确的设备上进行broadcast\n",
    "        dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # 同步优化器状态（如果有的话）\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param in optimizer.state:\n",
    "                for key, value in optimizer.state[param].items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        dist.broadcast(value, src=0)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, 3):  # Train for 2 epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # 步骤2: 每个设备使用本地模型参数进行前向传播和反向传播\n",
    "            # 计算n/d个样本的梯度\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            step_start = time.time()\n",
    "            loss.backward()\n",
    "            \n",
    "            comm_start = time.time()\n",
    "            # 步骤3: All-reduce梯度\n",
    "            # 将所有设备的梯度求平均，使每个设备都持有所有n个样本的平均梯度\n",
    "\n",
    "            # ===== 新的步骤3：Flattened gradient all-reduce =====\n",
    "            flat_grad = torch.zeros(total_grad_numel, device=device)\n",
    "\n",
    "            offset = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    numel = p.grad.numel()\n",
    "                    flat_grad[offset:offset + numel] = p.grad.view(-1)\n",
    "                    offset += numel\n",
    "\n",
    "            # 单次通信\n",
    "            dist.all_reduce(flat_grad, op=dist.ReduceOp.SUM)\n",
    "            flat_grad /= world_size\n",
    "\n",
    "            # 写回每个参数的梯度\n",
    "            offset = 0\n",
    "            for p, shape, numel in zip(model.parameters(), grad_shapes, grad_numels):\n",
    "                if p.grad is not None:\n",
    "                    p.grad.copy_(\n",
    "                        flat_grad[offset:offset + numel].view(shape)\n",
    "                    )\n",
    "                    offset += numel\n",
    "\n",
    "            comm_time = time.time() - comm_start\n",
    "            # 步骤4: 优化器步骤 - 每个设备使用相同的平均梯度更新参数\n",
    "            # 由于所有设备从相同初始状态开始并使用相同梯度，参数会保持同步\n",
    "            optimizer.step()\n",
    "            step_time = time.time() - step_start\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Rank {rank}, Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(f\"Step time: {step_time*1000:.2f} ms | Comm time: {comm_time*1000:.2f} ms\")\n",
    "\n",
    "    # 只在rank 0保存模型\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"Model saved!\")\n",
    "            \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = 1 # gpu只有一个就改成1，但是这样和平时普通的训练就没有区别了\n",
    "    backend = 'nccl'  # 使用gloo后端，更适合CPU训练\n",
    "    # 只检查数据集大小是否能被world_size整除\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    data_size = len(dataset)\n",
    "\n",
    "    if data_size % world_size != 0:\n",
    "        print(f\"Warning: Data size {data_size} is not divisible by world size {world_size}\")\n",
    "        print(f\"Some samples will be ignored to ensure equal distribution\")\n",
    "    \n",
    "    print(f\"Starting distributed training with {world_size} processes\")\n",
    "    print(f\"Each process will handle {data_size // world_size} samples\")\n",
    "    print(f\"Total samples: {data_size}\")\n",
    "    \n",
    "    # 启动分布式训练\n",
    "    spawn(ddp_train, args=(world_size, backend), nprocs=world_size, join=True)\n",
    "    \n",
    "    print(\"Distributed training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5eb2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/cs336_systems/minimal_ddp_flat_benchmarking.py\", line 4, in <module>\n",
      "    from torchvision import datasets, transforms\n",
      "ModuleNotFoundError: No module named 'torchvision'\n"
     ]
    }
   ],
   "source": [
    "!python minimal_ddp_flat_benchmarking.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c2f58",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "\n",
    "## 问题 (ddp_overlap_individual_parameters): 5 分\n",
    "\n",
    "实现一个 Python 类来处理分布式数据并行训练。该类应包装一个任意的 PyTorch `nn.Module` 并在训练之前处理权重的广播（以便所有 ranks 具有相同的初始参数）以及发出通信调用以进行梯度平均。我们建议以下公共接口：\n",
    "\n",
    "```python\n",
    "def __init__(self, module: torch.nn.Module): 给定一个实例化的 PyTorch nn.Module 要并行化，构建一个 DDP 容器，该容器将处理跨 ranks 的梯度同步。\n",
    "\n",
    "def forward(self, *inputs, **kwargs): 调用被包装模块的 `forward()` 方法，并使用提供的定位参数和关键字参数。\n",
    "\n",
    "def finish_gradient_synchronization(self): 当调用时，等待异步通信\n",
    "```\n",
    "\n",
    "*在高级情况下，如果你使用多个 CUDA 流，你可能需要显式同步跨流以确保输出准备好进行后续操作。* 参见 [CUDA Streams](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams)。\n",
    "\n",
    "### GPU 上的调用\n",
    "\n",
    "为了使用此类执行分布式训练，我们将一个模块传递给该模块进行包装，然后在我们运行 `optimizer.step()` 之前添加一个对 `finish_gradient_synchronization()` 的调用，以确保依赖于梯度的优化器步骤可以排队：\n",
    "\n",
    "```python\n",
    "model = ToyModel().to(device)\n",
    "ddp_model = DDP(model)\n",
    "\n",
    "for _ in range(train_steps):\n",
    "    x, y = get_batch()\n",
    "    logits = ddp_model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    ddp_model.finish_gradient_synchronization()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### 交付物：实现一个容器类来处理分布式数据并行训练。该类应重叠梯度通信和反向传播的计算。为了测试你的 DDP 类，首先实现适配器 `adapters.get_ddp_individual_parameters` 和 `adapters.ddp_individual_parameters_on_after_backward`（后者是可选的，取决于你的实现，你可能不需要它）。\n",
    "\n",
    "然后，执行测试，通过运行 `pytest tests/test_ddp_individual_parameters.py`。我们建议多次运行测试（例如，5 次）以确保其可靠通过。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ef9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_overlap_individual_parameters.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_overlap_individual_parameters.py\n",
    "'''\n",
    "实现一个手动版本的分布式数据并行（DDP）训练框架，核心特性是梯度分桶（Gradient Bucketing）+ 计算通信重叠（Computation-Communication Overlap）。\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 二、手动 DDP（按桶通信梯度）\n",
    "# ============================\n",
    "class DDPOverlapBucketed(nn.Module):\n",
    "    def __init__(self, module: torch.nn.Module, bucket_size_mb: float):\n",
    "        super(DDPOverlapBucketed, self).__init__()\n",
    "        self.module = module\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "        self.handles = []   # 保存异步 all_reduce 句柄\n",
    "        self.buckets = []   # 梯度桶\n",
    "        self.world_size = dist.get_world_size()\n",
    "        \n",
    "        # --------------------\n",
    "        # 初始化：广播参数 + 创建桶 + 注册钩子\n",
    "        # --------------------\n",
    "        self._broadcast_parameters()  # 初始参数广播\n",
    "        self._create_bucket()         # 按大小划分梯度桶\n",
    "        self._register_hook()         # 注册梯度钩子实现异步通信\n",
    "    \n",
    "    # --------------------\n",
    "    # 广播初始参数\n",
    "    # --------------------\n",
    "    def _broadcast_parameters(self):\n",
    "        \"\"\"\n",
    "        将 rank 0 的参数广播到所有进程，确保初始状态一致\n",
    "        \"\"\"\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # --------------------\n",
    "    # 按 bucket_size 创建梯度桶\n",
    "    # --------------------\n",
    "    def _create_bucket(self):\n",
    "        current_bucket_size = 0\n",
    "        current_bucket = []\n",
    "\n",
    "        # 倒序遍历参数，构建桶\n",
    "        for p in reversed(list(self.module.parameters())):\n",
    "            if p.requires_grad:\n",
    "                p_size = p.numel() * p.element_size()\n",
    "                \n",
    "                # 当前桶满了则保存并创建新桶\n",
    "                if p_size + current_bucket_size > self.bucket_size_bytes and current_bucket:\n",
    "                    self.buckets.append(current_bucket)\n",
    "                    current_bucket_size = 0\n",
    "                    current_bucket = []\n",
    "                \n",
    "                current_bucket.append(p)\n",
    "                current_bucket_size += p_size\n",
    "        \n",
    "        # 如果还有剩余参数，作为最后一个桶\n",
    "        if current_bucket:\n",
    "            self.buckets.append(current_bucket)\n",
    "        \n",
    "        # 为每个桶创建缓冲区\n",
    "        for i, bucket_params in enumerate(self.buckets):\n",
    "            if not bucket_params:\n",
    "                continue\n",
    "            buffer_size = sum(p.numel() for p in bucket_params)\n",
    "            buffer = torch.zeros(buffer_size, device=bucket_params[0].device, dtype=bucket_params[0].dtype)\n",
    "            self.buckets[i] = {\n",
    "                \"params\": bucket_params,\n",
    "                \"buffer\": buffer,\n",
    "                \"ready_params\": set(),\n",
    "                \"triggered\": False\n",
    "            }\n",
    "    \n",
    "    # --------------------\n",
    "    # 注册梯度钩子\n",
    "    # --------------------\n",
    "    def _register_hook(self):\n",
    "        for bucket_idx, bucket_info in enumerate(self.buckets):\n",
    "            for param in bucket_info[\"params\"]:\n",
    "                # 闭包捕获当前 bucket_idx\n",
    "                def make_hook(idx):\n",
    "                    return lambda grad, param=param: self._create_hook(grad, param, idx)\n",
    "                param.register_hook(make_hook(bucket_idx))\n",
    "    \n",
    "    # --------------------\n",
    "    # 梯度钩子逻辑：延迟执行 all_reduce\n",
    "    # --------------------\n",
    "    def _create_hook(self, grad, param, bucket_idx):\n",
    "        bucket_info = self.buckets[bucket_idx]\n",
    "        bucket_info[\"ready_params\"].add(param)\n",
    "\n",
    "        # 当桶内所有参数梯度都准备好，且未触发通信\n",
    "        if len(bucket_info[\"ready_params\"]) == len(bucket_info[\"params\"]) and not bucket_info[\"triggered\"]:\n",
    "            bucket_info[\"triggered\"] = True  # 标记为已触发\n",
    "\n",
    "            def delayed_sync():\n",
    "                # 将桶内所有梯度拷贝到扁平缓冲区\n",
    "                offset = 0\n",
    "                for p in bucket_info[\"params\"]:\n",
    "                    numel = p.numel()\n",
    "                    if p.grad is not None:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].copy_(p.grad.view(-1))\n",
    "                    else:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].zero_()\n",
    "                    offset += numel\n",
    "                \n",
    "                # 启动异步 all_reduce，实现计算与通信重叠\n",
    "                handle = dist.all_reduce(bucket_info[\"buffer\"], async_op=True)\n",
    "                self.handles.append((handle, bucket_idx))\n",
    "            \n",
    "            # 延迟执行，确保所有梯度计算完成\n",
    "            import torch.autograd as autograd\n",
    "            autograd.Variable._execution_engine.queue_callback(delayed_sync)\n",
    "    \n",
    "    # --------------------\n",
    "    # forward 前清理状态\n",
    "    # --------------------\n",
    "    def forward(self, x):\n",
    "        if self.world_size > 1:\n",
    "            for bucket in self.buckets:\n",
    "                bucket[\"triggered\"] = False\n",
    "                bucket[\"ready_params\"].clear()\n",
    "        self.handles.clear()\n",
    "        return self.module(x)\n",
    "\n",
    "    # --------------------\n",
    "    # 等待所有异步通信完成\n",
    "    # --------------------\n",
    "    def finish_gradient_synchronization(self):\n",
    "        \"\"\"\n",
    "        等待所有 all_reduce 完成，并将梯度写回各参数\n",
    "        需在 optimizer.step() 之前调用\n",
    "        \"\"\"\n",
    "        for handle, bucket_idx in self.handles:\n",
    "            handle.wait()  # 等待通信完成\n",
    "\n",
    "            bucket_info = self.buckets[bucket_idx]\n",
    "            buffer = bucket_info[\"buffer\"]\n",
    "\n",
    "            # 求平均梯度\n",
    "            buffer.div_(self.world_size)\n",
    "\n",
    "            offset = 0\n",
    "            for p in bucket_info[\"params\"]:\n",
    "                numel = p.numel()\n",
    "                if p.grad is not None:\n",
    "                    p.grad.view(-1).copy_(buffer[offset:offset+numel])\n",
    "                offset += numel\n",
    "        \n",
    "        # 清空 handles，为下一次迭代准备\n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、简单示例运行\n",
    "# ============================\n",
    "def run():\n",
    "    # 初始化分布式环境（torchrun 已设置环境变量）\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    model = nn.Linear(10, 5).cuda()\n",
    "    ddp_model = DDPOverlapBucketed(model, bucket_size_mb=1)  # 1 MB 小桶方便观察\n",
    "    opt = torch.optim.SGD(ddp_model.parameters(), lr=0.1)\n",
    "\n",
    "    for step in range(3):\n",
    "        x = torch.randn(4, 10).cuda()\n",
    "        loss = ddp_model(x).sum()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()  # 必须手动调用\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        print(f\"[rank {dist.get_rank()}] step {step} loss={loss.item()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060b549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank 0] step 0 loss=2.803687572479248\n",
      "[rank 0] step 1 loss=-8.734395980834961\n",
      "[rank 0] step 2 loss=-22.41156768798828\n",
      "[rank0]:[W122 14:35:46.682734635 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 ddp_overlap_individual_parameters.py # 有多少张卡设置nproc_per_node=几"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70de19",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "### 问题 (ddp_bucketed_benchmarking)：3分\n",
    "\n",
    "**(a)** 使用与之前实验相同的配置（1个节点，2个GPU，XL模型大小）对你的分桶DDP实现进行基准测试，变化最大桶大小（1, 10, 100, 1000 MB）。将你的结果与之前没有分桶的实验进行比较——结果是否符合你的预期？如果不符合，为什么？你可能需要使用PyTorch分析器来更好地理解通信调用的顺序和/或执行方式。你期望对实验设置进行哪些更改才能使结果符合预期？\n",
    "\n",
    "**交付物：** 各种桶大小下每次训练迭代的测量时间。对结果、你的预期以及任何不匹配的可能原因进行3-4句评论。\n",
    "\n",
    "**(b)** 假设计算一个桶的梯度所需的时间与通信该梯度桶所需的时间相同。写一个方程来建模DDP的通信开销（即反向传播后额外花费的时间），作为以下变量的函数：\n",
    "- 模型参数的总大小（字节）$s$\n",
    "- all-reduce算法带宽 $w$（计算为每个rank的数据大小除以完成all-reduce所需的时间）\n",
    "- 每次通信调用的开销（秒）$o$\n",
    "- 桶的数量 $n_b$\n",
    "\n",
    "从这个方程，写出最小化DDP开销的最优桶大小的方程。\n",
    "\n",
    "**交付物：** 建模DDP开销的方程，以及最优桶大小的方程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab59b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "\n",
    "'''\n",
    "实现了一个单节点多进程分布式通信性能基准测试工具，用于测量 PyTorch 中 all_reduce 操作的延迟和带宽性能。\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfa61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2266 ms\n",
      "  Bandwidth      : 4.6281 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0165 ms\n",
      "  Bandwidth      : 63.7398 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 2.0388 ms\n",
      "  Bandwidth      : 5.1430 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0149 ms\n",
      "  Bandwidth      : 704.8151 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 17.6973 ms\n",
      "  Bandwidth      : 5.9251 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0163 ms\n",
      "  Bandwidth      : 6448.7486 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 181.1478 ms\n",
      "  Bandwidth      : 5.7885 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0172 ms\n",
      "  Bandwidth      : 60914.7716 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49b94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_bucketed_benchmark_complete.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_bucketed_benchmark_complete.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.profiler\n",
    "from torch.multiprocessing import spawn, Manager\n",
    "import argparse\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "# ============================================================\n",
    "# 1. 无分桶DDP实现（用于对比）\n",
    "# ============================================================\n",
    "\n",
    "class DDPNoBucket(nn.Module):\n",
    "    \"\"\"\n",
    "    无分桶DDP：所有参数作为一个整体进行all_reduce\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.handles = []\n",
    "        \n",
    "        self._broadcast_parameters()\n",
    "        self._register_hook()\n",
    "    \n",
    "    def _broadcast_parameters(self):\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    def _register_hook(self):\n",
    "        self.params_with_grad = []\n",
    "        for param in self.module.parameters():\n",
    "            if param.requires_grad:\n",
    "                self.params_with_grad.append(param)\n",
    "                param.register_hook(lambda grad, p=param: self._hook_callback(p))\n",
    "    \n",
    "    def _hook_callback(self, param):\n",
    "        if all(p.grad is not None for p in self.params_with_grad):\n",
    "            flat_grads = []\n",
    "            for p in self.params_with_grad:\n",
    "                flat_grads.append(p.grad.view(-1))\n",
    "            \n",
    "            if flat_grads:\n",
    "                buffer = torch.cat(flat_grads)\n",
    "                handle = dist.all_reduce(buffer, async_op=True)\n",
    "                self.handles.append((handle, buffer, self.params_with_grad))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.handles.clear()\n",
    "        for p in self.params_with_grad:\n",
    "            p.grad = None\n",
    "        return self.module(x)\n",
    "    \n",
    "    def finish_gradient_synchronization(self):\n",
    "        for handle, buffer, params in self.handles:\n",
    "            handle.wait()\n",
    "            buffer.div_(self.world_size)\n",
    "            \n",
    "            offset = 0\n",
    "            for p in params:\n",
    "                numel = p.numel()\n",
    "                p.grad.view(-1).copy_(buffer[offset:offset+numel])\n",
    "                offset += numel\n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 分桶DDP实现（优化版）\n",
    "# ============================================================\n",
    "\n",
    "class DDPBucketed(nn.Module):\n",
    "    \"\"\"\n",
    "    分桶DDP：按指定桶大小分组梯度，实现计算通信重叠\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Module, bucket_size_mb: float):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.handles = []\n",
    "        self.buckets = []\n",
    "        \n",
    "        self._broadcast_parameters()\n",
    "        self._create_buckets()\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _broadcast_parameters(self):\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    def _create_buckets(self):\n",
    "        \"\"\"按桶大小创建梯度桶\"\"\"\n",
    "        current_bucket = []\n",
    "        current_bucket_size = 0\n",
    "        \n",
    "        for p in reversed(list(self.module.parameters())):\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            \n",
    "            p_size = p.numel() * p.element_size()\n",
    "            \n",
    "            if current_bucket and (current_bucket_size + p_size > self.bucket_size_bytes):\n",
    "                self._finalize_bucket(current_bucket)\n",
    "                current_bucket = []\n",
    "                current_bucket_size = 0\n",
    "            \n",
    "            current_bucket.append(p)\n",
    "            current_bucket_size += p_size\n",
    "        \n",
    "        if current_bucket:\n",
    "            self._finalize_bucket(current_bucket)\n",
    "    \n",
    "    def _finalize_bucket(self, params: List[torch.nn.Parameter]):\n",
    "        \"\"\"为桶创建缓冲区\"\"\"\n",
    "        if not params:\n",
    "            return\n",
    "        \n",
    "        buffer_size = sum(p.numel() for p in params)\n",
    "        buffer = torch.zeros(\n",
    "            buffer_size, \n",
    "            device=params[0].device, \n",
    "            dtype=params[0].dtype\n",
    "        )\n",
    "        \n",
    "        self.buckets.append({\n",
    "            \"params\": params,\n",
    "            \"buffer\": buffer,\n",
    "            \"ready_count\": 0,\n",
    "            \"triggered\": False,\n",
    "            \"total_params\": len(params)\n",
    "        })\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"为每个参数注册梯度钩子\"\"\"\n",
    "        for bucket_idx, bucket in enumerate(self.buckets):\n",
    "            for param in bucket[\"params\"]:\n",
    "                param.register_hook(\n",
    "                    lambda grad, b_idx=bucket_idx: self._on_gradient_ready(b_idx)\n",
    "                )\n",
    "    \n",
    "    def _on_gradient_ready(self, bucket_idx: int):\n",
    "        \"\"\"梯度就绪回调\"\"\"\n",
    "        bucket = self.buckets[bucket_idx]\n",
    "        bucket[\"ready_count\"] += 1\n",
    "        \n",
    "        if (bucket[\"ready_count\"] == bucket[\"total_params\"] and \n",
    "            not bucket[\"triggered\"]):\n",
    "            \n",
    "            bucket[\"triggered\"] = True\n",
    "            \n",
    "            def launch_all_reduce():\n",
    "                offset = 0\n",
    "                for p in bucket[\"params\"]:\n",
    "                    numel = p.numel()\n",
    "                    if p.grad is not None:\n",
    "                        bucket[\"buffer\"][offset:offset+numel].copy_(p.grad.view(-1))\n",
    "                    else:\n",
    "                        bucket[\"buffer\"][offset:offset+numel].zero_()\n",
    "                    offset += numel\n",
    "                \n",
    "                handle = dist.all_reduce(bucket[\"buffer\"], async_op=True)\n",
    "                self.handles.append((handle, bucket_idx))\n",
    "            \n",
    "            torch.autograd.Variable._execution_engine.queue_callback(launch_all_reduce)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播，重置状态\"\"\"\n",
    "        for bucket in self.buckets:\n",
    "            bucket[\"triggered\"] = False\n",
    "            bucket[\"ready_count\"] = 0\n",
    "        \n",
    "        self.handles.clear()\n",
    "        return self.module(x)\n",
    "    \n",
    "    def finish_gradient_synchronization(self):\n",
    "        \"\"\"等待所有通信完成并写回梯度\"\"\"\n",
    "        for handle, bucket_idx in self.handles:\n",
    "            handle.wait()\n",
    "            \n",
    "            bucket = self.buckets[bucket_idx]\n",
    "            bucket[\"buffer\"].div_(self.world_size)\n",
    "            \n",
    "            offset = 0\n",
    "            for p in bucket[\"params\"]:\n",
    "                numel = p.numel()\n",
    "                if p.grad is not None:\n",
    "                    p.grad.view(-1).copy_(bucket[\"buffer\"][offset:offset+numel])\n",
    "                offset += numel\n",
    "        \n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. 0.5B模型定义，可以自行修改参数配置，这里是让笔记本也能跑起来\n",
    "# ============================================================\n",
    "\n",
    "class XLModel(nn.Module):\n",
    "    \"\"\"\n",
    "    模拟XL大小的模型\n",
    "    总参数量约 0.5B（5亿参数）\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int = 1024, num_layers: int = 24):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding层: 50000 * 1024 = 51.2M\n",
    "        self.embedding = nn.Embedding(50000, hidden_size)\n",
    "        \n",
    "        # Transformer层: 24层\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=16,\n",
    "                dim_feedforward=hidden_size * 4,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 输出层: 1024 * 50000 = 51.2M\n",
    "        self.output = nn.Linear(hidden_size, 50000)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. 基准测试主逻辑\n",
    "# ============================================================\n",
    "\n",
    "def run_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    bucket_size_mb: float,\n",
    "    use_bucket: bool,\n",
    "    num_iterations: int,\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    result_queue=None,\n",
    "    result_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    运行DDP基准测试\n",
    "    \"\"\"\n",
    "    \n",
    "    # 设置设备\n",
    "    torch.cuda.set_device(global_rank)\n",
    "    device = torch.device(f\"cuda:{global_rank}\")\n",
    "    \n",
    "    # 初始化分布式环境 - 指定device_id消除警告\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    \n",
    "    # 修复：使用device_id指定设备，避免NCCL猜测\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        device_id=device  # 关键修复：明确指定设备ID\n",
    "    )\n",
    "    \n",
    "    if global_rank == 0:\n",
    "        config_str = f\"{'Bucketed' if use_bucket else 'No Bucket'}\"\n",
    "        if use_bucket:\n",
    "            config_str += f\" (bucket={bucket_size_mb}MB)\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {config_str}\")\n",
    "        print(f\"World size: {world_size}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    model = XLModel().to(device)\n",
    "    \n",
    "    # 包装DDP\n",
    "    if use_bucket:\n",
    "        ddp_model = DDPBucketed(model, bucket_size_mb=bucket_size_mb)\n",
    "    else:\n",
    "        ddp_model = DDPNoBucket(model)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # 预热\n",
    "    if global_rank == 0:\n",
    "        print(\"Warming up...\")\n",
    "    \n",
    "    for _ in range(3):\n",
    "        dummy_input = torch.randint(0, 50000, (2, 512), device=device)\n",
    "        output = ddp_model(dummy_input)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # 修复：使用device参数明确指定barrier设备\n",
    "    dist.barrier(device_ids=[global_rank])\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 正式测试\n",
    "    if global_rank == 0:\n",
    "        print(f\"Running benchmark ({num_iterations} iterations)...\")\n",
    "    \n",
    "    iteration_times = []\n",
    "    \n",
    "    for iter_idx in range(num_iterations):\n",
    "        input_ids = torch.randint(0, 50000, (2, 512), device=device)\n",
    "        \n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start_event.record()\n",
    "        \n",
    "        output = ddp_model(input_ids)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed_ms = start_event.elapsed_time(end_event)\n",
    "        iteration_times.append(elapsed_ms)\n",
    "    \n",
    "    # 收集所有rank的结果\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"bucket_size_mb\": bucket_size_mb if use_bucket else float('inf'),\n",
    "        \"use_bucket\": use_bucket,\n",
    "        \"iteration_times_ms\": iteration_times,\n",
    "        \"avg_time_ms\": sum(iteration_times) / len(iteration_times),\n",
    "        \"min_time_ms\": min(iteration_times),\n",
    "        \"max_time_ms\": max(iteration_times),\n",
    "    }\n",
    "    \n",
    "    all_results = [None] * world_size\n",
    "    dist.all_gather_object(all_results, local_result)\n",
    "    \n",
    "    # PyTorch Profiler（仅rank 0，且仅分桶模式）\n",
    "    if global_rank == 0 and use_bucket and bucket_size_mb <= 100:\n",
    "        try:\n",
    "            os.makedirs(\"./profiler_log\", exist_ok=True)\n",
    "            with torch.profiler.profile(\n",
    "                activities=[\n",
    "                    torch.profiler.ProfilerActivity.CPU,\n",
    "                    torch.profiler.ProfilerActivity.CUDA,\n",
    "                ],\n",
    "                schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "                    f\"./profiler_log/bucket_{bucket_size_mb}MB\"\n",
    "                ),\n",
    "                record_shapes=False,\n",
    "                profile_memory=False,\n",
    "                with_stack=False\n",
    "            ) as prof:\n",
    "                \n",
    "                for _ in range(4):\n",
    "                    input_ids = torch.randint(0, 50000, (2, 512), device=device)\n",
    "                    output = ddp_model(input_ids)\n",
    "                    loss = output.mean()\n",
    "                    loss.backward()\n",
    "                    ddp_model.finish_gradient_synchronization()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    prof.step()\n",
    "        except Exception as e:\n",
    "            print(f\"Profiler warning: {e}\")\n",
    "    \n",
    "    # 只有rank 0返回结果\n",
    "    if global_rank == 0:\n",
    "        result_data = {\n",
    "            \"config\": {\n",
    "                \"use_bucket\": use_bucket,\n",
    "                \"bucket_size_mb\": bucket_size_mb if use_bucket else None,\n",
    "                \"world_size\": world_size,\n",
    "            },\n",
    "            \"ranks\": all_results,\n",
    "            \"global_avg_time_ms\": sum(r[\"avg_time_ms\"] for r in all_results) / world_size,\n",
    "        }\n",
    "        \n",
    "        # 通过queue返回\n",
    "        if result_queue is not None:\n",
    "            result_queue.put(result_data)\n",
    "        \n",
    "        # 同时写入文件（备用）\n",
    "        if result_file is not None:\n",
    "            with open(result_file, 'wb') as f:\n",
    "                pickle.dump(result_data, f)\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def benchmark_worker(rank, world_size, bucket_size, use_bucket, num_iter, \n",
    "                     master_addr, master_port, result_queue, result_file):\n",
    "    \"\"\"包装函数用于spawn\"\"\"\n",
    "    try:\n",
    "        run_benchmark(rank, world_size, bucket_size, use_bucket, num_iter,\n",
    "                      master_addr, master_port, result_queue, result_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in worker {rank}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_single_test(world_size, bucket_size, use_bucket, num_iterations,\n",
    "                   master_addr, master_port):\n",
    "    \"\"\"运行单次测试并返回结果\"\"\"\n",
    "    \n",
    "    with Manager() as manager:\n",
    "        result_queue = manager.Queue()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pkl') as tmp:\n",
    "            result_file = tmp.name\n",
    "        \n",
    "        try:\n",
    "            spawn(\n",
    "                benchmark_worker,\n",
    "                args=(world_size, bucket_size, use_bucket, num_iterations,\n",
    "                      master_addr, master_port, result_queue, result_file),\n",
    "                nprocs=world_size,\n",
    "                join=True\n",
    "            )\n",
    "            \n",
    "            if not result_queue.empty():\n",
    "                return result_queue.get()\n",
    "            else:\n",
    "                with open(result_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        finally:\n",
    "            if os.path.exists(result_file):\n",
    "                os.unlink(result_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"DDP Bucketed Benchmark\")\n",
    "    parser.add_argument(\"--world_size\", type=int, default=2, \n",
    "                       help=\"GPU数量\")\n",
    "    parser.add_argument(\"--num_iterations\", type=int, default=20,\n",
    "                       help=\"每次测试的迭代次数\")\n",
    "    parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "    parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    assert torch.cuda.is_available(), \"需要CUDA\"\n",
    "    assert args.world_size <= torch.cuda.device_count(), \"GPU数量不足\"\n",
    "    \n",
    "    results = []\n",
    "    bucket_sizes = [1, 10, 100, 1000]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DDP Bucketed Benchmark\")\n",
    "    print(f\"Configuration: 1 node, {args.world_size} GPUs, 0.5B model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. 先测试无分桶版本（baseline）\n",
    "    print(\"\\n[1/5] Testing No Bucket (Baseline)...\")\n",
    "    result = run_single_test(\n",
    "        args.world_size, 0, False, args.num_iterations,\n",
    "        args.master_addr, args.master_port\n",
    "    )\n",
    "    results.append(result)\n",
    "    \n",
    "    # 2. 测试各种桶大小\n",
    "    for i, bucket_size in enumerate(bucket_sizes, 2):\n",
    "        print(f\"\\n[{i}/5] Testing Bucket Size = {bucket_size} MB...\")\n",
    "        result = run_single_test(\n",
    "            args.world_size, bucket_size, True, args.num_iterations,\n",
    "            args.master_addr, args.master_port\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # 汇总结果\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    baseline = None\n",
    "    bucketed_results = []\n",
    "    \n",
    "    for r in results:\n",
    "        if not r[\"config\"][\"use_bucket\"]:\n",
    "            baseline = r\n",
    "        else:\n",
    "            bucketed_results.append(r)\n",
    "    \n",
    "    if baseline is None:\n",
    "        print(\"ERROR: Baseline result not found!\")\n",
    "        return\n",
    "    \n",
    "    baseline_time = baseline[\"global_avg_time_ms\"]\n",
    "    print(f\"\\nBaseline (No Bucket): {baseline_time:.2f} ms/iteration\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Config':<20} {'Time (ms)':<12} {'Overhead':<12} {'Speedup':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"{'No Bucket':<20} {baseline_time:<12.2f} {0.0:<12.2f} {1.0:<10.2f}x\")\n",
    "    \n",
    "    for r in sorted(bucketed_results, key=lambda x: x[\"config\"][\"bucket_size_mb\"]):\n",
    "        bucket_size = r[\"config\"][\"bucket_size_mb\"]\n",
    "        time_ms = r[\"global_avg_time_ms\"]\n",
    "        overhead = time_ms - baseline_time\n",
    "        speedup = baseline_time / time_ms if time_ms > 0 else float('inf')\n",
    "        \n",
    "        config_str = f\"Bucket={bucket_size}MB\"\n",
    "        print(f\"{config_str:<20} {time_ms:<12.2f} {overhead:<12.2f} {speedup:<10.2f}x\")\n",
    "    \n",
    "    # 保存详细结果\n",
    "    with open(\"benchmark_results.json\", \"w\") as f:\n",
    "        json_results = []\n",
    "        for r in results:\n",
    "            json_r = {\n",
    "                \"config\": r[\"config\"],\n",
    "                \"global_avg_time_ms\": float(r[\"global_avg_time_ms\"]),\n",
    "                \"ranks\": [\n",
    "                    {\n",
    "                        \"rank\": rank_r[\"rank\"],\n",
    "                        \"bucket_size_mb\": float(rank_r[\"bucket_size_mb\"]) if rank_r[\"bucket_size_mb\"] != float('inf') else \"inf\",\n",
    "                        \"use_bucket\": rank_r[\"use_bucket\"],\n",
    "                        \"avg_time_ms\": float(rank_r[\"avg_time_ms\"]),\n",
    "                        \"min_time_ms\": float(rank_r[\"min_time_ms\"]),\n",
    "                        \"max_time_ms\": float(rank_r[\"max_time_ms\"]),\n",
    "                    }\n",
    "                    for rank_r in r[\"ranks\"]\n",
    "                ]\n",
    "            }\n",
    "            json_results.append(json_r)\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n详细结果已保存到 benchmark_results.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2998b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DDP Bucketed Benchmark\n",
      "Configuration: 1 node, 1 GPUs, 0.5B model\n",
      "======================================================================\n",
      "\n",
      "[1/5] Testing No Bucket (Baseline)...\n",
      "\n",
      "============================================================\n",
      "Testing: No Bucket\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[2/5] Testing Bucket Size = 1 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=1MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[3/5] Testing Bucket Size = 10 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=10MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[4/5] Testing Bucket Size = 100 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=100MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[5/5] Testing Bucket Size = 1000 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=1000MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Baseline (No Bucket): 8681.96 ms/iteration\n",
      "------------------------------------------------------------\n",
      "Config               Time (ms)    Overhead     Speedup   \n",
      "------------------------------------------------------------\n",
      "No Bucket            8681.96      0.00         1.00      x\n",
      "Bucket=1MB           11261.42     2579.46      0.77      x\n",
      "Bucket=10MB          9168.86      486.89       0.95      x\n",
      "Bucket=100MB         8094.60      -587.36      1.07      x\n",
      "Bucket=1000MB        6805.34      -1876.62     1.28      x\n",
      "\n",
      "详细结果已保存到 benchmark_results.json\n"
     ]
    }
   ],
   "source": [
    "!python ddp_bucketed_benchmark_complete.py --world_size 1 --num_iterations 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeedc51",
   "metadata": {},
   "source": [
    "# 作业五\n",
    "\n",
    "## 3 优化器状态分片\n",
    "\n",
    "分布式数据并行训练在概念上简单且通常非常高效，但要求每个进程持有模型参数和优化器状态的不同副本。这种冗余往往会导致显著的内存开销。例如，AdamW优化器为每个参数维护两个浮点数，这意味着其占用的内存是模型权重的两倍。Rajbhandari等人[2020]描述了几种方法，通过将（1）优化器状态、（2）梯度以及（3）参数在各进程间进行划分，并根据需要在不同工作节点之间通信，从而有效降低数据并行训练中的冗余问题。\n",
    "\n",
    "在本部分作业中，我们将通过实现优化器状态分片的简化版本，来降低每个Rank的内存消耗。具体而言，我们不会为所有参数都保存优化器状态，而是让每个Rank的优化器实例仅处理一部分参数（大约为1 / 世界规模）。当每个Rank的优化器执行一步更新时，它只会更新其分片中对应的那一部分模型参数。随后，各Rank会将其更新后的参数广播给其他Rank，以确保在每一步优化器更新后，模型参数始终保持同步。\n",
    "\n",
    "---\n",
    "\n",
    "### 问题（优化器状态分片）：15分\n",
    "\n",
    "实现一个Python类，用于处理优化器状态的分片。该类应封装任意输入的PyTorch `optim.Optimizer`，并在每次优化器步骤后负责同步更新的参数。我们建议采用以下公共接口：\n",
    "\n",
    "**`def __init__(self, params, optimizer_cls: Type[Optimizer], **kwargs: Any)`**：初始化分片状态优化器。`params` 是待优化的参数集合（或参数组，如果用户希望为模型的不同部分使用不同的超参数，例如学习率）；这些参数将在所有进程间进行分片。`optimizer_cls` 参数指定了要封装的优化器类型（例如，`optim.AdamW`）。最后，任何剩余的关键字参数将被转发到 `optimizer_cls` 的构造函数中。请务必在此方法中调用 `torch.optim.Optimizer` 超类的构造函数。\n",
    "\n",
    "**`def step(self, closure, **kwargs)`**：调用包装优化器的 `step()` 方法，并传入相应参数；提供了闭包和关键字参数。更新参数后，与其他进程同步。\n",
    "\n",
    "**`def add_param_group(self, param_group: dict[str, Any])`**：该方法应添加一个参数组传递给分片优化器。此方法在超类构造函数构建分片优化器时被调用，也可能在训练过程中被调用（例如，用于逐步解冻模型中的各层）。因此，该方法应负责将模型的参数分配到各个进程 ranks 中。\n",
    "\n",
    "---\n",
    "\n",
    "**交付物**：实现一个容器类，以支持优化器状态的分片处理。为测试你的分片优化器，请先实现适配器 `[adapters.get_sharded_optimizer]`。随后，运行 `uv run pytest tests/test_sharded_optimizer.py` 来执行测试。我们建议多次运行测试（例如5次），以确保测试能够稳定通过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8744ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharded_optimizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sharded_optimizer.py\n",
    "from typing import Any, Type, Iterable, Dict, List\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class ShardedOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    分片优化器（Optimizer State Sharding，简化版 ZeRO-1）\n",
    "\n",
    "    核心思想：\n",
    "    - 所有 rank 共享完整的模型参数（参数本身不切分）\n",
    "    - 但每个 rank 只“负责”其中一部分参数的优化器状态（如 Adam 的 m / v）\n",
    "    - 每一步：\n",
    "        1. 各 rank 只对自己负责的参数执行 optimizer.step()\n",
    "        2. 然后通过 broadcast，把更新后的参数同步到所有 rank\n",
    "\n",
    "    这样可以：\n",
    "    - 将优化器状态显存占用降低到原来的 1 / world_size\n",
    "    - 逻辑上等价于单卡训练（在同步完成后）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        optimizer_cls: Type[Optimizer],\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        # ZeRO / 分片优化器必须运行在 torch.distributed 初始化之后\n",
    "        if not dist.is_initialized():\n",
    "            raise RuntimeError(\"torch.distributed must be initialized\")\n",
    "\n",
    "        # 当前进程的 rank（全局唯一）\n",
    "        self.rank = dist.get_rank()\n",
    "\n",
    "        # 总进程数（world size）\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # 被包装的真实优化器类型（如 torch.optim.Adam）\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "\n",
    "        # 真实优化器的超参数（lr、betas 等）\n",
    "        self.optimizer_kwargs = kwargs\n",
    "\n",
    "        # 调用 Optimizer 的构造函数\n",
    "        # 作用：\n",
    "        # - 初始化 self.param_groups\n",
    "        # - 为每个参数组调用 add_param_group\n",
    "        super().__init__(params, defaults={})\n",
    "\n",
    "        # 构建“本 rank 专属”的本地优化器\n",
    "        self._build_local_optimizer()\n",
    "\n",
    "    def _build_local_optimizer(self):\n",
    "        \"\"\"\n",
    "        构建当前 rank 的本地优化器（local optimizer）\n",
    "\n",
    "        核心逻辑：\n",
    "        - 遍历所有参数\n",
    "        - 只挑选“属于当前 rank”的参数\n",
    "        - 用这些参数创建一个真正执行 step() 的 optimizer\n",
    "\n",
    "        注意：\n",
    "        - 每个 rank 的 local_optimizer 看到的参数子集不同\n",
    "        - 但 param_groups 的结构（lr / weight_decay 等）保持一致\n",
    "        \"\"\"\n",
    "        local_param_groups: List[Dict[str, Any]] = []\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # 从当前参数组中，筛选出 owner == 当前 rank 的参数\n",
    "            local_params = [\n",
    "                p for p in group[\"params\"]\n",
    "                if self._param_owner(p) == self.rank\n",
    "            ]\n",
    "\n",
    "            # 如果该参数组在当前 rank 没有任何参数，直接跳过\n",
    "            if len(local_params) == 0:\n",
    "                continue\n",
    "\n",
    "            # 复制一份参数组配置（避免修改原始 param_groups）\n",
    "            local_group = dict(group)\n",
    "\n",
    "            # 用本 rank 负责的参数替换 params\n",
    "            local_group[\"params\"] = local_params\n",
    "\n",
    "            local_param_groups.append(local_group)\n",
    "\n",
    "        # 用筛选后的参数组，实例化真实的优化器（如 Adam）\n",
    "        self.local_optimizer = self.optimizer_cls(\n",
    "            local_param_groups,\n",
    "            **self.optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "    def _param_owner(self, param: torch.nn.Parameter) -> int:\n",
    "        \"\"\"\n",
    "        判断某个参数“归属”哪个 rank\n",
    "\n",
    "        当前实现策略：\n",
    "        - 先为每个参数分配一个全局唯一的 index\n",
    "        - owner = index % world_size\n",
    "\n",
    "        这是最简单、最常见的静态分片方式\n",
    "        \"\"\"\n",
    "        return self._global_param_index(param) % self.world_size\n",
    "\n",
    "    def _global_param_index(self, param: torch.nn.Parameter) -> int:\n",
    "        \"\"\"\n",
    "        为每一个参数分配一个稳定的“全局索引”\n",
    "\n",
    "        设计要求：\n",
    "        - 所有 rank 上，参数遍历顺序必须一致\n",
    "        - 同一个参数在不同 rank 上，index 必须完全相同\n",
    "\n",
    "        实现方式：\n",
    "        - 第一次调用时，遍历所有 param_groups\n",
    "        - 按出现顺序给每个 parameter 编号\n",
    "        - 使用字典缓存 param -> index 的映射\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_param_to_index\"):\n",
    "            self._param_to_index = {}\n",
    "            idx = 0\n",
    "            for group in self.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    self._param_to_index[p] = idx\n",
    "                    idx += 1\n",
    "        return self._param_to_index[param]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None, **kwargs):\n",
    "        \"\"\"\n",
    "        执行一次优化步骤（等价于 Optimizer.step）\n",
    "\n",
    "        流程：\n",
    "        1. （可选）执行 closure 计算 loss\n",
    "        2. 仅对当前 rank 负责的参数执行 optimizer.step()\n",
    "        3. 将更新后的参数广播给所有 rank，同步模型参数\n",
    "\n",
    "        注意：\n",
    "        - 优化器状态（如 Adam 的动量）始终只存在于 owner rank\n",
    "        - 参数本身在 step 结束后在所有 rank 上保持一致\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            # closure 需要开启梯度\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        #  这里只会更新“本 rank 拥有的参数”\n",
    "        self.local_optimizer.step(**kwargs)\n",
    "\n",
    "        # 同步参数：owner rank → 其他所有 rank\n",
    "        self._sync_parameters()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _sync_parameters(self):\n",
    "        \"\"\"\n",
    "        参数同步逻辑（broadcast）\n",
    "\n",
    "        对每一个参数：\n",
    "        - 找到它的 owner rank\n",
    "        - 从 owner rank 将参数广播到所有 rank\n",
    "\n",
    "        这样可以保证：\n",
    "        - 虽然每个参数只在一个 rank 上被更新\n",
    "        - 但 step 结束后，所有 rank 上的参数值完全一致\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                owner = self._param_owner(p)\n",
    "                dist.broadcast(p.data, src=owner)\n",
    "\n",
    "    def add_param_group(self, param_group: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        动态添加参数组（与 PyTorch Optimizer 接口保持一致）\n",
    "\n",
    "        由于参数发生变化：\n",
    "        - 需要重新计算参数分片\n",
    "        - 并重建 local optimizer\n",
    "        \"\"\"\n",
    "        super().add_param_group(param_group)\n",
    "        self._build_local_optimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_sharded_optimizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sharded_optimizer.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from sharded_optimizer import ShardedOptimizer  \n",
    "\n",
    "\n",
    "def setup_distributed():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_distributed()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # 一个简单模型\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 10),\n",
    "    ).to(device)\n",
    "\n",
    "    # 确保模型初始化一致\n",
    "    for p in model.parameters():\n",
    "        dist.broadcast(p.data, src=0)\n",
    "\n",
    "    optimizer = ShardedOptimizer(\n",
    "        model.parameters(),\n",
    "        torch.optim.AdamW,\n",
    "        lr=1e-3\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(5):\n",
    "        x = torch.randn(8, 1024, device=device)\n",
    "        y = torch.randint(0, 10, (8,), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"step={step}, loss={loss.item():.4f}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f166dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=2.5581\n",
      "step=1, loss=2.4269\n",
      "step=2, loss=2.3533\n",
      "step=3, loss=2.2617\n",
      "step=4, loss=2.4645\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 test_sharded_optimizer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
