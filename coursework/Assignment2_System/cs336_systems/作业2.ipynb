{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8639c8be",
   "metadata": {},
   "source": [
    "# 四、分布式数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd826a49",
   "metadata": {},
   "source": [
    "### 一 分布式数据并行训练（Distributed Data Parallel Training）\n",
    "\n",
    "在作业的这一部分中，我们将探索使用多张 GPU 训练语言模型的方法，重点关注**数据并行（data parallelism）**。\n",
    "我们将首先对 PyTorch 中的分布式通信进行一个入门介绍。接着，我们会学习一种朴素（naive）的分布式数据并行训练实现方式，并在此基础上实现和评测多种用于提升通信效率的改进方案。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 PyTorch 中的单机分布式通信（Single-Node Distributed Communication in PyTorch）\n",
    "\n",
    "我们先来看一个 PyTorch 中的简单分布式应用，其目标是：\n",
    "**生成四个随机整数张量，并计算它们的和。**\n",
    "\n",
    "在下面的分布式示例中，我们会启动 **4 个 worker 进程**，每个进程都会生成一个随机整数张量。\n",
    "为了对这些进程中的张量求和，我们将调用 **all-reduce** 这一集合通信（collective communication）操作。\n",
    "\n",
    "`all-reduce` 的作用是：\n",
    "\n",
    "> 将所有进程中的张量进行规约（这里是求和），并用规约后的结果 **原地替换** 每个进程中的原始张量。\n",
    "\n",
    "也就是说，在 all-reduce 完成后，每个进程中的张量都会变成相同的“全局求和结果”。\n",
    "\n",
    "下面我们来看代码。\n",
    "\n",
    "---\n",
    "\n",
    "在运行下面的脚本后，我们会得到如下输出。\n",
    "可以看到：在 all-reduce 之前，每个 worker 进程持有的张量都不同；而在 all-reduce 操作之后，由于所有进程中的张量被求和并同步，每个进程中的 `data` 都被**原地修改**为相同的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "如果你多次运行这个脚本，你会发现打印输出的顺序并不是确定的。这是因为该应用运行在分布式环境中，我们无法控制各个进程执行打印语句的精确顺序。\n",
    "**唯一可以保证的是**：在 all-reduce 操作完成之后，所有进程都会持有**逐位（bitwise）完全一致**的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba55c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distributed_demo.py\n",
    "# =========================\n",
    "# 标准库 & PyTorch 相关导入\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 分布式环境初始化函数\n",
    "# =========================\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"\n",
    "    初始化分布式通信环境（Process Group）\n",
    "\n",
    "    参数：\n",
    "    - rank: 当前进程的编号（0 ~ world_size-1）\n",
    "    - world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # 指定“主进程”的地址\n",
    "    # 所有进程都会通过这个地址进行 rendezvous（集合）\n",
    "    # 单机多进程时使用 localhost\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "\n",
    "    # 指定主进程监听的端口号\n",
    "    # 所有进程必须保持一致\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组（非常关键的一步）\n",
    "    # backend=\"gloo\"：使用 gloo 通信后端（CPU 通用，支持多平台）\n",
    "    # rank：当前进程在所有进程中的唯一编号\n",
    "    # world_size：参与通信的总进程数\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\",\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 每个进程实际执行的函数\n",
    "# =========================\n",
    "def distributed_demo(rank, world_size):\n",
    "    \"\"\"\n",
    "    每个进程都会执行这个函数一次\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化分布式通信环境\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # 每个进程各自生成一个本地张量\n",
    "    # torch.randint 是“本地操作”，不同 rank 得到的值不同\n",
    "    # 这里生成一个 shape 为 (3,) 的一维张量\n",
    "    data = torch.randint(0, 10, (3,))\n",
    "\n",
    "    # 打印 all-reduce 之前的数据\n",
    "    # 注意：由于是多进程并行执行，打印顺序不确定\n",
    "    print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "\n",
    "    # 核心操作：all-reduce\n",
    "    # - 对所有 rank 上的 data 做规约（默认是 SUM）\n",
    "    # - 结果会“原地”写回到每个 rank 的 data 中\n",
    "    # - async_op=False 表示同步执行（阻塞，直到完成）\n",
    "    dist.all_reduce(data, async_op=False)\n",
    "\n",
    "    # 打印 all-reduce 之后的数据\n",
    "    # 此时每个 rank 的 data 都是完全相同的\n",
    "    print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 程序主入口\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # world_size 表示总进程数\n",
    "    # 在 DDP 中通常等于 GPU 数量\n",
    "    world_size = 4\n",
    "\n",
    "    # 使用 torch.multiprocessing.spawn 启动多进程\n",
    "    mp.spawn(\n",
    "        fn=distributed_demo,   # 每个进程要执行的函数\n",
    "        args=(world_size,),    # 传给函数的额外参数（rank 会自动作为第一个参数）\n",
    "        nprocs=world_size,     # 启动的进程数量\n",
    "        join=True              # 主进程是否等待所有子进程结束\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee989a51",
   "metadata": {},
   "source": [
    "由于分布式环境不支持ipynb的notebook格式，只能先写入.py文件再在notebook中运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11497694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank [Gloo] Rank [Gloo] Rank [Gloo] Rank 021 is connected to 3 is connected to  is connected to 3 is connected to 33 peer ranks. 3 peer ranks.  peer ranks. Expected number of connected peer ranks is :  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 3Expected number of connected peer ranks is : 33\n",
      "\n",
      "3\n",
      "\n",
      "rank 2 data (before all-reduce): tensor([1, 8, 8])rank 3 data (before all-reduce): tensor([2, 6, 8])rank 0 data (before all-reduce): tensor([0, 3, 4])\n",
      "\n",
      "\n",
      "rank 1 data (before all-reduce): tensor([1, 9, 3])\n",
      "rank 0 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 3 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 2 data (after all-reduce): tensor([ 4, 26, 23])rank 1 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_demo.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d60b11",
   "metadata": {},
   "source": [
    "## 这段代码在“分布式视角”下做了什么？\n",
    "\n",
    "> **它在一台机器上启动 4 个独立进程，每个进程生成一个随机张量，然后通过 `all_reduce` 把所有进程的张量求和，并让每个进程都拿到相同的结果。**\n",
    "\n",
    "这正是 **数据并行训练中“梯度同步”**的最小原型。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、整体执行流程（非常重要）\n",
    "\n",
    "当运行：\n",
    "\n",
    "```bash\n",
    "python distributed_hello_world.py\n",
    "```\n",
    "\n",
    "实际发生的是： **主进程启动**、 `mp.spawn` 启动 **4 个子进程**。\n",
    "\n",
    "每个子进程 被分配一个唯一的 `rank`（0～3），加入同一个 `process group`， 各自生成数据， 通过 `all_reduce` 进行通信。\n",
    "\n",
    "注意：**每个 rank 是一个独立的 Python 进程，不是线程**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、逐段精讲代码\n",
    "\n",
    "---\n",
    "\n",
    "### 1 导入模块\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "```\n",
    "\n",
    "作用拆解：\n",
    "\n",
    "| 模块                      | 作用             |\n",
    "| ----------------------- | -------------- |\n",
    "| `os`                    | 设置分布式通信所需的环境变量 |\n",
    "| `torch`                 | 张量与随机数         |\n",
    "| `torch.distributed`     | 分布式通信核心 API    |\n",
    "| `torch.multiprocessing` | 启动多个进程         |\n",
    "\n",
    "---\n",
    "\n",
    "### 2 setup：初始化分布式通信\n",
    "\n",
    "```python\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "```\n",
    "\n",
    "这是 **分布式程序的“入群”步骤**。\n",
    "\n",
    "#### （1）`MASTER_ADDR` & `MASTER_PORT`\n",
    "\n",
    "```text\n",
    "谁是“总协调者”（Rendezvous Server）\n",
    "```\n",
    "\n",
    "* 所有进程都会连到这个地址\n",
    "* 单机多进程 → 用 `localhost`\n",
    "* 多机 → 用主节点 IP\n",
    "\n",
    "---\n",
    "\n",
    "#### （2）`init_process_group`\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend=\"gloo\",\n",
    "    rank=rank,\n",
    "    world_size=world_size\n",
    ")\n",
    "```\n",
    "\n",
    "这是最关键的一行。\n",
    "\n",
    "参数解释：\n",
    "\n",
    "| 参数               | 含义             |\n",
    "| ---------------- | -------------- |\n",
    "| `backend=\"gloo\"` | 通信后端（CPU / 通用） |\n",
    "| `rank`           | 当前进程的唯一编号      |\n",
    "| `world_size`     | 进程总数           |\n",
    "\n",
    " **核心概念**：\n",
    "\n",
    "* `rank`：我是谁\n",
    "* `world_size`：我们一共有多少人\n",
    "\n",
    "在 DDP 里：\n",
    "\n",
    "* **1 个 GPU ≈ 1 个 rank**\n",
    "\n",
    "---\n",
    "\n",
    "### 3  distributed_demo：每个进程执行的逻辑\n",
    "\n",
    "```python\n",
    "def distributed_demo(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "```\n",
    "\n",
    "> 每个子进程都会执行这个函数一次\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1 生成本地数据\n",
    "\n",
    "```python\n",
    "data = torch.randint(0, 10, (3,))\n",
    "```\n",
    "\n",
    "* 每个 rank 生成 **不同的随机张量**\n",
    "* 例如：\n",
    "\n",
    "  * rank 0 → `[4, 4, 7]`\n",
    "  * rank 1 → `[9, 5, 3]`\n",
    "  * rank 2 → `[6, 0, 7]`\n",
    "  * rank 3 → `[3, 7, 8]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2 all-reduce 前打印\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "```\n",
    "\n",
    " 打印顺序**不保证**，因为 4 个进程是并行的。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3 核心：`all_reduce`\n",
    "\n",
    "```python\n",
    "dist.all_reduce(data, async_op=False)\n",
    "```\n",
    "\n",
    "这是整段代码的**灵魂**。\n",
    "\n",
    "### all-reduce 做了什么？\n",
    "\n",
    "对所有 rank 上的 `data`：\n",
    "\n",
    "1. **收集**\n",
    "2. **求和（默认是 SUM）**\n",
    "3. **把结果写回每个 rank 的 `data`（原地）**\n",
    "\n",
    "数学上等价于：\n",
    "\n",
    "```python\n",
    "data = data_rank0 + data_rank1 + data_rank2 + data_rank3\n",
    "```\n",
    "\n",
    "关键点：\n",
    "\n",
    "* **in-place 操作**\n",
    "* 所有 rank 最终结果 **完全一致**\n",
    "* 默认是 `SUM`（梯度同步就是用这个）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4 all-reduce 后打印\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "```\n",
    "\n",
    "你会看到所有 rank 都变成：\n",
    "\n",
    "```text\n",
    "tensor([22, 16, 25])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4 主入口：spawn 多进程\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "```\n",
    "\n",
    "* 定义进程数\n",
    "* 在 DDP 中通常 = GPU 数量\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "mp.spawn(\n",
    "    fn=distributed_demo,\n",
    "    args=(world_size,),\n",
    "    nprocs=world_size,\n",
    "    join=True\n",
    ")\n",
    "```\n",
    "\n",
    "解释：\n",
    "\n",
    "| 参数          | 含义           |\n",
    "| ----------- | ------------ |\n",
    "| `fn`        | 每个进程执行的函数    |\n",
    "| `args`      | 传给函数的参数      |\n",
    "| `nprocs`    | 启动多少个进程      |\n",
    "| `join=True` | 主进程等待所有子进程结束 |\n",
    "\n",
    "`spawn` 会自动给每个进程分配 `rank = 0, 1, 2, 3`，把 `rank` 作为 **第一个参数** 传入 `fn`。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、这段代码 ↔ 真正的 DDP 训练\n",
    "\n",
    "| 本例                 | DDP 中            |\n",
    "| ------------------ | ---------------- |\n",
    "| `data`             | 梯度（`param.grad`） |\n",
    "| `torch.randint`    | 前向 + 反向计算        |\n",
    "| `all_reduce(data)` | 梯度同步             |\n",
    "| 单次操作               | 每个 iteration 都做  |\n",
    "\n",
    "本质上：\n",
    "\n",
    "> **DDP = 前向各算各的，反向之后用 all-reduce 同步梯度**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc9cf6",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1.1 分布式应用基准测试的最佳实践\n",
    "\n",
    "（Best Practices for Benchmarking Distributed Applications）\n",
    "\n",
    "在本作业的这一部分中，你将对**分布式应用进行基准测试**，以更好地理解**通信带来的开销**。以下是一些推荐的最佳实践：\n",
    "\n",
    "* **尽可能在同一台机器上运行基准测试**，以便进行可控、公平的对比。\n",
    "\n",
    "* **在正式计时之前进行多次 warm-up（预热）**。\n",
    "  这一点对 **NCCL 通信调用**尤为重要。通常 **5 次预热迭代** 就足够了。\n",
    "\n",
    "* **在 GPU 上进行基准测试时，务必调用 `torch.cuda.synchronize()`**，以等待 CUDA 操作真正完成。\n",
    "  注意：即使在通信操作中设置了 `async_op=False`，这一步仍然是必须的，因为该调用只保证操作被**加入 GPU 队列**，而不是通信实际完成。³\n",
    "\n",
    "* **不同 rank 的测量时间可能略有差异**，因此通常会在所有 rank 之间**聚合测量结果**以提高估计的稳定性。\n",
    "  可以使用 **all-gather 集合通信操作**（特别是 `dist.all_gather_object`）来收集所有 rank 的结果。\n",
    "\n",
    "* **一般建议先在 CPU 上使用 Gloo 后端进行本地调试**，然后在实际问题中根据需要使用 GPU + NCCL 进行基准测试。\n",
    "  在两种后端之间切换通常只需要修改：\n",
    "\n",
    "  * `init_process_group` 中的 backend 参数\n",
    "  * 张量的设备（CPU / GPU）转换\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01bea2",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "## 问题（distributed_communication_single_node）：5 分\n",
    "\n",
    "请编写一个脚本，用于在**单节点多进程（single-node multi-process）**设置下，**基准测试 all-reduce 操作的运行时间**。\n",
    "\n",
    "你可以参考前面给出的示例代码作为起点，并尝试改变以下配置参数：\n",
    "\n",
    "### 实验设置要求\n",
    "\n",
    "* **后端 + 设备类型**\n",
    "\n",
    "  * Gloo + CPU\n",
    "  * NCCL + GPU\n",
    "\n",
    "* **all-reduce 数据规模**\n",
    "\n",
    "  * 数据类型：`float32`\n",
    "  * 张量大小：\n",
    "\n",
    "    * 1MB\n",
    "    * 10MB\n",
    "    * 100MB\n",
    "    * 1GB\n",
    "\n",
    "* **进程数量**\n",
    "\n",
    "  * 2 个进程\n",
    "  * 4 个进程\n",
    "  * 6 个进程\n",
    "\n",
    "### 资源限制\n",
    "\n",
    "* 最多可使用 **6 张 GPU**\n",
    "* 每一次基准测试运行时间 **不得超过 5 分钟**\n",
    "\n",
    "---\n",
    "\n",
    "## 交付内容（Deliverable）\n",
    "\n",
    "* 提供 **图表（plot）和/或表格（table）**，用于比较不同实验设置下的性能表现\n",
    "* 并附上 **2–3 句话的分析说明**，总结你的实验结果，以及你对各因素（后端、数据规模、进程数等）如何相互影响的理解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6259ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:# 有多少个GPU就增加测试用例，当然一个也能跑\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bec746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2600 ms\n",
      "  Bandwidth      : 4.0336 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0162 ms\n",
      "  Bandwidth      : 64.5348 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 1.9385 ms\n",
      "  Bandwidth      : 5.4092 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0178 ms\n",
      "  Bandwidth      : 589.5505 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 18.0035 ms\n",
      "  Bandwidth      : 5.8243 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0177 ms\n",
      "  Bandwidth      : 5927.2864 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 169.5600 ms\n",
      "  Bandwidth      : 6.1841 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0180 ms\n",
      "  Bandwidth      : 58252.2717 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32464a",
   "metadata": {},
   "source": [
    "## 一、Gloo和NCCL的基本概念\n",
    "\n",
    "在 PyTorch 中，多进程/多 GPU 之间进行通信（比如 `all_reduce`, `broadcast`, `all_gather`）需要 **通信后端（backend）**。\n",
    "常用的两种是：\n",
    "\n",
    "| 名称       | 适用设备            | 特点                                                 |\n",
    "| -------- | --------------- | -------------------------------------------------- |\n",
    "| **Gloo** | CPU / GPU（有限支持） | 跨平台，支持 CPU，多节点可以用 TCP/IP 通信，适合 CPU 张量，简单可靠         |\n",
    "| **NCCL** | GPU（NVIDIA GPU） | NVIDIA 官方库，专门优化 GPU 间通信，支持高速 PCIe、NVLink、网络通信，性能最好 |\n",
    "\n",
    "---\n",
    "\n",
    "## 二、Gloo\n",
    "\n",
    "Gloo 在 CPU 和 GPU 都可以用，但 GPU 支持有限，通常用在 CPU 张量上，它支持跨节点网络通信（TCP/IP）。并且使用方便，启动和调试比 NCCL 容易。在 CPU 环境下是**默认且可靠的选择**。\n",
    "\n",
    "它通常使用在单机 CPU 多进程训练，或者小规模实验和调试，没有 GPU 的服务器环境。\n",
    "\n",
    "在CPU 通信效率中等，GPU 上比 NCCL 慢，不适合大规模深度学习训练。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、NCCL（NVIDIA Collective Communication Library）\n",
    "\n",
    "\n",
    "它是专门为 NVIDIA GPU 设计的，支持多 GPU、多节点高速通信。并且优化了**PCIe 直连**，**NVLink（GPU 内部高速总线）**，**网络互联（Ethernet / InfiniBand）**，PyTorch 会自动使用 NCCL 的 ring / tree 通信算法做 `all_reduce`、`all_gather` 等操作。\n",
    "\n",
    "通常在单机多 GPU 训练（如 2~8 GPU）上使用。分布式训练深度学习大模型，高性能 GPU 集群使用较多。\n",
    "\n",
    "能将GPU 间带宽最大化，并且延迟极低（特别是大张量）， NCCL + CUDA 几乎是训练大模型的默认选择。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、总结对比\n",
    "\n",
    "| 特性           | Gloo            | NCCL               |\n",
    "| ------------ | --------------- | ------------------ |\n",
    "| 设备           | CPU（主要），GPU（有限） | GPU（NVIDIA 专用）     |\n",
    "| 性能           | 中等              | 很高                 |\n",
    "| 跨节点支持        | TCP/IP          | NVLink / PCIe / 网络 |\n",
    "| 用途           | CPU 分布式 / 小实验   | GPU 多卡训练 / 大模型     |\n",
    "| PyTorch 默认推荐 | CPU → Gloo      | GPU → NCCL         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00027",
   "metadata": {},
   "source": [
    "\n",
    "## 一、代码概览\n",
    "\n",
    "这段代码的目的只有一个：\n",
    "\n",
    "**在分布式环境下 benchmark `all_reduce` 的通信性能**\n",
    "测量指标包括：单次 `all_reduce` 的平均耗时，实际通信带宽（GB/s）适用场景在多进程（多卡 / 多节点），对比不同 backend（`gloo` / `nccl`）， 对比 CPU vs GPU 通信性能。\n",
    "\n",
    "## 二、模块介绍\n",
    "\n",
    "### 1. `torch.distributed as dist`\n",
    "\n",
    "它是PyTorch 的**分布式通信核心模块**，它提供进程组管理、collective ops（`all_reduce`, `broadcast`, `all_gather` 等）。\n",
    "\n",
    "### 2. `torch.multiprocessing.spawn`\n",
    "\n",
    "它是**多进程启动工具**，每个进程对应一个 `rank`，通常 rank + GPU id。当然分布式训练不等于多线程，**每个 rank 是一个独立 Python 进程**。\n",
    "\n",
    "\n",
    "## 三、初始化分布式环境\n",
    "\n",
    "```python\n",
    "def setup(master_addr, master_port, rank, world_size, backend):\n",
    "```\n",
    "\n",
    "### 1. MASTER_ADDR / MASTER_PORT\n",
    "\n",
    "```python\n",
    "os.environ['MASTER_ADDR'] = master_addr\n",
    "os.environ['MASTER_PORT'] = str(master_port)\n",
    "```\n",
    "\n",
    "定义**主节点（rank 0）的地址**，所有进程都通过这个地址进行 rendezvous，即使是**单机多卡**，也必须设置。\n",
    "\n",
    "### 2. 初始化进程组\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend,\n",
    "    rank=rank,\n",
    "    world_size=world_size\n",
    ")\n",
    "```\n",
    "\n",
    "| 参数         | 含义        |\n",
    "| ---------- | --------- |\n",
    "| backend    | 通信后端      |\n",
    "| rank       | 当前进程的全局编号 |\n",
    "| world_size | 总进程数      |\n",
    "\n",
    "#### 常见 backend 对比\n",
    "\n",
    "| backend | 适用场景            |\n",
    "| ------- | --------------- |\n",
    "| `gloo`  | CPU / 小规模       |\n",
    "| `nccl`  | GPU / 高性能（事实标准） |\n",
    "| `mpi`   | HPC 环境          |\n",
    "\n",
    "---\n",
    "\n",
    "## 四、`cleanup()`：资源回收\n",
    "\n",
    "```python\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```\n",
    "\n",
    "它显式释放通信资源，防止程序异常退出导致死锁\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "清空 CUDA cache（**不是释放显存，只是释放 PyTorch cache**），benchmark 中避免显存碎片影响。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、`benchmark_all_reduce()`\n",
    "\n",
    "\n",
    "### Step 1. 设置环境 & GPU 绑定\n",
    "\n",
    "```python\n",
    "setup(...)\n",
    "```\n",
    "\n",
    "#### GPU 场景下：\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(rank)\n",
    "```\n",
    "\n",
    "**为什么是 `rank`？**\n",
    "\n",
    "通常约定：\n",
    "\n",
    "```\n",
    "rank 0 → GPU 0\n",
    "rank 1 → GPU 1\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "### Step 2. 构造测试 Tensor\n",
    "\n",
    "```python\n",
    "tensor_size_bytes = tensor_size_mb * 1024 * 1024\n",
    "num_elements = tensor_size_bytes // 4\n",
    "tensor_data = torch.randn(num_elements, device=device)\n",
    "```\n",
    "\n",
    "`float32 = 4 bytes`，`randn` 只是为了填充数据，**值本身无关紧要**\n",
    "\n",
    "\n",
    "### Step 3 Warm-up\n",
    "\n",
    "```python\n",
    "for _ in range(5):\n",
    "    dist.all_reduce(tensor_data, op=dist.ReduceOp.SUM)\n",
    "```\n",
    "\n",
    "#### 为什么一定要 warm-up？\n",
    "\n",
    "1. CUDA kernel lazy initialization\n",
    "2. NCCL communicator 建立\n",
    "3. GPU 时钟频率爬升\n",
    "4. cache / pipeline 填充\n",
    "\n",
    "\n",
    "\n",
    "GPU 场景下：\n",
    "\n",
    "```python\n",
    "torch.cuda.synchronize()\n",
    "```\n",
    "\n",
    "CUDA 是**异步执行**不同步，测到的是 launch 时间。\n",
    "\n",
    "---\n",
    "\n",
    "### Barrier：所有进程对齐\n",
    "\n",
    "```python\n",
    "dist.barrier()\n",
    "```\n",
    "\n",
    "确保所有 rank **同时开始计时**，防止快的进程提前进入 benchmark。\n",
    "\n",
    "\n",
    "### Step 4. 正式 benchmark\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "```\n",
    "\n",
    "```python\n",
    "for _ in range(num_iterations):\n",
    "    dist.all_reduce(tensor_data, op=dist.ReduceOp.SUM)\n",
    "```\n",
    "\n",
    "`num_iterations = 20`，用来取平均，降低噪声。\n",
    "\n",
    "GPU 同步：\n",
    "\n",
    "```python\n",
    "torch.cuda.synchronize()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5. 计算耗时与带宽\n",
    "\n",
    "#### 平均耗时\n",
    "\n",
    "```python\n",
    "avg_time = duration / num_iterations\n",
    "```\n",
    "\n",
    "#### 带宽计算\n",
    "\n",
    "```python\n",
    "bandwidth_gbps = (tensor_size_bytes / avg_time) / 1e9\n",
    "```\n",
    "\n",
    "含义：\n",
    "\n",
    "```\n",
    "GB/s = 数据量 / 时间\n",
    "```\n",
    "\n",
    "当然这是 **简化模型**，没区分 ring / tree，适合对比，不是绝对值。\n",
    "\n",
    "\n",
    "### Step 6. 只让 rank 0 打印\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    print(...)\n",
    "```\n",
    "\n",
    "避免：N 个进程打印 N 份日志\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7. `all_gather_object` 汇总结果\n",
    "\n",
    "```python\n",
    "dist.all_gather_object(gathered_results, local_result)\n",
    "```\n",
    "\n",
    "它支持 **Python 对象**，比 `all_gather(tensor)` 更灵活，常用于 benchmark / profiling，这样 rank 0 能拿到所有 rank 的结果。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8. 清理 & 返回结果\n",
    "\n",
    "```python\n",
    "cleanup()\n",
    "```\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    return gathered_results\n",
    "```\n",
    "\n",
    "只有主进程返回， 子进程自动退出。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d57918",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "## 问题（naive_ddp）：5 分\n",
    "\n",
    "**交付物**：编写一个脚本来通过在反向传播后对各个参数梯度进行全规约，以简单方式执行分布式数据并行训练。为了验证你的DDP实现的正确性，使用它在随机生成的数据上训练一个小玩具模型，并验证其权重与单进程训练的结果相匹配。\n",
    "\n",
    "---\n",
    "\n",
    "## 问题（naive_ddp_benchmarking）：3 分\n",
    "\n",
    "在这种简单的DDP实现中，参数在每个反向传播后在各个rank之间单独进行全规约。为了更好地理解数据并行训练的开销，创建一个脚本来对之前实现的语言模型进行基准测试，该模型使用这种简单的DDP实现进行训练。测量每个训练步骤的总时间以及用于通信梯度的时间比例。收集在单节点设置（1个节点 x 2个GPU）下对XL模型的测量结果，如§1.1.2中所述。\n",
    "\n",
    "**交付物**：描述你的基准测试设置，以及每个训练迭代的测量时间以及用于通信梯度的时间。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a310004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_model_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_model_demo.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 一、定义一个简单的全连接神经网络\n",
    "# ============================\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    一个最简单的三层全连接网络，用于 MNIST 分类\n",
    "    输入：28×28 展平后的 784 维向量\n",
    "    输出：10 类（0~9）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一层全连接：输入层 -> 隐藏层\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # 第二层全连接：隐藏层 -> 隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 输出层：隐藏层 -> 类别数\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播逻辑\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # 使用 log_softmax，方便后续配合 NLLLoss\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 二、分布式环境初始化与清理\n",
    "# ============================\n",
    "def init_distributed_env(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    初始化分布式进程组（所有进程都必须调用）\n",
    "    \"\"\"\n",
    "    # 主进程地址和端口\n",
    "    # 在单机多卡场景中，localhost 即可\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_env():\n",
    "    \"\"\"\n",
    "    销毁进程组并清理资源\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、DDP 训练主逻辑（教学重点）\n",
    "# ============================\n",
    "def ddp_training_worker(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    每一个进程都会执行这个函数\n",
    "    rank      : 当前进程编号\n",
    "    world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. 固定随机种子（确保所有进程行为一致）\n",
    "    # ------------------------------------------------\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 确保 cudnn 的确定性行为（教学更容易对齐结果）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 初始化分布式环境\n",
    "    # ------------------------------------------------\n",
    "    init_distributed_env(rank, world_size, backend)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 为当前进程分配设备\n",
    "    # ------------------------------------------------\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(rank)\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "        print(f\"[进程 {rank}] 使用 GPU：{device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"[进程 {rank}] CUDA 不可用，使用 CPU\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 数据预处理与加载\n",
    "    # ------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        # 将 28×28 展平为 784\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.MNIST(\n",
    "        root=\"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. 手动划分数据（教学版 DDP）\n",
    "    #    每个进程只处理数据集的一部分\n",
    "    # ------------------------------------------------\n",
    "    total_samples = len(full_dataset)\n",
    "    samples_per_rank = total_samples // world_size\n",
    "\n",
    "    start_index = rank * samples_per_rank\n",
    "    end_index = start_index + samples_per_rank\n",
    "\n",
    "    subset_dataset = torch.utils.data.Subset(\n",
    "        full_dataset,\n",
    "        range(start_index, end_index)\n",
    "    )\n",
    "\n",
    "    # DataLoader 使用相同随机种子，确保 shuffle 行为可复现\n",
    "    data_generator = torch.Generator()\n",
    "    data_generator.manual_seed(seed)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        subset_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        generator=data_generator\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[进程 {rank}] 负责样本区间：\"\n",
    "        f\"{start_index} ~ {end_index - 1}，\"\n",
    "        f\"共 {samples_per_rank} 条数据\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 6. 创建模型与优化器\n",
    "    # ------------------------------------------------\n",
    "    model = SimpleNet(\n",
    "        input_dim=784,\n",
    "        hidden_dim=50,\n",
    "        num_classes=10\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 7. 步骤一：广播模型参数（关键教学点）\n",
    "    #    确保所有进程从“完全相同”的初始模型开始\n",
    "    # ------------------------------------------------\n",
    "    for param in model.parameters():\n",
    "        dist.broadcast(param.data, src=0)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 8. 正式开始训练\n",
    "    # ------------------------------------------------\n",
    "    for epoch in range(1, 3):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤二：前向 + 反向传播\n",
    "            # 每个进程只计算自己那一部分数据的梯度\n",
    "            # ----------------------------\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            step_start_time = time.time()\n",
    "            loss.backward()\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤三：梯度 All-Reduce\n",
    "            # 将所有进程的梯度相加，再取平均\n",
    "            # ----------------------------\n",
    "            comm_start_time = time.time()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    dist.all_reduce(\n",
    "                        param.grad.data,\n",
    "                        op=dist.ReduceOp.SUM\n",
    "                    )\n",
    "                    param.grad.data /= world_size\n",
    "\n",
    "            comm_time = time.time() - comm_start_time\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤四：参数更新\n",
    "            # 所有进程使用“完全相同”的平均梯度\n",
    "            # ----------------------------\n",
    "            optimizer.step()\n",
    "\n",
    "            step_time = time.time() - step_start_time\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[进程 {rank}] \"\n",
    "                    f\"Epoch {epoch} | \"\n",
    "                    f\"Batch {batch_idx} | \"\n",
    "                    f\"Loss = {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[主进程] 单步耗时：{step_time * 1000:.2f} ms，\"\n",
    "                    f\"通信耗时：{comm_time * 1000:.2f} ms\"\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 9. 仅在 rank 0 保存模型\n",
    "    # ------------------------------------------------\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"[主进程] 模型已保存：mnist_simple_ddp.pt\")\n",
    "\n",
    "    destroy_distributed_env()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 四、程序入口\n",
    "# ============================\n",
    "def main():\n",
    "    # world_size = 使用的进程（GPU）数量\n",
    "    # 如果只有一张 GPU，就设为 1（等价于普通训练）\n",
    "    world_size = 1\n",
    "\n",
    "    # NCCL：GPU 通信首选后端\n",
    "    backend = \"nccl\"\n",
    "\n",
    "    # 检查数据是否能被平均分配\n",
    "    dataset_size = len(\n",
    "        datasets.MNIST(\"../data\", train=True, download=True)\n",
    "    )\n",
    "\n",
    "    if dataset_size % world_size != 0:\n",
    "        print(\n",
    "            f\"警告：数据量 {dataset_size} 不能被 world_size={world_size} 整除，\"\n",
    "            f\"将丢弃部分样本以保证均分\"\n",
    "        )\n",
    "\n",
    "    print(f\"启动分布式训练，进程数：{world_size}\")\n",
    "    print(f\"每个进程处理样本数：{dataset_size // world_size}\")\n",
    "\n",
    "    # 启动多进程\n",
    "    spawn(\n",
    "        ddp_training_worker,\n",
    "        args=(world_size, backend),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "\n",
    "    print(\"分布式训练结束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32970985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动分布式训练，进程数：1\n",
      "每个进程处理样本数：60000\n",
      "[进程 0] 使用 GPU：cuda:0\n",
      "[进程 0] 负责样本区间：0 ~ 59999，共 60000 条数据\n",
      "[进程 0] Epoch 1 | Batch 0 | Loss = 2.294834\n",
      "[主进程] 单步耗时：178.22 ms，通信耗时：14.50 ms\n",
      "[进程 0] Epoch 1 | Batch 50 | Loss = 0.515532\n",
      "[主进程] 单步耗时：1.15 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 100 | Loss = 0.286108\n",
      "[主进程] 单步耗时：1.21 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 150 | Loss = 0.235657\n",
      "[主进程] 单步耗时：1.63 ms，通信耗时：0.49 ms\n",
      "[进程 0] Epoch 1 | Batch 200 | Loss = 0.305218\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 1 | Batch 250 | Loss = 0.421952\n",
      "[主进程] 单步耗时：1.36 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 300 | Loss = 0.376283\n",
      "[主进程] 单步耗时：1.17 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 350 | Loss = 0.159866\n",
      "[主进程] 单步耗时：1.60 ms，通信耗时：0.39 ms\n",
      "[进程 0] Epoch 1 | Batch 400 | Loss = 0.211988\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.65 ms\n",
      "[进程 0] Epoch 1 | Batch 450 | Loss = 0.401293\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 500 | Loss = 0.201109\n",
      "[主进程] 单步耗时：1.12 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 1 | Batch 550 | Loss = 0.257683\n",
      "[主进程] 单步耗时：1.62 ms，通信耗时：0.59 ms\n",
      "[进程 0] Epoch 1 | Batch 600 | Loss = 0.241983\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.67 ms\n",
      "[进程 0] Epoch 1 | Batch 650 | Loss = 0.171643\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 700 | Loss = 0.333229\n",
      "[主进程] 单步耗时：2.02 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 750 | Loss = 0.091621\n",
      "[主进程] 单步耗时：1.23 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 800 | Loss = 0.125746\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 850 | Loss = 0.194321\n",
      "[主进程] 单步耗时：1.01 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 900 | Loss = 0.435536\n",
      "[主进程] 单步耗时：1.33 ms，通信耗时：0.34 ms\n",
      "[进程 0] Epoch 2 | Batch 0 | Loss = 0.216730\n",
      "[主进程] 单步耗时：2.21 ms，通信耗时：0.42 ms\n",
      "[进程 0] Epoch 2 | Batch 50 | Loss = 0.168439\n",
      "[主进程] 单步耗时：1.09 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 100 | Loss = 0.067365\n",
      "[主进程] 单步耗时：1.25 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 2 | Batch 150 | Loss = 0.089863\n",
      "[主进程] 单步耗时：1.28 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 200 | Loss = 0.234549\n",
      "[主进程] 单步耗时：1.11 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 250 | Loss = 0.070795\n",
      "[主进程] 单步耗时：1.32 ms，通信耗时：0.41 ms\n",
      "[进程 0] Epoch 2 | Batch 300 | Loss = 0.088156\n",
      "[主进程] 单步耗时：1.68 ms，通信耗时：0.57 ms\n",
      "[进程 0] Epoch 2 | Batch 350 | Loss = 0.120999\n",
      "[主进程] 单步耗时：1.19 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 2 | Batch 400 | Loss = 0.121158\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.60 ms\n",
      "[进程 0] Epoch 2 | Batch 450 | Loss = 0.042282\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 500 | Loss = 0.265627\n",
      "[主进程] 单步耗时：1.38 ms，通信耗时：0.50 ms\n",
      "[进程 0] Epoch 2 | Batch 550 | Loss = 0.091650\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 600 | Loss = 0.142866\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.56 ms\n",
      "[进程 0] Epoch 2 | Batch 650 | Loss = 0.178375\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.68 ms\n",
      "[进程 0] Epoch 2 | Batch 700 | Loss = 0.181190\n",
      "[主进程] 单步耗时：1.10 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 750 | Loss = 0.107817\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 800 | Loss = 0.056186\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.63 ms\n",
      "[进程 0] Epoch 2 | Batch 850 | Loss = 0.103768\n",
      "[主进程] 单步耗时：1.04 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 900 | Loss = 0.167343\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.37 ms\n",
      "[主进程] 模型已保存：mnist_simple_ddp.pt\n",
      "分布式训练结束\n"
     ]
    }
   ],
   "source": [
    "!python ddp_model_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebc786",
   "metadata": {},
   "source": [
    "# 问题（minimal_ddp_flat_benchmarking）：2分\n",
    "\n",
    "修改您的最小DDP实现，以从所有参数中传递一个扁平化的梯度张量。将其性能与在先前使用条件下（1个节点 x 2个GPU，XL模型大小如§1.1.3中所述）为每个参数张量发出一个all-reduce的最小DDP实现进行比较。\n",
    "\n",
    "交付物：在单次批量all-reduce调用下，未分布式数据并行训练中每个训练迭代的测量时间以及梯度通信所花费的时间。1-2句话比较批量与单独通信梯度的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790a370",
   "metadata": {},
   "source": [
    "**验证：把“很多小的梯度通信”合并成“一次大的梯度通信”，能否显著降低 DDP 的通信开销**\n",
    "\n",
    "这在真实系统里是一个**非常核心的工程问题**。\n",
    "\n",
    "\n",
    "\n",
    "## naive DDP （对照基线）\n",
    "\n",
    "之前的最小 DDP 实现是这样的逻辑：\n",
    "\n",
    "```text\n",
    "for param in model.parameters():\n",
    "    all_reduce(param.grad)\n",
    "```\n",
    "\n",
    "也就是：**每个参数张量**，**单独一次 all-reduce**，通信调用次数 = 参数个数，XL 模型来说：参数张量数量几百～上千个，NCCL / GPU 通信，每一次 all-reduce 都有 **启动开销（latency）**，小 tensor → 极其低效，这就是作业的**优化的瓶颈**\n",
    "\n",
    "---\n",
    "\n",
    "## 改什么\n",
    "\n",
    "###  核心改动：**梯度展平 + 单次通信**\n",
    "\n",
    "把：\n",
    "\n",
    "```python\n",
    "grad_1, grad_2, grad_3, ..., grad_n\n",
    "```\n",
    "\n",
    "变成：\n",
    "\n",
    "```python\n",
    "flat_grad = concat([grad_1, grad_2, ..., grad_n])\n",
    "```\n",
    "\n",
    "然后只做：\n",
    "\n",
    "```python\n",
    "dist.all_reduce(flat_grad)\n",
    "```\n",
    "\n",
    "再把结果 **切回每个参数的 grad**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9739eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minimal_ddp_flat_benchmarking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minimal_ddp_flat_benchmarking.py\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # 定义第一个隐藏层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # 定义第二个隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 定义输出层\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  \n",
    "# 示例：创建一个SimpleNet实例\n",
    "\n",
    "def setup(rank, world_size, backend):\n",
    "    \"\"\" 初始化分布式环境 \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost' # 设置主节点IP地址以及端口，其他节点需要通过这个地址连接到主节点\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    # 根据后端初始化进程组\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size) # 初始化进程组，rank是当前进程的rank，world_size是总进程数，backend: 这是指定通信后端的参数。常见的后端有：gloo(CPU), nccl(GPU), mpi等。\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\" 清理分布式环境 \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    # 清理GPU缓存\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def ddp_train(rank, world_size, backend):\n",
    "    \"\"\" Naive DDP训练函数，实现图片中描述的4个步骤 \"\"\"\n",
    "    # 设置随机种子确保可重现性（所有进程使用相同种子）\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 设置确定性行为\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 初始化分布式环境\n",
    "    setup(rank, world_size, backend)\n",
    "    \n",
    "    # 为每个进程分配不同的GPU设备\n",
    "    # 使用GPU 0和1，它们当前是空闲的\n",
    "    gpu_id = rank  # rank 0 -> GPU 0, rank 1 -> GPU 1\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        print(f\"Rank {rank} using GPU: {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"CUDA not available, using CPU for rank {rank}\")\n",
    "    \n",
    "    # 数据加载和预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # 为了验证DDP正确性，我们让每个进程处理不同的数据子集\n",
    "    # 这样总的有效batch size = 64 * world_size\n",
    "    total_samples = len(dataset)\n",
    "    samples_per_process = total_samples // world_size\n",
    "    start_idx = rank * samples_per_process\n",
    "    end_idx = start_idx + samples_per_process\n",
    "    \n",
    "    # 创建当前进程的数据子集\n",
    "    subset = torch.utils.data.Subset(dataset, range(start_idx, end_idx))\n",
    "    \n",
    "    # 设置确定性的数据加载器\n",
    "    # 重要：为了与单进程训练比较，我们需要确保数据顺序一致\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)  # 所有进程使用相同的种子确保一致性\n",
    "    train_loader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=True, generator=generator)\n",
    "    \n",
    "    print(f\"Rank {rank} processing samples {start_idx} to {end_idx-1} ({samples_per_process} samples)\")\n",
    "    print(f\"Rank {rank} effective batch size: 64, total distributed batch size: {64 * world_size}\")\n",
    "    \n",
    "    # 创建模型和优化器（每个设备都创建相同的模型）\n",
    "    model = SimpleNet(input_size=784, hidden_size=50, num_classes=10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # ===== 新增：记录梯度展平所需的元信息 =====\n",
    "    grad_shapes = []\n",
    "    grad_numels = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "        grad_shapes.append(p.shape)\n",
    "        grad_numels.append(p.numel())\n",
    "\n",
    "    total_grad_numel = sum(grad_numels)\n",
    "\n",
    "    # 步骤1: Broadcast模型参数从rank 0到所有其他ranks\n",
    "    # 确保所有设备从相同的初始模型和优化器状态开始\n",
    "    for param in model.parameters():\n",
    "        # 确保参数在正确的设备上进行broadcast\n",
    "        dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # 同步优化器状态（如果有的话）\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param in optimizer.state:\n",
    "                for key, value in optimizer.state[param].items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        dist.broadcast(value, src=0)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, 3):  # Train for 2 epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # 步骤2: 每个设备使用本地模型参数进行前向传播和反向传播\n",
    "            # 计算n/d个样本的梯度\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            step_start = time.time()\n",
    "            loss.backward()\n",
    "            \n",
    "            comm_start = time.time()\n",
    "            # 步骤3: All-reduce梯度\n",
    "            # 将所有设备的梯度求平均，使每个设备都持有所有n个样本的平均梯度\n",
    "\n",
    "            # ===== 新的步骤3：Flattened gradient all-reduce =====\n",
    "            flat_grad = torch.zeros(total_grad_numel, device=device)\n",
    "\n",
    "            offset = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    numel = p.grad.numel()\n",
    "                    flat_grad[offset:offset + numel] = p.grad.view(-1)\n",
    "                    offset += numel\n",
    "\n",
    "            # 单次通信\n",
    "            dist.all_reduce(flat_grad, op=dist.ReduceOp.SUM)\n",
    "            flat_grad /= world_size\n",
    "\n",
    "            # 写回每个参数的梯度\n",
    "            offset = 0\n",
    "            for p, shape, numel in zip(model.parameters(), grad_shapes, grad_numels):\n",
    "                if p.grad is not None:\n",
    "                    p.grad.copy_(\n",
    "                        flat_grad[offset:offset + numel].view(shape)\n",
    "                    )\n",
    "                    offset += numel\n",
    "\n",
    "            comm_time = time.time() - comm_start\n",
    "            # 步骤4: 优化器步骤 - 每个设备使用相同的平均梯度更新参数\n",
    "            # 由于所有设备从相同初始状态开始并使用相同梯度，参数会保持同步\n",
    "            optimizer.step()\n",
    "            step_time = time.time() - step_start\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Rank {rank}, Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(f\"Step time: {step_time*1000:.2f} ms | Comm time: {comm_time*1000:.2f} ms\")\n",
    "\n",
    "    # 只在rank 0保存模型\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"Model saved!\")\n",
    "            \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = 1 # gpu只有一个就改成1，但是这样和平时普通的训练就没有区别了\n",
    "    backend = 'nccl'  # 使用gloo后端，更适合CPU训练\n",
    "    # 只检查数据集大小是否能被world_size整除\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    data_size = len(dataset)\n",
    "\n",
    "    if data_size % world_size != 0:\n",
    "        print(f\"Warning: Data size {data_size} is not divisible by world size {world_size}\")\n",
    "        print(f\"Some samples will be ignored to ensure equal distribution\")\n",
    "    \n",
    "    print(f\"Starting distributed training with {world_size} processes\")\n",
    "    print(f\"Each process will handle {data_size // world_size} samples\")\n",
    "    print(f\"Total samples: {data_size}\")\n",
    "    \n",
    "    # 启动分布式训练\n",
    "    spawn(ddp_train, args=(world_size, backend), nprocs=world_size, join=True)\n",
    "    \n",
    "    print(\"Distributed training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5eb2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/cs336_systems/minimal_ddp_flat_benchmarking.py\", line 4, in <module>\n",
      "    from torchvision import datasets, transforms\n",
      "ModuleNotFoundError: No module named 'torchvision'\n"
     ]
    }
   ],
   "source": [
    "!python minimal_ddp_flat_benchmarking.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c2f58",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "\n",
    "## 问题 (ddp_overlap_individual_parameters): 5 分\n",
    "\n",
    "实现一个 Python 类来处理分布式数据并行训练。该类应包装一个任意的 PyTorch `nn.Module` 并在训练之前处理权重的广播（以便所有 ranks 具有相同的初始参数）以及发出通信调用以进行梯度平均。我们建议以下公共接口：\n",
    "\n",
    "```python\n",
    "def __init__(self, module: torch.nn.Module): 给定一个实例化的 PyTorch nn.Module 要并行化，构建一个 DDP 容器，该容器将处理跨 ranks 的梯度同步。\n",
    "\n",
    "def forward(self, *inputs, **kwargs): 调用被包装模块的 `forward()` 方法，并使用提供的定位参数和关键字参数。\n",
    "\n",
    "def finish_gradient_synchronization(self): 当调用时，等待异步通信\n",
    "```\n",
    "\n",
    "*在高级情况下，如果你使用多个 CUDA 流，你可能需要显式同步跨流以确保输出准备好进行后续操作。* 参见 [CUDA Streams](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams)。\n",
    "\n",
    "### GPU 上的调用\n",
    "\n",
    "为了使用此类执行分布式训练，我们将一个模块传递给该模块进行包装，然后在我们运行 `optimizer.step()` 之前添加一个对 `finish_gradient_synchronization()` 的调用，以确保依赖于梯度的优化器步骤可以排队：\n",
    "\n",
    "```python\n",
    "model = ToyModel().to(device)\n",
    "ddp_model = DDP(model)\n",
    "\n",
    "for _ in range(train_steps):\n",
    "    x, y = get_batch()\n",
    "    logits = ddp_model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    ddp_model.finish_gradient_synchronization()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### 交付物：实现一个容器类来处理分布式数据并行训练。该类应重叠梯度通信和反向传播的计算。为了测试你的 DDP 类，首先实现适配器 `adapters.get_ddp_individual_parameters` 和 `adapters.ddp_individual_parameters_on_after_backward`（后者是可选的，取决于你的实现，你可能不需要它）。\n",
    "\n",
    "然后，执行测试，通过运行 `pytest tests/test_ddp_individual_parameters.py`。我们建议多次运行测试（例如，5 次）以确保其可靠通过。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ef9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_overlap_individual_parameters.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_overlap_individual_parameters.py\n",
    "'''\n",
    "实现一个手动版本的分布式数据并行（DDP）训练框架，核心特性是梯度分桶（Gradient Bucketing）+ 计算通信重叠（Computation-Communication Overlap）。\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 二、手动 DDP（按桶通信梯度）\n",
    "# ============================\n",
    "class DDPOverlapBucketed(nn.Module):\n",
    "    def __init__(self, module: torch.nn.Module, bucket_size_mb: float):\n",
    "        super(DDPOverlapBucketed, self).__init__()\n",
    "        self.module = module\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "        self.handles = []   # 保存异步 all_reduce 句柄\n",
    "        self.buckets = []   # 梯度桶\n",
    "        self.world_size = dist.get_world_size()\n",
    "        \n",
    "        # --------------------\n",
    "        # 初始化：广播参数 + 创建桶 + 注册钩子\n",
    "        # --------------------\n",
    "        self._broadcast_parameters()  # 初始参数广播\n",
    "        self._create_bucket()         # 按大小划分梯度桶\n",
    "        self._register_hook()         # 注册梯度钩子实现异步通信\n",
    "    \n",
    "    # --------------------\n",
    "    # 广播初始参数\n",
    "    # --------------------\n",
    "    def _broadcast_parameters(self):\n",
    "        \"\"\"\n",
    "        将 rank 0 的参数广播到所有进程，确保初始状态一致\n",
    "        \"\"\"\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # --------------------\n",
    "    # 按 bucket_size 创建梯度桶\n",
    "    # --------------------\n",
    "    def _create_bucket(self):\n",
    "        current_bucket_size = 0\n",
    "        current_bucket = []\n",
    "\n",
    "        # 倒序遍历参数，构建桶\n",
    "        for p in reversed(list(self.module.parameters())):\n",
    "            if p.requires_grad:\n",
    "                p_size = p.numel() * p.element_size()\n",
    "                \n",
    "                # 当前桶满了则保存并创建新桶\n",
    "                if p_size + current_bucket_size > self.bucket_size_bytes and current_bucket:\n",
    "                    self.buckets.append(current_bucket)\n",
    "                    current_bucket_size = 0\n",
    "                    current_bucket = []\n",
    "                \n",
    "                current_bucket.append(p)\n",
    "                current_bucket_size += p_size\n",
    "        \n",
    "        # 如果还有剩余参数，作为最后一个桶\n",
    "        if current_bucket:\n",
    "            self.buckets.append(current_bucket)\n",
    "        \n",
    "        # 为每个桶创建缓冲区\n",
    "        for i, bucket_params in enumerate(self.buckets):\n",
    "            if not bucket_params:\n",
    "                continue\n",
    "            buffer_size = sum(p.numel() for p in bucket_params)\n",
    "            buffer = torch.zeros(buffer_size, device=bucket_params[0].device, dtype=bucket_params[0].dtype)\n",
    "            self.buckets[i] = {\n",
    "                \"params\": bucket_params,\n",
    "                \"buffer\": buffer,\n",
    "                \"ready_params\": set(),\n",
    "                \"triggered\": False\n",
    "            }\n",
    "    \n",
    "    # --------------------\n",
    "    # 注册梯度钩子\n",
    "    # --------------------\n",
    "    def _register_hook(self):\n",
    "        for bucket_idx, bucket_info in enumerate(self.buckets):\n",
    "            for param in bucket_info[\"params\"]:\n",
    "                # 闭包捕获当前 bucket_idx\n",
    "                def make_hook(idx):\n",
    "                    return lambda grad, param=param: self._create_hook(grad, param, idx)\n",
    "                param.register_hook(make_hook(bucket_idx))\n",
    "    \n",
    "    # --------------------\n",
    "    # 梯度钩子逻辑：延迟执行 all_reduce\n",
    "    # --------------------\n",
    "    def _create_hook(self, grad, param, bucket_idx):\n",
    "        bucket_info = self.buckets[bucket_idx]\n",
    "        bucket_info[\"ready_params\"].add(param)\n",
    "\n",
    "        # 当桶内所有参数梯度都准备好，且未触发通信\n",
    "        if len(bucket_info[\"ready_params\"]) == len(bucket_info[\"params\"]) and not bucket_info[\"triggered\"]:\n",
    "            bucket_info[\"triggered\"] = True  # 标记为已触发\n",
    "\n",
    "            def delayed_sync():\n",
    "                # 将桶内所有梯度拷贝到扁平缓冲区\n",
    "                offset = 0\n",
    "                for p in bucket_info[\"params\"]:\n",
    "                    numel = p.numel()\n",
    "                    if p.grad is not None:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].copy_(p.grad.view(-1))\n",
    "                    else:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].zero_()\n",
    "                    offset += numel\n",
    "                \n",
    "                # 启动异步 all_reduce，实现计算与通信重叠\n",
    "                handle = dist.all_reduce(bucket_info[\"buffer\"], async_op=True)\n",
    "                self.handles.append((handle, bucket_idx))\n",
    "            \n",
    "            # 延迟执行，确保所有梯度计算完成\n",
    "            import torch.autograd as autograd\n",
    "            autograd.Variable._execution_engine.queue_callback(delayed_sync)\n",
    "    \n",
    "    # --------------------\n",
    "    # forward 前清理状态\n",
    "    # --------------------\n",
    "    def forward(self, x):\n",
    "        if self.world_size > 1:\n",
    "            for bucket in self.buckets:\n",
    "                bucket[\"triggered\"] = False\n",
    "                bucket[\"ready_params\"].clear()\n",
    "        self.handles.clear()\n",
    "        return self.module(x)\n",
    "\n",
    "    # --------------------\n",
    "    # 等待所有异步通信完成\n",
    "    # --------------------\n",
    "    def finish_gradient_synchronization(self):\n",
    "        \"\"\"\n",
    "        等待所有 all_reduce 完成，并将梯度写回各参数\n",
    "        需在 optimizer.step() 之前调用\n",
    "        \"\"\"\n",
    "        for handle, bucket_idx in self.handles:\n",
    "            handle.wait()  # 等待通信完成\n",
    "\n",
    "            bucket_info = self.buckets[bucket_idx]\n",
    "            buffer = bucket_info[\"buffer\"]\n",
    "\n",
    "            # 求平均梯度\n",
    "            buffer.div_(self.world_size)\n",
    "\n",
    "            offset = 0\n",
    "            for p in bucket_info[\"params\"]:\n",
    "                numel = p.numel()\n",
    "                if p.grad is not None:\n",
    "                    p.grad.view(-1).copy_(buffer[offset:offset+numel])\n",
    "                offset += numel\n",
    "        \n",
    "        # 清空 handles，为下一次迭代准备\n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、简单示例运行\n",
    "# ============================\n",
    "def run():\n",
    "    # 初始化分布式环境（torchrun 已设置环境变量）\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    model = nn.Linear(10, 5).cuda()\n",
    "    ddp_model = DDPOverlapBucketed(model, bucket_size_mb=1)  # 1 MB 小桶方便观察\n",
    "    opt = torch.optim.SGD(ddp_model.parameters(), lr=0.1)\n",
    "\n",
    "    for step in range(3):\n",
    "        x = torch.randn(4, 10).cuda()\n",
    "        loss = ddp_model(x).sum()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()  # 必须手动调用\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        print(f\"[rank {dist.get_rank()}] step {step} loss={loss.item()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060b549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank 0] step 0 loss=2.803687572479248\n",
      "[rank 0] step 1 loss=-8.734395980834961\n",
      "[rank 0] step 2 loss=-22.41156768798828\n",
      "[rank0]:[W122 14:35:46.682734635 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 ddp_overlap_individual_parameters.py # 有多少张卡设置nproc_per_node=几"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70de19",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "### 问题 (ddp_bucketed_benchmarking)：3分\n",
    "\n",
    "**(a)** 使用与之前实验相同的配置（1个节点，2个GPU，XL模型大小）对你的分桶DDP实现进行基准测试，变化最大桶大小（1, 10, 100, 1000 MB）。将你的结果与之前没有分桶的实验进行比较——结果是否符合你的预期？如果不符合，为什么？你可能需要使用PyTorch分析器来更好地理解通信调用的顺序和/或执行方式。你期望对实验设置进行哪些更改才能使结果符合预期？\n",
    "\n",
    "**交付物：** 各种桶大小下每次训练迭代的测量时间。对结果、你的预期以及任何不匹配的可能原因进行3-4句评论。\n",
    "\n",
    "**(b)** 假设计算一个桶的梯度所需的时间与通信该梯度桶所需的时间相同。写一个方程来建模DDP的通信开销（即反向传播后额外花费的时间），作为以下变量的函数：\n",
    "- 模型参数的总大小（字节）$s$\n",
    "- all-reduce算法带宽 $w$（计算为每个rank的数据大小除以完成all-reduce所需的时间）\n",
    "- 每次通信调用的开销（秒）$o$\n",
    "- 桶的数量 $n_b$\n",
    "\n",
    "从这个方程，写出最小化DDP开销的最优桶大小的方程。\n",
    "\n",
    "**交付物：** 建模DDP开销的方程，以及最优桶大小的方程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab59b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "\n",
    "'''\n",
    "实现了一个单节点多进程分布式通信性能基准测试工具，用于测量 PyTorch 中 all_reduce 操作的延迟和带宽性能。\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfa61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2266 ms\n",
      "  Bandwidth      : 4.6281 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0165 ms\n",
      "  Bandwidth      : 63.7398 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 2.0388 ms\n",
      "  Bandwidth      : 5.1430 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0149 ms\n",
      "  Bandwidth      : 704.8151 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 17.6973 ms\n",
      "  Bandwidth      : 5.9251 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0163 ms\n",
      "  Bandwidth      : 6448.7486 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 181.1478 ms\n",
      "  Bandwidth      : 5.7885 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0172 ms\n",
      "  Bandwidth      : 60914.7716 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49b94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_bucketed_benchmark_complete.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_bucketed_benchmark_complete.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.profiler\n",
    "from torch.multiprocessing import spawn, Manager\n",
    "import argparse\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "# ============================================================\n",
    "# 1. 无分桶DDP实现（用于对比）\n",
    "# ============================================================\n",
    "\n",
    "class DDPNoBucket(nn.Module):\n",
    "    \"\"\"\n",
    "    无分桶DDP：所有参数作为一个整体进行all_reduce\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.handles = []\n",
    "        \n",
    "        self._broadcast_parameters()\n",
    "        self._register_hook()\n",
    "    \n",
    "    def _broadcast_parameters(self):\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    def _register_hook(self):\n",
    "        self.params_with_grad = []\n",
    "        for param in self.module.parameters():\n",
    "            if param.requires_grad:\n",
    "                self.params_with_grad.append(param)\n",
    "                param.register_hook(lambda grad, p=param: self._hook_callback(p))\n",
    "    \n",
    "    def _hook_callback(self, param):\n",
    "        if all(p.grad is not None for p in self.params_with_grad):\n",
    "            flat_grads = []\n",
    "            for p in self.params_with_grad:\n",
    "                flat_grads.append(p.grad.view(-1))\n",
    "            \n",
    "            if flat_grads:\n",
    "                buffer = torch.cat(flat_grads)\n",
    "                handle = dist.all_reduce(buffer, async_op=True)\n",
    "                self.handles.append((handle, buffer, self.params_with_grad))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.handles.clear()\n",
    "        for p in self.params_with_grad:\n",
    "            p.grad = None\n",
    "        return self.module(x)\n",
    "    \n",
    "    def finish_gradient_synchronization(self):\n",
    "        for handle, buffer, params in self.handles:\n",
    "            handle.wait()\n",
    "            buffer.div_(self.world_size)\n",
    "            \n",
    "            offset = 0\n",
    "            for p in params:\n",
    "                numel = p.numel()\n",
    "                p.grad.view(-1).copy_(buffer[offset:offset+numel])\n",
    "                offset += numel\n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 分桶DDP实现（优化版）\n",
    "# ============================================================\n",
    "\n",
    "class DDPBucketed(nn.Module):\n",
    "    \"\"\"\n",
    "    分桶DDP：按指定桶大小分组梯度，实现计算通信重叠\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Module, bucket_size_mb: float):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.handles = []\n",
    "        self.buckets = []\n",
    "        \n",
    "        self._broadcast_parameters()\n",
    "        self._create_buckets()\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _broadcast_parameters(self):\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    def _create_buckets(self):\n",
    "        \"\"\"按桶大小创建梯度桶\"\"\"\n",
    "        current_bucket = []\n",
    "        current_bucket_size = 0\n",
    "        \n",
    "        for p in reversed(list(self.module.parameters())):\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            \n",
    "            p_size = p.numel() * p.element_size()\n",
    "            \n",
    "            if current_bucket and (current_bucket_size + p_size > self.bucket_size_bytes):\n",
    "                self._finalize_bucket(current_bucket)\n",
    "                current_bucket = []\n",
    "                current_bucket_size = 0\n",
    "            \n",
    "            current_bucket.append(p)\n",
    "            current_bucket_size += p_size\n",
    "        \n",
    "        if current_bucket:\n",
    "            self._finalize_bucket(current_bucket)\n",
    "    \n",
    "    def _finalize_bucket(self, params: List[torch.nn.Parameter]):\n",
    "        \"\"\"为桶创建缓冲区\"\"\"\n",
    "        if not params:\n",
    "            return\n",
    "        \n",
    "        buffer_size = sum(p.numel() for p in params)\n",
    "        buffer = torch.zeros(\n",
    "            buffer_size, \n",
    "            device=params[0].device, \n",
    "            dtype=params[0].dtype\n",
    "        )\n",
    "        \n",
    "        self.buckets.append({\n",
    "            \"params\": params,\n",
    "            \"buffer\": buffer,\n",
    "            \"ready_count\": 0,\n",
    "            \"triggered\": False,\n",
    "            \"total_params\": len(params)\n",
    "        })\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"为每个参数注册梯度钩子\"\"\"\n",
    "        for bucket_idx, bucket in enumerate(self.buckets):\n",
    "            for param in bucket[\"params\"]:\n",
    "                param.register_hook(\n",
    "                    lambda grad, b_idx=bucket_idx: self._on_gradient_ready(b_idx)\n",
    "                )\n",
    "    \n",
    "    def _on_gradient_ready(self, bucket_idx: int):\n",
    "        \"\"\"梯度就绪回调\"\"\"\n",
    "        bucket = self.buckets[bucket_idx]\n",
    "        bucket[\"ready_count\"] += 1\n",
    "        \n",
    "        if (bucket[\"ready_count\"] == bucket[\"total_params\"] and \n",
    "            not bucket[\"triggered\"]):\n",
    "            \n",
    "            bucket[\"triggered\"] = True\n",
    "            \n",
    "            def launch_all_reduce():\n",
    "                offset = 0\n",
    "                for p in bucket[\"params\"]:\n",
    "                    numel = p.numel()\n",
    "                    if p.grad is not None:\n",
    "                        bucket[\"buffer\"][offset:offset+numel].copy_(p.grad.view(-1))\n",
    "                    else:\n",
    "                        bucket[\"buffer\"][offset:offset+numel].zero_()\n",
    "                    offset += numel\n",
    "                \n",
    "                handle = dist.all_reduce(bucket[\"buffer\"], async_op=True)\n",
    "                self.handles.append((handle, bucket_idx))\n",
    "            \n",
    "            torch.autograd.Variable._execution_engine.queue_callback(launch_all_reduce)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播，重置状态\"\"\"\n",
    "        for bucket in self.buckets:\n",
    "            bucket[\"triggered\"] = False\n",
    "            bucket[\"ready_count\"] = 0\n",
    "        \n",
    "        self.handles.clear()\n",
    "        return self.module(x)\n",
    "    \n",
    "    def finish_gradient_synchronization(self):\n",
    "        \"\"\"等待所有通信完成并写回梯度\"\"\"\n",
    "        for handle, bucket_idx in self.handles:\n",
    "            handle.wait()\n",
    "            \n",
    "            bucket = self.buckets[bucket_idx]\n",
    "            bucket[\"buffer\"].div_(self.world_size)\n",
    "            \n",
    "            offset = 0\n",
    "            for p in bucket[\"params\"]:\n",
    "                numel = p.numel()\n",
    "                if p.grad is not None:\n",
    "                    p.grad.view(-1).copy_(bucket[\"buffer\"][offset:offset+numel])\n",
    "                offset += numel\n",
    "        \n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. 0.5B模型定义，可以自行修改参数配置，这里是让笔记本也能跑起来\n",
    "# ============================================================\n",
    "\n",
    "class XLModel(nn.Module):\n",
    "    \"\"\"\n",
    "    模拟XL大小的模型\n",
    "    总参数量约 0.5B（5亿参数）\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int = 1024, num_layers: int = 24):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding层: 50000 * 1024 = 51.2M\n",
    "        self.embedding = nn.Embedding(50000, hidden_size)\n",
    "        \n",
    "        # Transformer层: 24层\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=16,\n",
    "                dim_feedforward=hidden_size * 4,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 输出层: 1024 * 50000 = 51.2M\n",
    "        self.output = nn.Linear(hidden_size, 50000)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. 基准测试主逻辑\n",
    "# ============================================================\n",
    "\n",
    "def run_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    bucket_size_mb: float,\n",
    "    use_bucket: bool,\n",
    "    num_iterations: int,\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    result_queue=None,\n",
    "    result_file=None\n",
    "):\n",
    "    \"\"\"\n",
    "    运行DDP基准测试\n",
    "    \"\"\"\n",
    "    \n",
    "    # 设置设备\n",
    "    torch.cuda.set_device(global_rank)\n",
    "    device = torch.device(f\"cuda:{global_rank}\")\n",
    "    \n",
    "    # 初始化分布式环境 - 指定device_id消除警告\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    \n",
    "    # 修复：使用device_id指定设备，避免NCCL猜测\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        device_id=device  # 关键修复：明确指定设备ID\n",
    "    )\n",
    "    \n",
    "    if global_rank == 0:\n",
    "        config_str = f\"{'Bucketed' if use_bucket else 'No Bucket'}\"\n",
    "        if use_bucket:\n",
    "            config_str += f\" (bucket={bucket_size_mb}MB)\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {config_str}\")\n",
    "        print(f\"World size: {world_size}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    model = XLModel().to(device)\n",
    "    \n",
    "    # 包装DDP\n",
    "    if use_bucket:\n",
    "        ddp_model = DDPBucketed(model, bucket_size_mb=bucket_size_mb)\n",
    "    else:\n",
    "        ddp_model = DDPNoBucket(model)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # 预热\n",
    "    if global_rank == 0:\n",
    "        print(\"Warming up...\")\n",
    "    \n",
    "    for _ in range(3):\n",
    "        dummy_input = torch.randint(0, 50000, (2, 512), device=device)\n",
    "        output = ddp_model(dummy_input)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # 修复：使用device参数明确指定barrier设备\n",
    "    dist.barrier(device_ids=[global_rank])\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 正式测试\n",
    "    if global_rank == 0:\n",
    "        print(f\"Running benchmark ({num_iterations} iterations)...\")\n",
    "    \n",
    "    iteration_times = []\n",
    "    \n",
    "    for iter_idx in range(num_iterations):\n",
    "        input_ids = torch.randint(0, 50000, (2, 512), device=device)\n",
    "        \n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start_event.record()\n",
    "        \n",
    "        output = ddp_model(input_ids)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed_ms = start_event.elapsed_time(end_event)\n",
    "        iteration_times.append(elapsed_ms)\n",
    "    \n",
    "    # 收集所有rank的结果\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"bucket_size_mb\": bucket_size_mb if use_bucket else float('inf'),\n",
    "        \"use_bucket\": use_bucket,\n",
    "        \"iteration_times_ms\": iteration_times,\n",
    "        \"avg_time_ms\": sum(iteration_times) / len(iteration_times),\n",
    "        \"min_time_ms\": min(iteration_times),\n",
    "        \"max_time_ms\": max(iteration_times),\n",
    "    }\n",
    "    \n",
    "    all_results = [None] * world_size\n",
    "    dist.all_gather_object(all_results, local_result)\n",
    "    \n",
    "    # PyTorch Profiler（仅rank 0，且仅分桶模式）\n",
    "    if global_rank == 0 and use_bucket and bucket_size_mb <= 100:\n",
    "        try:\n",
    "            os.makedirs(\"./profiler_log\", exist_ok=True)\n",
    "            with torch.profiler.profile(\n",
    "                activities=[\n",
    "                    torch.profiler.ProfilerActivity.CPU,\n",
    "                    torch.profiler.ProfilerActivity.CUDA,\n",
    "                ],\n",
    "                schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "                    f\"./profiler_log/bucket_{bucket_size_mb}MB\"\n",
    "                ),\n",
    "                record_shapes=False,\n",
    "                profile_memory=False,\n",
    "                with_stack=False\n",
    "            ) as prof:\n",
    "                \n",
    "                for _ in range(4):\n",
    "                    input_ids = torch.randint(0, 50000, (2, 512), device=device)\n",
    "                    output = ddp_model(input_ids)\n",
    "                    loss = output.mean()\n",
    "                    loss.backward()\n",
    "                    ddp_model.finish_gradient_synchronization()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    prof.step()\n",
    "        except Exception as e:\n",
    "            print(f\"Profiler warning: {e}\")\n",
    "    \n",
    "    # 只有rank 0返回结果\n",
    "    if global_rank == 0:\n",
    "        result_data = {\n",
    "            \"config\": {\n",
    "                \"use_bucket\": use_bucket,\n",
    "                \"bucket_size_mb\": bucket_size_mb if use_bucket else None,\n",
    "                \"world_size\": world_size,\n",
    "            },\n",
    "            \"ranks\": all_results,\n",
    "            \"global_avg_time_ms\": sum(r[\"avg_time_ms\"] for r in all_results) / world_size,\n",
    "        }\n",
    "        \n",
    "        # 通过queue返回\n",
    "        if result_queue is not None:\n",
    "            result_queue.put(result_data)\n",
    "        \n",
    "        # 同时写入文件（备用）\n",
    "        if result_file is not None:\n",
    "            with open(result_file, 'wb') as f:\n",
    "                pickle.dump(result_data, f)\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def benchmark_worker(rank, world_size, bucket_size, use_bucket, num_iter, \n",
    "                     master_addr, master_port, result_queue, result_file):\n",
    "    \"\"\"包装函数用于spawn\"\"\"\n",
    "    try:\n",
    "        run_benchmark(rank, world_size, bucket_size, use_bucket, num_iter,\n",
    "                      master_addr, master_port, result_queue, result_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in worker {rank}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_single_test(world_size, bucket_size, use_bucket, num_iterations,\n",
    "                   master_addr, master_port):\n",
    "    \"\"\"运行单次测试并返回结果\"\"\"\n",
    "    \n",
    "    with Manager() as manager:\n",
    "        result_queue = manager.Queue()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pkl') as tmp:\n",
    "            result_file = tmp.name\n",
    "        \n",
    "        try:\n",
    "            spawn(\n",
    "                benchmark_worker,\n",
    "                args=(world_size, bucket_size, use_bucket, num_iterations,\n",
    "                      master_addr, master_port, result_queue, result_file),\n",
    "                nprocs=world_size,\n",
    "                join=True\n",
    "            )\n",
    "            \n",
    "            if not result_queue.empty():\n",
    "                return result_queue.get()\n",
    "            else:\n",
    "                with open(result_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        finally:\n",
    "            if os.path.exists(result_file):\n",
    "                os.unlink(result_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"DDP Bucketed Benchmark\")\n",
    "    parser.add_argument(\"--world_size\", type=int, default=2, \n",
    "                       help=\"GPU数量\")\n",
    "    parser.add_argument(\"--num_iterations\", type=int, default=20,\n",
    "                       help=\"每次测试的迭代次数\")\n",
    "    parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "    parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    assert torch.cuda.is_available(), \"需要CUDA\"\n",
    "    assert args.world_size <= torch.cuda.device_count(), \"GPU数量不足\"\n",
    "    \n",
    "    results = []\n",
    "    bucket_sizes = [1, 10, 100, 1000]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DDP Bucketed Benchmark\")\n",
    "    print(f\"Configuration: 1 node, {args.world_size} GPUs, 0.5B model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. 先测试无分桶版本（baseline）\n",
    "    print(\"\\n[1/5] Testing No Bucket (Baseline)...\")\n",
    "    result = run_single_test(\n",
    "        args.world_size, 0, False, args.num_iterations,\n",
    "        args.master_addr, args.master_port\n",
    "    )\n",
    "    results.append(result)\n",
    "    \n",
    "    # 2. 测试各种桶大小\n",
    "    for i, bucket_size in enumerate(bucket_sizes, 2):\n",
    "        print(f\"\\n[{i}/5] Testing Bucket Size = {bucket_size} MB...\")\n",
    "        result = run_single_test(\n",
    "            args.world_size, bucket_size, True, args.num_iterations,\n",
    "            args.master_addr, args.master_port\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # 汇总结果\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    baseline = None\n",
    "    bucketed_results = []\n",
    "    \n",
    "    for r in results:\n",
    "        if not r[\"config\"][\"use_bucket\"]:\n",
    "            baseline = r\n",
    "        else:\n",
    "            bucketed_results.append(r)\n",
    "    \n",
    "    if baseline is None:\n",
    "        print(\"ERROR: Baseline result not found!\")\n",
    "        return\n",
    "    \n",
    "    baseline_time = baseline[\"global_avg_time_ms\"]\n",
    "    print(f\"\\nBaseline (No Bucket): {baseline_time:.2f} ms/iteration\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Config':<20} {'Time (ms)':<12} {'Overhead':<12} {'Speedup':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"{'No Bucket':<20} {baseline_time:<12.2f} {0.0:<12.2f} {1.0:<10.2f}x\")\n",
    "    \n",
    "    for r in sorted(bucketed_results, key=lambda x: x[\"config\"][\"bucket_size_mb\"]):\n",
    "        bucket_size = r[\"config\"][\"bucket_size_mb\"]\n",
    "        time_ms = r[\"global_avg_time_ms\"]\n",
    "        overhead = time_ms - baseline_time\n",
    "        speedup = baseline_time / time_ms if time_ms > 0 else float('inf')\n",
    "        \n",
    "        config_str = f\"Bucket={bucket_size}MB\"\n",
    "        print(f\"{config_str:<20} {time_ms:<12.2f} {overhead:<12.2f} {speedup:<10.2f}x\")\n",
    "    \n",
    "    # 保存详细结果\n",
    "    with open(\"benchmark_results.json\", \"w\") as f:\n",
    "        json_results = []\n",
    "        for r in results:\n",
    "            json_r = {\n",
    "                \"config\": r[\"config\"],\n",
    "                \"global_avg_time_ms\": float(r[\"global_avg_time_ms\"]),\n",
    "                \"ranks\": [\n",
    "                    {\n",
    "                        \"rank\": rank_r[\"rank\"],\n",
    "                        \"bucket_size_mb\": float(rank_r[\"bucket_size_mb\"]) if rank_r[\"bucket_size_mb\"] != float('inf') else \"inf\",\n",
    "                        \"use_bucket\": rank_r[\"use_bucket\"],\n",
    "                        \"avg_time_ms\": float(rank_r[\"avg_time_ms\"]),\n",
    "                        \"min_time_ms\": float(rank_r[\"min_time_ms\"]),\n",
    "                        \"max_time_ms\": float(rank_r[\"max_time_ms\"]),\n",
    "                    }\n",
    "                    for rank_r in r[\"ranks\"]\n",
    "                ]\n",
    "            }\n",
    "            json_results.append(json_r)\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n详细结果已保存到 benchmark_results.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2998b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DDP Bucketed Benchmark\n",
      "Configuration: 1 node, 1 GPUs, 0.5B model\n",
      "======================================================================\n",
      "\n",
      "[1/5] Testing No Bucket (Baseline)...\n",
      "\n",
      "============================================================\n",
      "Testing: No Bucket\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[2/5] Testing Bucket Size = 1 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=1MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[3/5] Testing Bucket Size = 10 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=10MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[4/5] Testing Bucket Size = 100 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=100MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "[5/5] Testing Bucket Size = 1000 MB...\n",
      "\n",
      "============================================================\n",
      "Testing: Bucketed (bucket=1000MB)\n",
      "World size: 1\n",
      "============================================================\n",
      "Warming up...\n",
      "Running benchmark (20 iterations)...\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Baseline (No Bucket): 8681.96 ms/iteration\n",
      "------------------------------------------------------------\n",
      "Config               Time (ms)    Overhead     Speedup   \n",
      "------------------------------------------------------------\n",
      "No Bucket            8681.96      0.00         1.00      x\n",
      "Bucket=1MB           11261.42     2579.46      0.77      x\n",
      "Bucket=10MB          9168.86      486.89       0.95      x\n",
      "Bucket=100MB         8094.60      -587.36      1.07      x\n",
      "Bucket=1000MB        6805.34      -1876.62     1.28      x\n",
      "\n",
      "详细结果已保存到 benchmark_results.json\n"
     ]
    }
   ],
   "source": [
    "!python ddp_bucketed_benchmark_complete.py --world_size 1 --num_iterations 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeedc51",
   "metadata": {},
   "source": [
    "# 作业五\n",
    "\n",
    "## 3 优化器状态分片\n",
    "\n",
    "分布式数据并行训练在概念上简单且通常非常高效，但要求每个进程持有模型参数和优化器状态的不同副本。这种冗余往往会导致显著的内存开销。例如，AdamW优化器为每个参数维护两个浮点数，这意味着其占用的内存是模型权重的两倍。Rajbhandari等人[2020]描述了几种方法，通过将（1）优化器状态、（2）梯度以及（3）参数在各进程间进行划分，并根据需要在不同工作节点之间通信，从而有效降低数据并行训练中的冗余问题。\n",
    "\n",
    "在本部分作业中，我们将通过实现优化器状态分片的简化版本，来降低每个Rank的内存消耗。具体而言，我们不会为所有参数都保存优化器状态，而是让每个Rank的优化器实例仅处理一部分参数（大约为1 / 世界规模）。当每个Rank的优化器执行一步更新时，它只会更新其分片中对应的那一部分模型参数。随后，各Rank会将其更新后的参数广播给其他Rank，以确保在每一步优化器更新后，模型参数始终保持同步。\n",
    "\n",
    "---\n",
    "\n",
    "### 问题（优化器状态分片）：15分\n",
    "\n",
    "实现一个Python类，用于处理优化器状态的分片。该类应封装任意输入的PyTorch `optim.Optimizer`，并在每次优化器步骤后负责同步更新的参数。我们建议采用以下公共接口：\n",
    "\n",
    "**`def __init__(self, params, optimizer_cls: Type[Optimizer], **kwargs: Any)`**：初始化分片状态优化器。`params` 是待优化的参数集合（或参数组，如果用户希望为模型的不同部分使用不同的超参数，例如学习率）；这些参数将在所有进程间进行分片。`optimizer_cls` 参数指定了要封装的优化器类型（例如，`optim.AdamW`）。最后，任何剩余的关键字参数将被转发到 `optimizer_cls` 的构造函数中。请务必在此方法中调用 `torch.optim.Optimizer` 超类的构造函数。\n",
    "\n",
    "**`def step(self, closure, **kwargs)`**：调用包装优化器的 `step()` 方法，并传入相应参数；提供了闭包和关键字参数。更新参数后，与其他进程同步。\n",
    "\n",
    "**`def add_param_group(self, param_group: dict[str, Any])`**：该方法应添加一个参数组传递给分片优化器。此方法在超类构造函数构建分片优化器时被调用，也可能在训练过程中被调用（例如，用于逐步解冻模型中的各层）。因此，该方法应负责将模型的参数分配到各个进程 ranks 中。\n",
    "\n",
    "---\n",
    "\n",
    "**交付物**：实现一个容器类，以支持优化器状态的分片处理。为测试你的分片优化器，请先实现适配器 `[adapters.get_sharded_optimizer]`。随后，运行 `uv run pytest tests/test_sharded_optimizer.py` 来执行测试。我们建议多次运行测试（例如5次），以确保测试能够稳定通过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8744ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sharded_optimizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sharded_optimizer.py\n",
    "from typing import Any, Type, Iterable, Dict, List\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class ShardedOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    Optimizer State Sharding (ZeRO-1 style, simplified)\n",
    "\n",
    "    Each rank owns a shard of parameters and maintains optimizer\n",
    "    states only for those parameters. After each step, updated\n",
    "    parameters are broadcast to all other ranks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        optimizer_cls: Type[Optimizer],\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        if not dist.is_initialized():\n",
    "            raise RuntimeError(\"torch.distributed must be initialized\")\n",
    "\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.optimizer_kwargs = kwargs\n",
    "\n",
    "        # super() initializes param_groups and calls add_param_group\n",
    "        super().__init__(params, defaults={})\n",
    "\n",
    "        # Build the local (sharded) optimizer\n",
    "        self._build_local_optimizer()\n",
    "\n",
    "    def _build_local_optimizer(self):\n",
    "        \"\"\"\n",
    "        Create the wrapped optimizer using only parameters\n",
    "        assigned to this rank.\n",
    "        \"\"\"\n",
    "        local_param_groups: List[Dict[str, Any]] = []\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            local_params = [\n",
    "                p for p in group[\"params\"]\n",
    "                if self._param_owner(p) == self.rank\n",
    "            ]\n",
    "\n",
    "            if len(local_params) == 0:\n",
    "                continue\n",
    "\n",
    "            local_group = dict(group)\n",
    "            local_group[\"params\"] = local_params\n",
    "            local_param_groups.append(local_group)\n",
    "\n",
    "        self.local_optimizer = self.optimizer_cls(\n",
    "            local_param_groups,\n",
    "            **self.optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "    def _param_owner(self, param: torch.nn.Parameter) -> int:\n",
    "        \"\"\"\n",
    "        Determine which rank owns this parameter.\n",
    "        \"\"\"\n",
    "        return self._global_param_index(param) % self.world_size\n",
    "\n",
    "    def _global_param_index(self, param: torch.nn.Parameter) -> int:\n",
    "        \"\"\"\n",
    "        Assign a stable global index to each parameter.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_param_to_index\"):\n",
    "            self._param_to_index = {}\n",
    "            idx = 0\n",
    "            for group in self.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    self._param_to_index[p] = idx\n",
    "                    idx += 1\n",
    "        return self._param_to_index[param]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Run optimizer step on local shard, then synchronize parameters.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        # Step only updates local shard\n",
    "        self.local_optimizer.step(**kwargs)\n",
    "\n",
    "        # Synchronize updated parameters\n",
    "        self._sync_parameters()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _sync_parameters(self):\n",
    "        \"\"\"\n",
    "        Broadcast updated parameters from owning rank to all others.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                owner = self._param_owner(p)\n",
    "                dist.broadcast(p.data, src=owner)\n",
    "\n",
    "    def add_param_group(self, param_group: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Add a parameter group and rebuild local optimizer.\n",
    "        \"\"\"\n",
    "        super().add_param_group(param_group)\n",
    "        self._build_local_optimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2356358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_sharded_optimizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sharded_optimizer.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from sharded_optimizer import ShardedOptimizer   # 假设你把之前代码存成这个文件\n",
    "\n",
    "\n",
    "def setup_distributed():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_distributed()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # 一个简单模型\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 10),\n",
    "    ).to(device)\n",
    "\n",
    "    # 确保模型初始化一致\n",
    "    for p in model.parameters():\n",
    "        dist.broadcast(p.data, src=0)\n",
    "\n",
    "    optimizer = ShardedOptimizer(\n",
    "        model.parameters(),\n",
    "        torch.optim.AdamW,\n",
    "        lr=1e-3\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for step in range(5):\n",
    "        x = torch.randn(8, 1024, device=device)\n",
    "        y = torch.randint(0, 10, (8,), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"step={step}, loss={loss.item():.4f}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f166dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=2.5581\n",
      "step=1, loss=2.4269\n",
      "step=2, loss=2.3533\n",
      "step=3, loss=2.2617\n",
      "step=4, loss=2.4645\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 test_sharded_optimizer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
