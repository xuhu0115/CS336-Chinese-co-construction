{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8639c8be",
   "metadata": {},
   "source": [
    "# 四、分布式数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd826a49",
   "metadata": {},
   "source": [
    "### 一 分布式数据并行训练（Distributed Data Parallel Training）\n",
    "\n",
    "在作业的这一部分中，我们将探索使用多张 GPU 训练语言模型的方法，重点关注**数据并行（data parallelism）**。\n",
    "我们将首先对 PyTorch 中的分布式通信进行一个入门介绍。接着，我们会学习一种朴素（naive）的分布式数据并行训练实现方式，并在此基础上实现和评测多种用于提升通信效率的改进方案。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 PyTorch 中的单机分布式通信（Single-Node Distributed Communication in PyTorch）\n",
    "\n",
    "我们先来看一个 PyTorch 中的简单分布式应用，其目标是：\n",
    "**生成四个随机整数张量，并计算它们的和。**\n",
    "\n",
    "在下面的分布式示例中，我们会启动 **4 个 worker 进程**，每个进程都会生成一个随机整数张量。\n",
    "为了对这些进程中的张量求和，我们将调用 **all-reduce** 这一集合通信（collective communication）操作。\n",
    "\n",
    "`all-reduce` 的作用是：\n",
    "\n",
    "> 将所有进程中的张量进行规约（这里是求和），并用规约后的结果 **原地替换** 每个进程中的原始张量。\n",
    "\n",
    "也就是说，在 all-reduce 完成后，每个进程中的张量都会变成相同的“全局求和结果”。\n",
    "\n",
    "下面我们来看代码。\n",
    "\n",
    "---\n",
    "\n",
    "在运行下面的脚本后，我们会得到如下输出。\n",
    "可以看到：在 all-reduce 之前，每个 worker 进程持有的张量都不同；而在 all-reduce 操作之后，由于所有进程中的张量被求和并同步，每个进程中的 `data` 都被**原地修改**为相同的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "如果你多次运行这个脚本，你会发现打印输出的顺序并不是确定的。这是因为该应用运行在分布式环境中，我们无法控制各个进程执行打印语句的精确顺序。\n",
    "**唯一可以保证的是**：在 all-reduce 操作完成之后，所有进程都会持有**逐位（bitwise）完全一致**的结果张量。\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba55c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distributed_demo.py\n",
    "# =========================\n",
    "# 标准库 & PyTorch 相关导入\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 分布式环境初始化函数\n",
    "# =========================\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"\n",
    "    初始化分布式通信环境（Process Group）\n",
    "\n",
    "    参数：\n",
    "    - rank: 当前进程的编号（0 ~ world_size-1）\n",
    "    - world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # 指定“主进程”的地址\n",
    "    # 所有进程都会通过这个地址进行 rendezvous（集合）\n",
    "    # 单机多进程时使用 localhost\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "\n",
    "    # 指定主进程监听的端口号\n",
    "    # 所有进程必须保持一致\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组（非常关键的一步）\n",
    "    # backend=\"gloo\"：使用 gloo 通信后端（CPU 通用，支持多平台）\n",
    "    # rank：当前进程在所有进程中的唯一编号\n",
    "    # world_size：参与通信的总进程数\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\",\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 每个进程实际执行的函数\n",
    "# =========================\n",
    "def distributed_demo(rank, world_size):\n",
    "    \"\"\"\n",
    "    每个进程都会执行这个函数一次\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化分布式通信环境\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # 每个进程各自生成一个本地张量\n",
    "    # torch.randint 是“本地操作”，不同 rank 得到的值不同\n",
    "    # 这里生成一个 shape 为 (3,) 的一维张量\n",
    "    data = torch.randint(0, 10, (3,))\n",
    "\n",
    "    # 打印 all-reduce 之前的数据\n",
    "    # 注意：由于是多进程并行执行，打印顺序不确定\n",
    "    print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "\n",
    "    # 核心操作：all-reduce\n",
    "    # - 对所有 rank 上的 data 做规约（默认是 SUM）\n",
    "    # - 结果会“原地”写回到每个 rank 的 data 中\n",
    "    # - async_op=False 表示同步执行（阻塞，直到完成）\n",
    "    dist.all_reduce(data, async_op=False)\n",
    "\n",
    "    # 打印 all-reduce 之后的数据\n",
    "    # 此时每个 rank 的 data 都是完全相同的\n",
    "    print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 程序主入口\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # world_size 表示总进程数\n",
    "    # 在 DDP 中通常等于 GPU 数量\n",
    "    world_size = 4\n",
    "\n",
    "    # 使用 torch.multiprocessing.spawn 启动多进程\n",
    "    mp.spawn(\n",
    "        fn=distributed_demo,   # 每个进程要执行的函数\n",
    "        args=(world_size,),    # 传给函数的额外参数（rank 会自动作为第一个参数）\n",
    "        nprocs=world_size,     # 启动的进程数量\n",
    "        join=True              # 主进程是否等待所有子进程结束\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee989a51",
   "metadata": {},
   "source": [
    "由于分布式环境不支持ipynb的notebook格式，只能先写入.py文件再在notebook中运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11497694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank [Gloo] Rank [Gloo] Rank [Gloo] Rank 021 is connected to 3 is connected to  is connected to 3 is connected to 33 peer ranks. 3 peer ranks.  peer ranks. Expected number of connected peer ranks is :  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 3Expected number of connected peer ranks is : 33\n",
      "\n",
      "3\n",
      "\n",
      "rank 2 data (before all-reduce): tensor([1, 8, 8])rank 3 data (before all-reduce): tensor([2, 6, 8])rank 0 data (before all-reduce): tensor([0, 3, 4])\n",
      "\n",
      "\n",
      "rank 1 data (before all-reduce): tensor([1, 9, 3])\n",
      "rank 0 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 3 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "rank 2 data (after all-reduce): tensor([ 4, 26, 23])rank 1 data (after all-reduce): tensor([ 4, 26, 23])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_demo.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d60b11",
   "metadata": {},
   "source": [
    "## 这段代码在“分布式视角”下做了什么？\n",
    "\n",
    "> **它在一台机器上启动 4 个独立进程，每个进程生成一个随机张量，然后通过 `all_reduce` 把所有进程的张量求和，并让每个进程都拿到相同的结果。**\n",
    "\n",
    "这正是 **数据并行训练中“梯度同步”**的最小原型。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、整体执行流程（非常重要）\n",
    "\n",
    "当运行：\n",
    "\n",
    "```bash\n",
    "python distributed_hello_world.py\n",
    "```\n",
    "\n",
    "实际发生的是： **主进程启动**、 `mp.spawn` 启动 **4 个子进程**。\n",
    "\n",
    "每个子进程 被分配一个唯一的 `rank`（0～3），加入同一个 `process group`， 各自生成数据， 通过 `all_reduce` 进行通信。\n",
    "\n",
    "注意：**每个 rank 是一个独立的 Python 进程，不是线程**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、逐段精讲代码\n",
    "\n",
    "---\n",
    "\n",
    "### 1 导入模块\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "```\n",
    "\n",
    "作用拆解：\n",
    "\n",
    "| 模块                      | 作用             |\n",
    "| ----------------------- | -------------- |\n",
    "| `os`                    | 设置分布式通信所需的环境变量 |\n",
    "| `torch`                 | 张量与随机数         |\n",
    "| `torch.distributed`     | 分布式通信核心 API    |\n",
    "| `torch.multiprocessing` | 启动多个进程         |\n",
    "\n",
    "---\n",
    "\n",
    "### 2 setup：初始化分布式通信\n",
    "\n",
    "```python\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "```\n",
    "\n",
    "这是 **分布式程序的“入群”步骤**。\n",
    "\n",
    "#### （1）`MASTER_ADDR` & `MASTER_PORT`\n",
    "\n",
    "```text\n",
    "谁是“总协调者”（Rendezvous Server）\n",
    "```\n",
    "\n",
    "* 所有进程都会连到这个地址\n",
    "* 单机多进程 → 用 `localhost`\n",
    "* 多机 → 用主节点 IP\n",
    "\n",
    "---\n",
    "\n",
    "#### （2）`init_process_group`\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend=\"gloo\",\n",
    "    rank=rank,\n",
    "    world_size=world_size\n",
    ")\n",
    "```\n",
    "\n",
    "这是最关键的一行。\n",
    "\n",
    "参数解释：\n",
    "\n",
    "| 参数               | 含义             |\n",
    "| ---------------- | -------------- |\n",
    "| `backend=\"gloo\"` | 通信后端（CPU / 通用） |\n",
    "| `rank`           | 当前进程的唯一编号      |\n",
    "| `world_size`     | 进程总数           |\n",
    "\n",
    " **核心概念**：\n",
    "\n",
    "* `rank`：我是谁\n",
    "* `world_size`：我们一共有多少人\n",
    "\n",
    "在 DDP 里：\n",
    "\n",
    "* **1 个 GPU ≈ 1 个 rank**\n",
    "\n",
    "---\n",
    "\n",
    "### 3  distributed_demo：每个进程执行的逻辑\n",
    "\n",
    "```python\n",
    "def distributed_demo(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "```\n",
    "\n",
    "> 每个子进程都会执行这个函数一次\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1 生成本地数据\n",
    "\n",
    "```python\n",
    "data = torch.randint(0, 10, (3,))\n",
    "```\n",
    "\n",
    "* 每个 rank 生成 **不同的随机张量**\n",
    "* 例如：\n",
    "\n",
    "  * rank 0 → `[4, 4, 7]`\n",
    "  * rank 1 → `[9, 5, 3]`\n",
    "  * rank 2 → `[6, 0, 7]`\n",
    "  * rank 3 → `[3, 7, 8]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2 all-reduce 前打印\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (before all-reduce): {data}\")\n",
    "```\n",
    "\n",
    " 打印顺序**不保证**，因为 4 个进程是并行的。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3 核心：`all_reduce`\n",
    "\n",
    "```python\n",
    "dist.all_reduce(data, async_op=False)\n",
    "```\n",
    "\n",
    "这是整段代码的**灵魂**。\n",
    "\n",
    "### all-reduce 做了什么？\n",
    "\n",
    "对所有 rank 上的 `data`：\n",
    "\n",
    "1. **收集**\n",
    "2. **求和（默认是 SUM）**\n",
    "3. **把结果写回每个 rank 的 `data`（原地）**\n",
    "\n",
    "数学上等价于：\n",
    "\n",
    "```python\n",
    "data = data_rank0 + data_rank1 + data_rank2 + data_rank3\n",
    "```\n",
    "\n",
    "关键点：\n",
    "\n",
    "* **in-place 操作**\n",
    "* 所有 rank 最终结果 **完全一致**\n",
    "* 默认是 `SUM`（梯度同步就是用这个）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4 all-reduce 后打印\n",
    "\n",
    "```python\n",
    "print(f\"rank {rank} data (after all-reduce): {data}\")\n",
    "```\n",
    "\n",
    "你会看到所有 rank 都变成：\n",
    "\n",
    "```text\n",
    "tensor([22, 16, 25])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4 主入口：spawn 多进程\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "```\n",
    "\n",
    "* 定义进程数\n",
    "* 在 DDP 中通常 = GPU 数量\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "mp.spawn(\n",
    "    fn=distributed_demo,\n",
    "    args=(world_size,),\n",
    "    nprocs=world_size,\n",
    "    join=True\n",
    ")\n",
    "```\n",
    "\n",
    "解释：\n",
    "\n",
    "| 参数          | 含义           |\n",
    "| ----------- | ------------ |\n",
    "| `fn`        | 每个进程执行的函数    |\n",
    "| `args`      | 传给函数的参数      |\n",
    "| `nprocs`    | 启动多少个进程      |\n",
    "| `join=True` | 主进程等待所有子进程结束 |\n",
    "\n",
    "`spawn` 会自动给每个进程分配 `rank = 0, 1, 2, 3`，把 `rank` 作为 **第一个参数** 传入 `fn`。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、这段代码 ↔ 真正的 DDP 训练\n",
    "\n",
    "| 本例                 | DDP 中            |\n",
    "| ------------------ | ---------------- |\n",
    "| `data`             | 梯度（`param.grad`） |\n",
    "| `torch.randint`    | 前向 + 反向计算        |\n",
    "| `all_reduce(data)` | 梯度同步             |\n",
    "| 单次操作               | 每个 iteration 都做  |\n",
    "\n",
    "本质上：\n",
    "\n",
    "> **DDP = 前向各算各的，反向之后用 all-reduce 同步梯度**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc9cf6",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1.1 分布式应用基准测试的最佳实践\n",
    "\n",
    "（Best Practices for Benchmarking Distributed Applications）\n",
    "\n",
    "在本作业的这一部分中，你将对**分布式应用进行基准测试**，以更好地理解**通信带来的开销**。以下是一些推荐的最佳实践：\n",
    "\n",
    "* **尽可能在同一台机器上运行基准测试**，以便进行可控、公平的对比。\n",
    "\n",
    "* **在正式计时之前进行多次 warm-up（预热）**。\n",
    "  这一点对 **NCCL 通信调用**尤为重要。通常 **5 次预热迭代** 就足够了。\n",
    "\n",
    "* **在 GPU 上进行基准测试时，务必调用 `torch.cuda.synchronize()`**，以等待 CUDA 操作真正完成。\n",
    "  注意：即使在通信操作中设置了 `async_op=False`，这一步仍然是必须的，因为该调用只保证操作被**加入 GPU 队列**，而不是通信实际完成。³\n",
    "\n",
    "* **不同 rank 的测量时间可能略有差异**，因此通常会在所有 rank 之间**聚合测量结果**以提高估计的稳定性。\n",
    "  可以使用 **all-gather 集合通信操作**（特别是 `dist.all_gather_object`）来收集所有 rank 的结果。\n",
    "\n",
    "* **一般建议先在 CPU 上使用 Gloo 后端进行本地调试**，然后在实际问题中根据需要使用 GPU + NCCL 进行基准测试。\n",
    "  在两种后端之间切换通常只需要修改：\n",
    "\n",
    "  * `init_process_group` 中的 backend 参数\n",
    "  * 张量的设备（CPU / GPU）转换\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01bea2",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "## 问题（distributed_communication_single_node）：5 分\n",
    "\n",
    "请编写一个脚本，用于在**单节点多进程（single-node multi-process）**设置下，**基准测试 all-reduce 操作的运行时间**。\n",
    "\n",
    "你可以参考前面给出的示例代码作为起点，并尝试改变以下配置参数：\n",
    "\n",
    "### 实验设置要求\n",
    "\n",
    "* **后端 + 设备类型**\n",
    "\n",
    "  * Gloo + CPU\n",
    "  * NCCL + GPU\n",
    "\n",
    "* **all-reduce 数据规模**\n",
    "\n",
    "  * 数据类型：`float32`\n",
    "  * 张量大小：\n",
    "\n",
    "    * 1MB\n",
    "    * 10MB\n",
    "    * 100MB\n",
    "    * 1GB\n",
    "\n",
    "* **进程数量**\n",
    "\n",
    "  * 2 个进程\n",
    "  * 4 个进程\n",
    "  * 6 个进程\n",
    "\n",
    "### 资源限制\n",
    "\n",
    "* 最多可使用 **6 张 GPU**\n",
    "* 每一次基准测试运行时间 **不得超过 5 分钟**\n",
    "\n",
    "---\n",
    "\n",
    "## 交付内容（Deliverable）\n",
    "\n",
    "* 提供 **图表（plot）和/或表格（table）**，用于比较不同实验设置下的性能表现\n",
    "* 并附上 **2–3 句话的分析说明**，总结你的实验结果，以及你对各因素（后端、数据规模、进程数等）如何相互影响的理解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6259ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:# 有多少个GPU就增加测试用例，当然一个也能跑\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bec746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2600 ms\n",
      "  Bandwidth      : 4.0336 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0162 ms\n",
      "  Bandwidth      : 64.5348 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 1.9385 ms\n",
      "  Bandwidth      : 5.4092 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0178 ms\n",
      "  Bandwidth      : 589.5505 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 18.0035 ms\n",
      "  Bandwidth      : 5.8243 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0177 ms\n",
      "  Bandwidth      : 5927.2864 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 169.5600 ms\n",
      "  Bandwidth      : 6.1841 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0180 ms\n",
      "  Bandwidth      : 58252.2717 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32464a",
   "metadata": {},
   "source": [
    "## 一、Gloo和NCCL的基本概念\n",
    "\n",
    "在 PyTorch 中，多进程/多 GPU 之间进行通信（比如 `all_reduce`, `broadcast`, `all_gather`）需要 **通信后端（backend）**。\n",
    "常用的两种是：\n",
    "\n",
    "| 名称       | 适用设备            | 特点                                                 |\n",
    "| -------- | --------------- | -------------------------------------------------- |\n",
    "| **Gloo** | CPU / GPU（有限支持） | 跨平台，支持 CPU，多节点可以用 TCP/IP 通信，适合 CPU 张量，简单可靠         |\n",
    "| **NCCL** | GPU（NVIDIA GPU） | NVIDIA 官方库，专门优化 GPU 间通信，支持高速 PCIe、NVLink、网络通信，性能最好 |\n",
    "\n",
    "---\n",
    "\n",
    "## 二、Gloo\n",
    "\n",
    "Gloo 在 CPU 和 GPU 都可以用，但 GPU 支持有限，通常用在 CPU 张量上，它支持跨节点网络通信（TCP/IP）。并且使用方便，启动和调试比 NCCL 容易。在 CPU 环境下是**默认且可靠的选择**。\n",
    "\n",
    "它通常使用在单机 CPU 多进程训练，或者小规模实验和调试，没有 GPU 的服务器环境。\n",
    "\n",
    "在CPU 通信效率中等，GPU 上比 NCCL 慢，不适合大规模深度学习训练。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、NCCL（NVIDIA Collective Communication Library）\n",
    "\n",
    "\n",
    "它是专门为 NVIDIA GPU 设计的，支持多 GPU、多节点高速通信。并且优化了**PCIe 直连**，**NVLink（GPU 内部高速总线）**，**网络互联（Ethernet / InfiniBand）**，PyTorch 会自动使用 NCCL 的 ring / tree 通信算法做 `all_reduce`、`all_gather` 等操作。\n",
    "\n",
    "通常在单机多 GPU 训练（如 2~8 GPU）上使用。分布式训练深度学习大模型，高性能 GPU 集群使用较多。\n",
    "\n",
    "能将GPU 间带宽最大化，并且延迟极低（特别是大张量）， NCCL + CUDA 几乎是训练大模型的默认选择。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、总结对比\n",
    "\n",
    "| 特性           | Gloo            | NCCL               |\n",
    "| ------------ | --------------- | ------------------ |\n",
    "| 设备           | CPU（主要），GPU（有限） | GPU（NVIDIA 专用）     |\n",
    "| 性能           | 中等              | 很高                 |\n",
    "| 跨节点支持        | TCP/IP          | NVLink / PCIe / 网络 |\n",
    "| 用途           | CPU 分布式 / 小实验   | GPU 多卡训练 / 大模型     |\n",
    "| PyTorch 默认推荐 | CPU → Gloo      | GPU → NCCL         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b00027",
   "metadata": {},
   "source": [
    "\n",
    "## 一、代码概览\n",
    "\n",
    "这段代码的目的只有一个：\n",
    "\n",
    "**在分布式环境下 benchmark `all_reduce` 的通信性能**\n",
    "测量指标包括：单次 `all_reduce` 的平均耗时，实际通信带宽（GB/s）适用场景在多进程（多卡 / 多节点），对比不同 backend（`gloo` / `nccl`）， 对比 CPU vs GPU 通信性能。\n",
    "\n",
    "## 二、模块介绍\n",
    "\n",
    "### 1. `torch.distributed as dist`\n",
    "\n",
    "它是PyTorch 的**分布式通信核心模块**，它提供进程组管理、collective ops（`all_reduce`, `broadcast`, `all_gather` 等）。\n",
    "\n",
    "### 2. `torch.multiprocessing.spawn`\n",
    "\n",
    "它是**多进程启动工具**，每个进程对应一个 `rank`，通常 rank + GPU id。当然分布式训练不等于多线程，**每个 rank 是一个独立 Python 进程**。\n",
    "\n",
    "\n",
    "## 三、初始化分布式环境\n",
    "\n",
    "```python\n",
    "def setup(master_addr, master_port, rank, world_size, backend):\n",
    "```\n",
    "\n",
    "### 1. MASTER_ADDR / MASTER_PORT\n",
    "\n",
    "```python\n",
    "os.environ['MASTER_ADDR'] = master_addr\n",
    "os.environ['MASTER_PORT'] = str(master_port)\n",
    "```\n",
    "\n",
    "定义**主节点（rank 0）的地址**，所有进程都通过这个地址进行 rendezvous，即使是**单机多卡**，也必须设置。\n",
    "\n",
    "### 2. 初始化进程组\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend,\n",
    "    rank=rank,\n",
    "    world_size=world_size\n",
    ")\n",
    "```\n",
    "\n",
    "| 参数         | 含义        |\n",
    "| ---------- | --------- |\n",
    "| backend    | 通信后端      |\n",
    "| rank       | 当前进程的全局编号 |\n",
    "| world_size | 总进程数      |\n",
    "\n",
    "#### 常见 backend 对比\n",
    "\n",
    "| backend | 适用场景            |\n",
    "| ------- | --------------- |\n",
    "| `gloo`  | CPU / 小规模       |\n",
    "| `nccl`  | GPU / 高性能（事实标准） |\n",
    "| `mpi`   | HPC 环境          |\n",
    "\n",
    "---\n",
    "\n",
    "## 四、`cleanup()`：资源回收\n",
    "\n",
    "```python\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```\n",
    "\n",
    "它显式释放通信资源，防止程序异常退出导致死锁\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "清空 CUDA cache（**不是释放显存，只是释放 PyTorch cache**），benchmark 中避免显存碎片影响。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、`benchmark_all_reduce()`\n",
    "\n",
    "\n",
    "### Step 1. 设置环境 & GPU 绑定\n",
    "\n",
    "```python\n",
    "setup(...)\n",
    "```\n",
    "\n",
    "#### GPU 场景下：\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(rank)\n",
    "```\n",
    "\n",
    "**为什么是 `rank`？**\n",
    "\n",
    "通常约定：\n",
    "\n",
    "```\n",
    "rank 0 → GPU 0\n",
    "rank 1 → GPU 1\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "### Step 2. 构造测试 Tensor\n",
    "\n",
    "```python\n",
    "tensor_size_bytes = tensor_size_mb * 1024 * 1024\n",
    "num_elements = tensor_size_bytes // 4\n",
    "tensor_data = torch.randn(num_elements, device=device)\n",
    "```\n",
    "\n",
    "`float32 = 4 bytes`，`randn` 只是为了填充数据，**值本身无关紧要**\n",
    "\n",
    "\n",
    "### Step 3 Warm-up\n",
    "\n",
    "```python\n",
    "for _ in range(5):\n",
    "    dist.all_reduce(tensor_data, op=dist.ReduceOp.SUM)\n",
    "```\n",
    "\n",
    "#### 为什么一定要 warm-up？\n",
    "\n",
    "1. CUDA kernel lazy initialization\n",
    "2. NCCL communicator 建立\n",
    "3. GPU 时钟频率爬升\n",
    "4. cache / pipeline 填充\n",
    "\n",
    "\n",
    "\n",
    "GPU 场景下：\n",
    "\n",
    "```python\n",
    "torch.cuda.synchronize()\n",
    "```\n",
    "\n",
    "CUDA 是**异步执行**不同步，测到的是 launch 时间。\n",
    "\n",
    "---\n",
    "\n",
    "### Barrier：所有进程对齐\n",
    "\n",
    "```python\n",
    "dist.barrier()\n",
    "```\n",
    "\n",
    "确保所有 rank **同时开始计时**，防止快的进程提前进入 benchmark。\n",
    "\n",
    "\n",
    "### Step 4. 正式 benchmark\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "```\n",
    "\n",
    "```python\n",
    "for _ in range(num_iterations):\n",
    "    dist.all_reduce(tensor_data, op=dist.ReduceOp.SUM)\n",
    "```\n",
    "\n",
    "`num_iterations = 20`，用来取平均，降低噪声。\n",
    "\n",
    "GPU 同步：\n",
    "\n",
    "```python\n",
    "torch.cuda.synchronize()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5. 计算耗时与带宽\n",
    "\n",
    "#### 平均耗时\n",
    "\n",
    "```python\n",
    "avg_time = duration / num_iterations\n",
    "```\n",
    "\n",
    "#### 带宽计算\n",
    "\n",
    "```python\n",
    "bandwidth_gbps = (tensor_size_bytes / avg_time) / 1e9\n",
    "```\n",
    "\n",
    "含义：\n",
    "\n",
    "```\n",
    "GB/s = 数据量 / 时间\n",
    "```\n",
    "\n",
    "当然这是 **简化模型**，没区分 ring / tree，适合对比，不是绝对值。\n",
    "\n",
    "\n",
    "### Step 6. 只让 rank 0 打印\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    print(...)\n",
    "```\n",
    "\n",
    "避免：N 个进程打印 N 份日志\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7. `all_gather_object` 汇总结果\n",
    "\n",
    "```python\n",
    "dist.all_gather_object(gathered_results, local_result)\n",
    "```\n",
    "\n",
    "它支持 **Python 对象**，比 `all_gather(tensor)` 更灵活，常用于 benchmark / profiling，这样 rank 0 能拿到所有 rank 的结果。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8. 清理 & 返回结果\n",
    "\n",
    "```python\n",
    "cleanup()\n",
    "```\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    return gathered_results\n",
    "```\n",
    "\n",
    "只有主进程返回， 子进程自动退出。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d57918",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "## 问题（naive_ddp）：5 分\n",
    "\n",
    "**交付物**：编写一个脚本来通过在反向传播后对各个参数梯度进行全规约，以简单方式执行分布式数据并行训练。为了验证你的DDP实现的正确性，使用它在随机生成的数据上训练一个小玩具模型，并验证其权重与单进程训练的结果相匹配。\n",
    "\n",
    "---\n",
    "\n",
    "## 问题（naive_ddp_benchmarking）：3 分\n",
    "\n",
    "在这种简单的DDP实现中，参数在每个反向传播后在各个rank之间单独进行全规约。为了更好地理解数据并行训练的开销，创建一个脚本来对之前实现的语言模型进行基准测试，该模型使用这种简单的DDP实现进行训练。测量每个训练步骤的总时间以及用于通信梯度的时间比例。收集在单节点设置（1个节点 x 2个GPU）下对XL模型的测量结果，如§1.1.2中所述。\n",
    "\n",
    "**交付物**：描述你的基准测试设置，以及每个训练迭代的测量时间以及用于通信梯度的时间。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a310004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_model_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_model_demo.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 一、定义一个简单的全连接神经网络\n",
    "# ============================\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    一个最简单的三层全连接网络，用于 MNIST 分类\n",
    "    输入：28×28 展平后的 784 维向量\n",
    "    输出：10 类（0~9）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一层全连接：输入层 -> 隐藏层\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # 第二层全连接：隐藏层 -> 隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 输出层：隐藏层 -> 类别数\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播逻辑\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # 使用 log_softmax，方便后续配合 NLLLoss\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 二、分布式环境初始化与清理\n",
    "# ============================\n",
    "def init_distributed_env(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    初始化分布式进程组（所有进程都必须调用）\n",
    "    \"\"\"\n",
    "    # 主进程地址和端口\n",
    "    # 在单机多卡场景中，localhost 即可\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_env():\n",
    "    \"\"\"\n",
    "    销毁进程组并清理资源\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、DDP 训练主逻辑（教学重点）\n",
    "# ============================\n",
    "def ddp_training_worker(rank: int, world_size: int, backend: str):\n",
    "    \"\"\"\n",
    "    每一个进程都会执行这个函数\n",
    "    rank      : 当前进程编号\n",
    "    world_size: 总进程数\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. 固定随机种子（确保所有进程行为一致）\n",
    "    # ------------------------------------------------\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 确保 cudnn 的确定性行为（教学更容易对齐结果）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 初始化分布式环境\n",
    "    # ------------------------------------------------\n",
    "    init_distributed_env(rank, world_size, backend)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 为当前进程分配设备\n",
    "    # ------------------------------------------------\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(rank)\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "        print(f\"[进程 {rank}] 使用 GPU：{device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"[进程 {rank}] CUDA 不可用，使用 CPU\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 数据预处理与加载\n",
    "    # ------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        # 将 28×28 展平为 784\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.MNIST(\n",
    "        root=\"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. 手动划分数据（教学版 DDP）\n",
    "    #    每个进程只处理数据集的一部分\n",
    "    # ------------------------------------------------\n",
    "    total_samples = len(full_dataset)\n",
    "    samples_per_rank = total_samples // world_size\n",
    "\n",
    "    start_index = rank * samples_per_rank\n",
    "    end_index = start_index + samples_per_rank\n",
    "\n",
    "    subset_dataset = torch.utils.data.Subset(\n",
    "        full_dataset,\n",
    "        range(start_index, end_index)\n",
    "    )\n",
    "\n",
    "    # DataLoader 使用相同随机种子，确保 shuffle 行为可复现\n",
    "    data_generator = torch.Generator()\n",
    "    data_generator.manual_seed(seed)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        subset_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        generator=data_generator\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[进程 {rank}] 负责样本区间：\"\n",
    "        f\"{start_index} ~ {end_index - 1}，\"\n",
    "        f\"共 {samples_per_rank} 条数据\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 6. 创建模型与优化器\n",
    "    # ------------------------------------------------\n",
    "    model = SimpleNet(\n",
    "        input_dim=784,\n",
    "        hidden_dim=50,\n",
    "        num_classes=10\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 7. 步骤一：广播模型参数（关键教学点）\n",
    "    #    确保所有进程从“完全相同”的初始模型开始\n",
    "    # ------------------------------------------------\n",
    "    for param in model.parameters():\n",
    "        dist.broadcast(param.data, src=0)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 8. 正式开始训练\n",
    "    # ------------------------------------------------\n",
    "    for epoch in range(1, 3):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤二：前向 + 反向传播\n",
    "            # 每个进程只计算自己那一部分数据的梯度\n",
    "            # ----------------------------\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            step_start_time = time.time()\n",
    "            loss.backward()\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤三：梯度 All-Reduce\n",
    "            # 将所有进程的梯度相加，再取平均\n",
    "            # ----------------------------\n",
    "            comm_start_time = time.time()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    dist.all_reduce(\n",
    "                        param.grad.data,\n",
    "                        op=dist.ReduceOp.SUM\n",
    "                    )\n",
    "                    param.grad.data /= world_size\n",
    "\n",
    "            comm_time = time.time() - comm_start_time\n",
    "\n",
    "            # ----------------------------\n",
    "            # 步骤四：参数更新\n",
    "            # 所有进程使用“完全相同”的平均梯度\n",
    "            # ----------------------------\n",
    "            optimizer.step()\n",
    "\n",
    "            step_time = time.time() - step_start_time\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[进程 {rank}] \"\n",
    "                    f\"Epoch {epoch} | \"\n",
    "                    f\"Batch {batch_idx} | \"\n",
    "                    f\"Loss = {loss.item():.6f}\"\n",
    "                )\n",
    "\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    f\"[主进程] 单步耗时：{step_time * 1000:.2f} ms，\"\n",
    "                    f\"通信耗时：{comm_time * 1000:.2f} ms\"\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 9. 仅在 rank 0 保存模型\n",
    "    # ------------------------------------------------\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"[主进程] 模型已保存：mnist_simple_ddp.pt\")\n",
    "\n",
    "    destroy_distributed_env()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 四、程序入口\n",
    "# ============================\n",
    "def main():\n",
    "    # world_size = 使用的进程（GPU）数量\n",
    "    # 如果只有一张 GPU，就设为 1（等价于普通训练）\n",
    "    world_size = 1\n",
    "\n",
    "    # NCCL：GPU 通信首选后端\n",
    "    backend = \"nccl\"\n",
    "\n",
    "    # 检查数据是否能被平均分配\n",
    "    dataset_size = len(\n",
    "        datasets.MNIST(\"../data\", train=True, download=True)\n",
    "    )\n",
    "\n",
    "    if dataset_size % world_size != 0:\n",
    "        print(\n",
    "            f\"警告：数据量 {dataset_size} 不能被 world_size={world_size} 整除，\"\n",
    "            f\"将丢弃部分样本以保证均分\"\n",
    "        )\n",
    "\n",
    "    print(f\"启动分布式训练，进程数：{world_size}\")\n",
    "    print(f\"每个进程处理样本数：{dataset_size // world_size}\")\n",
    "\n",
    "    # 启动多进程\n",
    "    spawn(\n",
    "        ddp_training_worker,\n",
    "        args=(world_size, backend),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "\n",
    "    print(\"分布式训练结束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32970985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动分布式训练，进程数：1\n",
      "每个进程处理样本数：60000\n",
      "[进程 0] 使用 GPU：cuda:0\n",
      "[进程 0] 负责样本区间：0 ~ 59999，共 60000 条数据\n",
      "[进程 0] Epoch 1 | Batch 0 | Loss = 2.294834\n",
      "[主进程] 单步耗时：178.22 ms，通信耗时：14.50 ms\n",
      "[进程 0] Epoch 1 | Batch 50 | Loss = 0.515532\n",
      "[主进程] 单步耗时：1.15 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 100 | Loss = 0.286108\n",
      "[主进程] 单步耗时：1.21 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 150 | Loss = 0.235657\n",
      "[主进程] 单步耗时：1.63 ms，通信耗时：0.49 ms\n",
      "[进程 0] Epoch 1 | Batch 200 | Loss = 0.305218\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 1 | Batch 250 | Loss = 0.421952\n",
      "[主进程] 单步耗时：1.36 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 300 | Loss = 0.376283\n",
      "[主进程] 单步耗时：1.17 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 350 | Loss = 0.159866\n",
      "[主进程] 单步耗时：1.60 ms，通信耗时：0.39 ms\n",
      "[进程 0] Epoch 1 | Batch 400 | Loss = 0.211988\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.65 ms\n",
      "[进程 0] Epoch 1 | Batch 450 | Loss = 0.401293\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 500 | Loss = 0.201109\n",
      "[主进程] 单步耗时：1.12 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 1 | Batch 550 | Loss = 0.257683\n",
      "[主进程] 单步耗时：1.62 ms，通信耗时：0.59 ms\n",
      "[进程 0] Epoch 1 | Batch 600 | Loss = 0.241983\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.67 ms\n",
      "[进程 0] Epoch 1 | Batch 650 | Loss = 0.171643\n",
      "[主进程] 单步耗时：1.06 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 700 | Loss = 0.333229\n",
      "[主进程] 单步耗时：2.02 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 1 | Batch 750 | Loss = 0.091621\n",
      "[主进程] 单步耗时：1.23 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 800 | Loss = 0.125746\n",
      "[主进程] 单步耗时：1.91 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 1 | Batch 850 | Loss = 0.194321\n",
      "[主进程] 单步耗时：1.01 ms，通信耗时：0.27 ms\n",
      "[进程 0] Epoch 1 | Batch 900 | Loss = 0.435536\n",
      "[主进程] 单步耗时：1.33 ms，通信耗时：0.34 ms\n",
      "[进程 0] Epoch 2 | Batch 0 | Loss = 0.216730\n",
      "[主进程] 单步耗时：2.21 ms，通信耗时：0.42 ms\n",
      "[进程 0] Epoch 2 | Batch 50 | Loss = 0.168439\n",
      "[主进程] 单步耗时：1.09 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 100 | Loss = 0.067365\n",
      "[主进程] 单步耗时：1.25 ms，通信耗时：0.31 ms\n",
      "[进程 0] Epoch 2 | Batch 150 | Loss = 0.089863\n",
      "[主进程] 单步耗时：1.28 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 200 | Loss = 0.234549\n",
      "[主进程] 单步耗时：1.11 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 250 | Loss = 0.070795\n",
      "[主进程] 单步耗时：1.32 ms，通信耗时：0.41 ms\n",
      "[进程 0] Epoch 2 | Batch 300 | Loss = 0.088156\n",
      "[主进程] 单步耗时：1.68 ms，通信耗时：0.57 ms\n",
      "[进程 0] Epoch 2 | Batch 350 | Loss = 0.120999\n",
      "[主进程] 单步耗时：1.19 ms，通信耗时：0.32 ms\n",
      "[进程 0] Epoch 2 | Batch 400 | Loss = 0.121158\n",
      "[主进程] 单步耗时：1.45 ms，通信耗时：0.60 ms\n",
      "[进程 0] Epoch 2 | Batch 450 | Loss = 0.042282\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 500 | Loss = 0.265627\n",
      "[主进程] 单步耗时：1.38 ms，通信耗时：0.50 ms\n",
      "[进程 0] Epoch 2 | Batch 550 | Loss = 0.091650\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.62 ms\n",
      "[进程 0] Epoch 2 | Batch 600 | Loss = 0.142866\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.56 ms\n",
      "[进程 0] Epoch 2 | Batch 650 | Loss = 0.178375\n",
      "[主进程] 单步耗时：1.73 ms，通信耗时：0.68 ms\n",
      "[进程 0] Epoch 2 | Batch 700 | Loss = 0.181190\n",
      "[主进程] 单步耗时：1.10 ms，通信耗时：0.30 ms\n",
      "[进程 0] Epoch 2 | Batch 750 | Loss = 0.107817\n",
      "[主进程] 单步耗时：1.24 ms，通信耗时：0.36 ms\n",
      "[进程 0] Epoch 2 | Batch 800 | Loss = 0.056186\n",
      "[主进程] 单步耗时：1.46 ms，通信耗时：0.63 ms\n",
      "[进程 0] Epoch 2 | Batch 850 | Loss = 0.103768\n",
      "[主进程] 单步耗时：1.04 ms，通信耗时：0.29 ms\n",
      "[进程 0] Epoch 2 | Batch 900 | Loss = 0.167343\n",
      "[主进程] 单步耗时：1.67 ms，通信耗时：0.37 ms\n",
      "[主进程] 模型已保存：mnist_simple_ddp.pt\n",
      "分布式训练结束\n"
     ]
    }
   ],
   "source": [
    "!python ddp_model_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebc786",
   "metadata": {},
   "source": [
    "# 问题（minimal_ddp_flat_benchmarking）：2分\n",
    "\n",
    "修改您的最小DDP实现，以从所有参数中传递一个扁平化的梯度张量。将其性能与在先前使用条件下（1个节点 x 2个GPU，XL模型大小如§1.1.3中所述）为每个参数张量发出一个all-reduce的最小DDP实现进行比较。\n",
    "\n",
    "交付物：在单次批量all-reduce调用下，未分布式数据并行训练中每个训练迭代的测量时间以及梯度通信所花费的时间。1-2句话比较批量与单独通信梯度的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790a370",
   "metadata": {},
   "source": [
    "**验证：把“很多小的梯度通信”合并成“一次大的梯度通信”，能否显著降低 DDP 的通信开销**\n",
    "\n",
    "这在真实系统里是一个**非常核心的工程问题**。\n",
    "\n",
    "\n",
    "\n",
    "## naive DDP （对照基线）\n",
    "\n",
    "之前的最小 DDP 实现是这样的逻辑：\n",
    "\n",
    "```text\n",
    "for param in model.parameters():\n",
    "    all_reduce(param.grad)\n",
    "```\n",
    "\n",
    "也就是：**每个参数张量**，**单独一次 all-reduce**，通信调用次数 = 参数个数，XL 模型来说：参数张量数量几百～上千个，NCCL / GPU 通信，每一次 all-reduce 都有 **启动开销（latency）**，小 tensor → 极其低效，这就是作业的**优化的瓶颈**\n",
    "\n",
    "---\n",
    "\n",
    "## 改什么\n",
    "\n",
    "###  核心改动：**梯度展平 + 单次通信**\n",
    "\n",
    "把：\n",
    "\n",
    "```python\n",
    "grad_1, grad_2, grad_3, ..., grad_n\n",
    "```\n",
    "\n",
    "变成：\n",
    "\n",
    "```python\n",
    "flat_grad = concat([grad_1, grad_2, ..., grad_n])\n",
    "```\n",
    "\n",
    "然后只做：\n",
    "\n",
    "```python\n",
    "dist.all_reduce(flat_grad)\n",
    "```\n",
    "\n",
    "再把结果 **切回每个参数的 grad**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9739eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minimal_ddp_flat_benchmarking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minimal_ddp_flat_benchmarking.py\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.multiprocessing.spawn import spawn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # 定义第一个隐藏层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # 定义第二个隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # 定义输出层\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  \n",
    "# 示例：创建一个SimpleNet实例\n",
    "\n",
    "def setup(rank, world_size, backend):\n",
    "    \"\"\" 初始化分布式环境 \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost' # 设置主节点IP地址以及端口，其他节点需要通过这个地址连接到主节点\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    # 根据后端初始化进程组\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size) # 初始化进程组，rank是当前进程的rank，world_size是总进程数，backend: 这是指定通信后端的参数。常见的后端有：gloo(CPU), nccl(GPU), mpi等。\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\" 清理分布式环境 \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    # 清理GPU缓存\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def ddp_train(rank, world_size, backend):\n",
    "    \"\"\" Naive DDP训练函数，实现图片中描述的4个步骤 \"\"\"\n",
    "    # 设置随机种子确保可重现性（所有进程使用相同种子）\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 设置确定性行为\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # 初始化分布式环境\n",
    "    setup(rank, world_size, backend)\n",
    "    \n",
    "    # 为每个进程分配不同的GPU设备\n",
    "    # 使用GPU 0和1，它们当前是空闲的\n",
    "    gpu_id = rank  # rank 0 -> GPU 0, rank 1 -> GPU 1\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        print(f\"Rank {rank} using GPU: {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"CUDA not available, using CPU for rank {rank}\")\n",
    "    \n",
    "    # 数据加载和预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # 为了验证DDP正确性，我们让每个进程处理不同的数据子集\n",
    "    # 这样总的有效batch size = 64 * world_size\n",
    "    total_samples = len(dataset)\n",
    "    samples_per_process = total_samples // world_size\n",
    "    start_idx = rank * samples_per_process\n",
    "    end_idx = start_idx + samples_per_process\n",
    "    \n",
    "    # 创建当前进程的数据子集\n",
    "    subset = torch.utils.data.Subset(dataset, range(start_idx, end_idx))\n",
    "    \n",
    "    # 设置确定性的数据加载器\n",
    "    # 重要：为了与单进程训练比较，我们需要确保数据顺序一致\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)  # 所有进程使用相同的种子确保一致性\n",
    "    train_loader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=True, generator=generator)\n",
    "    \n",
    "    print(f\"Rank {rank} processing samples {start_idx} to {end_idx-1} ({samples_per_process} samples)\")\n",
    "    print(f\"Rank {rank} effective batch size: 64, total distributed batch size: {64 * world_size}\")\n",
    "    \n",
    "    # 创建模型和优化器（每个设备都创建相同的模型）\n",
    "    model = SimpleNet(input_size=784, hidden_size=50, num_classes=10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # ===== 新增：记录梯度展平所需的元信息 =====\n",
    "    grad_shapes = []\n",
    "    grad_numels = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "        grad_shapes.append(p.shape)\n",
    "        grad_numels.append(p.numel())\n",
    "\n",
    "    total_grad_numel = sum(grad_numels)\n",
    "\n",
    "    # 步骤1: Broadcast模型参数从rank 0到所有其他ranks\n",
    "    # 确保所有设备从相同的初始模型和优化器状态开始\n",
    "    for param in model.parameters():\n",
    "        # 确保参数在正确的设备上进行broadcast\n",
    "        dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # 同步优化器状态（如果有的话）\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param in optimizer.state:\n",
    "                for key, value in optimizer.state[param].items():\n",
    "                    if torch.is_tensor(value):\n",
    "                        dist.broadcast(value, src=0)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, 3):  # Train for 2 epochs\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # 步骤2: 每个设备使用本地模型参数进行前向传播和反向传播\n",
    "            # 计算n/d个样本的梯度\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            step_start = time.time()\n",
    "            loss.backward()\n",
    "            \n",
    "            comm_start = time.time()\n",
    "            # 步骤3: All-reduce梯度\n",
    "            # 将所有设备的梯度求平均，使每个设备都持有所有n个样本的平均梯度\n",
    "\n",
    "            # ===== 新的步骤3：Flattened gradient all-reduce =====\n",
    "            flat_grad = torch.zeros(total_grad_numel, device=device)\n",
    "\n",
    "            offset = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    numel = p.grad.numel()\n",
    "                    flat_grad[offset:offset + numel] = p.grad.view(-1)\n",
    "                    offset += numel\n",
    "\n",
    "            # 单次通信\n",
    "            dist.all_reduce(flat_grad, op=dist.ReduceOp.SUM)\n",
    "            flat_grad /= world_size\n",
    "\n",
    "            # 写回每个参数的梯度\n",
    "            offset = 0\n",
    "            for p, shape, numel in zip(model.parameters(), grad_shapes, grad_numels):\n",
    "                if p.grad is not None:\n",
    "                    p.grad.copy_(\n",
    "                        flat_grad[offset:offset + numel].view(shape)\n",
    "                    )\n",
    "                    offset += numel\n",
    "\n",
    "            comm_time = time.time() - comm_start\n",
    "            # 步骤4: 优化器步骤 - 每个设备使用相同的平均梯度更新参数\n",
    "            # 由于所有设备从相同初始状态开始并使用相同梯度，参数会保持同步\n",
    "            optimizer.step()\n",
    "            step_time = time.time() - step_start\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Rank {rank}, Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            if rank == 0 and batch_idx % 50 == 0:\n",
    "                print(f\"Step time: {step_time*1000:.2f} ms | Comm time: {comm_time*1000:.2f} ms\")\n",
    "\n",
    "    # 只在rank 0保存模型\n",
    "    if rank == 0:\n",
    "        torch.save(model.state_dict(), \"mnist_simple_ddp.pt\")\n",
    "        print(\"Model saved!\")\n",
    "            \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = 1 # gpu只有一个就改成1，但是这样和平时普通的训练就没有区别了\n",
    "    backend = 'nccl'  # 使用gloo后端，更适合CPU训练\n",
    "    # 只检查数据集大小是否能被world_size整除\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    data_size = len(dataset)\n",
    "\n",
    "    if data_size % world_size != 0:\n",
    "        print(f\"Warning: Data size {data_size} is not divisible by world size {world_size}\")\n",
    "        print(f\"Some samples will be ignored to ensure equal distribution\")\n",
    "    \n",
    "    print(f\"Starting distributed training with {world_size} processes\")\n",
    "    print(f\"Each process will handle {data_size // world_size} samples\")\n",
    "    print(f\"Total samples: {data_size}\")\n",
    "    \n",
    "    # 启动分布式训练\n",
    "    spawn(ddp_train, args=(world_size, backend), nprocs=world_size, join=True)\n",
    "    \n",
    "    print(\"Distributed training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5eb2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/cs336_systems/minimal_ddp_flat_benchmarking.py\", line 4, in <module>\n",
      "    from torchvision import datasets, transforms\n",
      "ModuleNotFoundError: No module named 'torchvision'\n"
     ]
    }
   ],
   "source": [
    "!python minimal_ddp_flat_benchmarking.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c2f58",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "## 问题 (ddp_overlap_individual_parameters): 5 分\n",
    "\n",
    "实现一个 Python 类来处理分布式数据并行训练。该类应包装任意 PyTorch `nn.Module` 并在训练前广播权重（以便所有等级具有相同的初始参数），并发出通信调用以进行梯度平均。我们建议以下公共接口：\n",
    "\n",
    "```python\n",
    "def __init__(self, module: torch.nn.Module): 给定一个实例化的 PyTorch nn.Module 要并行化，构建一个 DDP 容器，该容器将处理跨等级的梯度同步。\n",
    "\n",
    "def forward(self, *inputs, **kwargs): 调用包装模块的 forward() 方法，并使用提供的定位参数和关键字参数。\n",
    "\n",
    "def finish_gradient_synchronization(self): 当被调用时，等待异步通信\n",
    "```\n",
    "\n",
    "*在高级情况下，如果你使用多个 CUDA 流，你可能需要显式同步跨流以确保输出准备好进行后续操作。* 参见 [pytorch.org/docs/stable/notes/cuda.html#cuda-streams](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams)。\n",
    "\n",
    "### GPU 上的调用\n",
    "\n",
    "为了使用此类执行分布式训练，我们将一个模块传递给该模块进行包装，然后在我们运行 `optimizer.step()` 之前添加一个对 `finish_gradient_synchronization()` 的调用，以确保依赖于梯度的优化器步骤可以排队：\n",
    "\n",
    "```python\n",
    "model = ToyModel().to(device)\n",
    "ddp_model = DDP(model)\n",
    "\n",
    "for _ in range(train_steps):\n",
    "    x, y = get_batch()\n",
    "    logits = ddp_model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    ddp_model.finish_gradient_synchronization()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### 交付物：实现一个容器类来处理分布式数据并行训练。该类应重叠梯度通信和反向传播的计算。为了测试你的 DDP 类，首先实现适配器 [adapters.get_ddp_individual_parameters](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams) 和 [adapters.ddp_individual_parameters_on_after_backward](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams)（后者是可选的，取决于你的实现，你可能不需要它）。\n",
    "\n",
    "然后，执行测试，通过运行 `pytest tests/test_ddp_individual_parameters.py`。我们建议多次运行测试（例如，5 次）以确保其可靠通过。\n",
    "\n",
    "以下是翻译和整理为 Markdown 格式的内容：\n",
    "\n",
    "---\n",
    "\n",
    "## 问题 (ddp_overlap_individual_parameters): 5 分\n",
    "\n",
    "实现一个 Python 类来处理分布式数据并行训练。该类应包装一个任意的 PyTorch `nn.Module` 并在训练之前处理权重的广播（以便所有 ranks 具有相同的初始参数）以及发出通信调用以进行梯度平均。我们建议以下公共接口：\n",
    "\n",
    "```python\n",
    "def __init__(self, module: torch.nn.Module): 给定一个实例化的 PyTorch nn.Module 要并行化，构建一个 DDP 容器，该容器将处理跨 ranks 的梯度同步。\n",
    "\n",
    "def forward(self, *inputs, **kwargs): 调用被包装模块的 `forward()` 方法，并使用提供的定位参数和关键字参数。\n",
    "\n",
    "def finish_gradient_synchronization(self): 当调用时，等待异步通信\n",
    "```\n",
    "\n",
    "*在高级情况下，如果你使用多个 CUDA 流，你可能需要显式同步跨流以确保输出准备好进行后续操作。* 参见 [CUDA Streams](https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams)。\n",
    "\n",
    "### GPU 上的调用\n",
    "\n",
    "为了使用此类执行分布式训练，我们将一个模块传递给该模块进行包装，然后在我们运行 `optimizer.step()` 之前添加一个对 `finish_gradient_synchronization()` 的调用，以确保依赖于梯度的优化器步骤可以排队：\n",
    "\n",
    "```python\n",
    "model = ToyModel().to(device)\n",
    "ddp_model = DDP(model)\n",
    "\n",
    "for _ in range(train_steps):\n",
    "    x, y = get_batch()\n",
    "    logits = ddp_model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    ddp_model.finish_gradient_synchronization()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### 交付物：实现一个容器类来处理分布式数据并行训练。该类应重叠梯度通信和反向传播的计算。为了测试你的 DDP 类，首先实现适配器 `adapters.get_ddp_individual_parameters` 和 `adapters.ddp_individual_parameters_on_after_backward`（后者是可选的，取决于你的实现，你可能不需要它）。\n",
    "\n",
    "然后，执行测试，通过运行 `pytest tests/test_ddp_individual_parameters.py`。我们建议多次运行测试（例如，5 次）以确保其可靠通过。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ef9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_overlap_individual_parameters.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_overlap_individual_parameters.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 二、手动 DDP（按桶通信梯度）\n",
    "# ============================\n",
    "class DDPOverlapBucketed(nn.Module):\n",
    "    def __init__(self, module: torch.nn.Module, bucket_size_mb: float):\n",
    "        super(DDPOverlapBucketed, self).__init__()\n",
    "        self.module = module\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "        self.handles = []   # 保存异步 all_reduce 句柄\n",
    "        self.buckets = []   # 梯度桶\n",
    "        self.world_size = dist.get_world_size()\n",
    "        \n",
    "        # --------------------\n",
    "        # 初始化：广播参数 + 创建桶 + 注册钩子\n",
    "        # --------------------\n",
    "        self._broadcast_parameters()  # 初始参数广播\n",
    "        self._create_bucket()         # 按大小划分梯度桶\n",
    "        self._register_hook()         # 注册梯度钩子实现异步通信\n",
    "    \n",
    "    # --------------------\n",
    "    # 广播初始参数\n",
    "    # --------------------\n",
    "    def _broadcast_parameters(self):\n",
    "        \"\"\"\n",
    "        将 rank 0 的参数广播到所有进程，确保初始状态一致\n",
    "        \"\"\"\n",
    "        if self.world_size > 1:\n",
    "            for param in self.module.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "    \n",
    "    # --------------------\n",
    "    # 按 bucket_size 创建梯度桶\n",
    "    # --------------------\n",
    "    def _create_bucket(self):\n",
    "        current_bucket_size = 0\n",
    "        current_bucket = []\n",
    "\n",
    "        # 倒序遍历参数，构建桶\n",
    "        for p in reversed(list(self.module.parameters())):\n",
    "            if p.requires_grad:\n",
    "                p_size = p.numel() * p.element_size()\n",
    "                \n",
    "                # 当前桶满了则保存并创建新桶\n",
    "                if p_size + current_bucket_size > self.bucket_size_bytes and current_bucket:\n",
    "                    self.buckets.append(current_bucket)\n",
    "                    current_bucket_size = 0\n",
    "                    current_bucket = []\n",
    "                \n",
    "                current_bucket.append(p)\n",
    "                current_bucket_size += p_size\n",
    "        \n",
    "        # 如果还有剩余参数，作为最后一个桶\n",
    "        if current_bucket:\n",
    "            self.buckets.append(current_bucket)\n",
    "        \n",
    "        # 为每个桶创建缓冲区\n",
    "        for i, bucket_params in enumerate(self.buckets):\n",
    "            if not bucket_params:\n",
    "                continue\n",
    "            buffer_size = sum(p.numel() for p in bucket_params)\n",
    "            buffer = torch.zeros(buffer_size, device=bucket_params[0].device, dtype=bucket_params[0].dtype)\n",
    "            self.buckets[i] = {\n",
    "                \"params\": bucket_params,\n",
    "                \"buffer\": buffer,\n",
    "                \"ready_params\": set(),\n",
    "                \"triggered\": False\n",
    "            }\n",
    "    \n",
    "    # --------------------\n",
    "    # 注册梯度钩子\n",
    "    # --------------------\n",
    "    def _register_hook(self):\n",
    "        for bucket_idx, bucket_info in enumerate(self.buckets):\n",
    "            for param in bucket_info[\"params\"]:\n",
    "                # 闭包捕获当前 bucket_idx\n",
    "                def make_hook(idx):\n",
    "                    return lambda grad, param=param: self._create_hook(grad, param, idx)\n",
    "                param.register_hook(make_hook(bucket_idx))\n",
    "    \n",
    "    # --------------------\n",
    "    # 梯度钩子逻辑：延迟执行 all_reduce\n",
    "    # --------------------\n",
    "    def _create_hook(self, grad, param, bucket_idx):\n",
    "        bucket_info = self.buckets[bucket_idx]\n",
    "        bucket_info[\"ready_params\"].add(param)\n",
    "\n",
    "        # 当桶内所有参数梯度都准备好，且未触发通信\n",
    "        if len(bucket_info[\"ready_params\"]) == len(bucket_info[\"params\"]) and not bucket_info[\"triggered\"]:\n",
    "            bucket_info[\"triggered\"] = True  # 标记为已触发\n",
    "\n",
    "            def delayed_sync():\n",
    "                # 将桶内所有梯度拷贝到扁平缓冲区\n",
    "                offset = 0\n",
    "                for p in bucket_info[\"params\"]:\n",
    "                    numel = p.numel()\n",
    "                    if p.grad is not None:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].copy_(p.grad.view(-1))\n",
    "                    else:\n",
    "                        bucket_info[\"buffer\"][offset:offset+numel].zero_()\n",
    "                    offset += numel\n",
    "                \n",
    "                # 启动异步 all_reduce，实现计算与通信重叠\n",
    "                handle = dist.all_reduce(bucket_info[\"buffer\"], async_op=True)\n",
    "                self.handles.append((handle, bucket_idx))\n",
    "            \n",
    "            # 延迟执行，确保所有梯度计算完成\n",
    "            import torch.autograd as autograd\n",
    "            autograd.Variable._execution_engine.queue_callback(delayed_sync)\n",
    "    \n",
    "    # --------------------\n",
    "    # forward 前清理状态\n",
    "    # --------------------\n",
    "    def forward(self, x):\n",
    "        if self.world_size > 1:\n",
    "            for bucket in self.buckets:\n",
    "                bucket[\"triggered\"] = False\n",
    "                bucket[\"ready_params\"].clear()\n",
    "        self.handles.clear()\n",
    "        return self.module(x)\n",
    "\n",
    "    # --------------------\n",
    "    # 等待所有异步通信完成\n",
    "    # --------------------\n",
    "    def finish_gradient_synchronization(self):\n",
    "        \"\"\"\n",
    "        等待所有 all_reduce 完成，并将梯度写回各参数\n",
    "        需在 optimizer.step() 之前调用\n",
    "        \"\"\"\n",
    "        for handle, bucket_idx in self.handles:\n",
    "            handle.wait()  # 等待通信完成\n",
    "\n",
    "            bucket_info = self.buckets[bucket_idx]\n",
    "            buffer = bucket_info[\"buffer\"]\n",
    "\n",
    "            # 求平均梯度\n",
    "            buffer.div_(self.world_size)\n",
    "\n",
    "            offset = 0\n",
    "            for p in bucket_info[\"params\"]:\n",
    "                numel = p.numel()\n",
    "                if p.grad is not None:\n",
    "                    p.grad.view(-1).copy_(buffer[offset:offset+numel])\n",
    "                offset += numel\n",
    "        \n",
    "        # 清空 handles，为下一次迭代准备\n",
    "        self.handles.clear()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 三、简单示例运行\n",
    "# ============================\n",
    "def run():\n",
    "    # 初始化分布式环境（torchrun 已设置环境变量）\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    model = nn.Linear(10, 5).cuda()\n",
    "    ddp_model = DDPOverlapBucketed(model, bucket_size_mb=1)  # 1 MB 小桶方便观察\n",
    "    opt = torch.optim.SGD(ddp_model.parameters(), lr=0.1)\n",
    "\n",
    "    for step in range(3):\n",
    "        x = torch.randn(4, 10).cuda()\n",
    "        loss = ddp_model(x).sum()\n",
    "        loss.backward()\n",
    "        ddp_model.finish_gradient_synchronization()  # 必须手动调用\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        print(f\"[rank {dist.get_rank()}] step {step} loss={loss.item()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060b549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank 0] step 0 loss=2.803687572479248\n",
      "[rank 0] step 1 loss=-8.734395980834961\n",
      "[rank 0] step 2 loss=-22.41156768798828\n",
      "[rank0]:[W122 14:35:46.682734635 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=1 ddp_overlap_individual_parameters.py # 有多少张卡设置nproc_per_node=几"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab59b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting distributed_communication_single_node_demo.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile distributed_communication_single_node_demo.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "import torch\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 分布式环境初始化与清理\n",
    "# ============================================================\n",
    "\n",
    "def init_distributed_environment(\n",
    "    master_addr: str,\n",
    "    master_port: int,\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    backend: str\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 PyTorch 分布式通信环境（进程组）\n",
    "\n",
    "    参数说明：\n",
    "    - master_addr : rank 0 所在节点的 IP 地址\n",
    "    - master_port : 用于进程间通信的端口\n",
    "    - global_rank : 当前进程在所有进程中的唯一编号\n",
    "    - world_size  : 总进程数\n",
    "    - backend     : 通信后端（gloo / nccl / mpi）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.1 设置 rendezvous（所有进程汇合的地址）\n",
    "    # --------------------------------------------------------\n",
    "    # 所有进程必须通过 MASTER_ADDR:MASTER_PORT 建立初始连接\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1.2 初始化进程组（分布式的“总开关”）\n",
    "    # --------------------------------------------------------\n",
    "    # 这是使用 torch.distributed 的前置条件\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=global_rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "\n",
    "\n",
    "def destroy_distributed_environment():\n",
    "    \"\"\"\n",
    "    清理分布式环境并释放资源\n",
    "    \"\"\"\n",
    "\n",
    "    # 销毁进程组，防止资源泄漏或死锁\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    # GPU 场景下，清空 PyTorch 的 CUDA cache（非强制，但推荐）\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. All-Reduce 通信性能 Benchmark\n",
    "# ============================================================\n",
    "\n",
    "def run_all_reduce_benchmark(\n",
    "    global_rank: int,\n",
    "    world_size: int,\n",
    "    tensor_size_mb: int,\n",
    "    backend: str,\n",
    "    device: str,\n",
    "    master_addr: str,\n",
    "    master_port: int\n",
    "):\n",
    "    \"\"\"\n",
    "    对 dist.all_reduce 进行性能测试\n",
    "\n",
    "    测试指标：\n",
    "    - 单次 all-reduce 平均耗时\n",
    "    - 理论通信带宽（GB/s）\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 初始化分布式环境\n",
    "    # --------------------------------------------------------\n",
    "    init_distributed_environment(\n",
    "        master_addr=master_addr,\n",
    "        master_port=master_port,\n",
    "        global_rank=global_rank,\n",
    "        world_size=world_size,\n",
    "        backend=backend\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 绑定 GPU（仅在 CUDA 场景下）\n",
    "    # --------------------------------------------------------\n",
    "    if device == \"cuda\":\n",
    "        # 通常约定：rank i → GPU i\n",
    "        torch.cuda.set_device(global_rank)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 构造通信测试用的 Tensor\n",
    "    # --------------------------------------------------------\n",
    "    # 将 MB 转换为字节\n",
    "    tensor_num_bytes = tensor_size_mb * 1024 * 1024\n",
    "\n",
    "    # float32 占 4 字节\n",
    "    bytes_per_element = 4\n",
    "    num_elements = tensor_num_bytes // bytes_per_element\n",
    "\n",
    "    # 随机生成测试数据（数值本身不重要）\n",
    "    communication_tensor = torch.randn(\n",
    "        num_elements,\n",
    "        device=device,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Warm-up（非常重要）\n",
    "    # --------------------------------------------------------\n",
    "    # 原因：\n",
    "    # - NCCL 通信器初始化\n",
    "    # - CUDA kernel lazy initialization\n",
    "    # - GPU 频率爬升\n",
    "    warmup_iterations = 5\n",
    "    for _ in range(warmup_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "        # CUDA 是异步执行，必须显式同步\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # 确保所有进程在同一时刻开始正式测试\n",
    "    dist.barrier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 正式 benchmark（计时）\n",
    "    # --------------------------------------------------------\n",
    "    benchmark_iterations = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(benchmark_iterations):\n",
    "        dist.all_reduce(\n",
    "            communication_tensor,\n",
    "            op=dist.ReduceOp.SUM\n",
    "        )\n",
    "\n",
    "    # GPU 场景下等待所有 kernel 完成\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 性能指标计算\n",
    "    # --------------------------------------------------------\n",
    "    total_elapsed_time = end_time - start_time\n",
    "    avg_latency_seconds = total_elapsed_time / benchmark_iterations\n",
    "\n",
    "    # 带宽 = 数据量 / 时间（单位：GB/s）\n",
    "    bandwidth_gbps = (tensor_num_bytes / avg_latency_seconds) / 1e9\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.7 打印结果（仅 rank 0）\n",
    "    # --------------------------------------------------------\n",
    "    if global_rank == 0:\n",
    "        print(\n",
    "            f\"[All-Reduce Benchmark]\\n\"\n",
    "            f\"  Backend        : {backend}\\n\"\n",
    "            f\"  Device         : {device}\\n\"\n",
    "            f\"  World Size     : {world_size}\\n\"\n",
    "            f\"  Tensor Size    : {tensor_size_mb} MB\\n\"\n",
    "            f\"  Avg Latency    : {avg_latency_seconds * 1000:.4f} ms\\n\"\n",
    "            f\"  Bandwidth      : {bandwidth_gbps:.4f} GB/s\\n\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.8 汇总所有 rank 的结果（教学用）\n",
    "    # --------------------------------------------------------\n",
    "    local_result = {\n",
    "        \"rank\": global_rank,\n",
    "        \"world_size\": world_size,\n",
    "        \"backend\": backend,\n",
    "        \"device\": device,\n",
    "        \"tensor_size_mb\": tensor_size_mb,\n",
    "        \"avg_latency_ms\": avg_latency_seconds * 1000,\n",
    "        \"bandwidth_gbps\": bandwidth_gbps,\n",
    "    }\n",
    "\n",
    "    gathered_results = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(gathered_results, local_result)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2.9 清理环境\n",
    "    # --------------------------------------------------------\n",
    "    destroy_distributed_environment()\n",
    "\n",
    "    # 只让主进程返回结果\n",
    "    if global_rank == 0:\n",
    "        return gathered_results\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    for size in [1,10,100,1000]:\n",
    "        for Backend in [\"gloo\", \"nccl\"]:\n",
    "            for world_size in [1]:\n",
    "                parser = argparse.ArgumentParser(\"All-Reduce Benchmark\")\n",
    "\n",
    "                parser.add_argument(\"--world_size\", type=int, default=world_size,\n",
    "                                    help=\"进程总数（通常等于 GPU 数）\")\n",
    "                parser.add_argument(\"--tensor_size_mb\", type=int, default=size,\n",
    "                                    help=\"通信 tensor 大小（MB）\")\n",
    "                parser.add_argument(\"--backend\", type=str, default=Backend,\n",
    "                                    choices=[\"gloo\", \"nccl\"],\n",
    "                                    help=\"分布式通信后端\")\n",
    "                parser.add_argument(\"--device\", type=str, default=\"cuda\",\n",
    "                                    choices=[\"cpu\", \"cuda\"],\n",
    "                                    help=\"运行设备\")\n",
    "                parser.add_argument(\"--master_addr\", type=str, default=\"127.0.0.1\")\n",
    "                parser.add_argument(\"--master_port\", type=int, default=29500)\n",
    "\n",
    "                args = parser.parse_args()\n",
    "\n",
    "                # GPU 数量检查\n",
    "                if args.device == \"cuda\":\n",
    "                    assert torch.cuda.is_available(), \"CUDA 不可用\"\n",
    "                    assert args.world_size <= torch.cuda.device_count(), (\n",
    "                        \"world_size 不能超过 GPU 数量\"\n",
    "                    )\n",
    "\n",
    "                # 使用 spawn 启动多进程\n",
    "                spawn(\n",
    "                    fn=run_all_reduce_benchmark,\n",
    "                    args=(\n",
    "                        args.world_size,\n",
    "                        args.tensor_size_mb,\n",
    "                        args.backend,\n",
    "                        args.device,\n",
    "                        args.master_addr,\n",
    "                        args.master_port\n",
    "                    ),\n",
    "                    nprocs=args.world_size,\n",
    "                    join=True\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfa61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.2266 ms\n",
      "  Bandwidth      : 4.6281 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1 MB\n",
      "  Avg Latency    : 0.0165 ms\n",
      "  Bandwidth      : 63.7398 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 2.0388 ms\n",
      "  Bandwidth      : 5.1430 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 10 MB\n",
      "  Avg Latency    : 0.0149 ms\n",
      "  Bandwidth      : 704.8151 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 17.6973 ms\n",
      "  Bandwidth      : 5.9251 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 100 MB\n",
      "  Avg Latency    : 0.0163 ms\n",
      "  Bandwidth      : 6448.7486 GB/s\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : gloo\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 181.1478 ms\n",
      "  Bandwidth      : 5.7885 GB/s\n",
      "\n",
      "/mnt/d/code/项目/cs336/CS336-Chinese-co-construction/coursework/Assignment2_System/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[All-Reduce Benchmark]\n",
      "  Backend        : nccl\n",
      "  Device         : cuda\n",
      "  World Size     : 1\n",
      "  Tensor Size    : 1000 MB\n",
      "  Avg Latency    : 0.0172 ms\n",
      "  Bandwidth      : 60914.7716 GB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python distributed_communication_single_node_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0219cf",
   "metadata": {},
   "source": [
    "# 实现分布式优化器封装\n",
    "\n",
    "实现了一个**参数分片（sharding）的分布式优化器封装器**，思想上接近 **ZeRO Stage-1 / FSDP 的 optimizer state sharding**\n",
    "\n",
    "## 一、整体功能概览\n",
    "\n",
    "`ShardedOptimizer` 的目标是：\n",
    "\n",
    "> **在数据并行（DDP）场景下，让不同 rank 只负责一部分参数的 optimizer state 和更新计算，从而节省显存。**\n",
    "\n",
    "核心思想是对于**参数本身，每个 rank 都有完整模型参数**，对于**优化器状态（momentum / Adam 的 exp_avg 等），只在参数 owner rank 上存在**， **参数更新只在 owner rank 上进行**。\n",
    "* **参数同步：更新后通过 `broadcast` 把新参数发给其他 rank**。\n",
    "\n",
    "## 二、整体架构\n",
    "\n",
    "```\n",
    "所有 rank：\n",
    "  拥有完整模型参数\n",
    "       │\n",
    "       ▼\n",
    "参数被 round-robin 分配 owner_rank\n",
    "       │\n",
    "       ▼\n",
    "每个 rank：\n",
    "  ├─ 全局 Optimizer（只用于 param_groups 管理）\n",
    "  └─ 本地 optim（只包含自己拥有的 params）\n",
    "       │\n",
    "       ▼\n",
    "step():\n",
    "  1. 本地 optim.step()（只更新自己那一片）\n",
    "  2. dist.broadcast（把更新后的参数同步给所有 rank）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、代码解读\n",
    "\n",
    "\n",
    "### 1。 初始化：`__init__`\n",
    "\n",
    "```python\n",
    "class ShardedOptimizer(Optimizer):\n",
    "```\n",
    "\n",
    "继承 PyTorch 原生 `Optimizer`，因此：\n",
    "\n",
    "这样可以被 Trainer / 训练循环无缝使用，并且拥有 `param_groups / zero_grad / step` 接口\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.1 记录原始 optimizer 类型\n",
    "\n",
    "```python\n",
    "self.optimizer_cls = optimizer_cls\n",
    "self.optimizer_kwargs = kwargs\n",
    "```\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "ShardedOptimizer(\n",
    "    model.parameters(),\n",
    "    torch.optim.Adam,\n",
    "    lr=1e-4\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2 获取分布式信息\n",
    "\n",
    "```python\n",
    "self.rank = dist.get_rank()\n",
    "self.world_size = dist.get_world_size()\n",
    "```\n",
    "\n",
    "用于：\n",
    "\n",
    "* 决定参数 owner\n",
    "* 决定 broadcast 源\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.3 参数分配辅助变量\n",
    "\n",
    "```python\n",
    "self.global_param_counter = 0\n",
    "self.local_param_groups = []\n",
    "```\n",
    "\n",
    "* `global_param_counter`：**全局参数编号**\n",
    "* 用 round-robin 把参数均匀分配给各个 rank\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4 调用父类构造函数（关键）\n",
    "\n",
    "```python\n",
    "super().__init__(params, defaults=kwargs)\n",
    "```\n",
    "\n",
    "\n",
    "`Optimizer.__init__` 内部会调用 `self.add_param_group`，但这里的 `add_param_group` 已经被 **重写**\n",
    "\n",
    "这使得 **参数分片逻辑嵌入在初始化阶段**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5 创建“本地 optimizer”\n",
    "\n",
    "```python\n",
    "self.optim = self.optimizer_cls(self.local_param_groups, **self.optimizer_kwargs)\n",
    "```\n",
    "\n",
    "`self.optim` **只包含当前 rank 拥有的参数**，只有这些参数才会分配 optimizer state（节省显存）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 参数分片逻辑：`add_param_group`\n",
    "\n",
    "这是整个实现的**核心函数**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1 给每个参数打 owner 标记\n",
    "\n",
    "```python\n",
    "p._owner_rank = self.global_param_counter % self.world_size\n",
    "```\n",
    "\n",
    "round-robin 分配, `_owner_rank` 是动态属性（hack，但可行）\n",
    "\n",
    "示例（world_size=4）：\n",
    "\n",
    "| 参数编号 | owner_rank |\n",
    "| ---- | ---------- |\n",
    "| p0   | 0          |\n",
    "| p1   | 1          |\n",
    "| p2   | 2          |\n",
    "| p3   | 3          |\n",
    "| p4   | 0          |\n",
    "| p5   | 1          |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2 加入“全局 optimizer”\n",
    "\n",
    "```python\n",
    "super().add_param_group(param_group)\n",
    "```\n",
    "\n",
    "**为什么必须这么做？**\n",
    "\n",
    "`self.param_groups` 需要包含 **所有参数**,用于进行`zero_grad`，遍历所有参数做 `broadcast`，并且Trainer / AMP 兼容，**注意**：这个“全局 optimizer”**不做 step**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3 构造本地 param group\n",
    "\n",
    "```python\n",
    "local_group['params'] = [\n",
    "    p for p in param_group['params']\n",
    "    if p._owner_rank == self.rank\n",
    "]\n",
    "```\n",
    "\n",
    "它只保留属于当前 rank 的参数，其他超参（lr、weight_decay）保持一致。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.4 加入本地 optimizer\n",
    "\n",
    "```python\n",
    "self.optim.add_param_group(local_group)\n",
    "```\n",
    "\n",
    "或者（初始化期间）缓存起来：\n",
    "\n",
    "```python\n",
    "self.local_param_groups.append(local_group)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 参数更新逻辑：`step`\n",
    "\n",
    "```python\n",
    "def step(self, closure=None, **kwargs):\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1：本地更新\n",
    "\n",
    "```python\n",
    "self.optim.step(**kwargs)\n",
    "```\n",
    "\n",
    "* **只更新自己拥有的参数**\n",
    "* 只有这些参数有 optimizer state\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2：参数同步（核心）\n",
    "\n",
    "```python\n",
    "dist.broadcast(p.data, src=p._owner_rank)\n",
    "```\n",
    "\n",
    "如果是 owner 则发送最新参数，如果不是 owner 就接收参数覆盖本地副本。最终所有 rank 的模型参数 **完全一致**。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.梯度清零：`zero_grad`\n",
    "\n",
    "```python\n",
    "super().zero_grad(set_to_none=set_to_none)\n",
    "```\n",
    "\n",
    "清除所有参数的梯度，即使不是 owner，也要清 grad（否则梯度会累积）。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、这个实现“像什么”？\n",
    "\n",
    "| 技术           | 相似度  | 区别                        |\n",
    "| ------------ | ---- | ------------------------- |\n",
    "| ZeRO Stage-1 | ⭐⭐⭐⭐ | 真实 ZeRO 会用 reduce-scatter |\n",
    "| FSDP         | ⭐⭐   | 没有参数 flatten / overlap    |\n",
    "| DDP          | ⭐    | DDP 是全量 optimizer state   |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、优点与限制\n",
    "\n",
    "###  优点\n",
    "\n",
    "1. **显存节省**\n",
    "\n",
    "   * optimizer state 约减少到 `1/world_size`\n",
    "2. **逻辑清晰**\n",
    "\n",
    "   * 教学 & 原理验证非常好\n",
    "3. **兼容任意 Optimizer**\n",
    "\n",
    "   * Adam / SGD / Adafactor 等\n",
    "\n",
    "---\n",
    "\n",
    "### 局限\n",
    "\n",
    "1. **通信成本高**\n",
    "\n",
    "   ```python\n",
    "   dist.broadcast(p.data)\n",
    "   ```\n",
    "\n",
    "   * 每个参数一次 broadcast（非常慢）\n",
    "\n",
    "2. **参数粒度过细**\n",
    "\n",
    "   * 实际系统会 **flatten 后按 bucket 同步**\n",
    "\n",
    "3. **没有 overlap**\n",
    "\n",
    "   * 计算与通信完全串行\n",
    "\n",
    "4. **依赖 monkey-patch 参数属性**\n",
    "\n",
    "   * `_owner_rank` 不够“干净”\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结\n",
    "\n",
    "\n",
    "> 用 round-robin 分配参数 owner，\n",
    "> 只在 owner rank 上更新参数，\n",
    "> 再用 broadcast 同步权重，\n",
    "> 从而显著降低 optimizer 显存占用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a55c6",
   "metadata": {},
   "source": [
    "命令行参数\n",
    "   ↓\n",
    "   \n",
    "wandb 初始化（实验配置中心）\n",
    "   ↓\n",
    "\n",
    "加载 tokenized 数据（pkl）\n",
    "   ↓\n",
    "\n",
    "构造 DataLoader（按 context_length 切 batch）\n",
    "   ↓\n",
    "\n",
    "构造 Transformer 模型\n",
    "   ↓\n",
    "\n",
    "构造 AdamW + Cosine 学习率调度\n",
    "   ↓\n",
    "\n",
    "训练循环（epoch → step）\n",
    "   ↓\n",
    "\n",
    "验证 / 日志 / checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f007eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=====================================================\n",
    "Transformer Language Model Training Script (Teaching Version)\n",
    "=====================================================\n",
    "\n",
    "本脚本用于训练一个基于 Transformer 的自回归语言模型，\n",
    "数据集为 TinyStories（已提前 tokenized 并保存为 pkl）。\n",
    "\n",
    "主要包含：\n",
    "1. WandB 实验配置与日志\n",
    "2. 数据加载（自定义 DataLoader）\n",
    "3. 模型构建（TransformerModule）\n",
    "4. 训练循环（AdamW + Cosine LR）\n",
    "5. 验证与模型 checkpoint\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#导入自己的模块\n",
    "from cs336_myown import (\n",
    "    DataLoader,\n",
    "    CosineSchedule,\n",
    "    CrossEntropyLoss,\n",
    "    clip_gradient\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 一些全局工具变量\n",
    "# =====================================================\n",
    "\n",
    "# 用于区分不同实验的时间戳（用于 wandb 名字 & checkpoint）\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# WandB 初始化函数\n",
    "# =====================================================\n",
    "def init_wandb(\n",
    "    d_model: int,\n",
    "    d_ff: int,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    train_steps: int\n",
    "):\n",
    "    \"\"\"\n",
    "    初始化 WandB 实验，并统一管理所有超参数。\n",
    "\n",
    "    教学重点：\n",
    "    - WandB 的 config 本质上是“实验的单一事实源（single source of truth）”\n",
    "    - 后续训练中不应再散落硬编码超参数\n",
    "    \"\"\"\n",
    "\n",
    "    wandb.login()\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cs336_final_train\",\n",
    "        config={\n",
    "            # -----------------------------\n",
    "            # Experiment metadata\n",
    "            # -----------------------------\n",
    "            \"experiment_name\": f\"tinystories_17M_{TIMESTAMP}\",\n",
    "            \"total_tokens_processed\": 327_680_000,\n",
    "\n",
    "            # -----------------------------\n",
    "            # Dataset & Tokenizer\n",
    "            # -----------------------------\n",
    "            \"train_data_path\": \"../data/TinyStoriesV2-GPT4-train.txt\",\n",
    "            \"valid_data_path\": \"../data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "            \"encoded_ids_train_path\": \"./cs336_systems/encoded_ids_train.pkl\",\n",
    "            \"encoded_ids_valid_path\": \"./cs336_systems/encoded_ids_valid.pkl\",\n",
    "            \"vocab_path\": \"vocab.json\",\n",
    "            \"merges_path\": \"merges.txt\",\n",
    "\n",
    "            # -----------------------------\n",
    "            # Model hyperparameters\n",
    "            # -----------------------------\n",
    "            \"vocab_size\": 10_000,\n",
    "            \"context_length\": 256,\n",
    "            \"d_model\": d_model,        # Transformer hidden size\n",
    "            \"d_ff\": d_ff,              # FFN hidden size\n",
    "            \"n_layers\": n_layers,      # Transformer block 数\n",
    "            \"n_heads\": n_heads,        # Attention heads\n",
    "            \"rope_theta\": 10_000.0,    # RoPE 超参数\n",
    "\n",
    "            # -----------------------------\n",
    "            # Training hyperparameters\n",
    "            # -----------------------------\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"train_steps\": train_steps,\n",
    "\n",
    "            # Learning rate schedule\n",
    "            \"initial_lr\": 3e-5,\n",
    "            \"max_learning_rate\": 3e-5,\n",
    "            \"min_learning_rate\": 1e-5,\n",
    "            \"lr_warmup_steps\": 2000,\n",
    "            \"cosine_cycle_iters\": 10000,\n",
    "\n",
    "            # Optimizer (AdamW)\n",
    "            \"weight_decay\": 0.1,\n",
    "            \"adam_beta1\": 0.9,\n",
    "            \"adam_beta2\": 0.95,\n",
    "            \"eps\": 1e-8,\n",
    "\n",
    "            # Gradient clipping\n",
    "            \"grad_clip\": 1.0,\n",
    "\n",
    "            # -----------------------------\n",
    "            # Logging & Checkpoint\n",
    "            # -----------------------------\n",
    "            \"log_interval\": 20,\n",
    "            \"val_interval\": 20,\n",
    "            \"checkpoint_interval\": 60,\n",
    "            \"checkpoint_dir\": \"checkpoints\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return run.config\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 主训练函数\n",
    "# =====================================================\n",
    "def train_model(config, device):\n",
    "    \"\"\"\n",
    "    Transformer 语言模型训练主逻辑\n",
    "\n",
    "    教学重点：\n",
    "    - epoch vs step 的区别\n",
    "    - global_step 的意义（LR schedule 依赖）\n",
    "    - 训练 / 验证模式切换\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. 加载 tokenized 数据\n",
    "    # -----------------------------\n",
    "    with open(config[\"encoded_ids_train_path\"], \"rb\") as f:\n",
    "        train_token_ids = pickle.load(f)\n",
    "\n",
    "    with open(config[\"encoded_ids_valid_path\"], \"rb\") as f:\n",
    "        valid_token_ids = pickle.load(f)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_token_ids,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        context_length=config[\"context_length\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_token_ids,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        context_length=config[\"context_length\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. 构建模型\n",
    "    # -----------------------------\n",
    "    model = TransformerModule(\n",
    "        d_model=config[\"d_model\"],\n",
    "        n_heads=config[\"n_heads\"],\n",
    "        d_ff=config[\"d_ff\"],\n",
    "        context_length=config[\"context_length\"],\n",
    "        rope_theta=config[\"rope_theta\"],\n",
    "        n_layers=config[\"n_layers\"],\n",
    "        vocab_size=config[\"vocab_size\"],\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Optimizer & LR Scheduler\n",
    "    # -----------------------------\n",
    "    lr_scheduler = CosineSchedule(\n",
    "        max_lr=config[\"max_learning_rate\"],\n",
    "        min_lr=config[\"min_learning_rate\"],\n",
    "        warmup_steps=config[\"lr_warmup_steps\"],\n",
    "        cycle_iters=config[\"cosine_cycle_iters\"]\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"initial_lr\"],\n",
    "        betas=(config[\"adam_beta1\"], config[\"adam_beta2\"]),\n",
    "        eps=config[\"eps\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Training loop\n",
    "    # -----------------------------\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "        if epoch >= 9:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "        for step in range(config[\"train_steps\"]):\n",
    "\n",
    "            # 更新学习率（基于 global_step）\n",
    "            current_lr = lr_scheduler(global_step)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = current_lr\n",
    "\n",
    "            # 取一个 batch\n",
    "            x, y = train_loader.get_train_batch_data()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            logits = model(x)\n",
    "            loss = loss_fn.forward(logits, y)\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪（防止梯度爆炸）\n",
    "            clip_gradient(model.parameters(), config[\"grad_clip\"])\n",
    "            optimizer.step()\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch} | Step {step}] \"\n",
    "                    f\"LR={current_lr:.6e}, Loss={loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # epoch 计时\n",
    "        if epoch >= 9:\n",
    "            end_time = timeit.default_timer()\n",
    "            with open(\n",
    "                f\"{config['d_model']}_{config['d_ff']}_\"\n",
    "                f\"{config['n_layers']}_{config['n_heads']}_train_time.txt\",\n",
    "                \"a\"\n",
    "            ) as f:\n",
    "                f.write(f\"Epoch {epoch} took {end_time - start_time:.2f}s\\n\")\n",
    "\n",
    "        # WandB 记录\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": loss.item()})\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Validation\n",
    "        # -----------------------------\n",
    "        if (epoch + 1) % config[\"val_interval\"] == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_loader.get_valid_batch_data_iter():\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    logits = model(x)\n",
    "                    val_loss = loss_fn.forward(logits, y)\n",
    "                    wandb.log({\"epoch\": epoch, \"val_loss\": val_loss.item()})\n",
    "            model.train()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Checkpoint\n",
    "        # -----------------------------\n",
    "        if (epoch + 1) % config[\"checkpoint_interval\"] == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f\"checkpoints/model_epoch_{epoch}_{TIMESTAMP}.pth\"\n",
    "            )\n",
    "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"checkpoints/model_final_{TIMESTAMP}.pth\"\n",
    "    )\n",
    "    print(\"Final checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================\n",
    "# CLI 入口\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:6\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=40)\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=2000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "\n",
    "    parser.add_argument(\"--d_model\", type=int, default=768)\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=3072)\n",
    "    parser.add_argument(\"--n_layers\", type=int, default=12)\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=12)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(\n",
    "        args.device if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    config = init_wandb(\n",
    "        d_model=args.d_model,\n",
    "        d_ff=args.d_ff,\n",
    "        n_layers=args.n_layers,\n",
    "        n_heads=args.n_heads,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "        train_steps=args.train_steps\n",
    "    )\n",
    "\n",
    "    train_model(config, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2_System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
