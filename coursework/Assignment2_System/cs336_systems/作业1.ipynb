{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daa8d39",
   "metadata": {},
   "source": [
    "# 我们将完成\n",
    "1. 基准测试和性能分析框架\n",
    "2. Flash Attention \n",
    "2. Triton 内核\n",
    "3. 分布式数据并行训练\n",
    "4. 优化器状态分片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192270c9",
   "metadata": {},
   "source": [
    "# 前期准备工作\n",
    "\n",
    "## 在windows上安装Ubuntu-22.04（使用triton需要使用在linux上运行）\n",
    "\n",
    "由于triton在windows没有可用的版本，考虑到大家都是使用的windows系统的比较多，这里附一个安装教程，有linux系统或者不想运行triton模块的同学可以跳过\n",
    "\n",
    "### 安装步骤\n",
    "\n",
    "#### 1. 版本限制\n",
    "\n",
    "我们需要在windows上安装wsl2，限制在Windows 10 2004 (19041) 及以上 或者 Windows 11 任意版本\n",
    "\n",
    "#### 2.手动启用 WSL 相关功能（关键）\n",
    "\n",
    "按住win键搜索**启用或者关闭windows功能**，找到并勾选 **于Linux的Windows子系统**和 **虚拟机平台**，点击确定\n",
    "\n",
    "\n",
    "#### 3. 确认 CPU 虚拟化已开启\n",
    "\n",
    "按住Ctrl + Shift + Esc打开任务管理器，性能 -> CPU ，右侧应显示：虚拟化：已启用\n",
    "\n",
    "如果是“未启用”\n",
    "\n",
    "需要进 BIOS / UEFI，开启：\n",
    "\n",
    "Intel：Intel Virtualization Technology (VT-x)\n",
    "\n",
    "AMD：SVM Mode\n",
    "\n",
    "##### 4. 安装 Ubuntu\n",
    "\n",
    "在windows搜索页面中输入CMD，并且以管理员身份运行\n",
    "\n",
    "```bash\n",
    "wsl --install -d Ubuntu\n",
    "```\n",
    "\n",
    "之后设置账号密码，密码在输入的时候是不可见的，看不到是正常的\n",
    "\n",
    "要打开ubuntu的命令行只需要在cmd中上面一行的小三角中打开就行\n",
    "\n",
    "##### 5. 安装相关的工具 pip uv 等\n",
    "\n",
    "依次执行\n",
    "先使用管理员身份打开CMD\n",
    "\n",
    "输入\n",
    "```bash\n",
    "cd $env:USERPROFILE\n",
    "@\"\n",
    "[wsl2]\n",
    "networkingMode=mirrored\n",
    "dnsTunneling=true\n",
    "autoProxy=true\n",
    "firewall=true\n",
    "\"@ | Out-File -Encoding UTF8 .wslconfig\n",
    "\n",
    "\n",
    "wsl --shutdown     # 彻底关掉虚拟机\n",
    "wsl                # 再进子系统\n",
    "```\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install python3-pip\n",
    "\n",
    "sudo apt install python3.12-venv\n",
    "\n",
    "source ~/myenv/bin/activate\n",
    "pip install uv -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "sudo ln -s /home/kangkang/myenv/bin/uv /usr/local/bin/uv\n",
    "\n",
    "echo 'export UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple' >> ~/.bashrc\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### 6. 设置vscode的远程连接\n",
    "\n",
    "打开vscode ，先搜索插件wsl。安装完成后打开左侧一列的远程连接就可以连接到wsl的ubuntu系统中\n",
    "\n",
    "\n",
    "运行在CS336-Chinese-co-construction\\coursework\\Assignment2_System下\n",
    "```bash\n",
    "uv venv \n",
    "cd cs336_basics\n",
    "uv run\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d935",
   "metadata": {},
   "source": [
    "# 1. 基准测试和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f745648",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "（a）编写一个脚本，对模型的前向与反向传播进行基本的端到端基准测试。具体而言，你的脚本应支持以下功能：\n",
    "- 给定超参数（例如层数），初始化一个模型。\n",
    "- 生成一个随机批次的数据。\n",
    "- 先运行 w 个预热步骤（开始计时之前），然后计时执行 n 个步骤（仅前向或同时包含前向与反向，取决于参数）。计时可使用 Python 的 timeit 模块（例如直接调用 timeit 函数，或使用 timeit.default_timer()，后者提供系统最高分辨率的时钟，比 time.time() 更适合基准测试）。\n",
    "- 每一步之后调用 torch.cuda.synchronize()。\n",
    "\n",
    "交付物：一个脚本，能够根据给定超参数初始化基础 Transformer 模型，创建随机数据批次，并对前向与反向传播进行计时。\n",
    "\n",
    "（b）对 §1.2 中描述的模型规模，分别测试前向与反向传播耗时。  \n",
    "使用 5 个预热步骤，并在 10 次测量步骤上计算平均耗时与标准差。  \n",
    "一次前向传播耗时多久？反向传播呢？测量结果波动大吗，还是标准差很小？  \n",
    "\n",
    "交付物：1–2 句话给出你的计时结果。\n",
    "\n",
    "（c）基准测试的一个常见陷阱是跳过预热步骤。请在没有预热的情况下重复上述分析。结果如何变化？你认为原因是什么？再尝试仅使用 1 或 2 个预热步骤，为何结果仍可能不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9210e8",
   "metadata": {},
   "source": [
    "## 首先检查GPU是否可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea2358",
   "metadata": {},
   "source": [
    "这边通过pyproject.toml下载的torch是cpu版本\n",
    "如果要安装GPU版本：\n",
    "```bash\n",
    "uv pip uninstall torch \n",
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "cu121是我的cuda版本为12.1，可以改为自己的cuda版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036a681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.model import BasicsTransformerLM \n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "from cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.data import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a57e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "MPS（苹果的MPS） available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA GPU detected\")\n",
    "\n",
    "print(\"MPS（苹果的MPS） available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2625f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b8414",
   "metadata": {},
   "source": [
    "现在来测试一下sleep函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7986b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：50.1554012298584ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.1554012298584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(\"sleep\", lambda : time.sleep(50 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdfb5b",
   "metadata": {},
   "source": [
    "在误差范围之内"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6048",
   "metadata": {},
   "source": [
    "我们来实测一下transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5522e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 3e-5\n",
    "warmup_iters = 200\n",
    "cosine_cycle_iters = 10_000\n",
    "num_steps = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[189, 190, 191, 192, 193, 194, 195, 196],\n",
      "        [402, 403, 404, 405, 406, 407, 408, 409],\n",
      "        [581, 582, 583, 584, 585, 586, 587, 588],\n",
      "        [  7,   8,   9,  10,  11,  12,  13,  14]], device='cuda:0')\n",
      "tensor([[190, 191, 192, 193, 194, 195, 196, 197],\n",
      "        [403, 404, 405, 406, 407, 408, 409, 410],\n",
      "        [582, 583, 584, 585, 586, 587, 588, 589],\n",
      "        [  8,   9,  10,  11,  12,  13,  14,  15]], device='cuda:0')\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "def test_transformer():\n",
    "\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # 假设是 token 序列\n",
    "    dataset = np.arange(1000, dtype=np.int64)\n",
    "    x, y = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=4,\n",
    "        context_length=8,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(x.shape)  # (4, 8)\n",
    "    print(y.shape)  # (4, 8)\n",
    "    print(x)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    print(f'设备：{device}')\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x,y, optimizer,dataset\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8295b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8262 | lr 0.00e+00\n",
      "现在真正计时!\n",
      "0/1\n",
      "step      0 | loss 10.8334 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8391 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8361 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8282 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8266 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8343 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8268 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8296 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8359 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8282 | lr 0.00e+00\n",
      "单次耗时：834.6230745315552ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "834.6230745315552"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy  as np\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_model(num_steps = 1):\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "    \n",
    "\n",
    "        model = BasicsTransformerLM(\n",
    "                                vocab_size=vocab_size,\n",
    "                                context_length=context_length,\n",
    "                                d_model=d_model,\n",
    "                                num_layers=num_layers,\n",
    "                                num_heads=num_heads,\n",
    "                                d_ff=d_ff,\n",
    "                                rope_theta=rope_theta,\n",
    "                                ).to(device)\n",
    "        optimizer = AdamW(\n",
    "                            model.parameters(),\n",
    "                            lr=max_lr,\n",
    "                            weight_decay=0.1,\n",
    "                        )\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        # print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7日志\n",
    "    if it % 10 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dcaf6",
   "metadata": {},
   "source": [
    "## 推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46bb75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                            vocab_size=vocab_size,\n",
    "                            context_length=context_length,\n",
    "                            d_model=d_model,\n",
    "                            num_layers=num_layers,\n",
    "                            num_heads=num_heads,\n",
    "                            d_ff=d_ff,\n",
    "                            rope_theta=rope_theta,\n",
    "                            ).to(device)\n",
    "    optimizer = AdamW(\n",
    "                        model.parameters(),\n",
    "                        lr=max_lr,\n",
    "                        weight_decay=0.1,\n",
    "                    )\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 用 get_batch 拿一个 batch\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "        x, _ = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=1,\n",
    "            context_length= 1,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # 取第一个样本作为 prompt\n",
    "        prompt = x # shape: (context_length,)\n",
    "        # 调用模型自带的 generate\n",
    "        generated = model.generate(\n",
    "            x=prompt,\n",
    "            max_new_tokens=5,\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            eos_token_id=None,\n",
    "        )\n",
    "\n",
    "        print(generated.shape)  # (<=50,)\n",
    "        print(generated)\n",
    "        try:\n",
    "            del model, x, prompt, generated, dataset\n",
    "        except NameError:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ad903",
   "metadata": {},
   "source": [
    "### 性能分析工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db37a8a",
   "metadata": {},
   "source": [
    "使用wsl后无法测量cuda时间，这是正常现象，可以使用正常的linux系统或者windows系统，原因在于wsl的linux无法使用torch的一些接口来检测cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88bb1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.0+cu121\n",
      "cuda : 12.1\n",
      "CUPTI: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "import torch, ctypes, os\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda :', torch.version.cuda)\n",
    "try:\n",
    "    ctypes.CDLL('libcupti.so')\n",
    "    print('CUPTI: OK')\n",
    "except:\n",
    "    print('CUPTI: NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19c38995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8323 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8337 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8244 | lr 0.00e+00\n",
      "正在使用cuda\n",
      "0/1\n",
      "step      0 | loss 10.8257 | lr 0.00e+00\n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                             aten::empty         0.26%       2.167ms         0.26%       2.167ms      10.175us           213  \n",
      "                                                          aten::uniform_        46.59%     387.809ms        46.59%     387.809ms       8.814ms            44  \n",
      "                                                           aten::erfinv_         3.63%      30.232ms         3.63%      30.232ms     687.081us            44  \n",
      "                                                              aten::mul_         1.78%      14.811ms         1.92%      15.986ms     363.326us            44  \n",
      "                                                                aten::to         0.48%       4.011ms         7.63%      63.515ms     159.987us           397  \n",
      "                                                          aten::_to_copy         0.16%       1.321ms         7.15%      59.504ms     391.471us           152  \n",
      "                                                     aten::empty_strided         0.32%       2.697ms         0.38%       3.180ms      10.257us           310  \n",
      "                                                             aten::copy_         0.39%       3.255ms         7.42%      61.802ms     238.619us           259  \n",
      "                                                              aten::add_         1.85%      15.428ms         2.06%      17.131ms     146.421us           117  \n",
      "                                                            aten::clamp_         1.69%      14.087ms         1.69%      14.104ms     320.541us            44  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 832.419ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "\n",
    "print(profile(description ='transformer' , run = run_model, num_warmups = 3, with_stack = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e633611",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "然而，简单地将模型转换为低精度格式可能会导致模型精度降低。例如，许多梯度值在实践中太小，无法用 FP16 表示，因此在用 FP16 精度进行简单训练时会变成零。为了解决这个问题，通常在用 FP16 训练时使用损失缩放（loss is simply multiplied by a scaling factor, increasing gradient magnitudes so they don't flush to zero）。此外，FP16 的动态范围比 FP32 低，这可能导致溢出，表现为 NaN 损失。全精度（Full bfloat16）训练通常更稳定（因为 BF16 具有与 FP32 相同的动态范围），但与 FP32 相比仍可能影响最终模型性能。\n",
    "\n",
    "为了利用低精度数据类型的加速优势，通常使用混合精度训练。在 PyTorch 中，这通过 torch.autocast 上下文管理器实现。在这种情况下，某些操作（例如矩阵乘法）在低精度数据类型上执行，而需要 FP32 全动态范围的操作（例如累积和归约）保持不变。例如，以下代码将自动识别在前向传播期间需要在低精度上执行的操作，并将这些操作转换为指定的数据类型：\n",
    "\n",
    "```python\n",
    "model : torch.nn.Module = ...  # 例如，你的 Transformer 模型\n",
    "dtype : torch.dtype = ...  # 例如，torch.float16\n",
    "x : torch.Tensor = ...  # 输入数据\n",
    "\n",
    "with torch.autocast(device=\"cuda\", dtype=dtype):\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "如上所述，即使张量本身已被降级，通常最好在高精度下保持累积。以下练习将帮助你建立直觉，了解为什么会这样。\n",
    "\n",
    "问题（混合精度累积）：1 分\n",
    "\n",
    "运行以下代码并评论结果的准确性。\n",
    "\n",
    "```python\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "```\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "我们现在将混合精度应用于一个玩具模型进行直观理解，然后应用于我们的基准测试脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c20ec484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "\n",
    "\n",
    "#  清理显存引用\n",
    "try:\n",
    "    del s, x,i\n",
    "except NameError:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31646db",
   "metadata": {},
   "source": [
    "可以看到全精度最准确，半进度最不准确，混合精度介于二者之间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270f15e",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "问题（benchmarking_mixed_precision）：2 分\n",
    "\n",
    "（a）考虑以下模型：\n",
    "\n",
    "```python\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "假设我们在 GPU 上训练模型，并且模型参数最初是 FP32。我们希望使用 FP16 的自动混合精度。上述列出的每个组件的数据类型是什么？\n",
    "\n",
    "- autocast 上下文内的模型参数，\n",
    "- 第一个前馈层（ToyModel.fc1）的输出，\n",
    "- 层归一化（ToyModel.ln）的输出，\n",
    "- 模型的预测 logits，\n",
    "- 损失，\n",
    "- 以及模型的梯度？\n",
    "\n",
    "交付物：上述列出的每个组件的数据类型。\n",
    "\n",
    "（b）你应该已经看到，FP16 混合精度自动转换对层归一化层的处理与前馈层不同。层归一化的哪些部分对混合精度敏感？如果我们使用 BF16 而不是 FP16，是否仍需要对层归一化进行不同处理？为什么或为什么不？\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "（c）修改你的基准测试脚本，以便可以选择使用 BF16 混合精度运行模型。对 §1.1.2 中描述的每种语言模型规模，分别计时使用和不使用混合精度的前向和反向传播。比较使用全精度与混合精度的结果，并评论模型大小变化时的任何趋势。你可能会发现无上下文的 no-op 上下文管理器很有用。\n",
    "\n",
    "交付物：2–3 句话的回应，包含你的计时结果和评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7910a",
   "metadata": {},
   "source": [
    "# 解答\n",
    "\n",
    "\n",
    "### 问题 (a)\n",
    "\n",
    "在自动混合精度（Autocast）下，使用 FP16 训练时，各组件的数据类型如下：\n",
    "\n",
    "- **模型参数（model parameters）**：由于模型参数最初是 FP32，它们在 autocast 上下文内保持为 FP32。\n",
    "- **第一个前馈层的输出（output of the first feed-forward layerToyModel.fc1）**：在 autocast 上下文中，该层的输出将被转换为 FP16。\n",
    "- **层归一化的输出（output of layer norm ToyModel.ln）**：层归一化层的输出通常保持为 FP32，因为层归一化需要较高的数值稳定性，FP16 的动态范围较小，可能导致数值不稳定。\n",
    "- **模型的预测 logits**：在 autocast 上下文中，这些 logits 将被转换为 FP16。\n",
    "- **损失（loss）**：损失计算通常在 FP32 中进行，以保持数值稳定性。\n",
    "- **模型的梯度（model's gradients）**：梯度通常也保持为 FP32，以确保优化过程的稳定性。\n",
    "\n",
    "### 问题 (b)\n",
    "\n",
    "层归一化对混合精度的敏感性主要体现在其计算过程中需要较高的数值精度。层归一化涉及到均值和方差的计算，这些计算在 FP16 中可能会因为数值范围的限制而导致不准确，从而影响模型的训练效果。因此，即使在 FP16 混合精度下，层归一化通常也需要保持在 FP32 中进行，以确保数值稳定性。\n",
    "\n",
    "如果我们使用 BF16 而不是 FP16，我们可能仍然需要对层归一化进行特殊处理。BF16 虽然在动态范围上与 FP32 相同，但其精度较低，可能仍然不足以保证层归一化计算的稳定性。因此，为了保持数值稳定性，层归一化可能仍然需要在 FP32 中进行。\n",
    "\n",
    "### 问题 (c)\n",
    "\n",
    "为了在基准测试脚本中使用 BF16 混合精度，我们可以修改脚本以支持 BF16。以下是修改后的脚本示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 模型初始化\n",
    "model = ToyModel(100, 10)\n",
    "x = torch.randn(1, 100)\n",
    "\n",
    "# 使用 FP32 进行基准测试\n",
    "with torch.no_grad():\n",
    "    fp32_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "# 使用 BF16 进行基准测试\n",
    "with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    with torch.no_grad():\n",
    "        bf16_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "print(f\"FP32 time: {fp32_time}\")\n",
    "print(f\"BF16 time: {bf16_time}\")\n",
    "```\n",
    "\n",
    "在比较使用全精度（FP32）与混合精度（BF16）的结果时，我们可能会观察到 BF16 在某些情况下可以提供与 FP32 相似的精度，同时显著减少内存使用和加速计算。然而，对于需要高精度计算的层（如层归一化），BF16 可能不如 FP32 稳定。随着模型大小的变化，我们可能会看到不同规模的模型对混合精度的敏感性不同，这需要在实际应用中进行具体分析和调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 time: 0.0034561510001367424\n",
      "BF16 time: 0.002322755000932375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "import gc\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def test_fb32_BF16_time():\n",
    "\n",
    "    # 模型初始化，可以修改参数来测试效果\n",
    "    model = ToyModel(100, 10)\n",
    "    x = torch.randn(1, 100)\n",
    "\n",
    "\n",
    "\n",
    "    # 使用 FP32 进行基准测试\n",
    "    with torch.no_grad():\n",
    "        fp32_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "    # 使用 BF16 进行基准测试\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        with torch.no_grad():\n",
    "            bf16_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "    #  清理显存引用\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"FP32 time: {fp32_time}\")\n",
    "    print(f\"BF16 time: {bf16_time}\")\n",
    "    try:\n",
    "        del model, x, fp32_time, bf16_time\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81311fb",
   "metadata": {},
   "source": [
    "可以看到不同精度下，精度越低则速度越快"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eed308",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "\n",
    "到目前为止，我们一直在关注计算性能。现在我们将把注意力转向**内存**，这是语言模型训练和推理中的另一项关键资源。\n",
    "PyTorch 也提供了一个强大的**内存分析器（memory profiler）**，可以随时间跟踪内存分配情况。\n",
    "\n",
    "要使用内存分析器，你可以按如下方式修改你的 benchmark 脚本：\n",
    "\n",
    "```\n",
    "7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ... 在你的 benchmarking 脚本中的 warm-up 阶段\n",
    "\n",
    "# 开始记录内存历史\n",
    "torch.cuda.memory._record_memory_history(max_entries=1000000)\n",
    "\n",
    "# ... 在你的 benchmarking 脚本中你想要分析的部分\n",
    "\n",
    "# 保存一个 pickle 文件，用于 PyTorch 的在线工具加载\n",
    "torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "\n",
    "# 停止记录内存历史\n",
    "torch.cuda.memory._record_memory_history(enabled=None)\n",
    "```\n",
    "\n",
    "这将输出一个名为 `memory_snapshot.pickle` 的文件，你可以将其加载到如下在线工具中：\n",
    "[https://pytorch.org/memory_viz](https://pytorch.org/memory_viz)\n",
    "该工具可以让你查看整体内存使用时间线，以及每一次独立的内存分配是如何发生的，包括其大小以及指向分配来源代码位置的调用栈追踪。\n",
    "\n",
    "要使用该工具，你应当在浏览器中打开上述链接，然后将你的 pickle 文件拖拽到页面中。\n",
    "\n",
    "现在你将使用 PyTorch profiler 来分析你模型的内存使用情况。\n",
    "\n",
    "---\n",
    "\n",
    "### Problem（memory profiling）：4 分\n",
    "\n",
    "对 **表 1 中的 2.7B 模型**进行 profiling，分析其 **前向传播（forward pass）**、**反向传播（backward pass）** 以及 **优化器步骤（optimizer step）**，上下文长度分别为 **128、256 和 512**。\n",
    "\n",
    "---\n",
    "\n",
    "**(a)**\n",
    "在你的 profiling 脚本中加入一个选项，使模型运行时能够经过内存分析器。\n",
    "你可能需要复用之前的一些基础设施（例如：激活 mixed-precision、加载特定的模型大小等）。\n",
    "\n",
    "然后，在以下两种情况下，为 2.7B 模型获取内存 profile：\n",
    "\n",
    "* 只做推理（仅前向传播）\n",
    "* 完整训练一步（前向 + 反向 + 优化器步骤）\n",
    "\n",
    "你的内存时间线看起来如何？你能否根据看到的峰值判断当前运行的是哪个阶段？\n",
    "\n",
    "**Deliverable：**\n",
    "来自 `pytorch.org/memory_viz` 工具中 **“Active Memory Timeline”** 的两张截图（针对 context length = 512）：\n",
    "\n",
    "* 一张是仅前向传播\n",
    "* 一张是完整训练步骤（前向和反向传播，不含 optimizer step），\n",
    "  并附上一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(b)**\n",
    "在仅进行前向传播时，不同 context length 下的峰值内存使用是多少？\n",
    "在执行完整训练步骤时又是多少？\n",
    "\n",
    "**Deliverable：**\n",
    "一张表格，每个 context length 对应两个数值。\n",
    "\n",
    "---\n",
    "\n",
    "**(c)**\n",
    "在使用 **mixed-precision** 的情况下，找出 2.7B 模型在以下两种情形下的峰值内存使用：\n",
    "\n",
    "* 前向传播\n",
    "* 完整优化器步骤\n",
    "\n",
    "Mixed-precision 是否显著影响了内存使用？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(d)**\n",
    "考虑 2.7B 模型。给定我们的参考超参数，在 **single-precision** 下，\n",
    "Transformer 残差流（residual stream）中 **激活张量（activations）** 的大小是多少？\n",
    "\n",
    "请以 **MB** 为单位给出结果（即用字节数除以 (1024^2)）。\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的回答，并给出你的推导过程。\n",
    "\n",
    "---\n",
    "\n",
    "**(e)**\n",
    "仔细查看 `pytorch.org/memory_viz` 中，2.7B 模型在前向传播时的 **“Active Memory Timeline”**。\n",
    "\n",
    "当你将 **“Detail”** 级别调低时，工具会显示最小分配的快照\n",
    "（例如，将 “Detail” 调到 **10%**，表示展示 **10% 最大的分配**）。\n",
    "\n",
    "最大的分配大小是多少？\n",
    "查看其调用栈（stack trace），你能判断这些分配来自哪里吗？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的文字说明。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import Callable\n",
    "import gc\n",
    "def memory_profile(\n",
    "    description: str,\n",
    "    run,\n",
    "    num_warmups: int = 0,\n",
    "    out_dir: str = \"memory_snapshots\",\n",
    "):\n",
    "    assert torch.cuda.is_available(), \"Memory profiling requires CUDA\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    snapshot_path = os.path.join(out_dir, f\"{description}.pickle\")\n",
    "\n",
    "    # ----------------------\n",
    "    # Warm-up（不记录）\n",
    "    # ----------------------\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # ----------------------\n",
    "    # 开始记录内存历史（LEGACY SAFE）\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._record_memory_history(True)\n",
    "\n",
    "    # ----------------------\n",
    "    # 被 profile 的运行\n",
    "    # ----------------------\n",
    "    run()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # ----------------------\n",
    "    # 导出 snapshot\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._dump_snapshot(snapshot_path)\n",
    "\n",
    "    # ----------------------\n",
    "    # 停止记录\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._record_memory_history(False)\n",
    "\n",
    "    print(f\"[Memory snapshot saved] {snapshot_path}\")\n",
    "\n",
    "    return snapshot_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(num_steps = 1):\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    batch_size = 32\n",
    "    \n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    \n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    warmup_iters = 200\n",
    "    cosine_cycle_iters = 10_000\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        #print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "        #  清理显存引用\n",
    "        try:\n",
    "            del X, attention_fn, out, loss\n",
    "        except NameError:\n",
    "            pass\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    # 7日志\n",
    "    if it % 100 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dee061ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8345 | lr 0.00e+00\n",
      "[Memory snapshot saved] memory_snapshots/前向传播.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'memory_snapshots/前向传播.pickle'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_profile(description=\"前向传播\",run=run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dbd6",
   "metadata": {},
   "source": [
    "# Flash Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4215642",
   "metadata": {},
   "source": [
    "\n",
    "# 作业 五\n",
    "\n",
    "你的性能分析结果很可能表明，在注意力层中，无论是**内存**还是**计算**方面，都存在优化空间。从高层次来看，注意力操作由一次矩阵乘法、一次 softmax，然后再一次矩阵乘法组成：\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\mathrm{mask}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\\right)V\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "朴素的注意力实现需要为每个 batch / head 元素保存形状为\n",
    "\n",
    "${seqlen} \\times {seqlen}$ 的注意力分数矩阵。随着序列长度变长，这个矩阵会迅速变得非常大，从而在长输入或长输出任务中导致 **out-of-memory (OOM)** 错误。\n",
    "我们将实现一个遵循 **FlashAttention-2** 论文的注意力 kernel，它通过 **按 tile 方式计算注意力**，避免显式地物化 ${seqlen} \\times {seqlen}$ 的注意力分数矩阵，从而支持更长的序列长度。\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem (pytorch_attention): 2 points**\n",
    "\n",
    "#### (a) 在不同规模下对你的注意力实现进行基准测试。编写一个脚本来完成以下任务：\n",
    "\n",
    "(a) 将 batch size 固定为 **8**，并且 **不使用多头注意力**（即移除 head 这一维）。\n",
    "\n",
    "(b) 遍历如下笛卡尔积配置：\n",
    "\n",
    "* head embedding 维度 $d_{\\text{model}} \\in {16, 32, 64, 128}$\n",
    "* 序列长度 ${seqlen} \\in {256, 1024, 4096, 8192, 16384}$\n",
    "\n",
    "(c) 为对应的尺寸生成随机输入 (Q, K, V)。\n",
    "\n",
    "(d) 使用这些输入，对注意力前向传播 **计时 100 次**。\n",
    "\n",
    "(e) 测量在 **反向传播开始之前** 的显存使用量，并对 **反向传播计时 100 次**。\n",
    "\n",
    "(f) 确保进行 **warm-up**，并在每次前向 / 反向传播之后调用\n",
    "`torch.cuda.synchronize()`。\n",
    "\n",
    "---\n",
    "\n",
    "请报告你在这些配置下得到的 **时间结果（或 out-of-memory 错误）**。\n",
    "**在什么规模下会出现 out-of-memory？**\n",
    "\n",
    "请选择你发现的 **最小的、仍然会 OOM 的配置之一**，对注意力的内存使用进行详细的**显存占用核算**（你可以使用 Assignment 1 中 Transformer 内存使用的公式）。\n",
    "\n",
    "* 反向传播中节省的显存会如何随着序列长度变化？\n",
    "* 你会如何 **消除反向传播中的这部分内存开销**？\n",
    "\n",
    "---\n",
    "\n",
    "### **Deliverable（提交内容）**\n",
    "\n",
    "* 一张包含 **时间测量结果** 的表格\n",
    "* 你对 **内存使用的推导过程**\n",
    "* 一段 **1–2 段的文字分析回答**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "707a115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头、朴素自注意力实现\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 线性投影（Q, K, V）\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        return: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model\n",
    "\n",
    "        # 1. 线性投影\n",
    "        Q = self.q_proj(x)  # (B, T, D)\n",
    "        K = self.k_proj(x)  # (B, T, D)\n",
    "        V = self.v_proj(x)  # (B, T, D)\n",
    "\n",
    "        # 2. 注意力分数 (B, T, T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "\n",
    "        # 3. softmax\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # 4. 加权求和\n",
    "        out = torch.matmul(attn, V)  # (B, T, D)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3cf245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OOM: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OOM: d=32, T=16384\n",
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OOM: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OOM: d=128, T=16384\n",
      "[{'d_model': 16, 'seqlen': 256, 'forward_time_ms': 0.4015684127807617, 'backward_time_ms': 1.744985580444336, 'mem_MB': 1185.04736328125}, {'d_model': 16, 'seqlen': 1024, 'forward_time_ms': 1.4081478118896484, 'backward_time_ms': 4.13205623626709, 'mem_MB': 1217.29736328125}, {'d_model': 16, 'seqlen': 4096, 'forward_time_ms': 18.352532386779785, 'backward_time_ms': 50.609660148620605, 'mem_MB': 1706.70361328125}, {'d_model': 16, 'seqlen': 8192, 'forward_time_ms': 1144.6343421936035, 'backward_time_ms': 3062.2467041015625, 'mem_MB': 3254.29736328125}, {'d_model': 16, 'seqlen': 16384, 'OOM': True}, {'d_model': 32, 'seqlen': 256, 'forward_time_ms': 0.21080970764160156, 'backward_time_ms': 1.3709783554077148, 'mem_MB': 1185.81494140625}, {'d_model': 32, 'seqlen': 1024, 'forward_time_ms': 1.080942153930664, 'backward_time_ms': 3.2602548599243164, 'mem_MB': 1220.31494140625}, {'d_model': 32, 'seqlen': 4096, 'forward_time_ms': 15.6574010848999, 'backward_time_ms': 48.03934097290039, 'mem_MB': 1718.31494140625}, {'d_model': 32, 'seqlen': 8192, 'forward_time_ms': 1265.7791376113892, 'backward_time_ms': 2883.9706420898438, 'mem_MB': 3278.31494140625}, {'d_model': 32, 'seqlen': 16384, 'OOM': True}, {'d_model': 64, 'seqlen': 256, 'forward_time_ms': 0.3102540969848633, 'backward_time_ms': 1.2479066848754883, 'mem_MB': 1187.38525390625}, {'d_model': 64, 'seqlen': 1024, 'forward_time_ms': 1.239013671875, 'backward_time_ms': 3.4499645233154297, 'mem_MB': 1226.79150390625}, {'d_model': 64, 'seqlen': 4096, 'forward_time_ms': 16.402578353881836, 'backward_time_ms': 50.11599063873291, 'mem_MB': 1742.38525390625}, {'d_model': 64, 'seqlen': 8192, 'forward_time_ms': 1110.0043058395386, 'backward_time_ms': 2907.641077041626, 'mem_MB': 3326.38525390625}, {'d_model': 64, 'seqlen': 16384, 'OOM': True}, {'d_model': 128, 'seqlen': 256, 'forward_time_ms': 0.28061866760253906, 'backward_time_ms': 1.4456987380981445, 'mem_MB': 1190.66650390625}, {'d_model': 128, 'seqlen': 1024, 'forward_time_ms': 1.6154289245605469, 'backward_time_ms': 4.666328430175781, 'mem_MB': 1238.66650390625}, {'d_model': 128, 'seqlen': 4096, 'forward_time_ms': 20.27733325958252, 'backward_time_ms': 61.95337772369385, 'mem_MB': 1790.66650390625}, {'d_model': 128, 'seqlen': 8192, 'forward_time_ms': 1159.9766969680786, 'backward_time_ms': 2989.28861618042, 'mem_MB': 3422.66650390625}, {'d_model': 128, 'seqlen': 16384, 'OOM': True}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "                X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "         \n",
    "                attention_fn = NaiveSelfAttention(d_model= d_model).to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                # warm-up\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                # forward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.time() - start) / 10\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 10\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "          \n",
    "            finally:\n",
    "                #  清理显存引用\n",
    "                try:\n",
    "                    del X, attention_fn, out, loss\n",
    "                except NameError:\n",
    "                    pass\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    return results\n",
    "benchmark_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0e5d",
   "metadata": {},
   "source": [
    "**精确核算**。\n",
    "\n",
    "## 已知输入张量形状\n",
    "\n",
    "```python\n",
    "x.shape = [8, 8192, 16]\n",
    "dtype = torch.float32  # 默认\n",
    "```\n",
    "\n",
    "* batch size (B = 8)\n",
    "* sequence length (T = 8192)\n",
    "* hidden dim (D = 16)\n",
    "* float32 = **4 bytes**\n",
    "\n",
    "---\n",
    "\n",
    "##  元素总数\n",
    "\n",
    "$$\n",
    "8 \\times 8192 \\times 16 = 1{,}048{,}576 \\text{ elements}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 显存占用（仅 input x）\n",
    "\n",
    "$$\n",
    "1{,}048{,}576 \\times 4 \\text{ bytes}\n",
    "= 4{,}194{,}304 \\text{ bytes}\n",
    "\\approx 4.0 \\text{ MB}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> **输入 `torch.Size([8, 8192, 16])` 本身仅占约 `4 MB` 显存**\n",
    "\n",
    "\n",
    "真正爆显存的是注意力里的\n",
    "\n",
    "### attention score 矩阵\n",
    "\n",
    "```python\n",
    "scores.shape = [8, 8192, 8192]\n",
    "```\n",
    "\n",
    "元素数：\n",
    "\n",
    "$$\n",
    "8 \\times 8192^2 = 536{,}870{,}912\n",
    "$$\n",
    "\n",
    "显存：\n",
    "\n",
    "$$\n",
    "536{,}870{,}912 \\times 4 \\approx 2.0 \\text{ GB}\n",
    "$$\n",
    "\n",
    "而且：\n",
    "\n",
    "* scores 要存\n",
    "* softmax 输出要存\n",
    "* backward 还要再存一份中间量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cbd65",
   "metadata": {},
   "source": [
    "我们将在这里实现一个flash attention，我们需要实现两个部分，一个是分块，第二个是数值稳定softmax的方式，思想上非常接近 FlashAttention v1，具体可以看第六章：GPU和GPU相关的优化，有详细介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f7859",
   "metadata": {},
   "source": [
    "# 作业六\n",
    "Problem (torch_compile): 2 points\n",
    "------------------------------------------------------------\n",
    "\n",
    "(a) 扩展你的 attention 基准测试脚本，\n",
    "    加入你使用 PyTorch 实现的 attention 的\n",
    "    **编译版本（compiled version）**，\n",
    "    并在与上一个 pytorch_attention 问题\n",
    "    相同的配置下，将其性能与未编译版本进行比较。\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比你 **编译后的 attention 模块**\n",
    "    与 **pytorch_attention 问题中未编译版本**\n",
    "    在前向传播和反向传播上的耗时。\n",
    "\n",
    "(b) 接下来，在端到端的基准测试脚本中，\n",
    "    将你的整个 Transformer 模型进行编译。\n",
    "    前向传播的性能会如何变化？\n",
    "    那么 **前向 + 反向传播 + 优化器更新步骤**\n",
    "    的整体性能又会如何变化？\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比 **原始（vanilla）**\n",
    "    与 **编译后（compiled）** 的 Transformer 模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52a54e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OOM: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OOM: d=32, T=16384\n",
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OOM: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OOM: d=128, T=16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d_model': 16,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 129.60939407348633,\n",
       "  'backward_time_ms': 1.1349678039550781,\n",
       "  'mem_MB': 292.1552734375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 118.07444095611572,\n",
       "  'backward_time_ms': 3.0669689178466797,\n",
       "  'mem_MB': 292.533203125},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 121.02212905883789,\n",
       "  'backward_time_ms': 29.390263557434082,\n",
       "  'mem_MB': 294.0361328125},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 172.49445915222168,\n",
       "  'backward_time_ms': 594.4693088531494,\n",
       "  'mem_MB': 296.0390625},\n",
       " {'d_model': 16, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 133.33213329315186,\n",
       "  'backward_time_ms': 1.1520147323608398,\n",
       "  'mem_MB': 292.2890625},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 120.98090648651123,\n",
       "  'backward_time_ms': 3.391122817993164,\n",
       "  'mem_MB': 293.05078125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 132.49444961547852,\n",
       "  'backward_time_ms': 29.76512908935547,\n",
       "  'mem_MB': 296.0625},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 154.30927276611328,\n",
       "  'backward_time_ms': 617.0315504074097,\n",
       "  'mem_MB': 300.07421875},\n",
       " {'d_model': 32, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 119.28505897521973,\n",
       "  'backward_time_ms': 2.0403623580932617,\n",
       "  'mem_MB': 292.57421875},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 113.94946575164795,\n",
       "  'backward_time_ms': 1.7974615097045898,\n",
       "  'mem_MB': 294.12109375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 119.61464881896973,\n",
       "  'backward_time_ms': 26.551437377929688,\n",
       "  'mem_MB': 300.16796875},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 156.477952003479,\n",
       "  'backward_time_ms': 587.8675937652588,\n",
       "  'mem_MB': 308.21484375},\n",
       " {'d_model': 64, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 123.97098541259766,\n",
       "  'backward_time_ms': 2.677583694458008,\n",
       "  'mem_MB': 293.21484375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 120.5125093460083,\n",
       "  'backward_time_ms': 5.651998519897461,\n",
       "  'mem_MB': 296.40234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 133.5397720336914,\n",
       "  'backward_time_ms': 33.28080177307129,\n",
       "  'mem_MB': 308.58984375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 179.05967235565186,\n",
       "  'backward_time_ms': 1049.7406244277954,\n",
       "  'mem_MB': 324.77734375},\n",
       " {'d_model': 128, 'seqlen': 16384, 'OOM': True}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import time\n",
    "DTYPES = torch.float32\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "import torch._inductor.config as inductor_config\n",
    "inductor_config.triton.cudagraphs = False\n",
    "\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "                X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                model = NaiveSelfAttention(d_model)\n",
    "                attention_fn = torch.compile(\n",
    "\n",
    "                                            model,\n",
    "                                            mode=\"max-autotune\"\n",
    "\n",
    "                                            ).to(device).to(DTYPES)\n",
    "                torch.cuda.empty_cache()\n",
    "                # warm-up\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # forward timing\n",
    "                    start = time.time()\n",
    "                    for _ in range(10):\n",
    "                        out = attention_fn(X)\n",
    "                    torch.cuda.synchronize()\n",
    "                    forward_time = (time.time() - start) / 10\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 10\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "            #  清理显存引用\n",
    "            try:\n",
    "                del X, attention_fn, out, loss\n",
    "            except NameError:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "benchmark_attention()\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6a2aa563",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e02546",
   "metadata": {},
   "source": [
    "请参考图上图，了解 tile 的示意图以及如何推进块指针。上述的加权求和函数如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import rearrange, einsum\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,       # 输入指针\n",
    "    output_ptr,              # 输出指针\n",
    "    x_stride_row, x_stride_dim,  # strides 告诉我们如何在张量的每个轴上移动一个元素\n",
    "    weight_stride_dim,       # 很可能是 1\n",
    "    output_stride_row,       # 很可能是 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # tile 的形状必须在编译时已知\n",
    "):\n",
    "    # 每个实例将计算 x 的一块行的加权和\n",
    "    # `tl.program_id` 给我们一个方法来检查当前线程块的编号\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # block 指针给我们一个方法，从 ND 内存区域中选择数据\n",
    "    # 并移动选择位置\n",
    "    # block 指针必须知道：\n",
    "    # - 张量第一个元素的指针\n",
    "    # - 张量的整体形状以处理越界访问\n",
    "    # - 每个维度的步长，用于使用内存布局属性\n",
    "    # - 起始块的ND坐标，即“偏移量”\n",
    "    # - 一次使用/存储的块形状\n",
    "    # - 内存中主要到次要的维度顺序\n",
    "    # axes（=np.argsort(strides)）用于优化，特别是在H100上\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D,),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # 初始化一个缓冲区以写入到输出\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    for i in range(tl.div(D, D_TILE_SIZE)):\n",
    "        # 加载当前块指针\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，且D_TILE_SIZE可能不能整除D，\n",
    "        # 我们需要为两个维度进行边界检查\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "\n",
    "        # 计算行的加权和。\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "\n",
    "        # 移动到下一个块。\n",
    "        # 这些是（行，列）坐标的增量\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # 在最后一个维度移动D_TILE_SIZE\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # 移动D_TILE_SIZE\n",
    "\n",
    "        # 将输出写入输出块指针（每行一个标量）\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，我们需要边界检查\n",
    "        tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "\n",
    "# 现在我们将这个内核包装在一个PyTorch自动微分函数中，该函数将与PyTorch（即，接受张量作为输入，输出张量，并在反向传播期间与自动微分引擎一起工作）一起解释\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # 在反向传递中缓存x和weight，当我们\n",
    "        # 只接收到输出张量的梯度时，将使用它们\n",
    "        # 需要计算x和weight的梯度。\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # 将输入张量重塑为2D\n",
    "        input_shape = x.shape\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"维度不匹配\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"期望CUDA张量\"\n",
    "        assert x.is_contiguous(), \"我们的指针算术将假设x是连续的\"\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # 大约16次循环遍历嵌入维度\n",
    "        ctx.ROWS_TILE_SIZE = 16  # 每个线程同时处理16个批次元素\n",
    "        ctx.input_shape = input_shape\n",
    "\n",
    "        # 需要初始化一个空的结果张量。注意这些元素不一定是0！\n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "\n",
    "        # 在我们的1D网格中启动我们的内核，有n个实例。\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e917aa",
   "metadata": {},
   "source": [
    "这段代码实现了一个 **自定义 Triton GPU 算子**，用来完成“对二维矩阵的每一行做加权求和”这一操作，并通过 PyTorch 的 `autograd.Function` 封装，使其可以像普通 PyTorch 函数一样前向/反向传播\n",
    "\n",
    "------------------------------------------------\n",
    "一、Triton 内核 `weighted_sum_fwd`\n",
    "------------------------------------------------\n",
    "1. 并行划分  \n",
    "   `@triton.jit` 把函数编译成 CUDA kernel。  \n",
    "   启动时只给了一维 grid：`weighted_sum_fwd[(grid_dim,)](...)`，  \n",
    "   所以 `tl.program_id(0)` 就是当前线程块（program instance）在一维网格里的编号，对应“第几个行块”。\n",
    "\n",
    "2. 块指针（block pointer）  \n",
    "   `tl.make_block_ptr` 把“张量指针 + 形状/步长/偏移”包装成一个 **可滑动窗口**。  \n",
    "   `x_ptr` 指向二维矩阵 `(ROWS, D)`  \n",
    "   `weight_ptr` 指向一维向量 `(D,)`  \n",
    "   `output_ptr` 指向输出向量 `(ROWS,)`  \n",
    "   窗口大小由 `ROWS_TILE_SIZE`（=16）和 `D_TILE_SIZE`（编译时常量）决定。  \n",
    "   `order` 告诉 Triton 哪一维在内存里是主序（row-major 对应 `(1,0)`）。\n",
    "\n",
    "3. 主循环  \n",
    "   沿特征维 `D` 以 `D_TILE_SIZE` 为步长滑动窗口：  \n",
    "   - 加载当前 tile：  \n",
    "     `row` 形状 `(16, D_TILE_SIZE)`  \n",
    "     `weight` 形状 `(D_TILE_SIZE,)`  \n",
    "   - 计算：  \n",
    "     `row * weight[None, :]` → 广播成 `(16, D_TILE_SIZE)`  \n",
    "     `tl.sum(..., axis=1)` → 对每行求和，得到 `(16,)`  \n",
    "     累加到 `output` 寄存器。  \n",
    "   - 移动窗口：  \n",
    "     `x_block_ptr.advance((0, D_TILE_SIZE))`  \n",
    "     `weight_block_ptr.advance((D_TILE_SIZE,))`  \n",
    "   循环结束后 `output` 里就是 **这 16 行的最终结果**。\n",
    "\n",
    "4. 写回全局内存  \n",
    "   `tl.store(output_block_ptr, output, boundary_check=(0,))`  \n",
    "   如果总行数不能被 16 整除，最末一块会越界，`boundary_check` 自动屏蔽无效元素。\n",
    "\n",
    "------------------------------------------------\n",
    "二、PyTorch 封装 `WeightedSumFunc`\n",
    "------------------------------------------------\n",
    "1. 参数整理  \n",
    "   - `x` 可以是任意阶张量，最后一维是特征 `D`；前面所有维度当成“行”。  \n",
    "   - `weight` 是一维向量，长度必须等于 `D`。  \n",
    "   用 `einops.rearrange` 把 `x` 压成二维 `(n_rows, D)`，方便内核处理。\n",
    "\n",
    "2. 静态超参  \n",
    "   - `ROWS_TILE_SIZE = 16`  \n",
    "   - `D_TILE_SIZE` 取 `≥D/16` 且为 2 的幂，保证循环次数≈16 次，兼顾寄存器占用与并行度。\n",
    "\n",
    "3. 输出张量  \n",
    "   `y = torch.empty(output_dims)` 形状与 `x[..., 0]` 相同。\n",
    "\n",
    "4. 启动 kernel  \n",
    "   grid 大小 = `ceil(n_rows / 16)`，每个线程块负责 16 行。  \n",
    "   把张量底层指针、步长、行列数等全部传进 Triton 内核。\n",
    "\n",
    "5. 返回  \n",
    "   把二维结果再 `view` 回原来的 batch 形状，前端用户感知不到内存重排。\n",
    "\n",
    "------------------------------------------------\n",
    "三、整体流程\n",
    "------------------------------------------------\n",
    "```python\n",
    "out = WeightedSumFunc.apply(x, weight)\n",
    "```\n",
    "\n",
    "等价于  \n",
    "```python\n",
    "out = (x * weight).sum(dim=-1)\n",
    "```\n",
    "\n",
    "但 **完全跑在 GPU 上，手工调度共享内存/寄存器，比 PyTorch 原生 reduce 更快**，尤其当 `D` 很大、batch 很多时。后续只要再补一个 `backward` 函数，就可以像普通 `nn.Module` 一样训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c8f8",
   "metadata": {},
   "source": [
    "### **FlashAttention 中「分块（blocking）」与「数值稳定 softmax」为什么能带来加速**\n",
    "\n",
    "\n",
    "### 一、加速原理\n",
    "\n",
    "FlashAttention 的加速**不是因为算得更少 FLOPs**，而是因为：\n",
    "\n",
    "> **极大减少了显存读写（memory I/O），并将计算变成“流式（streaming）”**\n",
    "\n",
    "核心公式仍然是论文中的公式：\n",
    "\n",
    "$$\n",
    "\\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "但**计算路径完全改变**。\n",
    "\n",
    "####  标准 Attention 的真实成本\n",
    "\n",
    "以 sequence length = (N)，head dim = (d)：\n",
    "\n",
    "| 操作      | FLOPs      | 显存占用          |\n",
    "| ------- | ---------- | ------------- |\n",
    "| QKᵀ     | (O(N^2 d)) | **存 N×N**     |\n",
    "| softmax | (O(N^2))   | **读 + 写 N×N** |\n",
    "| AV      | (O(N^2 d)) | 读 N×N         |\n",
    "\n",
    "**关键瓶颈不是算力，而是显存带宽**\n",
    "\n",
    "在 GPU 上：N×N的attention matrix **必须写入 HBM**，* 然后再 **从 HBM 读回** 做 softmax，大模型场景下 **显存访问 >> 计算时间**。\n",
    "\n",
    "#### GPU 性能的现实约束\n",
    "\n",
    "GPU 有三种“速度等级”存储：\n",
    "\n",
    "| 存储            | 速度 | 容量 |\n",
    "| ------------- | -- | -- |\n",
    "| Register      | 极快 | 极小 |\n",
    "| Shared Memory | 快  | 小  |\n",
    "| HBM (显存)      | 慢  | 很大 |\n",
    "\n",
    "标准 Attention 的问题是：\n",
    "\n",
    "> **N×N attention matrix 太大，只能进 HBM**\n",
    "\n",
    "#### 分块（Blocking）为什么能加速\n",
    "\n",
    "**分块的本质就是让数据“活在片上”**\n",
    "\n",
    "FlashAttention 将 Attention 重写为：\n",
    "\n",
    "```text\n",
    "for each Q_block:\n",
    "    for each K/V_block:\n",
    "        计算局部 QK^T\n",
    "        立刻参与 softmax\n",
    "        立刻乘 V\n",
    "```\n",
    "\n",
    "核心变化是不再生成完整的 QKᵀ，不再写入 N×N matrix，局部结果 **只存在寄存器 / shared memory**。从**全文总结变成了走一步看一步**。\n",
    "\n",
    "#### 分块前 vs 分块后（显存行为对比）\n",
    "\n",
    "**标准 Attention**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  写 QK^T (N×N)\n",
    "  读 QK^T\n",
    "  写 softmax(QK^T)\n",
    "  读 softmax(QK^T)\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N²)**\n",
    "\n",
    "---\n",
    "\n",
    "**FlashAttention（分块）**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  读 Q_block\n",
    "  读 K_block, V_block\n",
    "  写 O_block\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N·d)**\n",
    "\n",
    "**这是数量级的下降**\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么“只对 K/V 分块也有效”\n",
    "\n",
    "分块有两种实现：\n",
    "\n",
    "* Q 不分块\n",
    "* Q 分块（论文 / CUDA 版）\n",
    "\n",
    "两者都满足：\n",
    "\n",
    "> **K/V 被流式读取，attention matrix 不落地**\n",
    "\n",
    "Q 分块是为了，提高 occupancy，并且控制寄存器使用，适配 CUDA block 结构。\n",
    "\n",
    "---\n",
    "\n",
    "#### 二、在线 softmax 的加速原理”\n",
    "\n",
    "**标准 softmax 的结构问题**\n",
    "\n",
    "标准 softmax：\n",
    "\n",
    "```python\n",
    "S = QK^T\n",
    "P = exp(S) / sum(exp(S))\n",
    "```\n",
    "\n",
    "问题在于softmax **必须看完整一行**，无法流式，依赖全量 S。\n",
    "\n",
    "**这强制我们先存下整个 QKᵀ**\n",
    "\n",
    "\n",
    "**running max / running sum 的革命性意义**\n",
    "\n",
    "FlashAttention 使用：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_i &= \\max_j S_{ij} \\\n",
    "l_i &= \\sum_j e^{S_{ij} - m_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "并改写为 **可递推形式**：\n",
    "\n",
    "$$\n",
    "m_i^{(t)} = \\max(m_i^{(t-1)}, \\max S_{ij}^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "l_i^{(t)} =\n",
    "l_i^{(t-1)} e^{m_i^{(t-1)} - m_i^{(t)}} +\n",
    "\\sum e^{S_{ij}^{(t)} - m_i^{(t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "这一步“决定了 FlashAttention 能不能存在”，它带来了三件**决定性好处**，将softmax 变成 **在线算法**。\n",
    "\n",
    "再这个算法中我们不需要完整 S，每来一个 block 就能更新，天然适配 streaming。\n",
    "\n",
    "**softmax 与 AV 可以融合**\n",
    "\n",
    "标准 Attention：\n",
    "\n",
    "```text\n",
    "QK^T → softmax → @V\n",
    "```\n",
    "\n",
    "FlashAttention：\n",
    "\n",
    "```text\n",
    "(QK^T → exp → sum → @V)  全部在 block 内完成\n",
    "```\n",
    "\n",
    "**算子融合（kernel fusion）成为可能**\n",
    "\n",
    "\n",
    "**backward 也不需要 attention matrix**\n",
    "\n",
    "由于 forward 保存了：\n",
    "\n",
    "* `L = logsumexp`\n",
    "* `O = softmax @ V`\n",
    "\n",
    "backward 可写成：\n",
    "\n",
    "$$\n",
    "dS_{ij} = P_{ij} (dP_{ij} - D_i)\n",
    "$$\n",
    "\n",
    "**反向同样 O(N) 显存**\n",
    "\n",
    "> FlashAttention 的加速不是减少计算，而是通过 **分块 + 在线 softmax**，\n",
    "> **避免了 N×N attention matrix 的显存读写**，\n",
    "> 将 attention 计算变成一个 **完全流式、可融合的过程**，\n",
    "> 从而把瓶颈从显存带宽转回到算力。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cc2ba",
   "metadata": {},
   "source": [
    "# 作业七：使用pytorch实现flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a41b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    FlashAttention（forward + backward）\n",
    "\n",
    "    输入：\n",
    "        Q: (B, Nq, d)\n",
    "        K: (B, Nk, d)\n",
    "        V: (B, Nk, d)\n",
    "\n",
    "    输出：\n",
    "        O: (B, Nq, d)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        # -----------------------------\n",
    "        # block size\n",
    "        # -----------------------------\n",
    "        Bq = 64  # Query block 的尺寸\n",
    "        Bk = 64  # Key / Value block 的尺寸\n",
    "\n",
    "        # 我们将KV矩阵分成一个个小矩阵\n",
    "\n",
    "        Tq = Nq // Bq  # Q block 数\n",
    "        Tk = Nk // Bk  # K/V block 数\n",
    "\n",
    "        scale = d ** -0.5\n",
    "\n",
    "        # -----------------------------\n",
    "        # 输出与中间量\n",
    "        # -----------------------------\n",
    "        O = torch.zeros_like(Q)              # Attention 输出\n",
    "        L = torch.zeros(B, Nq, device=Q.device)  # log-sum-exp（给 backward 用）\n",
    "\n",
    "        # =========================================================\n",
    "        # 对 batch 维度显式循环\n",
    "        # =========================================================\n",
    "        for b in range(B):\n",
    "            Q_b = Q[b]  # (Nq, d)\n",
    "            K_b = K[b]  # (Nk, d)\n",
    "            V_b = V[b]  # (Nk, d)\n",
    "\n",
    "            # =====================================================\n",
    "            # 外层循环：Query block\n",
    "            # =====================================================\n",
    "            for i in range(Tq):\n",
    "                q_i = Q_b[i * Bq:(i + 1) * Bq]  # (Bq, d)\n",
    "\n",
    "                # -----------------------------\n",
    "                # FlashAttention 核心状态\n",
    "                # -----------------------------\n",
    "                m = torch.full((Bq, 1), -float(\"inf\"), device=Q.device)  # running max\n",
    "                l = torch.zeros((Bq, 1), device=Q.device)               # running sum\n",
    "                O_acc = torch.zeros((Bq, d), device=Q.device)           # 输出累积（未归一化）\n",
    "\n",
    "                # =================================================\n",
    "                # 内层循环：Key / Value block\n",
    "                # =================================================\n",
    "                for j in range(Tk):\n",
    "                    k_j = K_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "                    v_j = V_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 计算局部 attention score\n",
    "                    # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale  # (Bq, Bk)\n",
    "\n",
    "                    # 当前 block 内，每一行的最大值\n",
    "                    row_max = S.max(dim=1, keepdim=True).values  # (Bq, 1)\n",
    "\n",
    "                    # 更新 running max\n",
    "                    m_new = torch.maximum(m, row_max)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 数值稳定 softmax（FlashAttention 关键）\n",
    "                    # --------------------------------------------\n",
    "                    # 旧 block 贡献修正\n",
    "                    exp_old = torch.exp(m - m_new) * l\n",
    "\n",
    "                    # 当前 block 的 exp\n",
    "                    P = torch.exp(S - m_new)\n",
    "\n",
    "                    # 更新分母\n",
    "                    l = exp_old + P.sum(dim=1, keepdim=True)\n",
    "\n",
    "                    # 更新未归一化输出\n",
    "                    O_acc = (\n",
    "                        torch.exp(m - m_new) * O_acc +\n",
    "                        P @ v_j\n",
    "                    )\n",
    "\n",
    "                    # 写回 running max\n",
    "                    m = m_new\n",
    "\n",
    "                # =================================================\n",
    "                # block 内 softmax 归一化\n",
    "                # =================================================\n",
    "                O_i = O_acc / l  # (Bq, d)\n",
    "\n",
    "                # log-sum-exp：L = m + log(l)\n",
    "                L_i = m.squeeze(1) + torch.log(l.squeeze(1))\n",
    "\n",
    "                # 写回全局输出\n",
    "                O[b, i * Bq:(i + 1) * Bq] = O_i\n",
    "                L[b, i * Bq:(i + 1) * Bq] = L_i\n",
    "\n",
    "        # 保存 backward 所需变量\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.scale = scale\n",
    "        ctx.Bq = Bq\n",
    "        ctx.Bk = Bk\n",
    "\n",
    "        return O\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, L = ctx.saved_tensors\n",
    "        scale = ctx.scale\n",
    "        Bq = ctx.Bq\n",
    "        Bk = ctx.Bk\n",
    "\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Tq = Nq // Bq\n",
    "        Tk = Nk // Bk\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # FlashAttention backward 中的重要中间量\n",
    "        # D_i = sum_j O_ij * dO_ij\n",
    "        # --------------------------------------------\n",
    "        D = torch.sum(O * dO, dim=-1)  # (B, Nq)\n",
    "\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        for b in range(B):\n",
    "            for i in range(Tq):\n",
    "                q_i = Q[b, i * Bq:(i + 1) * Bq]\n",
    "                dO_i = dO[b, i * Bq:(i + 1) * Bq]\n",
    "                L_i = L[b, i * Bq:(i + 1) * Bq]\n",
    "                D_i = D[b, i * Bq:(i + 1) * Bq]\n",
    "\n",
    "                for j in range(Tk):\n",
    "                    k_j = K[b, j * Bk:(j + 1) * Bk]\n",
    "                    v_j = V[b, j * Bk:(j + 1) * Bk]\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 重算局部 attention 概率 P_ij\n",
    "                    # P_ij = exp(S_ij - L_i)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale\n",
    "                    P = torch.exp(S - L_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dV --------\n",
    "                    dV[b, j * Bk:(j + 1) * Bk] += P.T @ dO_i\n",
    "\n",
    "                    # -------- dS --------\n",
    "                    dP = dO_i @ v_j.T\n",
    "                    dS = P * (dP - D_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dQ --------\n",
    "                    dQ[b, i * Bq:(i + 1) * Bq] += dS @ k_j * scale\n",
    "\n",
    "                    # -------- dK --------\n",
    "                    dK[b, j * Bk:(j + 1) * Bk] += dS.T @ q_i * scale\n",
    "\n",
    "        return dQ, dK, dV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0992c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：87.63003349304199ms\n",
      "正在使用cuda\n",
      "[FlashAttention] B=1, N=1024, d=64, time=87630.033 ms\n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                      Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    FlashAttentionAutograd        20.67%      28.515ms        99.98%     137.947ms     137.947ms             1  \n",
      "                                          aten::zeros_like         0.03%      41.979us        12.83%      17.707ms      17.707ms             1  \n",
      "                                          aten::empty_like         0.05%      68.855us         8.10%      11.169ms      11.169ms             1  \n",
      "                                       aten::empty_strided         8.05%      11.100ms         8.05%      11.100ms      11.100ms             1  \n",
      "                                               aten::zero_         0.10%     143.870us         5.07%       6.995ms     205.727us            34  \n",
      "                                               aten::fill_         0.22%     306.888us         5.21%       7.184ms     143.672us            50  \n",
      "                                          cudaLaunchKernel        37.66%      51.956ms        37.66%      51.956ms      11.040us          4706  \n",
      "                                               aten::zeros         0.09%     123.045us         0.61%     842.604us      25.533us            33  \n",
      "                                               aten::empty         0.24%     329.117us         0.24%     329.117us       6.717us            49  \n",
      "                                              aten::select         0.16%     222.160us         0.19%     256.725us       7.335us            35  \n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 137.968ms\n",
      "\n",
      "现在真正计时!\n",
      "单次耗时：0.17102559407552084ms\n",
      "正在使用cuda\n",
      "[NaiveAttention]  B=1, N=1024, d=64, time=171.026 ms\n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                      Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::transpose         0.09%      10.350us         0.11%      12.802us      12.802us             1  \n",
      "                                          aten::as_strided         0.06%       7.041us         0.06%       7.041us       1.408us             5  \n",
      "                                              aten::matmul         1.33%     156.529us        96.39%      11.367ms       5.683ms             2  \n",
      "                                              aten::expand         0.17%      19.721us         0.21%      24.310us       6.077us             4  \n",
      "                                             aten::reshape         0.36%      42.484us         0.48%      56.335us      14.084us             4  \n",
      "                                                aten::view         0.09%      10.646us         0.09%      10.646us       3.549us             3  \n",
      "                                      aten::_reshape_alias         0.03%       3.205us         0.03%       3.205us       3.205us             1  \n",
      "                                                 aten::bmm        90.31%      10.650ms        94.24%      11.113ms       5.557ms             2  \n",
      "    cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags         0.18%      21.679us         0.18%      21.679us      10.840us             2  \n",
      "                                      cudaFuncSetAttribute         0.03%       3.124us         0.03%       3.124us       1.562us             2  \n",
      "----------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.792ms\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_flash_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionAutograd.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_flash_attention',run = run)\n",
    "    t1 = profile(description ='test_flash_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttention] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [1024]:\n",
    "    test_flash_attention(B=1, N=N, d=64, device=device)\n",
    "    test_naive_attention(B=1, N=N, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a110c30",
   "metadata": {},
   "source": [
    "# 2. Triton 内核实现flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d049031",
   "metadata": {},
   "source": [
    "Triton 是一个用 Python 写 GPU kernel 的 DSL，本质上是“显式控制数据分块与内存层级的编程模型”，它不是NumPy，也不是高层 PyTorch API，它是一个 CUDA kernel 的“可读版本”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12251ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sympy import Q\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def flash_attention_fwd_kernel(\n",
    "    # -------- 全局内存指针 --------\n",
    "    Q_ptr, K_ptr, V_ptr,           # 输入 Q, K, V\n",
    "    O_ptr, L_ptr,                  # 输出 O, logsumexp L\n",
    "\n",
    "    # -------- stride 信息（用于 block ptr）--------\n",
    "    stride_qb, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vn, stride_vd,\n",
    "    stride_ob, stride_on, stride_od,\n",
    "    stride_lb, stride_ln,\n",
    "\n",
    "    # -------- 序列长度 --------\n",
    "    Nq, Nk,\n",
    "\n",
    "    # -------- scale --------\n",
    "    scale,\n",
    "\n",
    "    # -------- 编译期常量 --------\n",
    "    D: tl.constexpr,\n",
    "    Q_BLOCK: tl.constexpr,\n",
    "    K_BLOCK: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    每个 Triton program 负责：\n",
    "        - 一个 batch\n",
    "        - 一个 Query block（Q_BLOCK 行）\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Triton 并行索引\n",
    "    # ------------------------------------------------\n",
    "    q_block_id = tl.program_id(0)     # 第几个 Q block\n",
    "    batch_id   = tl.program_id(1)     # 第几个 batch\n",
    "\n",
    "    # =================================================\n",
    "    # 构造 block pointer（这是 Triton 的关键）\n",
    "    # =================================================\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_qn, stride_qd),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # ⚠️ K / V 从第 0 行开始，后面在 loop 中 advance\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K_ptr + batch_id * stride_kb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_kn, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V_ptr + batch_id * stride_vb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_vn, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O_ptr + batch_id * stride_ob,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_on, stride_od),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        base=L_ptr + batch_id * stride_lb,\n",
    "        shape=(Nq, 1),\n",
    "        strides=(stride_ln, 1),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # =================================================\n",
    "    # 加载 Query block\n",
    "    # =================================================\n",
    "    Q_i = tl.load(Q_block_ptr)  # (Q_BLOCK, D)\n",
    "\n",
    "    # =================================================\n",
    "    # FlashAttention 核心状态（每个 Q block 独立）\n",
    "    # =================================================\n",
    "    O_acc = tl.zeros((Q_BLOCK, D), dtype=tl.float32)       # 未归一化输出累积\n",
    "    L_acc = tl.zeros((Q_BLOCK, 1), dtype=tl.float32)      # softmax 分母\n",
    "    M_acc = tl.full((Q_BLOCK, 1), -float(\"inf\"), tl.float32)  # running max\n",
    "\n",
    "    # =================================================\n",
    "    # 内层循环：遍历所有 K/V block\n",
    "    # =================================================\n",
    "    for k_block_id in range(tl.cdiv(Nk, K_BLOCK)):\n",
    "        K_j = tl.load(K_block_ptr)   # (K_BLOCK, D)\n",
    "        V_j = tl.load(V_block_ptr)   # (K_BLOCK, D)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "        # ---------------------------------------------\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale  # (Q_BLOCK, K_BLOCK)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # Causal mask（编译期 if）\n",
    "        # ---------------------------------------------\n",
    "        if is_causal:\n",
    "            q_idx = q_block_id * Q_BLOCK + tl.arange(0, Q_BLOCK)[:, None]\n",
    "            k_idx = k_block_id * K_BLOCK + tl.arange(0, K_BLOCK)[None, :]\n",
    "            causal_mask = q_idx >= k_idx\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 数值稳定 softmax（FlashAttention 核心）\n",
    "        # ---------------------------------------------\n",
    "        M_block = tl.max(S_ij, axis=1, keep_dims=True)\n",
    "        M_new = tl.maximum(M_acc, M_block)\n",
    "\n",
    "        P_ij = tl.exp(S_ij - M_block)\n",
    "\n",
    "        L_new = (\n",
    "            tl.exp(M_acc - M_new) * L_acc +\n",
    "            tl.exp(M_block - M_new) * tl.sum(P_ij, axis=1, keep_dims=True)\n",
    "        )\n",
    "\n",
    "        # 类型对齐（Triton 细节）\n",
    "        P_cast = P_ij.to(V_block_ptr.type.element_ty)\n",
    "\n",
    "        O_new = (\n",
    "            tl.exp(M_acc - M_new) * O_acc +\n",
    "            tl.exp(M_block - M_new) * tl.dot(P_cast, V_j)\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 更新 running 状态\n",
    "        # ---------------------------------------------\n",
    "        M_acc = M_new\n",
    "        L_acc = L_new\n",
    "        O_acc = O_new\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 移动 K/V block 指针\n",
    "        # ---------------------------------------------\n",
    "        K_block_ptr = K_block_ptr.advance((K_BLOCK, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_BLOCK, 0))\n",
    "\n",
    "    # =================================================\n",
    "    # softmax 归一化\n",
    "    # =================================================\n",
    "    O_i = O_acc / L_acc\n",
    "    L_i = M_acc + tl.log(L_acc)\n",
    "\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(L_block_ptr, L_i)\n",
    "\n",
    "@triton.jit\n",
    "def flash_bwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr, dO_ptr, D_ptr,\n",
    "    dQ_ptr, dK_ptr, dV_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    stride_dob, stride_doq, stride_dod,\n",
    "    stride_db, stride_dq_d,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # program ids\n",
    "    k_tile_id = tl.program_id(0)\n",
    "    batch_id = tl.program_id(1)\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (Q-side)\n",
    "    # -------------------------\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_id * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dO_block_ptr = tl.make_block_ptr(\n",
    "        dO_ptr + batch_id * stride_dob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_doq, stride_dod),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_id * stride_lb,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_lq, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    D_block_ptr = tl.make_block_ptr(\n",
    "        D_ptr + batch_id * stride_db,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_dq_d, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (K/V-side)\n",
    "    # -------------------------\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dK_block_ptr = tl.make_block_ptr(\n",
    "        dK_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dV_block_ptr = tl.make_block_ptr(\n",
    "        dV_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # load K/V tile\n",
    "    # -------------------------\n",
    "    K_j = tl.load(K_block_ptr)  # (K_TILE_SIZE, D)\n",
    "    V_j = tl.load(V_block_ptr)\n",
    "\n",
    "    dK_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "    dV_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # loop over Q tiles\n",
    "    # -------------------------\n",
    "    for q_tile_id in range(tl.cdiv(N_QUERIES, Q_TILE_SIZE)):\n",
    "        Q_i = tl.load(Q_block_ptr)\n",
    "        O_i = tl.load(O_block_ptr)\n",
    "        dO_i = tl.load(dO_block_ptr)\n",
    "        L_i = tl.load(L_block_ptr)\n",
    "        D_i = tl.load(D_block_ptr)\n",
    "\n",
    "        # S = QK^T\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale\n",
    "\n",
    "        if is_causal:\n",
    "            q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "            k_idx = k_tile_id * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)[None, :]\n",
    "            S_ij = tl.where(q_idx >= k_idx, S_ij, -1e6)\n",
    "\n",
    "        # P = softmax\n",
    "        P_ij = tl.exp(S_ij - L_i)\n",
    "\n",
    "        # dV\n",
    "        dV_j += tl.dot(P_ij.to(V_j.dtype).T, dO_i)\n",
    "\n",
    "        # dP\n",
    "        dP_ij = tl.dot(dO_i, V_j.T)\n",
    "\n",
    "        # dS\n",
    "        dS_ij = P_ij * (dP_ij - D_i)\n",
    "        dS_ij = dS_ij * scale\n",
    "\n",
    "        # dQ (atomic add)\n",
    "        dQ_i = tl.dot(dS_ij, K_j)\n",
    "\n",
    "        q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "        d_idx = tl.arange(0, D)[None, :]\n",
    "        mask = (q_idx < N_QUERIES) & (d_idx < D)\n",
    "        dq_ptrs = dQ_ptr + batch_id * stride_qb + q_idx * stride_qq + d_idx * stride_qd\n",
    "        tl.atomic_add(dq_ptrs, dQ_i.to(dQ_ptr.dtype.element_ty), mask=mask)\n",
    "\n",
    "        # dK\n",
    "        dK_j += tl.dot(dS_ij.T, Q_i)\n",
    "\n",
    "        # advance Q-side blocks\n",
    "        Q_block_ptr = Q_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        O_block_ptr = O_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        dO_block_ptr = dO_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        L_block_ptr = L_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        D_block_ptr = D_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "\n",
    "    # -------------------------\n",
    "    # store dK / dV\n",
    "    # -------------------------\n",
    "    tl.store(dK_block_ptr, dK_j.to(dK_ptr.dtype.element_ty))\n",
    "    tl.store(dV_block_ptr, dV_j.to(dV_ptr.dtype.element_ty))\n",
    "\n",
    "\n",
    "\n",
    "class FlashAttentionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        \"\"\"\n",
    "        Q, K, V: (B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Nq, D = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Q_BLOCK = 32\n",
    "        K_BLOCK = 32\n",
    "\n",
    "        scale = D ** -0.5\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        L = torch.empty(B, Nq, device=Q.device)\n",
    "\n",
    "        grid = (Nq // Q_BLOCK, B)\n",
    "\n",
    "        flash_attention_fwd_kernel[grid](\n",
    "            Q, K, V, O, L,\n",
    "            Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "            K.stride(0), K.stride(1), K.stride(2),\n",
    "            V.stride(0), V.stride(1), V.stride(2),\n",
    "            O.stride(0), O.stride(1), O.stride(2),\n",
    "            L.stride(0), L.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_BLOCK=Q_BLOCK,\n",
    "            K_BLOCK=K_BLOCK,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, l = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "    \n",
    "        B, Nq, D = q.shape\n",
    "        Nk = k.shape[1]\n",
    "    \n",
    "        Bq = 32\n",
    "        Bk = 32\n",
    "        scale = D ** -0.5\n",
    "    \n",
    "        dq = torch.zeros_like(q)\n",
    "        dk = torch.zeros_like(k)\n",
    "        dv = torch.zeros_like(v)\n",
    "    \n",
    "        # D_i = sum_j dO_ij * O_ij\n",
    "        D_sum = torch.sum(o * do, dim=-1, keepdim=True)\n",
    "    \n",
    "        grid = (triton.cdiv(Nk, Bk), B)\n",
    "    \n",
    "        flash_bwd_kernel[grid](\n",
    "            q, k, v, o, l, do, D_sum,\n",
    "            dq, dk, dv,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            o.stride(0), o.stride(1), o.stride(2),\n",
    "            l.stride(0), l.stride(1),\n",
    "            do.stride(0), do.stride(1), do.stride(2),\n",
    "            D_sum.stride(0), D_sum.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_TILE_SIZE=Bq,\n",
    "            K_TILE_SIZE=Bk,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "    \n",
    "        return dq, dk, dv, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OK: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OK: d=32, T=16384\n",
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OK: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OK: d=128, T=16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d_model': 16,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.07023334503173828,\n",
       "  'mem_MB': 361.93896484375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.8378815650939941,\n",
       "  'mem_MB': 363.46240234375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 2.7483487129211426,\n",
       "  'mem_MB': 369.55615234375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 16.507973670959473,\n",
       "  'mem_MB': 377.68115234375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 27.576420307159424,\n",
       "  'mem_MB': 393.93115234375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.04380464553833008,\n",
       "  'mem_MB': 362.43896484375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.20469427108764648,\n",
       "  'mem_MB': 365.46240234375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 3.0948972702026367,\n",
       "  'mem_MB': 377.55615234375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 12.246737480163574,\n",
       "  'mem_MB': 393.68115234375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 47.23048448562622,\n",
       "  'mem_MB': 425.93115234375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.05492210388183594,\n",
       "  'mem_MB': 363.43896484375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.24171829223632812,\n",
       "  'mem_MB': 369.46240234375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 3.587462902069092,\n",
       "  'mem_MB': 393.55615234375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 14.247164726257324,\n",
       "  'mem_MB': 425.68115234375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 56.60130739212036,\n",
       "  'mem_MB': 489.93115234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.06808996200561523,\n",
       "  'mem_MB': 365.43896484375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.5030989646911621,\n",
       "  'mem_MB': 377.46240234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 7.622871398925781,\n",
       "  'mem_MB': 425.55615234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 30.193157196044922,\n",
       "  'mem_MB': 489.68115234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 119.87650156021118,\n",
       "  'mem_MB': 617.93115234375}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "               \n",
    "                Q = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                K = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                V = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "                # warm-up\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(10):\n",
    "                        out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                        #loss = out.sum()\n",
    "                        #loss.backward()\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                # forward timing\n",
    "                start = time.time()\n",
    "                for _ in range(100):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.time() - start) / 100\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                \n",
    "                '''\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(100):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 100\n",
    "                '''\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    #\"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    return results\n",
    "benchmark_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a712f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a186243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd13a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：27.439117431640625ms\n",
      "正在使用cuda\n",
      "[FlashAttentionTriton] B=4, N=4096, d=64, time=27439.117 ms\n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                    FlashAttentionTriton         1.65%     667.277us        41.03%      16.642ms      16.642ms             1  \n",
      "                                                        aten::empty_like         0.10%      40.169us        24.69%      10.013ms       2.003ms             5  \n",
      "                                                     aten::empty_strided        24.59%       9.972ms        24.59%       9.972ms       1.994ms             5  \n",
      "                                                             aten::empty         0.11%      46.145us         0.11%      46.145us      46.145us             1  \n",
      "                                                          cuLaunchKernel        14.90%       6.042ms        14.90%       6.042ms       3.021ms             2  \n",
      "                                                               aten::sum         0.42%     172.180us         0.78%     317.125us     158.563us             2  \n",
      "                                                        aten::as_strided         0.02%       7.651us         0.02%       7.651us       3.826us             2  \n",
      "                                                         cudaMemsetAsync         0.17%      68.495us         0.17%      68.495us      68.495us             1  \n",
      "                                                        cudaLaunchKernel         0.54%     219.955us         0.54%     219.955us      21.996us            10  \n",
      "                                                         aten::ones_like         0.05%      20.130us         0.30%     119.871us     119.871us             1  \n",
      "------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 40.559ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_flash_AttentionTriton(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionTriton.apply(Q, K, V)\n",
    "        loss = O.sum()\n",
    "        loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'FlashAttentionTriton',run = run)\n",
    "    t1 = profile(description ='FlashAttentionTriton' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttentionTriton] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "test_flash_AttentionTriton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：4801.045497258504ms\n",
      "正在使用cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        loss = O.sum()\n",
    "        loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [4096]:\n",
    "    test_flash_attention(B=4, N=4096, d=64, device=device)\n",
    "    test_naive_attention(B=4, N=4096, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ca0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Q\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr,\n",
    "    O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # 这里program_id(0)和program_id(1)分别对应启动triton kernel的grid的顺序\n",
    "    i = tl.program_id(0) #i就代表Q的第i个tile\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    # 这里offset每个指针，对应batch index乘以batch stride\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(i * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0), #注意K，V的初始化偏移是0，它们会在后续的循环中被更新\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(i * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_lq, 1),\n",
    "        offsets=(i * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    #对于因果掩码，主要针对Q和K的矩阵乘法，最终目标是求得S矩阵的上三角区域的值变成-inf\n",
    "    #在FLashAttention操作中，目标仍然是求大的S矩阵的上三角区域的值变成-inf，但因为是分块求的S_ij，所以如果i<j即代表S_ij处在大S矩阵的上三角区域，所以需要把S_ij变成-inf;如果i>j即代表S_ij处在大S矩阵的下三角区域，不用管。\n",
    "    #一类特殊情况就是i=j，此时的S_ij正好处在对角上，这里的小S_ij也要去把上三角给变成-inf\n",
    "\n",
    "    Q_i = tl.load(Q_block_ptr) #(Q_TILE_SIZE, D)\n",
    "    O_i_acc = tl.zeros((Q_TILE_SIZE, D), dtype=tl.float32) #(Q_TILE_SIZE, D)\n",
    "    L_i_acc = tl.zeros((Q_TILE_SIZE, 1), dtype=tl.float32) #(Q_TILE_SIZE, 1)\n",
    "    M_i_acc = tl.full((Q_TILE_SIZE, 1), float('-inf'), dtype=tl.float32) #(Q_TILE_SIZE, 1)\n",
    "    # 外层循环：对 K/V 的序列长度进行分块\n",
    "    for j in range(tl.cdiv(N_KEYS, K_TILE_SIZE)):\n",
    "        K_j = tl.load(K_block_ptr) #(K_TILE_SIZE, D)\n",
    "        V_j = tl.load(V_block_ptr) #(K_TILE_SIZE, D)\n",
    "        # 计算attention scores\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale #(Q_TILE_SIZE, K_TILE_SIZE)\n",
    "        \n",
    "        # 应用因果掩码 - 使用一个统一的掩码方法\n",
    "        if is_causal:\n",
    "        #     if i < j: #i<j即代表S_ij处在大S矩阵的上三角区域，所以需要把S_ij变成-inf\n",
    "        #         S_ij = tl.full((Q_TILE_SIZE, K_TILE_SIZE), -1e6, dtype=tl.float32)\n",
    "        #     elif j == i:#如果j==i，则代表S_ij正好处在对角上，这里的小S_ij也要去把上三角给变成-inf\n",
    "        #         mask = tl.arange(0,Q_TILE_SIZE)[:, None] >= tl.arange(0, K_TILE_SIZE)[None, :]\n",
    "        #         S_ij = tl.where(mask, S_ij, -1e6)\n",
    "            #创建位置掩码,在triton里面必须要用这种写法，因为triton在编译的时候是不通过if else的。\n",
    "            q_idx = i * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None] #(Q_TILE_SIZE, 1) #第一项实际上就是偏移的位置了\n",
    "            k_idx = j * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)[None, :] #(1, K_TILE_SIZE)\n",
    "            causal_mask = q_idx >= k_idx #(Q_TILE_SIZE, K_TILE_SIZE) 这里比较的思路就是把每次的偏移之后的行、列索引都进行比较，如果q_idx>=k_idx，则代表S_ij处在大S矩阵的上三角区域，所以需要把S_ij变成-inf\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        M_ij = tl.max(S_ij, axis=1, keep_dims=True) #(Q_TILE_SIZE, 1) - 当前块的最大值\n",
    "        M_i_new = tl.maximum(M_i_acc, M_ij) #(Q_TILE_SIZE, 1) - 与累积最大值比较\n",
    "        P_ij = tl.exp(S_ij - M_ij) #(Q_TILE_SIZE, K_TILE_SIZE) - 注意：使用M_ij而不是M_i_new\n",
    "        L_i_new = tl.exp(M_i_acc - M_i_new) * L_i_acc + tl.exp(M_ij - M_i_new) * tl.sum(P_ij, axis=1, keep_dims=True) #(Q_TILE_SIZE, 1)\n",
    "        #数据对齐\n",
    "        P_ij_cast = P_ij.to(V_j.dtype)#这代表从指针全局内存中读取类型，用V_j.dype理论上也行。\n",
    "        O_i_new = tl.exp(M_i_acc - M_i_new) * O_i_acc + tl.exp(M_ij - M_i_new) * tl.dot(P_ij_cast, V_j)  #(Q_TILE_SIZE, D)\n",
    "\n",
    "        #与pytorch代码不同，这里必须显式地更新M_i_acc和O_i_acc，因为下一个循环不会记住上一次的值\n",
    "        M_i_acc = M_i_new\n",
    "        O_i_acc = O_i_new\n",
    "        L_i_acc = L_i_new\n",
    "\n",
    "        # 只有 K 和 V 指针在 K 维度上前进，遍历所有 K/V 分块\n",
    "        K_block_ptr = K_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "\n",
    "\n",
    "    O_i = O_i_acc / L_i_acc\n",
    "    L_i = M_i_acc + tl.log(L_i_acc)\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(L_block_ptr, L_i)\n",
    "\n",
    "@triton.jit\n",
    "def flash_bwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr, dO_ptr, D_ptr,\n",
    "    dQ_ptr, dK_ptr, dV_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    stride_dob, stride_doq, stride_dod,\n",
    "    stride_db, stride_dq_d,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    j = tl.program_id(0)\n",
    "    batch_index = tl.program_id(1)\n",
    "    \n",
    "    \n",
    "    # 这里offset每个指针，对应batch index乘以batch stride\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),#stride是每个元素存储布局的方式\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),#(1, 0)代表行主序，(0, 1)代表列主序\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_lq, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dO_block_ptr = tl.make_block_ptr(\n",
    "        dO_ptr + batch_index * stride_dob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_doq, stride_dod),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    D_block_ptr = tl.make_block_ptr(\n",
    "        D_ptr + batch_index * stride_db,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_dq_d, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dQ_block_ptr = tl.make_block_ptr(\n",
    "        dQ_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(j * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(j * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dK_block_ptr = tl.make_block_ptr(\n",
    "        dK_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(j * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dV_block_ptr = tl.make_block_ptr(\n",
    "        dV_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(j * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "\n",
    "    # 加载当前K/V tile，这些在整个内层循环中保持不变\n",
    "    K_j = tl.load(K_block_ptr) #(K_TILE_SIZE, D)\n",
    "    V_j = tl.load(V_block_ptr) #(K_TILE_SIZE, D)\n",
    "    \n",
    "    # 初始化dK和dV的累积器，按照伪代码要求\n",
    "    dK_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "    dV_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "    \n",
    "    # 内层循环：遍历所有Q tiles\n",
    "    for i in range(tl.cdiv(N_QUERIES, Q_TILE_SIZE)):\n",
    "        # 加载当前Q tile的所有相关数据\n",
    "        Q_i = tl.load(Q_block_ptr) #(Q_TILE_SIZE, D)\n",
    "        O_i = tl.load(O_block_ptr) #(Q_TILE_SIZE, D)\n",
    "        dO_i = tl.load(dO_block_ptr) #(Q_TILE_SIZE, D)\n",
    "        L_i = tl.load(L_block_ptr) #(Q_TILE_SIZE, 1)\n",
    "        \n",
    "        # 加载预计算的D_i = rowsum(dO_i ○ O_i)，按照伪代码\n",
    "        D_i = tl.load(D_block_ptr) #(Q_TILE_SIZE, 1)\n",
    "\n",
    "        # 计算attention scores S_ij = Q_i @ K_j^T / √d\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale #(Q_TILE_SIZE, K_TILE_SIZE)\n",
    "        \n",
    "        # 应用因果掩码\n",
    "        if is_causal:\n",
    "            q_idx = i * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "            k_idx = j * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)[None, :]\n",
    "            causal_mask = q_idx >= k_idx\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        P_ij = tl.exp(S_ij - L_i) #(Q_TILE_SIZE, K_TILE_SIZE)\n",
    "\n",
    "        dV_j += tl.dot(P_ij.to(V_j.dtype).T, dO_i)\n",
    "        \n",
    "        dP_ij = tl.dot(dO_i, V_j.T) #(Q_TILE_SIZE, K_TILE_SIZE)\n",
    "        \n",
    "        dS_ij = P_ij * (dP_ij - D_i) * scale #(Q_TILE_SIZE, K_TILE_SIZE)\n",
    "        dS_ij = dS_ij.to(Q_i.dtype)\n",
    "        \n",
    "        # 计算dQ的贡献，使用block指针进行原子累积\n",
    "        dQ_contribution = tl.dot(dS_ij, K_j) #(Q_TILE_SIZE, D)\n",
    "        # 生成Q tile和D的索引\n",
    "        q_idx = i * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "        d_idx = tl.arange(0, D)[None, :]\n",
    "        # 创建边界掩码，避免对越界的无效的位置进行操作\n",
    "        q_mask = q_idx < N_QUERIES\n",
    "        d_mask = d_idx < D\n",
    "        mask = q_mask & d_mask\n",
    "        # 计算偏移地址\n",
    "        offset = batch_index * stride_qb + q_idx * stride_qq + d_idx * stride_qd\n",
    "        dq_ptrs = dQ_ptr + offset\n",
    "        # 使用原子操作累积\n",
    "        tl.atomic_add(dq_ptrs, dQ_contribution.to(dQ_ptr.dtype.element_ty), mask=mask)\n",
    "        \n",
    "        # 累积到dK_j\n",
    "        dK_j += tl.dot(dS_ij.T, Q_i)\n",
    "        \n",
    "        # 前进到下一个Q tile\n",
    "        Q_block_ptr = Q_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        O_block_ptr = O_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        dO_block_ptr = dO_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        L_block_ptr = L_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        D_block_ptr = D_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        dQ_block_ptr = dQ_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "\n",
    "    # 写回累积的dK和dV梯度\n",
    "    tl.store(dK_block_ptr, dK_j.to(dK_ptr.dtype.element_ty))\n",
    "    tl.store(dV_block_ptr, dV_j.to(dV_ptr.dtype.element_ty))\n",
    "\n",
    "class FlashAttentionAutogradFunctionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, is_causal=False):\n",
    "        # 处理批处理维度: q, k, v 的形状是 (batch_size, seq_len, d_model)\n",
    "        batch_size = q.shape[0]\n",
    "        Nq = q.shape[1] # q 的序列长度\n",
    "        Nk = k.shape[1] # k 的序列长度\n",
    "        d = q.shape[2]  # 特征维度\n",
    "        \n",
    "        Bq = 32  # q 的分块大小,是一个单位 (节省共享内存)\n",
    "        Bk = 32  # k 的分块大小\n",
    "        Tq = Nq // Bq\n",
    "\n",
    "        scale = 1 / d**0.5\n",
    "\n",
    "        O = torch.zeros_like(q)\n",
    "        L = torch.zeros(batch_size, Nq, device=q.device)\n",
    "        #这里的grid代表启动triton kernel的grid的形状，也就是并行的线程，往往会把batch_size这种常用的并行方式放在后面，但对性能没有影响\n",
    "        grid = (Tq, batch_size)\n",
    "\n",
    "        flash_fwd_kernel[grid](q, k, v, O, L,\n",
    "                               q.stride(0), q.stride(1), q.stride(2),\n",
    "                               k.stride(0), k.stride(1), k.stride(2),\n",
    "                               v.stride(0), v.stride(1), v.stride(2),\n",
    "                               O.stride(0), O.stride(1), O.stride(2),\n",
    "                               L.stride(0), L.stride(1),\n",
    "                               Nq, Nk,\n",
    "                               scale,\n",
    "                               D=d, Q_TILE_SIZE=Bq, K_TILE_SIZE=Bk,\n",
    "                               is_causal=is_causal)\n",
    "        \n",
    "        ctx.save_for_backward(q, k, v, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, l = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "        Nq = q.shape[1]\n",
    "        Nk = k.shape[1]\n",
    "        d = q.shape[2]\n",
    "\n",
    "        Bq = 32\n",
    "        Bk = 32\n",
    "        # Tq = Nq // Bq\n",
    "        Tk = Nk // Bk\n",
    "        scale = 1 / d**0.5\n",
    "\n",
    "        dq = torch.zeros_like(q)\n",
    "        dk = torch.zeros_like(k)\n",
    "        dv = torch.zeros_like(v)\n",
    "        D_sum = torch.sum(o * do, dim=-1, keepdim=True)  # 计算D = rowsum(dO ○ O)\n",
    "        grid = (Tk, batch_size) \n",
    "        flash_bwd_kernel[grid](\n",
    "            q, k, v, o, l, do, D_sum,\n",
    "            dq, dk, dv,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            o.stride(0), o.stride(1), o.stride(2),\n",
    "            l.stride(0), l.stride(1),\n",
    "            do.stride(0), do.stride(1), do.stride(2),\n",
    "            D_sum.stride(0), D_sum.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=d, Q_TILE_SIZE=Bq, K_TILE_SIZE=Bk,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        return dq, dk, dv, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cdd5f",
   "metadata": {},
   "source": [
    "\n",
    "# Triton 一般语法与编程模型说明\n",
    "\n",
    "Triton 是一种面向深度学习算子的 DSL（Domain-Specific Language），其设计目标并不是替代 PyTorch，而是 **在不直接编写 CUDA 的前提下，精确控制 GPU 上的并行计算与内存访问**。\n",
    "\n",
    "理解 Triton，需要从它的编程模型开始，而不是从 API 细节入手。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、Triton 的核心抽象：program\n",
    "\n",
    "在 Triton 中，**program 是最小的并行执行单元**。\n",
    "\n",
    "```python\n",
    "pid = tl.program_id(axis)\n",
    "```\n",
    "\n",
    "这一概念对应于 CUDA 中的：\n",
    "\n",
    "| Triton  | CUDA                    |\n",
    "| ------- | ----------------------- |\n",
    "| program | thread block            |\n",
    "| grid    | grid                    |\n",
    "| axis    | blockIdx.x / blockIdx.y |\n",
    "\n",
    "关键区别在于：\n",
    "\n",
    "* Triton 的 program **天然是 SIMD / 向量化的**\n",
    "* 一个 program 内部，操作的是一个 **tile（块）**，而不是单个标量\n",
    "\n",
    "因此，Triton 的思维方式是：\n",
    "\n",
    "> **“一个 program 负责一块数据”**\n",
    "\n",
    "而不是：\n",
    "\n",
    "> “一个 thread 负责一个元素”\n",
    "\n",
    "---\n",
    "\n",
    "## 二、grid 的含义：并行维度的显式声明\n",
    "\n",
    "Triton kernel 的调用形式：\n",
    "\n",
    "```python\n",
    "kernel[grid](...)\n",
    "```\n",
    "\n",
    "其中：\n",
    "\n",
    "```python\n",
    "grid = (grid_dim_0, grid_dim_1, ...)\n",
    "```\n",
    "\n",
    "对应：\n",
    "\n",
    "```python\n",
    "tl.program_id(0)  # 第 0 个并行维度\n",
    "tl.program_id(1)  # 第 1 个并行维度\n",
    "```\n",
    "\n",
    "常见用法：\n",
    "\n",
    "| grid 维度 | 语义           |\n",
    "| ------- | ------------ |\n",
    "| axis=0  | 序列 / 行 block |\n",
    "| axis=1  | batch        |\n",
    "| axis=2  | head         |\n",
    "\n",
    "Triton 不关心“线程数量”，它只关心：\n",
    "\n",
    "> **一共启动多少个 program，以及每个 program 处理哪一块数据**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、Triton 的内存访问模型\n",
    "\n",
    "### 1. 指针是“裸指针”，但带 shape 和 stride\n",
    "\n",
    "在 Triton kernel 中，所有张量参数本质上都是 **指针**：\n",
    "\n",
    "```python\n",
    "Q_ptr, K_ptr, V_ptr\n",
    "```\n",
    "\n",
    "它们没有 shape、没有 stride，只有地址。\n",
    "\n",
    "因此，Triton 需要你**显式告诉它**：\n",
    "\n",
    "* 张量的逻辑形状\n",
    "* 张量在内存中的步长\n",
    "* 当前 program 访问哪一块\n",
    "\n",
    "---\n",
    "\n",
    "### 2. make_block_ptr：结构化内存访问的核心\n",
    "\n",
    "```python\n",
    "block_ptr = tl.make_block_ptr(\n",
    "    base,\n",
    "    shape,\n",
    "    strides,\n",
    "    offsets,\n",
    "    block_shape,\n",
    "    order\n",
    ")\n",
    "```\n",
    "\n",
    "这不是“语法糖”，而是 Triton 最重要的抽象之一。\n",
    "\n",
    "它的含义是：\n",
    "\n",
    "> **定义一个“可被向量化 load/store 的二维 tile”**\n",
    "\n",
    "各参数的含义：\n",
    "\n",
    "| 参数          | 说明                      |\n",
    "| ----------- | ----------------------- |\n",
    "| base        | 数据起始指针                  |\n",
    "| shape       | 整个张量的逻辑 shape           |\n",
    "| strides     | 每个维度的步长                 |\n",
    "| offsets     | 当前 program 的起始偏移        |\n",
    "| block_shape | 一次 load/store 的 tile 大小 |\n",
    "| order       | 内存访问的主序（影响 coalescing）  |\n",
    "\n",
    "一旦定义了 `block_ptr`：\n",
    "\n",
    "```python\n",
    "x = tl.load(block_ptr)\n",
    "tl.store(block_ptr, x)\n",
    "```\n",
    "\n",
    "就等价于一次 **矢量化、对齐的内存操作**。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、Triton 中的“张量”是什么\n",
    "\n",
    "Triton 中的张量并不是 PyTorch Tensor，而是：\n",
    "\n",
    "> **编译期已知 shape 的寄存器向量**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "```\n",
    "\n",
    "意味着：\n",
    "\n",
    "* shape 在编译期是常量\n",
    "* 存储在寄存器 / SRAM 中\n",
    "* 所有运算是 element-wise 或 block-wise 的\n",
    "\n",
    "这也是为什么 Triton 要求很多参数是 `tl.constexpr`。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、tl.constexpr：编译期常量，而不是运行时参数\n",
    "\n",
    "```python\n",
    "BLOCK_SIZE: tl.constexpr\n",
    "```\n",
    "\n",
    "含义是：\n",
    "\n",
    "> **该值在 kernel 编译时已知**\n",
    "\n",
    "这对 Triton 非常重要，因为：\n",
    "\n",
    "* block_shape\n",
    "* loop unrolling\n",
    "* 向量化宽度\n",
    "* 内存布局\n",
    "\n",
    "都依赖这些值。\n",
    "\n",
    "经验规则：\n",
    "\n",
    "* **影响 shape / 循环次数 / 内存布局的参数 → 必须是 constexpr**\n",
    "* 纯数值缩放（如 scale）可以是运行时参数\n",
    "\n",
    "---\n",
    "\n",
    "## 六、算子语义：没有 Python 循环的“逐元素”\n",
    "\n",
    "Triton 的算子都是 **逐元素并行** 的。\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.arange(0, BLOCK)\n",
    "```\n",
    "\n",
    "产生的是：\n",
    "\n",
    "```text\n",
    "[0, 1, 2, ..., BLOCK-1]\n",
    "```\n",
    "\n",
    "而不是单个值。\n",
    "\n",
    "再例如：\n",
    "\n",
    "```python\n",
    "mask = q_idx >= k_idx\n",
    "```\n",
    "\n",
    "这是一个 **逐元素比较，返回布尔矩阵**。\n",
    "\n",
    "`tl.where` 的语义是：\n",
    "\n",
    "```python\n",
    "out[i] = a[i] if cond[i] else b[i]\n",
    "```\n",
    "\n",
    "而不是 Python 分支。\n",
    "\n",
    "---\n",
    "\n",
    "## 七、控制流：为什么 Triton “不鼓励 if”\n",
    "\n",
    "Triton 中的 `if`：\n",
    "\n",
    "```python\n",
    "if is_causal:\n",
    "    ...\n",
    "```\n",
    "\n",
    "只有在 **`is_causal` 是 tl.constexpr** 时才安全。\n",
    "\n",
    "原因是：\n",
    "\n",
    "* Triton kernel 是 **单一静态程序**\n",
    "* 所有 program 必须执行同一条指令流\n",
    "\n",
    "因此：\n",
    "\n",
    "* **数据相关分支 → 必须用 mask**\n",
    "* **配置相关分支 → 可以用 constexpr if**\n",
    "\n",
    "这也是为什么 causal mask 必须写成：\n",
    "\n",
    "```python\n",
    "S = tl.where(mask, S, -inf)\n",
    "```\n",
    "\n",
    "而不是 `if row > col`.\n",
    "\n",
    "---\n",
    "\n",
    "## 八、数学运算与矩阵乘法\n",
    "\n",
    "### 1. tl.dot\n",
    "\n",
    "```python\n",
    "C = tl.dot(A, B)\n",
    "```\n",
    "\n",
    "这是 Triton 提供的 **块级矩阵乘法原语**：\n",
    "\n",
    "* 输入是 `(M, K)` 和 `(K, N)`\n",
    "* 输出是 `(M, N)`\n",
    "* 自动映射到 tensor core（如果 dtype / shape 合适）\n",
    "\n",
    "这是 Triton 实现 FlashAttention、GEMM、LayerNorm 融合的基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 广播规则\n",
    "\n",
    "Triton 的广播行为和 NumPy / PyTorch 基本一致，但前提是：\n",
    "\n",
    "> **shape 必须在编译期可推导**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "M = tl.max(S, axis=1, keep_dims=True)\n",
    "```\n",
    "\n",
    "这是合法的，因为 axis 和 shape 都是常量。\n",
    "\n",
    "---\n",
    "\n",
    "## 九、Triton kernel 与 PyTorch Autograd 的关系\n",
    "\n",
    "Triton kernel 本身：\n",
    "\n",
    "* 不关心梯度\n",
    "* 不保存中间状态\n",
    "\n",
    "因此常见结构是：\n",
    "\n",
    "```python\n",
    "class MyOp(torch.autograd.Function):\n",
    "    def forward(...):\n",
    "        launch triton kernel\n",
    "        save tensors\n",
    "    def backward(...):\n",
    "        launch triton kernel\n",
    "```\n",
    "\n",
    "Triton 负责 **高性能数值计算**\n",
    "PyTorch 负责 **计算图与调度**\n",
    "\n",
    "这是一个明确的职责划分。\n",
    "\n",
    "---\n",
    "\n",
    "## 十、总结：如何正确理解 Triton\n",
    "\n",
    "一句话总结 Triton：\n",
    "\n",
    "> **Triton 是一种“显式控制内存与并行，但不暴露线程”的 GPU 编程模型**\n",
    "\n",
    "理解 Triton 的关键不是 API，而是以下三点：\n",
    "\n",
    "1. 一个 program 对应一个 tile\n",
    "2. 所有 shape 在编译期已知\n",
    "3. 控制流必须数据并行\n",
    "\n",
    "一旦接受这三点，FlashAttention、GEMM、LayerNorm 在 Triton 中的实现方式，都会变得自然且统一。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c954e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Optimizer\n",
    "from typing import Any, Type, Dict, Iterable\n",
    "\n",
    "\n",
    "class TeachingShardedOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    教学版：Optimizer State Sharding（类似 ZeRO Stage-1）\n",
    "\n",
    "    核心思想：\n",
    "    - 每个 rank 拥有【完整模型参数】\n",
    "    - 每个参数被分配一个唯一的 owner rank\n",
    "    - 只有 owner rank：\n",
    "        - 持有该参数的 optimizer state\n",
    "        - 负责执行 optimizer.step()\n",
    "    - 参数更新后，通过 dist.broadcast 同步到所有 rank\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        base_optimizer_cls: Type[Optimizer],\n",
    "        **base_optimizer_kwargs: Any\n",
    "    ):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        - params: model.parameters()\n",
    "        - base_optimizer_cls: 如 torch.optim.Adam\n",
    "        - base_optimizer_kwargs: 如 lr, betas, weight_decay 等\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== 1. 保存原始 Optimizer 信息 =====\n",
    "        self.base_optimizer_cls = base_optimizer_cls\n",
    "        self.base_optimizer_kwargs = base_optimizer_kwargs\n",
    "\n",
    "        # ===== 2. 分布式环境信息 =====\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # ===== 3. 参数分片辅助变量 =====\n",
    "        # 用于给参数做 round-robin 编号\n",
    "        self.global_param_index = 0\n",
    "\n",
    "        # 在 __init__ 期间暂存属于本 rank 的 param groups\n",
    "        self._buffered_local_param_groups = []\n",
    "\n",
    "        # ===== 4. 初始化“全局 Optimizer”父类 =====\n",
    "        # 注意：Optimizer.__init__ 内部会调用 self.add_param_group\n",
    "        #       而我们在下面重写了 add_param_group\n",
    "        super().__init__(params, defaults=base_optimizer_kwargs)\n",
    "\n",
    "        # ===== 5. 创建“本地 shard Optimizer” =====\n",
    "        # 只包含当前 rank 拥有的参数\n",
    "        if self._buffered_local_param_groups:\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                self._buffered_local_param_groups,\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "        else:\n",
    "            # 边缘情况：当前 rank 没有分到任何参数\n",
    "            self.local_shard_optimizer = self.base_optimizer_cls(\n",
    "                [],\n",
    "                **self.base_optimizer_kwargs\n",
    "            )\n",
    "\n",
    "    def add_param_group(self, param_group: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        教学重点函数：参数分片逻辑发生在这里\n",
    "\n",
    "        一个 param_group 会被“拆成两份”：\n",
    "        1. 全量 param_group → 交给父类 Optimizer（用于遍历 & 同步）\n",
    "        2. 本地 param_group → 只保留 owner == 当前 rank 的参数\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step A: 给 param_group 中的每个参数分配 owner rank\n",
    "        # -------------------------------------------------------\n",
    "        for param in param_group[\"params\"]:\n",
    "            if not hasattr(param, \"_owner_rank\"):\n",
    "                # round-robin 分配\n",
    "                param._owner_rank = self.global_param_index % self.world_size\n",
    "                self.global_param_index += 1\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step B: 注册到“全局 Optimizer”\n",
    "        # -------------------------------------------------------\n",
    "        # 这个 Optimizer：\n",
    "        # - 包含所有参数\n",
    "        # - 不负责 step\n",
    "        # - 只用于：\n",
    "        #     * zero_grad\n",
    "        #     * 参数遍历\n",
    "        #     * step 后的 broadcast\n",
    "        super().add_param_group(param_group)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step C: 构造“本地 shard param_group”\n",
    "        # -------------------------------------------------------\n",
    "        # 拷贝除 params 以外的超参（lr / weight_decay 等）\n",
    "        local_param_group = {\n",
    "            key: value\n",
    "            for key, value in param_group.items()\n",
    "            if key != \"params\"\n",
    "        }\n",
    "\n",
    "        # 只保留 owner 是当前 rank 的参数\n",
    "        local_param_group[\"params\"] = [\n",
    "            param\n",
    "            for param in param_group[\"params\"]\n",
    "            if param._owner_rank == self.rank\n",
    "        ]\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step D: 加入本地 shard Optimizer\n",
    "        # -------------------------------------------------------\n",
    "        if hasattr(self, \"local_shard_optimizer\"):\n",
    "            # 正常情况：__init__ 已结束\n",
    "            if local_param_group[\"params\"]:\n",
    "                self.local_shard_optimizer.add_param_group(local_param_group)\n",
    "        else:\n",
    "            # __init__ 过程中，先缓存\n",
    "            self._buffered_local_param_groups.append(local_param_group)\n",
    "\n",
    "    def step(self, closure=None, **kwargs):\n",
    "        \"\"\"\n",
    "        一个 step 分为两步：\n",
    "\n",
    "        1. 本地更新（只更新 owner 是当前 rank 的参数）\n",
    "        2. 参数同步（broadcast 最新参数到所有 rank）\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 1: 本地 shard Optimizer 更新\n",
    "        # -------------------------------------------------------\n",
    "        # 只有 owner rank 会真正更新对应参数\n",
    "        self.local_shard_optimizer.step(**kwargs)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # Step 2: 参数同步（broadcast）\n",
    "        # -------------------------------------------------------\n",
    "        # 遍历“全局 Optimizer”中的所有参数\n",
    "        for param_group in self.param_groups:\n",
    "            for param in param_group[\"params\"]:\n",
    "                # owner rank 作为 src\n",
    "                # 其他 rank 接收并覆盖本地参数\n",
    "                dist.broadcast(\n",
    "                    tensor=param.data,\n",
    "                    src=param._owner_rank\n",
    "                )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = True):\n",
    "        \"\"\"\n",
    "        梯度必须在所有 rank 上清零：\n",
    "        即使该 rank 不是参数 owner，也会参与反向传播\n",
    "        \"\"\"\n",
    "        super().zero_grad(set_to_none=set_to_none)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0c987",
   "metadata": {},
   "source": [
    "\n",
    "# **手写 DDP 梯度通信 + overlap 的实现**\n",
    "\n",
    "## 一、整体目标：你在“手写 DDP”\n",
    "\n",
    "这份代码的核心目标只有一句话：\n",
    "\n",
    "> **在反向传播过程中，尽早、异步地同步梯度，实现计算与通信 overlap**\n",
    "\n",
    "它实现了 DDP 的三个关键步骤：\n",
    "\n",
    "| 阶段       | 你做了什么                            |\n",
    "| -------- | -------------------------------- |\n",
    "| 初始化      | broadcast 参数和 buffer             |\n",
    "| backward | 注册 grad hook，触发 async all-reduce |\n",
    "| step 前   | 等待通信完成，平均梯度                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 二、`DDPOverlapIndividual`：逐参数 All-Reduce（教学入门版）\n",
    "\n",
    "这是一个**最直观、但效率最低**的 DDP overlap 实现。\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ 初始化阶段\n",
    "\n",
    "```python\n",
    "class DDPOverlapIndividual(torch.nn.Module):\n",
    "```\n",
    "\n",
    "这是一个 **Module wrapper**，不是 Optimizer。\n",
    "\n",
    "---\n",
    "\n",
    "#### (1) 必须先调用 `super().__init__()`\n",
    "\n",
    "```python\n",
    "super().__init__()\n",
    "```\n",
    "\n",
    "否则：\n",
    "\n",
    "* 参数不会被正确注册\n",
    "* hooks 行为不确定\n",
    "\n",
    "---\n",
    "\n",
    "#### (2) 保存状态\n",
    "\n",
    "```python\n",
    "self.module = module\n",
    "self.handles = []\n",
    "self.world_size = dist.get_world_size()\n",
    "```\n",
    "\n",
    "* `handles`：保存所有 async 通信句柄\n",
    "* `world_size`：用于梯度平均\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 初始化参数同步（等价 DDP）\n",
    "\n",
    "```python\n",
    "for p in self.module.parameters():\n",
    "    dist.broadcast(p.data, src=0)\n",
    "```\n",
    "\n",
    "**目的**：\n",
    "\n",
    "* 保证所有 rank 起始参数完全一致\n",
    "* 等价于 DDP 的 initial sync\n",
    "\n",
    "buffers（BN running_mean 等）也必须同步：\n",
    "\n",
    "```python\n",
    "for b in self.module.buffers():\n",
    "    dist.broadcast(b.data, src=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 注册 backward hook（核心）\n",
    "\n",
    "```python\n",
    "p.register_post_accumulate_grad_hook(self._make_hook())\n",
    "```\n",
    "\n",
    "**这个 hook 的触发时机非常关键**：\n",
    "\n",
    "> 在 **该参数的梯度刚刚算完并累积完成之后**\n",
    "\n",
    "这意味着：\n",
    "\n",
    "* backward 还没结束\n",
    "* 上游 layer 还在算\n",
    "* 你已经可以通信了 → overlap 的基础\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ `_make_hook`：逐参数 async all-reduce\n",
    "\n",
    "```python\n",
    "handle = dist.all_reduce(\n",
    "    param.grad,\n",
    "    op=dist.ReduceOp.SUM,\n",
    "    async_op=True\n",
    ")\n",
    "```\n",
    "\n",
    "关键点：\n",
    "\n",
    "* **通信对象是 param.grad**\n",
    "* **async_op=True** → 不阻塞反向传播\n",
    "* 返回 `handle`，用于之后 `wait()`\n",
    "\n",
    "⚠️ hook **不能返回任何值**\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ `finish_gradient_synchronization`\n",
    "\n",
    "```python\n",
    "for h in self.handles:\n",
    "    h.wait()\n",
    "```\n",
    "\n",
    "在 optimizer.step() 之前：\n",
    "\n",
    "1. 等待所有梯度通信完成\n",
    "2. 对梯度做平均：\n",
    "\n",
    "```python\n",
    "p.grad.div_(self.world_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 这一版本你学到什么？\n",
    "\n",
    "* backward hook 的时机\n",
    "* async all-reduce 的基本用法\n",
    "* overlap 的最小实现\n",
    "\n",
    "❌ 问题：\n",
    "\n",
    "* 每个参数一次通信\n",
    "* NCCL 启动开销极大\n",
    "* 工业中 **不会这么做**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、`DDPBucketed`：真实 DDP 的核心思想\n",
    "\n",
    "这是 **PyTorch DDP 的核心结构复刻版**。\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Bucket 类：梯度通信的基本单位\n",
    "\n",
    "### Bucket 是什么？\n",
    "\n",
    "> **一组参数的梯度，被 flatten 成一个连续 buffer，一起通信**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 分配连续 buffer\n",
    "\n",
    "```python\n",
    "total_numel = sum(p.numel() for p in params)\n",
    "self.buffer = torch.zeros(total_numel, ...)\n",
    "```\n",
    "\n",
    "好处：\n",
    "\n",
    "* 单次 NCCL 调用\n",
    "* 连续内存，通信效率高\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 为每个参数创建 view\n",
    "\n",
    "```python\n",
    "self.param_views[p] = self.buffer[offset:...].view(p.shape)\n",
    "```\n",
    "\n",
    "这样：\n",
    "\n",
    "* `p.grad ↔ buffer 的一段`\n",
    "* copy 成本低\n",
    "* 不需要重新分配内存\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 `reset()`\n",
    "\n",
    "```python\n",
    "self.count_ready = 0\n",
    "self.comm_handle = None\n",
    "```\n",
    "\n",
    "每次 forward 前必须重置：\n",
    "\n",
    "* 反向传播是逐参数完成的\n",
    "* bucket 是“状态机”\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ DDPBucketed 初始化流程\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 broadcast 初始化参数\n",
    "\n",
    "和前一个类完全一致，省略。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 参数分桶（非常关键）\n",
    "\n",
    "```python\n",
    "reversed_params = list(reversed(trainable_params))\n",
    "```\n",
    "\n",
    "**为什么反向？**\n",
    "\n",
    "* backward 顺序：输出 → 输入\n",
    "* 最先 ready 的梯度 → 靠近输出层\n",
    "* bucket 应该按这个顺序装\n",
    "\n",
    "👉 **这是 overlap 的关键细节**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 bucket size 控制\n",
    "\n",
    "```python\n",
    "if current_bucket_size + p_bytes > bucket_size:\n",
    "    close bucket\n",
    "```\n",
    "\n",
    "目的：\n",
    "\n",
    "* 避免 bucket 太小（通信多）\n",
    "* 避免 bucket 太大（overlap 变差）\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ hook：bucket 级别的 overlap\n",
    "\n",
    "### hook_fn 的完整逻辑\n",
    "\n",
    "#### A. 拷贝梯度到 bucket buffer\n",
    "\n",
    "```python\n",
    "bucket.param_views[p].copy_(p.grad)\n",
    "```\n",
    "\n",
    "#### B. 计数\n",
    "\n",
    "```python\n",
    "bucket.count_ready += 1\n",
    "```\n",
    "\n",
    "#### C. bucket 满 → 发起通信\n",
    "\n",
    "```python\n",
    "if bucket.count_ready == len(bucket.params):\n",
    "    bucket.comm_handle = dist.all_reduce(..., async_op=True)\n",
    "```\n",
    "\n",
    "⚠️ 注意：\n",
    "\n",
    "* 不是等 backward 结束\n",
    "* 是等 **这个 bucket 内的参数都 ready**\n",
    "\n",
    "👉 **真正的 overlap**\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ `finish_gradient_synchronization`\n",
    "\n",
    "```python\n",
    "bucket.comm_handle.wait()\n",
    "bucket.buffer.div_(world_size)\n",
    "p.grad.copy_(bucket.param_views[p])\n",
    "```\n",
    "\n",
    "步骤：\n",
    "\n",
    "1. 等 bucket 通信完成\n",
    "2. 一次性平均整个 buffer\n",
    "3. 拷贝回各参数的 `p.grad`\n",
    "\n",
    "---\n",
    "\n",
    "## 四、两种实现的对比（非常重要）\n",
    "\n",
    "| 对比项         | Individual | Bucketed  |\n",
    "| ----------- | ---------- | --------- |\n",
    "| 通信粒度        | 每个参数       | 每个 bucket |\n",
    "| 通信次数        | 很多         | 很少        |\n",
    "| overlap 效果  | 有，但低效      | 工业级       |\n",
    "| PyTorch DDP | ❌          | ✅         |\n",
    "\n",
    "---\n",
    "\n",
    "## 五、它和 PyTorch DDP 的关系\n",
    "\n",
    "| PyTorch DDP 内部 | 你这份代码                              |\n",
    "| -------------- | ---------------------------------- |\n",
    "| Reducer        | Bucket                             |\n",
    "| Grad bucket    | buffer + views                     |\n",
    "| Autograd hook  | register_post_accumulate_grad_hook |\n",
    "| Overlap        | async all_reduce                   |\n",
    "| Finalize       | finish_gradient_synchronization    |\n",
    "\n",
    "你已经在 **“复刻 DDP 核心”** 了。\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结（面试 / 教学级）\n",
    "\n",
    "> **DDP 的本质不是 all-reduce，而是：\n",
    "> 在反向传播尚未结束时，\n",
    "> 以 bucket 为单位，\n",
    "> 对已 ready 的梯度发起异步通信，\n",
    "> 并在 step 前完成梯度一致性。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ecc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 1. 逐参数 Overlap 版本（最直观的 DDP 教学实现）\n",
    "# ============================================================\n",
    "\n",
    "class TeachingDDPOverlapPerParam(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    教学版 DDP（逐参数通信）：\n",
    "\n",
    "    - 每个参数在 backward 完成后，立刻触发 async all-reduce\n",
    "    - 实现计算（backward）与通信的 overlap\n",
    "    - 逻辑直观，但通信粒度太细（工业中不用）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        # ⚠️ 必须最先初始化父类\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # 保存所有 async all-reduce 的句柄\n",
    "        self.async_handles = []\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 初始化阶段：同步模型参数 & buffer\n",
    "        # ----------------------------------------------------\n",
    "        # 等价于 PyTorch DDP 的 initial broadcast\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "\n",
    "            for buffer in self.model.buffers():\n",
    "                dist.broadcast(buffer.data, src=0)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 注册 backward hook（核心）\n",
    "        # ----------------------------------------------------\n",
    "        # 当某个参数的梯度刚算完，就立刻通信\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.register_post_accumulate_grad_hook(\n",
    "                    self._create_grad_hook()\n",
    "                )\n",
    "\n",
    "    def _create_grad_hook(self):\n",
    "        \"\"\"\n",
    "        创建一个 backward hook（closure）\n",
    "\n",
    "        hook 的触发时机：\n",
    "        - 当前参数的梯度已经计算完成\n",
    "        - 但整个 backward 还没结束\n",
    "        \"\"\"\n",
    "\n",
    "        def hook_fn(param: torch.nn.Parameter):\n",
    "            # 有些参数可能没有梯度（如被冻结）\n",
    "            if param.grad is None:\n",
    "                return\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # 对该参数的梯度做 async all-reduce\n",
    "            # ------------------------------------------------\n",
    "            handle = dist.all_reduce(\n",
    "                param.grad,\n",
    "                op=dist.ReduceOp.SUM,\n",
    "                async_op=True\n",
    "            )\n",
    "\n",
    "            # 保存句柄，step 前需要 wait\n",
    "            self.async_handles.append(handle)\n",
    "\n",
    "            # ⚠️ register_post_accumulate_grad_hook\n",
    "            #    不允许返回任何值\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def finalize_gradients(self):\n",
    "        \"\"\"\n",
    "        在 optimizer.step() 之前调用：\n",
    "\n",
    "        - 等待所有 async all-reduce 完成\n",
    "        - 对梯度做平均\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. 等待所有通信完成\n",
    "        for handle in self.async_handles:\n",
    "            handle.wait()\n",
    "        self.async_handles.clear()\n",
    "\n",
    "        # 2. 梯度平均\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.div_(self.world_size)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 2. Bucket 类：DDP 的核心数据结构\n",
    "# ============================================================\n",
    "\n",
    "class GradientBucket:\n",
    "    \"\"\"\n",
    "    一个 bucket 管理一组参数的梯度通信：\n",
    "\n",
    "    - 多个参数的梯度被 flatten 到一个连续 buffer\n",
    "    - 当 bucket 中所有参数梯度 ready 后，触发一次 all-reduce\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: list[torch.nn.Parameter], bucket_id: int):\n",
    "        self.bucket_id = bucket_id\n",
    "        self.params = params\n",
    "\n",
    "        # 已经 ready 的参数数量\n",
    "        self.ready_count = 0\n",
    "\n",
    "        # async all-reduce 的通信句柄\n",
    "        self.comm_handle = None\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 创建连续的梯度 buffer\n",
    "        # ----------------------------------------------------\n",
    "        total_numel = sum(p.numel() for p in params)\n",
    "        dtype = params[0].dtype\n",
    "        device = params[0].device\n",
    "\n",
    "        self.flat_buffer = torch.zeros(\n",
    "            total_numel, dtype=dtype, device=device\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 为每个参数创建 buffer 视图\n",
    "        # ----------------------------------------------------\n",
    "        self.param_to_view = {}\n",
    "        offset = 0\n",
    "        for p in params:\n",
    "            numel = p.numel()\n",
    "            self.param_to_view[p] = (\n",
    "                self.flat_buffer[offset: offset + numel]\n",
    "                .view(p.shape)\n",
    "            )\n",
    "            offset += numel\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        在每个 forward 之前调用：\n",
    "        - 清空计数器\n",
    "        - 清空通信句柄\n",
    "        \"\"\"\n",
    "        self.ready_count = 0\n",
    "        self.comm_handle = None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Part 3. Bucket 化 DDP（接近 PyTorch DDP 真正实现）\n",
    "# ============================================================\n",
    "\n",
    "class TeachingDDPBucketed(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    教学版 Bucket DDP（工业级思路）：\n",
    "\n",
    "    - 参数按 bucket 分组\n",
    "    - 每个 bucket 一次 all-reduce\n",
    "    - 在 backward 中实现计算 / 通信 overlap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, bucket_size_mb: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # bucket 大小（MB -> Bytes）\n",
    "        self.bucket_size_bytes = bucket_size_mb * 1024 * 1024\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 1. 初始化参数同步\n",
    "        # ----------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                dist.broadcast(param.data, src=0)\n",
    "            for buffer in self.model.buffers():\n",
    "                dist.broadcast(buffer.data, src=0)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 2. 参数分桶（非常关键）\n",
    "        # ----------------------------------------------------\n",
    "        self.buckets: list[GradientBucket] = []\n",
    "        self.param_to_bucket: dict[torch.nn.Parameter, GradientBucket] = {}\n",
    "\n",
    "        # 只考虑需要梯度的参数\n",
    "        trainable_params = [\n",
    "            p for p in self.model.parameters() if p.requires_grad\n",
    "        ]\n",
    "\n",
    "        # ⚠️ 反向传播顺序：从输出层到输入层\n",
    "        # 所以 bucket 要按“反向顺序”构建\n",
    "        reversed_params = list(reversed(trainable_params))\n",
    "\n",
    "        current_bucket_params = []\n",
    "        current_bucket_bytes = 0\n",
    "\n",
    "        for param in reversed_params:\n",
    "            param_bytes = param.numel() * param.element_size()\n",
    "\n",
    "            # 如果当前 bucket 已满，先创建 bucket\n",
    "            if (\n",
    "                current_bucket_params\n",
    "                and current_bucket_bytes + param_bytes > self.bucket_size_bytes\n",
    "            ):\n",
    "                self._create_bucket(current_bucket_params)\n",
    "                current_bucket_params = []\n",
    "                current_bucket_bytes = 0\n",
    "\n",
    "            current_bucket_params.append(param)\n",
    "            current_bucket_bytes += param_bytes\n",
    "\n",
    "        # 最后一个 bucket\n",
    "        if current_bucket_params:\n",
    "            self._create_bucket(current_bucket_params)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Step 3. 注册 backward hooks\n",
    "        # ----------------------------------------------------\n",
    "        for param in trainable_params:\n",
    "            param.register_post_accumulate_grad_hook(\n",
    "                self._create_bucket_hook(param)\n",
    "            )\n",
    "\n",
    "    def _create_bucket(self, params: list[torch.nn.Parameter]):\n",
    "        bucket = GradientBucket(params, bucket_id=len(self.buckets))\n",
    "        self.buckets.append(bucket)\n",
    "\n",
    "        for p in params:\n",
    "            self.param_to_bucket[p] = bucket\n",
    "\n",
    "    def _create_bucket_hook(self, param: torch.nn.Parameter):\n",
    "        \"\"\"\n",
    "        每个参数的 hook：\n",
    "        - 把梯度拷贝进 bucket buffer\n",
    "        - 如果 bucket 满了，立刻发起 async all-reduce\n",
    "        \"\"\"\n",
    "\n",
    "        def hook_fn(p: torch.nn.Parameter):\n",
    "            if p.grad is None:\n",
    "                return\n",
    "\n",
    "            bucket = self.param_to_bucket[p]\n",
    "\n",
    "            # A. 拷贝梯度到 bucket buffer\n",
    "            bucket.param_to_view[p].copy_(p.grad)\n",
    "\n",
    "            # B. 标记一个参数 ready\n",
    "            bucket.ready_count += 1\n",
    "\n",
    "            # C. bucket 内所有参数 ready → 启动通信\n",
    "            if bucket.ready_count == len(bucket.params):\n",
    "                bucket.comm_handle = dist.all_reduce(\n",
    "                    bucket.flat_buffer,\n",
    "                    op=dist.ReduceOp.SUM,\n",
    "                    async_op=True\n",
    "                )\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # 每次 forward 前重置 bucket 状态\n",
    "        for bucket in self.buckets:\n",
    "            bucket.reset()\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def finalize_gradients(self):\n",
    "        \"\"\"\n",
    "        在 optimizer.step() 之前调用：\n",
    "\n",
    "        - 等待所有 bucket 的通信完成\n",
    "        - 对 bucket buffer 做平均\n",
    "        - 拷贝回各参数的 p.grad\n",
    "        \"\"\"\n",
    "\n",
    "        for bucket in self.buckets:\n",
    "            # 1. 等待通信\n",
    "            if bucket.comm_handle is not None:\n",
    "                bucket.comm_handle.wait()\n",
    "\n",
    "            # 2. 平均梯度\n",
    "            bucket.flat_buffer.div_(self.world_size)\n",
    "\n",
    "            # 3. 拷贝回参数梯度\n",
    "            for p in bucket.params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.copy_(bucket.param_to_view[p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "from cs336_myown import DataLoader, TransformerModule, AdamW, CosineSchedule, CrossEntropyLoss, clip_gradient\n",
    "# from transformer import get_linear_schedule_with_warmup\n",
    "import timeit\n",
    "import torch.cuda.nvtx as nvtx\n",
    "\n",
    "\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def wandb_init(d_model, d_ff, n_layers, n_heads, batch_size, epochs, train_steps):\n",
    "    wandb.login()\n",
    "    run = wandb.init(project=\"cs336_final_train\", \n",
    "                    config = {\n",
    "                    # Experiment\n",
    "                    \"experiment_name\": f\"tinystories_17M_{timestamp}\",\n",
    "                    \"total_tokens_processed\": 327_680_000,\n",
    "                    \n",
    "                    # Data\n",
    "                    \"train_data_path\": \"../data/TinyStoriesV2-GPT4-train.txt\",\n",
    "                    \"valid_data_path\": \"../data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "                    \"encoded_ids_train_path\": \"./cs336_systems/encoded_ids_train.pkl\",\n",
    "                    \"encoded_ids_valid_path\": \"./cs336_systems/encoded_ids_valid.pkl\",\n",
    "                    \"vocab_path\": \"vocab.json\",\n",
    "                    \"merges_path\": \"merges.txt\",\n",
    "\n",
    "                    # Model\n",
    "                    \"vocab_size\": 10000,\n",
    "                    \"context_length\": 256,\n",
    "                    \"d_model\": d_model,\n",
    "                    \"d_ff\": d_ff,\n",
    "                    \"n_layers\": n_layers,\n",
    "                    \"n_heads\": n_heads,\n",
    "                    \"rope_theta\": 10000.0,\n",
    "\n",
    "                    # Training\n",
    "                    \"batch_size\": batch_size, # Adjust based on your GPU memory\n",
    "                    # \"learning_rate\": 3e-5,\n",
    "                    #学习率退火相关参数\n",
    "                    \"initial_lr\": 3e-5,\n",
    "                    \"max_learning_rate\": 3e-5,\n",
    "                    \"min_learning_rate\": 1e-5,\n",
    "                    \"lr_warmup_steps\": 2000,\n",
    "                    \"cosine_cycle_iters\": 10000,\n",
    "\n",
    "                    #优化器相关参数\n",
    "                    \"weight_decay\": 0.1,\n",
    "                    \"adam_beta1\": 0.9,\n",
    "                    \"adam_beta2\": 0.95,\n",
    "                    \"eps\": 1e-8,\n",
    "\n",
    "                    #梯度裁剪\n",
    "                    \"grad_clip\": 1.0,\n",
    "\n",
    "                    #训练相关参数\n",
    "                    \"epochs\": epochs,\n",
    "                    \"train_steps\": train_steps,\n",
    "                    \n",
    "                    # Logging & Checkpointing\n",
    "                    \"log_interval\": 20,\n",
    "                    \"val_interval\": 20,\n",
    "                    \"checkpoint_interval\": 60,\n",
    "                    \"checkpoint_dir\": \"checkpoints\",\n",
    "                }\n",
    "                )\n",
    "    config = run.config\n",
    "    return config\n",
    "\n",
    "\n",
    "def train(config,device,train_steps):\n",
    "    # data_path = config[\"train_data_path\"]\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    encoded_ids_train_path = config[\"encoded_ids_train_path\"]\n",
    "    encoded_ids_valid_path = config[\"encoded_ids_valid_path\"]\n",
    "    #直接导入编码后的数据\n",
    "    with open(encoded_ids_train_path, \"rb\") as f:\n",
    "        train_encode_ids = pickle.load(f)\n",
    "    with open(encoded_ids_valid_path, \"rb\") as f:\n",
    "        valid_encode_ids = pickle.load(f)\n",
    "\n",
    "    train_data_loader = DataLoader(train_encode_ids, config[\"batch_size\"],config[\"context_length\"],shuffle=True) # 训练集导入\n",
    "    valid_data_loader = DataLoader(valid_encode_ids, config[\"batch_size\"],config[\"context_length\"],shuffle=True) # 验证集导入\n",
    "  \n",
    "    # 加载模型\n",
    "    model = TransformerModule(config[\"d_model\"], config[\"n_heads\"], config[\"d_ff\"], config[\"context_length\"], config[\"rope_theta\"], config[\"n_layers\"], vocab_size, device).to(device)\n",
    "    # 加载优化器\n",
    "    lr_scheduler = CosineSchedule(config[\"max_learning_rate\"], config[\"min_learning_rate\"], config[\"lr_warmup_steps\"], config[\"cosine_cycle_iters\"]) # 学习率退火   \n",
    "    optimizer = AdamW(model.parameters(), config[\"initial_lr\"], (config[\"adam_beta1\"], config[\"adam_beta2\"]), config[\"eps\"], config[\"weight_decay\"])\n",
    "    # 加载损失函数\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    # print(\"模型加载完成\")\n",
    "    # 训练循环\n",
    "    model.train()\n",
    "    global_step = 0  # 初始化全局步数\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):#每个epoch代表训练完了一整个所有批次的数据，不能轻易和step混淆\n",
    "        if (epoch+1) >= 10:\n",
    "            timer_func = timeit.default_timer\n",
    "            start_time = timer_func()\n",
    "        for step in range(train_steps):\n",
    "            # 更新学习率\n",
    "            new_lr = lr_scheduler(global_step)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            # print(\"epoch\",epoch,\"lr\",new_lr)\n",
    "            x,y = train_data_loader.get_train_batch_data()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)  # shape: (batch, seq, vocab_size)\n",
    "            \n",
    "            loss = loss_fn.forward(logits, y)\n",
    "            # loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            # start_backward_time = timer_func()\n",
    "            loss.backward() \n",
    "            # end_backward_time = timer_func()\n",
    "            clip_gradient(model.parameters(), config[\"grad_clip\"])\n",
    "            optimizer.step()\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            global_step += 1 # 增加全局步数\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch} Step {step} LR {new_lr:.6f} Loss: {loss.item()}\")\n",
    "        if (epoch+1) >= 10:\n",
    "            end_time = timer_func()\n",
    "        # print(f\"Epoch {epoch} took {end_time - start_time:.2f} seconds\")\n",
    "            with open(f\"{config[\"d_model\"]}_{config[\"d_ff\"]}_{config[\"n_layers\"]}_{config[\"n_heads\"]}_train_time.txt\", \"a\") as f:\n",
    "                f.write(f\"Epoch {epoch} took {end_time - start_time:.2f} seconds\\n\")\n",
    "        # print(\"训练完了\")\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "        if (epoch+1) % config[\"val_interval\"] == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_data_loader.get_valid_batch_data_iter():\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    logits = model(x)\n",
    "                    #   loss = loss_fn.forward(logits, y)\n",
    "                    # loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "                    loss = loss_fn.forward(logits, y)\n",
    "                    wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "        # print(\"经过验证集\")\n",
    "        if (epoch+1) % config[\"checkpoint_interval\"] == 0:\n",
    "            # torch.save(model.state_dict(), f\"checkpoints/model_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                # 可以加上'loss': loss.item()等\n",
    "            }, f\"checkpoints/model_epoch_{epoch}_{timestamp}.pth\")\n",
    "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"checkpoints/model_final_{timestamp}.pth\")\n",
    "    print(\"Final checkpoint saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:6\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=40)\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=2000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument('--no-rmsnorm', dest='use_rmsnorm', action='store_false', help=\"Disable RMSNorm and use LayerNorm instead\")\n",
    "\n",
    "    parser.add_argument(\"--d_model\", type=int, default=768)\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=3072)\n",
    "    parser.add_argument(\"--n_layers\", type=int, default=12)\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=12)\n",
    "\n",
    "\n",
    "    parser.set_defaults(use_rmsnorm=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = args.device\n",
    "    epochs = args.epochs\n",
    "    train_steps = args.train_steps\n",
    "    batch_size = args.batch_size\n",
    "    d_model = args.d_model\n",
    "    d_ff = args.d_ff\n",
    "    n_layers = args.n_layers\n",
    "    n_heads = args.n_heads\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    config = wandb_init(d_model, d_ff, n_layers, n_heads, batch_size, epochs, train_steps)\n",
    "\n",
    "    train(config,device,train_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2_System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
