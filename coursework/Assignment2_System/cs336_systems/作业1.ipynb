{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192270c9",
   "metadata": {},
   "source": [
    "# 前期准备工作\n",
    "\n",
    "## 在windows上安装Ubuntu-22.04（使用triton需要使用在linux上运行）\n",
    "\n",
    "由于triton在windows没有可用的版本，考虑到大家都是使用的windows系统的比较多，这里附一个安装教程，有linux系统或者不想运行triton模块的同学可以跳过，\n",
    "整个作业使用**linux**进行，读者可以使用**windows的wsl**来在windows中跑通作本次作业是关于GPU和分布式训练的，如果有多个GPU就可以跑通，只有一个GPU也可以，但是就没有分布式训练的速度优势，但是作为教学学习是足够的。\n",
    "\n",
    "如果您想使用自己作业1的实现，请将`cs336-basics`目录替换为您自己的实现，或编辑外层`pyproject.toml`文件以指向您自己的实现\n",
    "\n",
    "### 安装步骤\n",
    "\n",
    "#### 1. 版本限制\n",
    "\n",
    "我们需要在windows上安装wsl2，限制在Windows 10 2004 (19041) 及以上 或者 Windows 11 任意版本\n",
    "\n",
    "#### 2.手动启用 WSL 相关功能（关键）\n",
    "\n",
    "按住win键搜索**启用或者关闭windows功能**，找到并勾选 **于Linux的Windows子系统**和 **虚拟机平台**，点击确定\n",
    "\n",
    "\n",
    "#### 3. 确认 CPU 虚拟化已开启\n",
    "\n",
    "按住Ctrl + Shift + Esc打开任务管理器，性能 -> CPU ，右侧应显示：虚拟化：已启用\n",
    "\n",
    "如果是“未启用”\n",
    "\n",
    "需要进 BIOS / UEFI，开启：\n",
    "\n",
    "Intel：Intel Virtualization Technology (VT-x)\n",
    "\n",
    "AMD：SVM Mode\n",
    "\n",
    "##### 4. 安装 Ubuntu\n",
    "\n",
    "在windows搜索页面中输入CMD，并且以**管理员身份**运行\n",
    "\n",
    "```bash\n",
    "wsl --install -d Ubuntu\n",
    "```\n",
    "\n",
    "之后设置账号密码，密码在输入的时候是不可见的，看不到是正常的\n",
    "\n",
    "要打开ubuntu的命令行只需要在CMD命令行中输入\n",
    "```bash\n",
    "wsl\n",
    "```\n",
    "\n",
    "##### 5. 安装相关的工具 pip uv 等\n",
    "\n",
    "依次执行\n",
    "先使用管理员身份打开CMD\n",
    "\n",
    "输入\n",
    "```bash\n",
    "\n",
    "cd $env:USERPROFILE\n",
    "@\"\n",
    "[wsl2]\n",
    "networkingMode=mirrored\n",
    "dnsTunneling=true\n",
    "autoProxy=true\n",
    "firewall=true\n",
    "\"@ | Out-File -Encoding UTF8 .wslconfig\n",
    "\n",
    "\n",
    "wsl --shutdown     # 彻底关掉虚拟机\n",
    "wsl                # 再进子系统\n",
    "```\n",
    "\n",
    "```bash\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install python3-pip\n",
    "\n",
    "sudo apt install python3.12-venv\n",
    "\n",
    "source ~/myenv/bin/activate\n",
    "pip install uv -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "sudo ln -s /home/kangkang/myenv/bin/uv /usr/local/bin/uv\n",
    "\n",
    "echo 'export UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple' >> ~/.bashrc\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### 6. 设置vscode的远程连接\n",
    "\n",
    "打开vscode ，先搜索插件wsl。安装完成后打开左侧一列的远程连接就可以连接到wsl的ubuntu系统中\n",
    "\n",
    "\n",
    "运行在CS336-Chinese-co-construction\\coursework\\Assignment2_System下\n",
    "```bash\n",
    "cd CS336-Chinese-co-construction\\coursework\\Assignment2_System\n",
    "uv sync\n",
    "uv run\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d935",
   "metadata": {},
   "source": [
    "# 1. 基准测试和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f745648",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "（a）编写一个脚本，对模型的前向与反向传播进行基本的端到端基准测试。具体而言，你的脚本应支持以下功能：\n",
    "- 给定超参数（例如层数），初始化一个模型。\n",
    "- 生成一个随机批次的数据。\n",
    "- 先运行 w 个预热步骤（开始计时之前），然后计时执行 n 个步骤（仅前向或同时包含前向与反向，取决于参数）。计时可使用 Python 的 timeit 模块（例如直接调用 timeit 函数，或使用 timeit.default_timer()，后者提供系统最高分辨率的时钟，比 time.time() 更适合基准测试）。\n",
    "- 每一步之后调用 torch.cuda.synchronize()。\n",
    "\n",
    "交付物：一个脚本，能够根据给定超参数初始化基础 Transformer 模型，创建随机数据批次，并对前向与反向传播进行计时。\n",
    "\n",
    "（b）对 §1.2 中描述的模型规模，分别测试前向与反向传播耗时。  \n",
    "使用 5 个预热步骤，并在 10 次测量步骤上计算平均耗时与标准差。  \n",
    "一次前向传播耗时多久？反向传播呢？测量结果波动大吗，还是标准差很小？  \n",
    "\n",
    "交付物：1–2 句话给出你的计时结果。\n",
    "\n",
    "（c）基准测试的一个常见陷阱是跳过预热步骤。请在没有预热的情况下重复上述分析。结果如何变化？你认为原因是什么？再尝试仅使用 1 或 2 个预热步骤，为何结果仍可能不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9210e8",
   "metadata": {},
   "source": [
    "## 首先检查GPU是否可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea2358",
   "metadata": {},
   "source": [
    "这边通过pyproject.toml下载的torch是cpu版本\n",
    "如果要安装GPU版本：\n",
    "```bash\n",
    "uv pip uninstall torch \n",
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "cu121是我的cuda版本为12.1，可以改为自己的cuda版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036a681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.model import BasicsTransformerLM \n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "from cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.data import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a57e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "MPS（苹果的MPS） available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA GPU detected\")\n",
    "\n",
    "print(\"MPS（苹果的MPS） available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2625f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b8414",
   "metadata": {},
   "source": [
    "现在来测试一下sleep函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7986b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：50.19839604695638ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.19839604695638"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(\"sleep\", lambda : time.sleep(50 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdfb5b",
   "metadata": {},
   "source": [
    "在误差范围之内"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6048",
   "metadata": {},
   "source": [
    "我们来实测一下transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5522e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 3e-5\n",
    "warmup_iters = 200\n",
    "cosine_cycle_iters = 10_000\n",
    "num_steps = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[184, 185, 186, 187, 188, 189, 190, 191],\n",
      "        [130, 131, 132, 133, 134, 135, 136, 137],\n",
      "        [360, 361, 362, 363, 364, 365, 366, 367],\n",
      "        [683, 684, 685, 686, 687, 688, 689, 690]], device='cuda:0')\n",
      "tensor([[185, 186, 187, 188, 189, 190, 191, 192],\n",
      "        [131, 132, 133, 134, 135, 136, 137, 138],\n",
      "        [361, 362, 363, 364, 365, 366, 367, 368],\n",
      "        [684, 685, 686, 687, 688, 689, 690, 691]], device='cuda:0')\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import gc\n",
    "def test_transformer():\n",
    "\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # 假设是 token 序列\n",
    "    dataset = np.arange(1000, dtype=np.int64)\n",
    "    x, y = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=4,\n",
    "        context_length=8,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(x.shape)  # (4, 8)\n",
    "    print(y.shape)  # (4, 8)\n",
    "    print(x)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    model(x)\n",
    "    print(f'设备：{device}')\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x,y, optimizer,dataset\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8295b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "现在真正计时!\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "单次耗时：7469.104385375977ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7469.104385375977"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy  as np\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_model(num_steps = 1):\n",
    "    min_lr = 3e-5\n",
    "    warmup_iters = 200\n",
    "    cosine_cycle_iters = 10_000\n",
    "    num_steps = 10\n",
    "    batch_size = 32\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    min_lr = 3e-5\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "    \n",
    "\n",
    "        model = BasicsTransformerLM(\n",
    "                                vocab_size=vocab_size,\n",
    "                                context_length=context_length,\n",
    "                                d_model=d_model,\n",
    "                                num_layers=num_layers,\n",
    "                                num_heads=num_heads,\n",
    "                                d_ff=d_ff,\n",
    "                                rope_theta=rope_theta,\n",
    "                                ).to(device)\n",
    "        optimizer = AdamW(\n",
    "                            model.parameters(),\n",
    "                            lr=max_lr,\n",
    "                            weight_decay=0.1,\n",
    "                        )\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        # print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7日志\n",
    "    if it % 10 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dcaf6",
   "metadata": {},
   "source": [
    "## 推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                            vocab_size=vocab_size,\n",
    "                            context_length=context_length,\n",
    "                            d_model=d_model,\n",
    "                            num_layers=num_layers,\n",
    "                            num_heads=num_heads,\n",
    "                            d_ff=d_ff,\n",
    "                            rope_theta=rope_theta,\n",
    "                            ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 用 get_batch 拿一个 batch\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "        x, _ = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=1,\n",
    "            context_length= 1,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # 取第一个样本作为 prompt\n",
    "        prompt = x # shape: (context_length,)\n",
    "        # 调用模型自带的 generate\n",
    "        generated = model.generate(\n",
    "            x=prompt,\n",
    "            max_new_tokens=5,\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            eos_token_id=None,\n",
    "        )\n",
    "\n",
    "        print(generated.shape)  # (<=50,)\n",
    "        print(generated)\n",
    "        try:\n",
    "            del model, x, prompt, generated, dataset\n",
    "        except NameError:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ad903",
   "metadata": {},
   "source": [
    "### 性能分析工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db37a8a",
   "metadata": {},
   "source": [
    "使用wsl后无法测量cuda时间，这是正常现象，可以使用正常的linux系统或者windows系统，原因在于wsl的linux无法使用torch的一些接口(CUPTI)来检测cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88bb1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda : 12.8\n",
      "CUPTI: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "import torch, ctypes, os\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda :', torch.version.cuda)\n",
    "try:\n",
    "    ctypes.CDLL('libcupti.so')\n",
    "    print('CUPTI: OK')\n",
    "except:\n",
    "    print('CUPTI: NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c38995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "正在使用cuda\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         0.58%      39.712ms         1.13%      77.319ms      46.860us        1.413s        48.65%        1.413s     856.397us          1650  \n",
      "                                                           ampere_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     479.847ms        16.52%     479.847ms       1.116ms           430  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us     420.992ms        14.50%     420.992ms      42.099ms            10  \n",
      "                                                                       aten::mul         0.91%      62.183ms         8.75%     600.001ms      96.931us     384.862ms        13.25%     401.992ms      64.942us          6190  \n",
      "                                                                     aten::copy_         0.29%      19.929ms         6.35%     435.595ms     168.835us     378.042ms        13.02%     389.980ms     151.155us          2580  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_nt_align1>(cutlass_80...         0.00%       0.000us         0.00%       0.000us       0.000us     351.013ms        12.09%     351.013ms       1.847ms           190  \n",
      "                                                Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     323.554ms        11.14%     323.554ms     557.853us           580  \n",
      "                                                           ampere_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     247.754ms         8.53%     247.754ms     589.890us           420  \n",
      "                                                          ampere_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     225.779ms         7.77%     225.779ms      22.578ms            10  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us     210.675ms         7.25%     210.675ms     210.675us          1000  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.858s\n",
      "Self CUDA time total: 2.904s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "\n",
    "print(profile(description ='transformer' , run = run_model, num_warmups = 3, with_stack = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e633611",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "然而，简单地将模型转换为低精度格式可能会导致模型精度降低。例如，许多梯度值在实践中太小，无法用 FP16 表示，因此在用 FP16 精度进行简单训练时会变成零。为了解决这个问题，通常在用 FP16 训练时使用损失缩放（loss is simply multiplied by a scaling factor, increasing gradient magnitudes so they don't flush to zero）。此外，FP16 的动态范围比 FP32 低，这可能导致溢出，表现为 NaN 损失。全精度（Full bfloat16）训练通常更稳定（因为 BF16 具有与 FP32 相同的动态范围），但与 FP32 相比仍可能影响最终模型性能。\n",
    "\n",
    "为了利用低精度数据类型的加速优势，通常使用混合精度训练。在 PyTorch 中，这通过 torch.autocast 上下文管理器实现。在这种情况下，某些操作（例如矩阵乘法）在低精度数据类型上执行，而需要 FP32 全动态范围的操作（例如累积和归约）保持不变。例如，以下代码将自动识别在前向传播期间需要在低精度上执行的操作，并将这些操作转换为指定的数据类型：\n",
    "\n",
    "```python\n",
    "model : torch.nn.Module = ...  # 例如，你的 Transformer 模型\n",
    "dtype : torch.dtype = ...  # 例如，torch.float16\n",
    "x : torch.Tensor = ...  # 输入数据\n",
    "\n",
    "with torch.autocast(device=\"cuda\", dtype=dtype):\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "如上所述，即使张量本身已被降级，通常最好在高精度下保持累积。以下练习将帮助你建立直觉，了解为什么会这样。\n",
    "\n",
    "问题（混合精度累积）：1 分\n",
    "\n",
    "运行以下代码并评论结果的准确性。\n",
    "\n",
    "```python\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "```\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "我们现在将混合精度应用于一个玩具模型进行直观理解，然后应用于我们的基准测试脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20ec484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "\n",
    "\n",
    "#  清理显存引用\n",
    "try:\n",
    "    del s, x,i\n",
    "except NameError:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31646db",
   "metadata": {},
   "source": [
    "可以看到全精度最准确，半进度最不准确，混合精度介于二者之间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270f15e",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "问题（benchmarking_mixed_precision）：2 分\n",
    "\n",
    "（a）考虑以下模型：\n",
    "\n",
    "```python\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "假设我们在 GPU 上训练模型，并且模型参数最初是 FP32。我们希望使用 FP16 的自动混合精度。上述列出的每个组件的数据类型是什么？\n",
    "\n",
    "- autocast 上下文内的模型参数，\n",
    "- 第一个前馈层（ToyModel.fc1）的输出，\n",
    "- 层归一化（ToyModel.ln）的输出，\n",
    "- 模型的预测 logits，\n",
    "- 损失，\n",
    "- 以及模型的梯度？\n",
    "\n",
    "交付物：上述列出的每个组件的数据类型。\n",
    "\n",
    "（b）你应该已经看到，FP16 混合精度自动转换对层归一化层的处理与前馈层不同。层归一化的哪些部分对混合精度敏感？如果我们使用 BF16 而不是 FP16，是否仍需要对层归一化进行不同处理？为什么或为什么不？\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "（c）修改你的基准测试脚本，以便可以选择使用 BF16 混合精度运行模型。对 §1.1.2 中描述的每种语言模型规模，分别计时使用和不使用混合精度的前向和反向传播。比较使用全精度与混合精度的结果，并评论模型大小变化时的任何趋势。你可能会发现无上下文的 no-op 上下文管理器很有用。\n",
    "\n",
    "交付物：2–3 句话的回应，包含你的计时结果和评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7910a",
   "metadata": {},
   "source": [
    "# 解答\n",
    "\n",
    "\n",
    "### 问题 (a)\n",
    "\n",
    "在自动混合精度（Autocast）下，使用 FP16 训练时，各组件的数据类型如下：\n",
    "\n",
    "- **模型参数（model parameters）**：由于模型参数最初是 FP32，它们在 autocast 上下文内保持为 FP32。\n",
    "- **第一个前馈层的输出（output of the first feed-forward layerToyModel.fc1）**：在 autocast 上下文中，该层的输出将被转换为 FP16。\n",
    "- **层归一化的输出（output of layer norm ToyModel.ln）**：层归一化层的输出通常保持为 FP32，因为层归一化需要较高的数值稳定性，FP16 的动态范围较小，可能导致数值不稳定。\n",
    "- **模型的预测 logits**：在 autocast 上下文中，这些 logits 将被转换为 FP16。\n",
    "- **损失（loss）**：损失计算通常在 FP32 中进行，以保持数值稳定性。\n",
    "- **模型的梯度（model's gradients）**：梯度通常也保持为 FP32，以确保优化过程的稳定性。\n",
    "\n",
    "### 问题 (b)\n",
    "\n",
    "层归一化对混合精度的敏感性主要体现在其计算过程中需要较高的数值精度。层归一化涉及到均值和方差的计算，这些计算在 FP16 中可能会因为数值范围的限制而导致不准确，从而影响模型的训练效果。因此，即使在 FP16 混合精度下，层归一化通常也需要保持在 FP32 中进行，以确保数值稳定性。\n",
    "\n",
    "如果我们使用 BF16 而不是 FP16，我们可能仍然需要对层归一化进行特殊处理。BF16 虽然在动态范围上与 FP32 相同，但其精度较低，可能仍然不足以保证层归一化计算的稳定性。因此，为了保持数值稳定性，层归一化可能仍然需要在 FP32 中进行。\n",
    "\n",
    "### 问题 (c)\n",
    "\n",
    "为了在基准测试脚本中使用 BF16 混合精度，我们可以修改脚本以支持 BF16。以下是修改后的脚本示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 模型初始化\n",
    "model = ToyModel(100, 10)\n",
    "x = torch.randn(1, 100)\n",
    "\n",
    "# 使用 FP32 进行基准测试\n",
    "with torch.no_grad():\n",
    "    fp32_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "# 使用 BF16 进行基准测试\n",
    "with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    with torch.no_grad():\n",
    "        bf16_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "print(f\"FP32 time: {fp32_time}\")\n",
    "print(f\"BF16 time: {bf16_time}\")\n",
    "```\n",
    "\n",
    "在比较使用全精度（FP32）与混合精度（BF16）的结果时，我们可能会观察到 BF16 在某些情况下可以提供与 FP32 相似的精度，同时显著减少内存使用和加速计算。然而，对于需要高精度计算的层（如层归一化），BF16 可能不如 FP32 稳定。随着模型大小的变化，我们可能会看到不同规模的模型对混合精度的敏感性不同，这需要在实际应用中进行具体分析和调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4eea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "import gc\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def test_fb32_BF16_time():\n",
    "\n",
    "    # 模型初始化，可以修改参数来测试效果\n",
    "    model = ToyModel(100, 10)\n",
    "    x = torch.randn(1, 100)\n",
    "\n",
    "\n",
    "\n",
    "    # 使用 FP32 进行基准测试\n",
    "    with torch.no_grad():\n",
    "        fp32_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "    # 使用 BF16 进行基准测试\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        with torch.no_grad():\n",
    "            bf16_time = timeit.timeit(lambda: model(x), number=100)\n",
    "\n",
    "    #  清理显存引用\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"FP32 time: {fp32_time}\")\n",
    "    print(f\"BF16 time: {bf16_time}\")\n",
    "    try:\n",
    "        del model, x, fp32_time, bf16_time\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd52bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 time: 0.039825333005865104\n",
      "BF16 time: 0.0021841169946128502\n"
     ]
    }
   ],
   "source": [
    "test_fb32_BF16_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81311fb",
   "metadata": {},
   "source": [
    "可以看到不同精度下，精度越低则速度越快"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eed308",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "\n",
    "到目前为止，我们一直在关注计算性能。现在我们将把注意力转向**内存**，这是语言模型训练和推理中的另一项关键资源。\n",
    "PyTorch 也提供了一个强大的**内存分析器（memory profiler）**，可以随时间跟踪内存分配情况。\n",
    "\n",
    "要使用内存分析器，你可以按如下方式修改你的 benchmark 脚本：\n",
    "\n",
    "```\n",
    "7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ... 在你的 benchmarking 脚本中的 warm-up 阶段\n",
    "\n",
    "# 开始记录内存历史\n",
    "torch.cuda.memory._record_memory_history(max_entries=1000000)\n",
    "\n",
    "# ... 在你的 benchmarking 脚本中你想要分析的部分\n",
    "\n",
    "# 保存一个 pickle 文件，用于 PyTorch 的在线工具加载\n",
    "torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "\n",
    "# 停止记录内存历史\n",
    "torch.cuda.memory._record_memory_history(enabled=None)\n",
    "```\n",
    "\n",
    "这将输出一个名为 `memory_snapshot.pickle` 的文件，你可以将其加载到如下在线工具中：\n",
    "[https://pytorch.org/memory_viz](https://pytorch.org/memory_viz)\n",
    "该工具可以让你查看整体内存使用时间线，以及每一次独立的内存分配是如何发生的，包括其大小以及指向分配来源代码位置的调用栈追踪。\n",
    "\n",
    "要使用该工具，你应当在浏览器中打开上述链接，然后将你的 pickle 文件拖拽到页面中。\n",
    "\n",
    "现在你将使用 PyTorch profiler 来分析你模型的内存使用情况。\n",
    "\n",
    "---\n",
    "\n",
    "### Problem（memory profiling）：4 分\n",
    "\n",
    "对 **表 1 中的 2.7B 模型**进行 profiling，分析其 **前向传播（forward pass）**、**反向传播（backward pass）** 以及 **优化器步骤（optimizer step）**，上下文长度分别为 **128、256 和 512**。\n",
    "\n",
    "---\n",
    "\n",
    "**(a)**\n",
    "在你的 profiling 脚本中加入一个选项，使模型运行时能够经过内存分析器。\n",
    "你可能需要复用之前的一些基础设施（例如：激活 mixed-precision、加载特定的模型大小等）。\n",
    "\n",
    "然后，在以下两种情况下，为 2.7B 模型获取内存 profile：\n",
    "\n",
    "* 只做推理（仅前向传播）\n",
    "* 完整训练一步（前向 + 反向 + 优化器步骤）\n",
    "\n",
    "你的内存时间线看起来如何？你能否根据看到的峰值判断当前运行的是哪个阶段？\n",
    "\n",
    "**Deliverable：**\n",
    "来自 `pytorch.org/memory_viz` 工具中 **“Active Memory Timeline”** 的两张截图（针对 context length = 512）：\n",
    "\n",
    "* 一张是仅前向传播\n",
    "* 一张是完整训练步骤（前向和反向传播，不含 optimizer step），\n",
    "  并附上一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(b)**\n",
    "在仅进行前向传播时，不同 context length 下的峰值内存使用是多少？\n",
    "在执行完整训练步骤时又是多少？\n",
    "\n",
    "**Deliverable：**\n",
    "一张表格，每个 context length 对应两个数值。\n",
    "\n",
    "---\n",
    "\n",
    "**(c)**\n",
    "在使用 **mixed-precision** 的情况下，找出 2.7B 模型在以下两种情形下的峰值内存使用：\n",
    "\n",
    "* 前向传播\n",
    "* 完整优化器步骤\n",
    "\n",
    "Mixed-precision 是否显著影响了内存使用？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(d)**\n",
    "考虑 2.7B 模型。给定我们的参考超参数，在 **single-precision** 下，\n",
    "Transformer 残差流（residual stream）中 **激活张量（activations）** 的大小是多少？\n",
    "\n",
    "请以 **MB** 为单位给出结果（即用字节数除以 (1024^2)）。\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的回答，并给出你的推导过程。\n",
    "\n",
    "---\n",
    "\n",
    "**(e)**\n",
    "仔细查看 `pytorch.org/memory_viz` 中，2.7B 模型在前向传播时的 **“Active Memory Timeline”**。\n",
    "\n",
    "当你将 **“Detail”** 级别调低时，工具会显示最小分配的快照\n",
    "（例如，将 “Detail” 调到 **10%**，表示展示 **10% 最大的分配**）。\n",
    "\n",
    "最大的分配大小是多少？\n",
    "查看其调用栈（stack trace），你能判断这些分配来自哪里吗？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的文字说明。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b4afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import Callable\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "def memory_profile(\n",
    "    description: str,\n",
    "    run,\n",
    "    num_warmups: int = 0,\n",
    "    out_dir: str = \"memory_snapshots\",\n",
    "):\n",
    "    assert torch.cuda.is_available(), \"Memory profiling requires CUDA\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    snapshot_path = os.path.join(out_dir, f\"{description}.pickle\")\n",
    "\n",
    "    # ----------------------\n",
    "    # Warm-up（不记录）\n",
    "    # ----------------------\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # ----------------------\n",
    "    # 开始记录内存历史（LEGACY SAFE）\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._record_memory_history(True)\n",
    "\n",
    "    # ----------------------\n",
    "    # 被 profile 的运行\n",
    "    # ----------------------\n",
    "    run()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # ----------------------\n",
    "    # 导出 snapshot\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._dump_snapshot(snapshot_path)\n",
    "\n",
    "    # ----------------------\n",
    "    # 停止记录\n",
    "    # ----------------------\n",
    "    torch.cuda.memory._record_memory_history(False)\n",
    "\n",
    "    print(f\"[Memory snapshot saved] {snapshot_path}\")\n",
    "\n",
    "    return snapshot_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e7b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.optimizer import AdamW, get_cosine_lr\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.data import get_batch\n",
    "import torch.nn.functional as F\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_model(num_steps = 1):\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    batch_size = 32\n",
    "    \n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    \n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    warmup_iters = 200\n",
    "    cosine_cycle_iters = 10_000\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    for it in range(num_steps):\n",
    "        print(f'{it}/{num_steps}')\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        #print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "        #  清理显存引用\n",
    "        try:\n",
    "            del X, attention_fn, out, loss\n",
    "        except NameError:\n",
    "            pass\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    # 7日志\n",
    "    if it % 100 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee061ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8304 | lr 0.00e+00\n",
      "[Memory snapshot saved] memory_snapshots/前向传播.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'memory_snapshots/前向传播.pickle'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_profile(description=\"前向传播\",run=run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dbd6",
   "metadata": {},
   "source": [
    "# Flash Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4215642",
   "metadata": {},
   "source": [
    "\n",
    "# 作业 五\n",
    "\n",
    "你的性能分析结果很可能表明，在注意力层中，无论是**内存**还是**计算**方面，都存在优化空间。从高层次来看，注意力操作由一次矩阵乘法、一次 softmax，然后再一次矩阵乘法组成：\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\mathrm{mask}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\\right)V\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "朴素的注意力实现需要为每个 batch / head 元素保存形状为\n",
    "\n",
    "${seqlen} \\times {seqlen}$ 的注意力分数矩阵。随着序列长度变长，这个矩阵会迅速变得非常大，从而在长输入或长输出任务中导致 **out-of-memory (OOM)** 错误。\n",
    "我们将实现一个遵循 **FlashAttention-2** 论文的注意力 kernel，它通过 **按 tile 方式计算注意力**，避免显式地物化 ${seqlen} \\times {seqlen}$ 的注意力分数矩阵，从而支持更长的序列长度。\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem (pytorch_attention): 2 points**\n",
    "\n",
    "#### (a) 在不同规模下对你的注意力实现进行基准测试。编写一个脚本来完成以下任务：\n",
    "\n",
    "(a) 将 batch size 固定为 **8**，并且 **不使用多头注意力**（即移除 head 这一维）。\n",
    "\n",
    "(b) 遍历如下笛卡尔积配置：\n",
    "\n",
    "* head embedding 维度 $d_{\\text{model}} \\in {16, 32, 64, 128}$\n",
    "* 序列长度 ${seqlen} \\in {256, 1024, 4096, 8192, 16384}$\n",
    "\n",
    "(c) 为对应的尺寸生成随机输入 (Q, K, V)。\n",
    "\n",
    "(d) 使用这些输入，对注意力前向传播 **计时 100 次**。\n",
    "\n",
    "(e) 测量在 **反向传播开始之前** 的显存使用量，并对 **反向传播计时 100 次**。\n",
    "\n",
    "(f) 确保进行 **warm-up**，并在每次前向 / 反向传播之后调用\n",
    "`torch.cuda.synchronize()`。\n",
    "\n",
    "---\n",
    "\n",
    "请报告你在这些配置下得到的 **时间结果（或 out-of-memory 错误）**。\n",
    "**在什么规模下会出现 out-of-memory？**\n",
    "\n",
    "请选择你发现的 **最小的、仍然会 OOM 的配置之一**，对注意力的内存使用进行详细的**显存占用核算**（你可以使用 Assignment 1 中 Transformer 内存使用的公式）。\n",
    "\n",
    "* 反向传播中节省的显存会如何随着序列长度变化？\n",
    "* 你会如何 **消除反向传播中的这部分内存开销**？\n",
    "\n",
    "---\n",
    "\n",
    "### **Deliverable（提交内容）**\n",
    "\n",
    "* 一张包含 **时间测量结果** 的表格\n",
    "* 你对 **内存使用的推导过程**\n",
    "* 一段 **1–2 段的文字分析回答**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707a115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头、朴素自注意力实现\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 线性投影（Q, K, V）\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        return: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model\n",
    "\n",
    "        # 1. 线性投影\n",
    "        Q = self.q_proj(x)  # (B, T, D)\n",
    "        K = self.k_proj(x)  # (B, T, D)\n",
    "        V = self.v_proj(x)  # (B, T, D)\n",
    "\n",
    "        # 2. 注意力分数 (B, T, T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "\n",
    "        # 3. softmax\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # 4. 加权求和\n",
    "        out = torch.matmul(attn, V)  # (B, T, D)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cf245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OOM: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OOM: d=32, T=16384\n",
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OOM: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OOM: d=128, T=16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d_model': 16,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.19712448120117188,\n",
       "  'backward_time_ms': 1.0917901992797852,\n",
       "  'mem_MB': 19.00634765625},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.0440826416015625,\n",
       "  'backward_time_ms': 3.292059898376465,\n",
       "  'mem_MB': 51.25634765625},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 15.674781799316406,\n",
       "  'backward_time_ms': 47.9339599609375,\n",
       "  'mem_MB': 540.25634765625},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 322.64397144317627,\n",
       "  'backward_time_ms': 1256.8419456481934,\n",
       "  'mem_MB': 2088.25634765625},\n",
       " {'d_model': 16, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.17549991607666016,\n",
       "  'backward_time_ms': 1.0680675506591797,\n",
       "  'mem_MB': 19.77392578125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.1481761932373047,\n",
       "  'backward_time_ms': 3.741121292114258,\n",
       "  'mem_MB': 54.27392578125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 16.518235206604004,\n",
       "  'backward_time_ms': 50.249552726745605,\n",
       "  'mem_MB': 552.27392578125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 243.1901216506958,\n",
       "  'backward_time_ms': 413.28980922698975,\n",
       "  'mem_MB': 2112.27392578125},\n",
       " {'d_model': 32, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.15537738800048828,\n",
       "  'backward_time_ms': 1.0670185089111328,\n",
       "  'mem_MB': 21.34423828125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.265549659729004,\n",
       "  'backward_time_ms': 3.5120248794555664,\n",
       "  'mem_MB': 60.34423828125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 16.463303565979004,\n",
       "  'backward_time_ms': 50.7037878036499,\n",
       "  'mem_MB': 576.34423828125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 236.27283573150635,\n",
       "  'backward_time_ms': 824.6513605117798,\n",
       "  'mem_MB': 2160.34423828125},\n",
       " {'d_model': 64, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.21293163299560547,\n",
       "  'backward_time_ms': 1.2351036071777344,\n",
       "  'mem_MB': 24.62548828125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.272892951965332,\n",
       "  'backward_time_ms': 3.793001174926758,\n",
       "  'mem_MB': 72.62548828125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 16.986703872680664,\n",
       "  'backward_time_ms': 50.85306167602539,\n",
       "  'mem_MB': 624.62548828125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 250.72762966156003,\n",
       "  'backward_time_ms': 867.3455715179443,\n",
       "  'mem_MB': 2256.62548828125},\n",
       " {'d_model': 128, 'seqlen': 16384, 'OOM': True}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "                X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "         \n",
    "                attention_fn = NaiveSelfAttention(d_model= d_model).to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                # warm-up\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                # forward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.time() - start) / 10\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 10\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "          \n",
    "            finally:\n",
    "                #  清理显存引用\n",
    "                try:\n",
    "                    del X, attention_fn, out, loss\n",
    "                except NameError:\n",
    "                    pass\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    return results\n",
    "benchmark_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0e5d",
   "metadata": {},
   "source": [
    "**已知输入张量形状**\n",
    "\n",
    "```python\n",
    "x.shape = [8, 8192, 16]\n",
    "dtype = torch.float32  # 默认\n",
    "```\n",
    "\n",
    "* batch size (B = 8)\n",
    "* sequence length (T = 8192)\n",
    "* hidden dim (D = 16)\n",
    "* float32 = **4 bytes**\n",
    "\n",
    "---\n",
    "\n",
    "**元素总数**\n",
    "\n",
    "$$\n",
    "8 \\times 8192 \\times 16 = 1{,}048{,}576 \\text{ elements}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**显存占用（仅 input x）**\n",
    "\n",
    "$$\n",
    "1{,}048{,}576 \\times 4 \\text{ bytes}\n",
    "= 4{,}194{,}304 \\text{ bytes}\n",
    "\\approx 4.0 \\text{ MB}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> **输入 `torch.Size([8, 8192, 16])` 本身仅占约 `4 MB` 显存**\n",
    "\n",
    "\n",
    "真正爆显存的是注意力里的**attention score 矩阵**\n",
    "\n",
    "```python\n",
    "scores.shape = [8, 8192, 8192]\n",
    "```\n",
    "\n",
    "元素数：\n",
    "\n",
    "$$\n",
    "8 \\times 8192^2 = 536{,}870{,}912\n",
    "$$\n",
    "\n",
    "显存：\n",
    "\n",
    "\n",
    "$$\n",
    "536{,}870{,}912 \\times 4 \\approx 2.0 \\text{ GB}\n",
    "$$\n",
    "\n",
    "而且，scores 要存，softmax 输出要存，backward 还要再存一份中间量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cbd65",
   "metadata": {},
   "source": [
    "我们将在这里实现一个flash attention，我们需要实现两个部分，一个是分块，第二个是数值稳定softmax的方式，思想上非常接近 FlashAttention v1，具体可以看第六章：GPU和GPU相关的优化，有详细介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f7859",
   "metadata": {},
   "source": [
    "# 作业六\n",
    "Problem (torch_compile): 2 points\n",
    "------------------------------------------------------------\n",
    "\n",
    "(a) 扩展你的 attention 基准测试脚本，\n",
    "    加入你使用 PyTorch 实现的 attention 的\n",
    "    **编译版本（compiled version）**，\n",
    "    并在与上一个 pytorch_attention 问题\n",
    "    相同的配置下，将其性能与未编译版本进行比较。\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比你 **编译后的 attention 模块**\n",
    "    与 **pytorch_attention 问题中未编译版本**\n",
    "    在前向传播和反向传播上的耗时。\n",
    "\n",
    "(b) 接下来，在端到端的基准测试脚本中，\n",
    "    将你的整个 Transformer 模型进行编译。\n",
    "    前向传播的性能会如何变化？\n",
    "    那么 **前向 + 反向传播 + 优化器更新步骤**\n",
    "    的整体性能又会如何变化？\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比 **原始（vanilla）**\n",
    "    与 **编译后（compiled）** 的 Transformer 模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d277fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OOM: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OOM: d=32, T=16384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 23:41:05.236000 153201 torch/_dynamo/convert_frame.py:1358] [7/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0121 23:41:05.236000 153201 torch/_dynamo/convert_frame.py:1358] [7/8]    function: 'forward' (/tmp/ipykernel_153201/3913498325.py:20)\n",
      "W0121 23:41:05.236000 153201 torch/_dynamo/convert_frame.py:1358] [7/8]    last reason: 7/7: GLOBAL_STATE changed: grad_mode \n",
      "W0121 23:41:05.236000 153201 torch/_dynamo/convert_frame.py:1358] [7/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0121 23:41:05.236000 153201 torch/_dynamo/convert_frame.py:1358] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OOM: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OOM: d=128, T=16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d_model': 16,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 16.332650184631348,\n",
       "  'backward_time_ms': 1.815176010131836,\n",
       "  'mem_MB': 17.31640625},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 22.020459175109863,\n",
       "  'backward_time_ms': 1.8056154251098633,\n",
       "  'mem_MB': 17.8154296875},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 10.751724243164062,\n",
       "  'backward_time_ms': 29.122447967529297,\n",
       "  'mem_MB': 20.8154296875},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 40.813422203063965,\n",
       "  'backward_time_ms': 107.90963172912598,\n",
       "  'mem_MB': 24.8154296875},\n",
       " {'d_model': 16, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 14.969635009765625,\n",
       "  'backward_time_ms': 1.2181520462036133,\n",
       "  'mem_MB': 17.3330078125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.5947589874267578,\n",
       "  'backward_time_ms': 1.7226696014404297,\n",
       "  'mem_MB': 18.8330078125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 9.15524959564209,\n",
       "  'backward_time_ms': 24.99227523803711,\n",
       "  'mem_MB': 24.8330078125},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 42.124128341674805,\n",
       "  'backward_time_ms': 98.32925796508789,\n",
       "  'mem_MB': 32.8330078125},\n",
       " {'d_model': 32, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 5.90672492980957,\n",
       "  'backward_time_ms': 1.1570453643798828,\n",
       "  'mem_MB': 17.9033203125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.0004520416259766,\n",
       "  'backward_time_ms': 2.016925811767578,\n",
       "  'mem_MB': 20.9033203125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 14.969539642333984,\n",
       "  'backward_time_ms': 28.618526458740234,\n",
       "  'mem_MB': 32.9033203125},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 60.47871112823486,\n",
       "  'backward_time_ms': 123.23949337005615,\n",
       "  'mem_MB': 48.9033203125},\n",
       " {'d_model': 64, 'seqlen': 16384, 'OOM': True},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.45876502990722656,\n",
       "  'backward_time_ms': 1.9208669662475586,\n",
       "  'mem_MB': 19.1845703125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 1.6371965408325195,\n",
       "  'backward_time_ms': 4.649972915649414,\n",
       "  'mem_MB': 25.1845703125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 19.71259117126465,\n",
       "  'backward_time_ms': 53.25422286987305,\n",
       "  'mem_MB': 49.1845703125},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 71.34931087493896,\n",
       "  'backward_time_ms': 493.1082010269165,\n",
       "  'mem_MB': 81.1845703125},\n",
       " {'d_model': 128, 'seqlen': 16384, 'OOM': True}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头、朴素自注意力实现\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 线性投影（Q, K, V）\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        return: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model\n",
    "\n",
    "        # 1. 线性投影\n",
    "        Q = self.q_proj(x)  # (B, T, D)\n",
    "        K = self.k_proj(x)  # (B, T, D)\n",
    "        V = self.v_proj(x)  # (B, T, D)\n",
    "\n",
    "        # 2. 注意力分数 (B, T, T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "\n",
    "        # 3. softmax\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # 4. 加权求和\n",
    "        out = torch.matmul(attn, V)  # (B, T, D)\n",
    "\n",
    "        return out\n",
    "\n",
    "import time\n",
    "import torch._inductor.config as inductor_config\n",
    "import torch.compiler as tc\n",
    "import gc\n",
    "\n",
    "\n",
    "DTYPES = torch.float32\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "inductor_config.triton.cudagraphs = False\n",
    "\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "                \n",
    "                model = NaiveSelfAttention(d_model).to(device).to(DTYPES)\n",
    "                attention_fn = torch.compile(\n",
    "\n",
    "                                            model,\n",
    "                                            mode=\"default\",\n",
    "\n",
    "                                            )\n",
    "                torch.cuda.empty_cache()\n",
    "                # warm-up\n",
    "                for _ in range(10):\n",
    "                    X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                    \n",
    "                    _ = attention_fn(X).sum().backward()\n",
    "                    #del out\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # forward timing\n",
    "                    start = time.time()\n",
    "                    for _ in range(10):\n",
    "                        X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                        \n",
    "                        _ = attention_fn(X)\n",
    "                        #del out\n",
    "                    torch.cuda.synchronize()\n",
    "                    forward_time = (time.time() - start) / 10\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(10):\n",
    "                    X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                    \n",
    "                    _ = attention_fn(X).sum().backward()\n",
    "                    #del out\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 10\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "            #  清理显存引用\n",
    "            try:\n",
    "                del X, attention_fn, out, loss, model,_\n",
    "            except NameError:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "benchmark_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062be29d",
   "metadata": {},
   "source": [
    "对transformer模型进行torch_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b503846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "import numpy as np \n",
    "import gc\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.optimizer import AdamW,get_cosine_lr\n",
    "from cs336_basics.data import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time \n",
    "\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca32dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[251, 252, 253, 254, 255, 256, 257, 258],\n",
      "        [489, 490, 491, 492, 493, 494, 495, 496],\n",
      "        [238, 239, 240, 241, 242, 243, 244, 245],\n",
      "        [208, 209, 210, 211, 212, 213, 214, 215]], device='cuda:0')\n",
      "tensor([[252, 253, 254, 255, 256, 257, 258, 259],\n",
      "        [490, 491, 492, 493, 494, 495, 496, 497],\n",
      "        [239, 240, 241, 242, 243, 244, 245, 246],\n",
      "        [209, 210, 211, 212, 213, 214, 215, 216]], device='cuda:0')\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_transformer():\n",
    "\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # 假设是 token 序列\n",
    "    dataset = np.arange(1000, dtype=np.int64)\n",
    "    x, y = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=4,\n",
    "        context_length=8,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(x.shape)  # (4, 8)\n",
    "    print(y.shape)  # (4, 8)\n",
    "    print(x)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    print(f'设备：{device}')\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x,y, optimizer,dataset\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf1eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8281 | lr 0.00e+00\n",
      "现在真正计时!\n",
      "0/1\n",
      "step      0 | loss 10.8248 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8252 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8254 | lr 0.00e+00\n",
      "单次耗时：225.66509246826172ms\n",
      "0/1\n",
      "step      0 | loss 10.8287 | lr 0.00e+00\n",
      "正在使用cuda\n",
      "0/1\n",
      "step      0 | loss 10.8318 | lr 0.00e+00\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm         0.79%       1.577ms         1.28%       2.560ms      19.842us      93.764ms        48.90%      93.764ms     726.854us           129  \n",
      "## Call CompiledFxGraph fd5bhv26pk4bconban4k3x5gfik7nk4tka527yg7zdicgkqzi5rc ...         0.00%       0.000us         0.00%       0.000us       0.000us      77.318ms        40.32%      77.318ms      77.318ms             1  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      42.988ms        22.42%      42.988ms      42.988ms             1  \n",
      "## Call CompiledFxGraph fqhuqcctcz3lijrnxzxpoyxva37pkkwtwjut7wqhndi4nbdjt7pj ...         0.00%       0.000us         0.00%       0.000us       0.000us      39.376ms        20.54%      39.376ms      39.376ms             1  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x256_16x4_tn_align4>(cu...         0.00%       0.000us         0.00%       0.000us       0.000us      32.667ms        17.04%      32.667ms     759.706us            43  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nt_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us      15.064ms         7.86%      15.064ms      15.064ms             1  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nn_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us      14.436ms         7.53%      14.436ms      14.436ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      13.705ms         7.15%      13.705ms      60.110us           228  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x64_16x6_nn_align4>(cut...         0.00%       0.000us         0.00%       0.000us       0.000us      12.806ms         6.68%      12.806ms     266.797us            48  \n",
      "                                                                       aten::mul         1.16%       2.311ms         7.73%      15.444ms      45.027us      12.498ms         6.52%      14.465ms      42.172us           343  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 199.712ms\n",
      "Self CUDA time total: 191.737ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_benchmack():\n",
    "    \n",
    "    batch_size = 32\n",
    "    warmup_iters = 200\n",
    "    DTYPES = torch.float32\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    cosine_cycle_iters = 10_000\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    context_length=context_length,\n",
    "                                    d_model=d_model,\n",
    "                                    num_layers=num_layers,\n",
    "                                    num_heads=num_heads,\n",
    "                                    d_ff=d_ff,\n",
    "                                    rope_theta=rope_theta,\n",
    "                                    ).to(device).to(DTYPES)\n",
    "    model_compile = torch.compile(\n",
    "\n",
    "                                                model,\n",
    "                                                mode=\"default\",\n",
    "                                               \n",
    "\n",
    "                                                )\n",
    "    def run_model_complile(num_steps = 1):\n",
    "       \n",
    "        \n",
    "        for it in range(num_steps):\n",
    "            print(f'{it}/{num_steps}')\n",
    "            dataset = np.arange(1000, dtype=np.int64)\n",
    "        \n",
    "\n",
    "            \n",
    "            optimizer = AdamW(\n",
    "                                model.parameters(),\n",
    "                                lr=max_lr,\n",
    "                                weight_decay=0.1,\n",
    "                            )\n",
    "            # 1️更新学习率（每 step）\n",
    "            lr = get_cosine_lr(\n",
    "                it=it,\n",
    "                max_learning_rate=max_lr,\n",
    "                min_learning_rate=min_lr,\n",
    "                warmup_iters=warmup_iters,\n",
    "                cosine_cycle_iters=cosine_cycle_iters,\n",
    "            )\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            # 2取 batch\n",
    "            x, y = get_batch(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                context_length=context_length,\n",
    "                device=device,\n",
    "            )\n",
    "            # print(x)\n",
    "\n",
    "            # 3前向\n",
    "            logits = model_compile(x)\n",
    "\n",
    "            # 4loss\n",
    "            loss = lm_loss(logits, y)\n",
    "\n",
    "            # 5反向\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            # （可选）梯度裁剪（强烈推荐）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 6更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "        # 7日志\n",
    "        if it % 10 == 0:\n",
    "            print(\n",
    "                f\"step {it:6d} | \"\n",
    "                f\"loss {loss.item():.4f} | \"\n",
    "                f\"lr {lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        #  清理显存引用\n",
    "\n",
    "    benchmark(description = 'transformer模型的基准测试', run =  run_model_complile, num_warmups = 1, num_trials = 3)\n",
    "    t = profile(description='transformers的性能分析',run=run_model_complile)\n",
    "    print(t)\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer, model_compile\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "run_benchmack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c74a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "step      0 | loss 10.8298 | lr 0.00e+00\n",
      "现在真正计时!\n",
      "0/1\n",
      "step      0 | loss 10.8309 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8332 | lr 0.00e+00\n",
      "0/1\n",
      "step      0 | loss 10.8316 | lr 0.00e+00\n",
      "单次耗时：221.30799293518066ms\n",
      "0/1\n",
      "step      0 | loss 10.8299 | lr 0.00e+00\n",
      "正在使用cuda\n",
      "0/1\n",
      "step      0 | loss 10.8377 | lr 0.00e+00\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         2.13%       4.408ms         3.22%       6.645ms      40.272us      95.843ms        45.48%      95.843ms     580.867us           165  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      42.762ms        20.29%      42.762ms      42.762ms             1  \n",
      "                                                                       aten::mul         2.45%       5.063ms        21.68%      44.775ms      72.451us      39.153ms        18.58%      42.303ms      68.451us           618  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x256_16x4_tn_align4>(cu...         0.00%       0.000us         0.00%       0.000us       0.000us      32.293ms        15.32%      32.293ms     750.991us            43  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us      20.595ms         9.77%      20.595ms     205.953us           100  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      18.952ms         8.99%      18.952ms      55.253us           343  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nt_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us      14.342ms         6.81%      14.342ms      14.342ms             1  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nn_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us      14.217ms         6.75%      14.217ms      14.217ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us      13.245ms         6.29%      13.245ms      35.895us           369  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x64_16x6_nn_align4>(cut...         0.00%       0.000us         0.00%       0.000us       0.000us      12.631ms         5.99%      12.631ms     263.152us            48  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 206.529ms\n",
      "Self CUDA time total: 210.730ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_benchmack():\n",
    "    \n",
    "    batch_size = 32\n",
    "    warmup_iters = 200\n",
    "    DTYPES = torch.float32\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    cosine_cycle_iters = 10_000\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    context_length=context_length,\n",
    "                                    d_model=d_model,\n",
    "                                    num_layers=num_layers,\n",
    "                                    num_heads=num_heads,\n",
    "                                    d_ff=d_ff,\n",
    "                                    rope_theta=rope_theta,\n",
    "                                    ).to(device).to(DTYPES)\n",
    "    def run_model(num_steps = 1):\n",
    "       \n",
    "        \n",
    "        for it in range(num_steps):\n",
    "            print(f'{it}/{num_steps}')\n",
    "            dataset = np.arange(1000, dtype=np.int64)\n",
    "        \n",
    "\n",
    "            \n",
    "            optimizer = AdamW(\n",
    "                                model.parameters(),\n",
    "                                lr=max_lr,\n",
    "                                weight_decay=0.1,\n",
    "                            )\n",
    "            # 1️更新学习率（每 step）\n",
    "            lr = get_cosine_lr(\n",
    "                it=it,\n",
    "                max_learning_rate=max_lr,\n",
    "                min_learning_rate=min_lr,\n",
    "                warmup_iters=warmup_iters,\n",
    "                cosine_cycle_iters=cosine_cycle_iters,\n",
    "            )\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            # 2取 batch\n",
    "            x, y = get_batch(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                context_length=context_length,\n",
    "                device=device,\n",
    "            )\n",
    "            # print(x)\n",
    "\n",
    "            # 3前向\n",
    "            logits = model(x)\n",
    "\n",
    "            # 4loss\n",
    "            loss = lm_loss(logits, y)\n",
    "\n",
    "            # 5反向\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            # （可选）梯度裁剪（强烈推荐）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 6更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "        # 7日志\n",
    "        if it % 10 == 0:\n",
    "            print(\n",
    "                f\"step {it:6d} | \"\n",
    "                f\"loss {loss.item():.4f} | \"\n",
    "                f\"lr {lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        #  清理显存引用\n",
    "\n",
    "    benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 3)\n",
    "    t = profile(description='transformers的性能分析',run=run_model)\n",
    "    print(t)\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer, model_compile\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "run_benchmack()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6a2aa563",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e02546",
   "metadata": {},
   "source": [
    "请参考图上图，了解 tile 的示意图以及如何推进块指针。上述的加权求和函数如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import rearrange, einsum\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,       # 输入指针\n",
    "    output_ptr,              # 输出指针\n",
    "    x_stride_row, x_stride_dim,  # strides 告诉我们如何在张量的每个轴上移动一个元素\n",
    "    weight_stride_dim,       # 很可能是 1\n",
    "    output_stride_row,       # 很可能是 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # tile 的形状必须在编译时已知\n",
    "):\n",
    "    # 每个实例将计算 x 的一块行的加权和\n",
    "    # `tl.program_id` 给我们一个方法来检查当前线程块的编号\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # block 指针给我们一个方法，从 ND 内存区域中选择数据\n",
    "    # 并移动选择位置\n",
    "    # block 指针必须知道：\n",
    "    # - 张量第一个元素的指针\n",
    "    # - 张量的整体形状以处理越界访问\n",
    "    # - 每个维度的步长，用于使用内存布局属性\n",
    "    # - 起始块的ND坐标，即“偏移量”\n",
    "    # - 一次使用/存储的块形状\n",
    "    # - 内存中主要到次要的维度顺序\n",
    "    # axes（=np.argsort(strides)）用于优化，特别是在H100上\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D,),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # 初始化一个缓冲区以写入到输出\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    for i in range(tl.div(D, D_TILE_SIZE)):\n",
    "        # 加载当前块指针\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，且D_TILE_SIZE可能不能整除D，\n",
    "        # 我们需要为两个维度进行边界检查\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "\n",
    "        # 计算行的加权和。\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "\n",
    "        # 移动到下一个块。\n",
    "        # 这些是（行，列）坐标的增量\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # 在最后一个维度移动D_TILE_SIZE\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # 移动D_TILE_SIZE\n",
    "\n",
    "        # 将输出写入输出块指针（每行一个标量）\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，我们需要边界检查\n",
    "        tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "\n",
    "# 现在我们将这个内核包装在一个PyTorch自动微分函数中，该函数将与PyTorch（即，接受张量作为输入，输出张量，并在反向传播期间与自动微分引擎一起工作）一起解释\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # 在反向传递中缓存x和weight，当我们\n",
    "        # 只接收到输出张量的梯度时，将使用它们\n",
    "        # 需要计算x和weight的梯度。\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # 将输入张量重塑为2D\n",
    "        input_shape = x.shape\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"维度不匹配\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"期望CUDA张量\"\n",
    "        assert x.is_contiguous(), \"我们的指针算术将假设x是连续的\"\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # 大约16次循环遍历嵌入维度\n",
    "        ctx.ROWS_TILE_SIZE = 16  # 每个线程同时处理16个批次元素\n",
    "        ctx.input_shape = input_shape\n",
    "\n",
    "        # 需要初始化一个空的结果张量。注意这些元素不一定是0！\n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "\n",
    "        # 在我们的1D网格中启动我们的内核，有n个实例。\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e917aa",
   "metadata": {},
   "source": [
    "实现了一个 **自定义 Triton GPU 算子**，用来完成“对二维矩阵的每一行做加权求和”这一操作，并通过 PyTorch 的 `autograd.Function` 封装，使其可以像普通 PyTorch 函数一样前向/反向传播\n",
    "\n",
    "------------------------------------------------\n",
    "一、Triton 内核 `weighted_sum_fwd`\n",
    "------------------------------------------------\n",
    "1. 并行划分  \n",
    "   `@triton.jit` 把函数编译成 CUDA kernel。  \n",
    "   启动时只给了一维 grid：`weighted_sum_fwd[(grid_dim,)](...)`，  \n",
    "   所以 `tl.program_id(0)` 就是当前线程块（program instance）在一维网格里的编号，对应“第几个行块”。\n",
    "\n",
    "2. 块指针（block pointer）  \n",
    "   `tl.make_block_ptr` 把“张量指针 + 形状/步长/偏移”包装成一个 **可滑动窗口**。  \n",
    "   `x_ptr` 指向二维矩阵 `(ROWS, D)`  \n",
    "   `weight_ptr` 指向一维向量 `(D,)`  \n",
    "   `output_ptr` 指向输出向量 `(ROWS,)`  \n",
    "   窗口大小由 `ROWS_TILE_SIZE`（=16）和 `D_TILE_SIZE`（编译时常量）决定。  \n",
    "   `order` 告诉 Triton 哪一维在内存里是主序（row-major 对应 `(1,0)`）。\n",
    "\n",
    "3. 主循环  \n",
    "   沿特征维 `D` 以 `D_TILE_SIZE` 为步长滑动窗口：  \n",
    "   - 加载当前 tile：  \n",
    "     `row` 形状 `(16, D_TILE_SIZE)`  \n",
    "     `weight` 形状 `(D_TILE_SIZE,)`  \n",
    "   - 计算：  \n",
    "     `row * weight[None, :]` → 广播成 `(16, D_TILE_SIZE)`  \n",
    "     `tl.sum(..., axis=1)` → 对每行求和，得到 `(16,)`  \n",
    "     累加到 `output` 寄存器。  \n",
    "   - 移动窗口：  \n",
    "     `x_block_ptr.advance((0, D_TILE_SIZE))`  \n",
    "     `weight_block_ptr.advance((D_TILE_SIZE,))`  \n",
    "   循环结束后 `output` 里就是 **这 16 行的最终结果**。\n",
    "\n",
    "4. 写回全局内存  \n",
    "   `tl.store(output_block_ptr, output, boundary_check=(0,))`  \n",
    "   如果总行数不能被 16 整除，最末一块会越界，`boundary_check` 自动屏蔽无效元素。\n",
    "\n",
    "------------------------------------------------\n",
    "二、PyTorch 封装 `WeightedSumFunc`\n",
    "------------------------------------------------\n",
    "1. 参数整理  \n",
    "   - `x` 可以是任意阶张量，最后一维是特征 `D`；前面所有维度当成“行”。  \n",
    "   - `weight` 是一维向量，长度必须等于 `D`。  \n",
    "   用 `einops.rearrange` 把 `x` 压成二维 `(n_rows, D)`，方便内核处理。\n",
    "\n",
    "2. 静态超参  \n",
    "   - `ROWS_TILE_SIZE = 16`  \n",
    "   - `D_TILE_SIZE` 取 `≥D/16` 且为 2 的幂，保证循环次数≈16 次，兼顾寄存器占用与并行度。\n",
    "\n",
    "3. 输出张量  \n",
    "   `y = torch.empty(output_dims)` 形状与 `x[..., 0]` 相同。\n",
    "\n",
    "4. 启动 kernel  \n",
    "   grid 大小 = `ceil(n_rows / 16)`，每个线程块负责 16 行。  \n",
    "   把张量底层指针、步长、行列数等全部传进 Triton 内核。\n",
    "\n",
    "5. 返回  \n",
    "   把二维结果再 `view` 回原来的 batch 形状，前端用户感知不到内存重排。\n",
    "\n",
    "------------------------------------------------\n",
    "三、整体流程\n",
    "------------------------------------------------\n",
    "```python\n",
    "out = WeightedSumFunc.apply(x, weight)\n",
    "```\n",
    "\n",
    "等价于  \n",
    "```python\n",
    "out = (x * weight).sum(dim=-1)\n",
    "```\n",
    "\n",
    "但 **完全跑在 GPU 上，手工调度共享内存/寄存器，比 PyTorch 原生 reduce 更快**，尤其当 `D` 很大、batch 很多时。后续只要再补一个 `backward` 函数，就可以像普通 `nn.Module` 一样训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c8f8",
   "metadata": {},
   "source": [
    "### **FlashAttention 中「分块（blocking）」与「数值稳定 softmax」为什么能带来加速**\n",
    "\n",
    "\n",
    "### 一、加速原理\n",
    "\n",
    "FlashAttention 的加速**不是因为算得更少 FLOPs**，而是因为：\n",
    "\n",
    "> **极大减少了显存读写（memory I/O），并将计算变成“流式（streaming）”**\n",
    "\n",
    "核心公式仍然是论文中的公式：\n",
    "\n",
    "$$\n",
    "\\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "但**计算路径完全改变**。\n",
    "\n",
    "####  标准 Attention 的真实成本\n",
    "\n",
    "以 sequence length = (N)，head dim = (d)：\n",
    "\n",
    "| 操作      | FLOPs      | 显存占用          |\n",
    "| ------- | ---------- | ------------- |\n",
    "| QKᵀ     | (O(N^2 d)) | **存 N×N**     |\n",
    "| softmax | (O(N^2))   | **读 + 写 N×N** |\n",
    "| AV      | (O(N^2 d)) | 读 N×N         |\n",
    "\n",
    "**关键瓶颈不是算力，而是显存带宽**\n",
    "\n",
    "在 GPU 上：N×N的attention matrix **必须写入 HBM**，* 然后再 **从 HBM 读回** 做 softmax，大模型场景下 **显存访问 >> 计算时间**。\n",
    "\n",
    "#### GPU 性能的现实约束\n",
    "\n",
    "GPU 有三种“速度等级”存储：\n",
    "\n",
    "| 存储            | 速度 | 容量 |\n",
    "| ------------- | -- | -- |\n",
    "| Register      | 极快 | 极小 |\n",
    "| Shared Memory | 快  | 小  |\n",
    "| HBM (显存)      | 慢  | 很大 |\n",
    "\n",
    "标准 Attention 的问题是：\n",
    "\n",
    "> **N×N attention matrix 太大，只能进 HBM**\n",
    "\n",
    "#### 分块（Blocking）为什么能加速\n",
    "\n",
    "**分块的本质就是让数据“活在片上”**\n",
    "\n",
    "FlashAttention 将 Attention 重写为：\n",
    "\n",
    "```text\n",
    "for each Q_block:\n",
    "    for each K/V_block:\n",
    "        计算局部 QK^T\n",
    "        立刻参与 softmax\n",
    "        立刻乘 V\n",
    "```\n",
    "\n",
    "核心变化是不再生成完整的 QKᵀ，不再写入 N×N matrix，局部结果 **只存在寄存器 / shared memory**。从**全文总结变成了走一步看一步**。\n",
    "\n",
    "#### 分块前 vs 分块后（显存行为对比）\n",
    "\n",
    "**标准 Attention**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  写 QK^T (N×N)\n",
    "  读 QK^T\n",
    "  写 softmax(QK^T)\n",
    "  读 softmax(QK^T)\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N²)**\n",
    "\n",
    "---\n",
    "\n",
    "**FlashAttention（分块）**\n",
    "\n",
    "```text\n",
    "HBM:\n",
    "  读 Q_block\n",
    "  读 K_block, V_block\n",
    "  写 O_block\n",
    "```\n",
    "\n",
    "**HBM I/O = O(N·d)**\n",
    "\n",
    "**这是数量级的下降**\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么“只对 K/V 分块也有效”\n",
    "\n",
    "分块有两种实现：\n",
    "\n",
    "* Q 不分块\n",
    "* Q 分块（论文 / CUDA 版）\n",
    "\n",
    "两者都满足：\n",
    "\n",
    "> **K/V 被流式读取，attention matrix 不落地**\n",
    "\n",
    "Q 分块是为了，提高 occupancy，并且控制寄存器使用，适配 CUDA block 结构。\n",
    "\n",
    "---\n",
    "\n",
    "#### 二、在线 softmax 的加速原理”\n",
    "\n",
    "**标准 softmax 的结构问题**\n",
    "\n",
    "标准 softmax：\n",
    "\n",
    "```python\n",
    "S = QK^T\n",
    "P = exp(S) / sum(exp(S))\n",
    "```\n",
    "\n",
    "问题在于softmax **必须看完整一行**，无法流式，依赖全量 S。\n",
    "\n",
    "**这强制我们先存下整个 QKᵀ**\n",
    "\n",
    "\n",
    "**running max / running sum 的革命性意义**\n",
    "\n",
    "FlashAttention 使用：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_i &= \\max_j S_{ij} \\\n",
    "l_i &= \\sum_j e^{S_{ij} - m_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "并改写为 **可递推形式**：\n",
    "\n",
    "$$\n",
    "m_i^{(t)} = \\max(m_i^{(t-1)}, \\max S_{ij}^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "l_i^{(t)} =\n",
    "l_i^{(t-1)} e^{m_i^{(t-1)} - m_i^{(t)}} +\n",
    "\\sum e^{S_{ij}^{(t)} - m_i^{(t)}}\n",
    "$$\n",
    "\n",
    "\n",
    "这一步“决定了 FlashAttention 能不能存在”，它带来了三件**决定性好处**，将softmax 变成 **在线算法**。\n",
    "\n",
    "再这个算法中我们不需要完整 S，每来一个 block 就能更新，天然适配 streaming。\n",
    "\n",
    "**softmax 与 AV 可以融合**\n",
    "\n",
    "标准 Attention：\n",
    "\n",
    "```text\n",
    "QK^T → softmax → @V\n",
    "```\n",
    "\n",
    "FlashAttention：\n",
    "\n",
    "```text\n",
    "(QK^T → exp → sum → @V)  全部在 block 内完成\n",
    "```\n",
    "\n",
    "**算子融合（kernel fusion）成为可能**\n",
    "\n",
    "\n",
    "**backward 也不需要 attention matrix**\n",
    "\n",
    "由于 forward 保存了：\n",
    "\n",
    "* `L = logsumexp`\n",
    "* `O = softmax @ V`\n",
    "\n",
    "backward 可写成：\n",
    "\n",
    "$$\n",
    "dS_{ij} = P_{ij} (dP_{ij} - D_i)\n",
    "$$\n",
    "\n",
    "**反向同样 O(N) 显存**\n",
    "\n",
    "> FlashAttention 的加速不是减少计算，而是通过 **分块 + 在线 softmax**，\n",
    "> **避免了 N×N attention matrix 的显存读写**，\n",
    "> 将 attention 计算变成一个 **完全流式、可融合的过程**，\n",
    "> 从而把瓶颈从显存带宽转回到算力。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cc2ba",
   "metadata": {},
   "source": [
    "# 作业七：使用pytorch实现flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a41b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    FlashAttention（forward + backward）\n",
    "\n",
    "    输入：\n",
    "        Q: (B, Nq, d)\n",
    "        K: (B, Nk, d)\n",
    "        V: (B, Nk, d)\n",
    "\n",
    "    输出：\n",
    "        O: (B, Nq, d)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        # -----------------------------\n",
    "        # block size\n",
    "        # -----------------------------\n",
    "        Bq = 64  # Query block 的尺寸\n",
    "        Bk = 64  # Key / Value block 的尺寸\n",
    "\n",
    "        # 我们将KV矩阵分成一个个小矩阵\n",
    "\n",
    "        Tq = Nq // Bq  # Q block 数\n",
    "        Tk = Nk // Bk  # K/V block 数\n",
    "\n",
    "        scale = d ** -0.5\n",
    "\n",
    "        # -----------------------------\n",
    "        # 输出与中间量\n",
    "        # -----------------------------\n",
    "        O = torch.zeros_like(Q)              # Attention 输出\n",
    "        L = torch.zeros(B, Nq, device=Q.device)  # log-sum-exp（给 backward 用）\n",
    "\n",
    "        # =========================================================\n",
    "        # 对 batch 维度显式循环\n",
    "        # =========================================================\n",
    "        for b in range(B):\n",
    "            Q_b = Q[b]  # (Nq, d)\n",
    "            K_b = K[b]  # (Nk, d)\n",
    "            V_b = V[b]  # (Nk, d)\n",
    "\n",
    "            # =====================================================\n",
    "            # 外层循环：Query block\n",
    "            # =====================================================\n",
    "            for i in range(Tq):\n",
    "                q_i = Q_b[i * Bq:(i + 1) * Bq]  # (Bq, d)\n",
    "\n",
    "                # -----------------------------\n",
    "                # FlashAttention 核心状态\n",
    "                # -----------------------------\n",
    "                m = torch.full((Bq, 1), -float(\"inf\"), device=Q.device)  # running max\n",
    "                l = torch.zeros((Bq, 1), device=Q.device)               # running sum\n",
    "                O_acc = torch.zeros((Bq, d), device=Q.device)           # 输出累积（未归一化）\n",
    "\n",
    "                # =================================================\n",
    "                # 内层循环：Key / Value block\n",
    "                # =================================================\n",
    "                for j in range(Tk):\n",
    "                    k_j = K_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "                    v_j = V_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 计算局部 attention score\n",
    "                    # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale  # (Bq, Bk)\n",
    "\n",
    "                    # 当前 block 内，每一行的最大值\n",
    "                    row_max = S.max(dim=1, keepdim=True).values  # (Bq, 1)\n",
    "\n",
    "                    # 更新 running max\n",
    "                    m_new = torch.maximum(m, row_max)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 数值稳定 softmax（FlashAttention 关键）\n",
    "                    # --------------------------------------------\n",
    "                    # 旧 block 贡献修正\n",
    "                    exp_old = torch.exp(m - m_new) * l\n",
    "\n",
    "                    # 当前 block 的 exp\n",
    "                    P = torch.exp(S - m_new)\n",
    "\n",
    "                    # 更新分母\n",
    "                    l = exp_old + P.sum(dim=1, keepdim=True)\n",
    "\n",
    "                    # 更新未归一化输出\n",
    "                    O_acc = (\n",
    "                        torch.exp(m - m_new) * O_acc +\n",
    "                        P @ v_j\n",
    "                    )\n",
    "\n",
    "                    # 写回 running max\n",
    "                    m = m_new\n",
    "\n",
    "                # =================================================\n",
    "                # block 内 softmax 归一化\n",
    "                # =================================================\n",
    "                O_i = O_acc / l  # (Bq, d)\n",
    "\n",
    "                # log-sum-exp：L = m + log(l)\n",
    "                L_i = m.squeeze(1) + torch.log(l.squeeze(1))\n",
    "\n",
    "                # 写回全局输出\n",
    "                O[b, i * Bq:(i + 1) * Bq] = O_i\n",
    "                L[b, i * Bq:(i + 1) * Bq] = L_i\n",
    "\n",
    "        # 保存 backward 所需变量\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.scale = scale\n",
    "        ctx.Bq = Bq\n",
    "        ctx.Bk = Bk\n",
    "\n",
    "        return O\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, L = ctx.saved_tensors\n",
    "        scale = ctx.scale\n",
    "        Bq = ctx.Bq\n",
    "        Bk = ctx.Bk\n",
    "\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Tq = Nq // Bq\n",
    "        Tk = Nk // Bk\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # FlashAttention backward 中的重要中间量\n",
    "        # D_i = sum_j O_ij * dO_ij\n",
    "        # --------------------------------------------\n",
    "        D = torch.sum(O * dO, dim=-1)  # (B, Nq)\n",
    "\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        for b in range(B):\n",
    "            for i in range(Tq):\n",
    "                q_i = Q[b, i * Bq:(i + 1) * Bq]\n",
    "                dO_i = dO[b, i * Bq:(i + 1) * Bq]\n",
    "                L_i = L[b, i * Bq:(i + 1) * Bq]\n",
    "                D_i = D[b, i * Bq:(i + 1) * Bq]\n",
    "\n",
    "                for j in range(Tk):\n",
    "                    k_j = K[b, j * Bk:(j + 1) * Bk]\n",
    "                    v_j = V[b, j * Bk:(j + 1) * Bk]\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 重算局部 attention 概率 P_ij\n",
    "                    # P_ij = exp(S_ij - L_i)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale\n",
    "                    P = torch.exp(S - L_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dV --------\n",
    "                    dV[b, j * Bk:(j + 1) * Bk] += P.T @ dO_i\n",
    "\n",
    "                    # -------- dS --------\n",
    "                    dP = dO_i @ v_j.T\n",
    "                    dS = P * (dP - D_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dQ --------\n",
    "                    dQ[b, i * Bq:(i + 1) * Bq] += dS @ k_j * scale\n",
    "\n",
    "                    # -------- dK --------\n",
    "                    dK[b, j * Bk:(j + 1) * Bk] += dS.T @ q_i * scale\n",
    "\n",
    "        return dQ, dK, dV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7a947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from typing import Callable\n",
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "\n",
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time \n",
    "\n",
    "def profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0992c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：50.60640970865885ms\n",
      "正在使用cuda\n",
      "[FlashAttention] B=1, N=1024, d=64, time=50606.410 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm         4.09%       7.780ms        13.67%      26.013ms      50.806us      63.793ms        58.75%      65.305ms     127.549us           512  \n",
      "                                                 ampere_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us      26.187ms        24.12%      26.187ms     102.293us           256  \n",
      "                                                 ampere_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      20.442ms        18.83%      20.442ms      79.853us           256  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, f...         0.00%       0.000us         0.00%       0.000us       0.000us      17.164ms        15.81%      17.164ms      33.523us           512  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      10.032ms         9.24%      10.032ms       9.646us          1040  \n",
      "                                                                       aten::max         1.76%       3.343ms         3.13%       5.953ms      23.255us       9.286ms         8.55%       9.343ms      36.495us           256  \n",
      "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native...         0.00%       0.000us         0.00%       0.000us       0.000us       9.286ms         8.55%       9.286ms      36.272us           256  \n",
      "                                                                       aten::exp         2.44%       4.639ms         5.79%      11.021ms      14.350us       6.905ms         6.36%       6.989ms       9.101us           768  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::exp_kernel_cuda...         0.00%       0.000us         0.00%       0.000us       0.000us       6.905ms         6.36%       6.905ms       8.990us           768  \n",
      "                                                                       aten::sub         3.75%       7.133ms         8.61%      16.394ms      21.347us       6.795ms         6.26%       7.228ms       9.412us           768  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 190.324ms\n",
      "Self CUDA time total: 108.579ms\n",
      "\n",
      "现在真正计时!\n",
      "单次耗时：0.40650367736816406ms\n",
      "正在使用cuda\n",
      "[NaiveAttention]  B=1, N=1024, d=64, time=406.504 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         7.28%      99.294us        27.75%     378.486us     189.243us     183.804us        78.10%     279.404us     139.702us             2  \n",
      "                                                         Activity Buffer Request        16.33%     222.659us        16.33%     222.659us     222.659us      95.600us        40.62%      95.600us      95.600us             1  \n",
      "                                                          ampere_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us      95.600us        40.62%      95.600us      95.600us             1  \n",
      "                                                 ampere_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us      87.135us        37.03%      87.135us      87.135us             1  \n",
      "                                                                  aten::_softmax         2.23%      30.420us         3.63%      49.575us      49.575us      30.084us        12.78%      30.084us      30.084us             1  \n",
      "void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, fal...         0.00%       0.000us         0.00%       0.000us       0.000us      30.084us        12.78%      30.084us      30.084us             1  \n",
      "                                                                       aten::div         1.18%      16.026us         2.06%      28.106us      28.106us      21.442us         9.11%      21.442us      21.442us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us      21.442us         9.11%      21.442us      21.442us             1  \n",
      "                                                                 Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.069us         0.45%       1.069us       1.069us             1  \n",
      "                                                                 aten::transpose         0.83%      11.355us         1.04%      14.124us      14.124us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.364ms\n",
      "Self CUDA time total: 235.330us\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_flash_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionAutograd.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_flash_attention',run = run)\n",
    "    t1 = profile(description ='test_flash_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttention] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [1024]:\n",
    "    test_flash_attention(B=1, N=N, d=64, device=device)\n",
    "    test_naive_attention(B=1, N=N, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a110c30",
   "metadata": {},
   "source": [
    "# 2. Triton 内核实现flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d049031",
   "metadata": {},
   "source": [
    "Triton 是一个用 Python 写 GPU kernel 的 DSL，本质上是“显式控制数据分块与内存层级的编程模型”，它不是NumPy，也不是高层 PyTorch API，它是一个 CUDA kernel 的“可读版本”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12251ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sympy import Q\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def flash_attention_fwd_kernel(\n",
    "    # -------- 全局内存指针 --------\n",
    "    Q_ptr, K_ptr, V_ptr,           # 输入 Q, K, V\n",
    "    O_ptr, L_ptr,                  # 输出 O, logsumexp L\n",
    "\n",
    "    # -------- stride 信息（用于 block ptr）--------\n",
    "    stride_qb, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vn, stride_vd,\n",
    "    stride_ob, stride_on, stride_od,\n",
    "    stride_lb, stride_ln,\n",
    "\n",
    "    # -------- 序列长度 --------\n",
    "    Nq, Nk,\n",
    "\n",
    "    # -------- scale --------\n",
    "    scale,\n",
    "\n",
    "    # -------- 编译期常量 --------\n",
    "    D: tl.constexpr,\n",
    "    Q_BLOCK: tl.constexpr,\n",
    "    K_BLOCK: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    每个 Triton program 负责：\n",
    "        - 一个 batch\n",
    "        - 一个 Query block（Q_BLOCK 行）\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Triton 并行索引\n",
    "    # ------------------------------------------------\n",
    "    q_block_id = tl.program_id(0)     # 第几个 Q block\n",
    "    batch_id   = tl.program_id(1)     # 第几个 batch\n",
    "\n",
    "    # =================================================\n",
    "    # 构造 block pointer（这是 Triton 的关键）\n",
    "    # =================================================\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_qn, stride_qd),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    #  K / V 从第 0 行开始，后面在 loop 中 advance\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K_ptr + batch_id * stride_kb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_kn, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V_ptr + batch_id * stride_vb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_vn, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O_ptr + batch_id * stride_ob,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_on, stride_od),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        base=L_ptr + batch_id * stride_lb,\n",
    "        shape=(Nq, 1),\n",
    "        strides=(stride_ln, 1),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # =================================================\n",
    "    # 加载 Query block\n",
    "    # =================================================\n",
    "    Q_i = tl.load(Q_block_ptr)  # (Q_BLOCK, D)\n",
    "\n",
    "    # =================================================\n",
    "    # FlashAttention 核心状态（每个 Q block 独立）\n",
    "    # =================================================\n",
    "    O_acc = tl.zeros((Q_BLOCK, D), dtype=tl.float32)       # 未归一化输出累积\n",
    "    L_acc = tl.zeros((Q_BLOCK, 1), dtype=tl.float32)      # softmax 分母\n",
    "    M_acc = tl.full((Q_BLOCK, 1), -float(\"inf\"), tl.float32)  # running max\n",
    "\n",
    "    # =================================================\n",
    "    # 内层循环：遍历所有 K/V block\n",
    "    # =================================================\n",
    "    for k_block_id in range(tl.cdiv(Nk, K_BLOCK)):\n",
    "        K_j = tl.load(K_block_ptr)   # (K_BLOCK, D)\n",
    "        V_j = tl.load(V_block_ptr)   # (K_BLOCK, D)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "        # ---------------------------------------------\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale  # (Q_BLOCK, K_BLOCK)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # Causal mask（编译期 if）\n",
    "        # ---------------------------------------------\n",
    "        if is_causal:\n",
    "            q_idx = q_block_id * Q_BLOCK + tl.arange(0, Q_BLOCK)[:, None]\n",
    "            k_idx = k_block_id * K_BLOCK + tl.arange(0, K_BLOCK)[None, :]\n",
    "            causal_mask = q_idx >= k_idx\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 数值稳定 softmax（FlashAttention 核心）\n",
    "        # ---------------------------------------------\n",
    "        M_block = tl.max(S_ij, axis=1, keep_dims=True)\n",
    "        M_new = tl.maximum(M_acc, M_block)\n",
    "\n",
    "        P_ij = tl.exp(S_ij - M_block)\n",
    "\n",
    "        L_new = (\n",
    "            tl.exp(M_acc - M_new) * L_acc +\n",
    "            tl.exp(M_block - M_new) * tl.sum(P_ij, axis=1, keep_dims=True)\n",
    "        )\n",
    "\n",
    "        # 类型对齐（Triton 细节）\n",
    "        P_cast = P_ij.to(V_block_ptr.type.element_ty)\n",
    "\n",
    "        O_new = (\n",
    "            tl.exp(M_acc - M_new) * O_acc +\n",
    "            tl.exp(M_block - M_new) * tl.dot(P_cast, V_j)\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 更新 running 状态\n",
    "        # ---------------------------------------------\n",
    "        M_acc = M_new\n",
    "        L_acc = L_new\n",
    "        O_acc = O_new\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 移动 K/V block 指针\n",
    "        # ---------------------------------------------\n",
    "        K_block_ptr = K_block_ptr.advance((K_BLOCK, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_BLOCK, 0))\n",
    "\n",
    "    # =================================================\n",
    "    # softmax 归一化\n",
    "    # =================================================\n",
    "    O_i = O_acc / L_acc\n",
    "    L_i = M_acc + tl.log(L_acc)\n",
    "\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(L_block_ptr, L_i)\n",
    "\n",
    "@triton.jit\n",
    "def flash_bwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr, dO_ptr, D_ptr,\n",
    "    dQ_ptr, dK_ptr, dV_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    stride_dob, stride_doq, stride_dod,\n",
    "    stride_db, stride_dq_d,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # program ids\n",
    "    k_tile_id = tl.program_id(0)\n",
    "    batch_id = tl.program_id(1)\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (Q-side)\n",
    "    # -------------------------\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_id * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dO_block_ptr = tl.make_block_ptr(\n",
    "        dO_ptr + batch_id * stride_dob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_doq, stride_dod),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_id * stride_lb,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_lq, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    D_block_ptr = tl.make_block_ptr(\n",
    "        D_ptr + batch_id * stride_db,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_dq_d, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (K/V-side)\n",
    "    # -------------------------\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dK_block_ptr = tl.make_block_ptr(\n",
    "        dK_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dV_block_ptr = tl.make_block_ptr(\n",
    "        dV_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # load K/V tile\n",
    "    # -------------------------\n",
    "    K_j = tl.load(K_block_ptr)  # (K_TILE_SIZE, D)\n",
    "    V_j = tl.load(V_block_ptr)\n",
    "\n",
    "    dK_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "    dV_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # loop over Q tiles\n",
    "    # -------------------------\n",
    "    for q_tile_id in range(tl.cdiv(N_QUERIES, Q_TILE_SIZE)):\n",
    "        Q_i = tl.load(Q_block_ptr)\n",
    "        O_i = tl.load(O_block_ptr)\n",
    "        dO_i = tl.load(dO_block_ptr)\n",
    "        L_i = tl.load(L_block_ptr)\n",
    "        D_i = tl.load(D_block_ptr)\n",
    "\n",
    "        # S = QK^T\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale\n",
    "\n",
    "        if is_causal:\n",
    "            q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "            k_idx = k_tile_id * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)[None, :]\n",
    "            S_ij = tl.where(q_idx >= k_idx, S_ij, -1e6)\n",
    "\n",
    "        # P = softmax\n",
    "        P_ij = tl.exp(S_ij - L_i)\n",
    "\n",
    "        # dV\n",
    "        dV_j += tl.dot(P_ij.to(V_j.dtype).T, dO_i)\n",
    "\n",
    "        # dP\n",
    "        dP_ij = tl.dot(dO_i, V_j.T)\n",
    "\n",
    "        # dS\n",
    "        dS_ij = P_ij * (dP_ij - D_i)\n",
    "        dS_ij = dS_ij * scale\n",
    "\n",
    "        # dQ (atomic add)\n",
    "        dQ_i = tl.dot(dS_ij, K_j)\n",
    "\n",
    "        q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "        d_idx = tl.arange(0, D)[None, :]\n",
    "        mask = (q_idx < N_QUERIES) & (d_idx < D)\n",
    "        dq_ptrs = dQ_ptr + batch_id * stride_qb + q_idx * stride_qq + d_idx * stride_qd\n",
    "        tl.atomic_add(dq_ptrs, dQ_i.to(dQ_ptr.dtype.element_ty), mask=mask)\n",
    "\n",
    "        # dK\n",
    "        dK_j += tl.dot(dS_ij.T, Q_i)\n",
    "\n",
    "        # advance Q-side blocks\n",
    "        Q_block_ptr = Q_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        O_block_ptr = O_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        dO_block_ptr = dO_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        L_block_ptr = L_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        D_block_ptr = D_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "\n",
    "    # -------------------------\n",
    "    # store dK / dV\n",
    "    # -------------------------\n",
    "    tl.store(dK_block_ptr, dK_j.to(dK_ptr.dtype.element_ty))\n",
    "    tl.store(dV_block_ptr, dV_j.to(dV_ptr.dtype.element_ty))\n",
    "\n",
    "\n",
    "\n",
    "class FlashAttentionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        \"\"\"\n",
    "        Q, K, V: (B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Nq, D = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Q_BLOCK = 16\n",
    "        K_BLOCK = 16\n",
    "\n",
    "        scale = D ** -0.5\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        L = torch.empty(B, Nq, device=Q.device)\n",
    "\n",
    "        grid = (Nq // Q_BLOCK, B)\n",
    "\n",
    "        flash_attention_fwd_kernel[grid](\n",
    "            Q, K, V, O, L,\n",
    "            Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "            K.stride(0), K.stride(1), K.stride(2),\n",
    "            V.stride(0), V.stride(1), V.stride(2),\n",
    "            O.stride(0), O.stride(1), O.stride(2),\n",
    "            L.stride(0), L.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_BLOCK=Q_BLOCK,\n",
    "            K_BLOCK=K_BLOCK,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, l = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "    \n",
    "        B, Nq, D = q.shape\n",
    "        Nk = k.shape[1]\n",
    "    \n",
    "        Bq = 16\n",
    "        Bk = 16\n",
    "        scale = D ** -0.5\n",
    "    \n",
    "        dq = torch.zeros_like(q)\n",
    "        dk = torch.zeros_like(k)\n",
    "        dv = torch.zeros_like(v)\n",
    "    \n",
    "        # D_i = sum_j dO_ij * O_ij\n",
    "        D_sum = torch.sum(o * do, dim=-1, keepdim=True)\n",
    "    \n",
    "        grid = (triton.cdiv(Nk, Bk), B)\n",
    "    \n",
    "        flash_bwd_kernel[grid](\n",
    "            q, k, v, o, l, do, D_sum,\n",
    "            dq, dk, dv,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            o.stride(0), o.stride(1), o.stride(2),\n",
    "            l.stride(0), l.stride(1),\n",
    "            do.stride(0), do.stride(1), do.stride(2),\n",
    "            D_sum.stride(0), D_sum.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_TILE_SIZE=Bq,\n",
    "            K_TILE_SIZE=Bk,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "    \n",
    "        return dq, dk, dv, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a0638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                # 构造输入（无多头）\n",
    "               \n",
    "                Q = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                K = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "                V = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "                # warm-up\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(10):\n",
    "                        out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                        #loss = out.sum()\n",
    "                        #loss.backward()\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                # forward timing\n",
    "                start = time.time()\n",
    "                for _ in range(100):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.time() - start) / 100\n",
    "\n",
    "                # memory before backward\n",
    "                mem_before_backward = torch.cuda.memory_allocated()\n",
    "\n",
    "                \n",
    "                #---------------------\n",
    "                # backward timing\n",
    "                start = time.time()\n",
    "                for _ in range(100):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.time() - start) / 100\n",
    "                #-------------------------\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    #\"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_before_backward / 1024**2,\n",
    "                })\n",
    "\n",
    "                print(f\"OK: d={d_model}, T={seqlen}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"OOM: d={d_model}, T={seqlen}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46bbd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: d=16, T=256\n",
      "OK: d=16, T=1024\n",
      "OK: d=16, T=4096\n",
      "OK: d=16, T=8192\n",
      "OK: d=16, T=16384\n",
      "OK: d=32, T=256\n",
      "OK: d=32, T=1024\n",
      "OK: d=32, T=4096\n",
      "OK: d=32, T=8192\n",
      "OK: d=32, T=16384\n",
      "OK: d=64, T=256\n",
      "OK: d=64, T=1024\n",
      "OK: d=64, T=4096\n",
      "OK: d=64, T=8192\n",
      "OK: d=64, T=16384\n",
      "OK: d=128, T=256\n",
      "OK: d=128, T=1024\n",
      "OK: d=128, T=4096\n",
      "OK: d=128, T=8192\n",
      "OK: d=128, T=16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d_model': 16,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.18090486526489258,\n",
       "  'mem_MB': 7.5244140625},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.220947265625,\n",
       "  'mem_MB': 9.79833984375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 3.0855512619018555,\n",
       "  'mem_MB': 18.14208984375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 12.349913120269775,\n",
       "  'mem_MB': 35.26708984375},\n",
       " {'d_model': 16,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 78.64851951599121,\n",
       "  'mem_MB': 63.51708984375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.0536036491394043,\n",
       "  'mem_MB': 56.02490234375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.2613353729248047,\n",
       "  'mem_MB': 12.54833984375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 4.055523872375488,\n",
       "  'mem_MB': 29.14208984375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 15.804851055145265,\n",
       "  'mem_MB': 63.26708984375},\n",
       " {'d_model': 32,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 63.974981307983406,\n",
       "  'mem_MB': 119.51708984375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.06345272064208984,\n",
       "  'mem_MB': 105.02490234375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.356600284576416,\n",
       "  'mem_MB': 18.04833984375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 5.967504978179932,\n",
       "  'mem_MB': 51.14208984375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 23.900372982025146,\n",
       "  'mem_MB': 119.26708984375},\n",
       " {'d_model': 64,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 126.5398907661438,\n",
       "  'mem_MB': 231.51708984375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 256,\n",
       "  'forward_time_ms': 0.0557708740234375,\n",
       "  'mem_MB': 203.02490234375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 1024,\n",
       "  'forward_time_ms': 0.7149362564086914,\n",
       "  'mem_MB': 29.04833984375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 4096,\n",
       "  'forward_time_ms': 11.509106159210205,\n",
       "  'mem_MB': 95.14208984375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 8192,\n",
       "  'forward_time_ms': 42.86984920501709,\n",
       "  'mem_MB': 231.26708984375},\n",
       " {'d_model': 128,\n",
       "  'seqlen': 16384,\n",
       "  'forward_time_ms': 187.2625470161438,\n",
       "  'mem_MB': 455.51708984375}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "benchmark_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf1f8f",
   "metadata": {},
   "source": [
    "如果报错out of resource: shared memory, Required: 115200, Hardware limit: 101376. Reducing block sizes or num_stages may help.\n",
    "\n",
    "适当将Q_BLOCK = 16 和 K_BLOCK = 16还有Bq = 16 ，Bk = 16调低，如果你的GPU非常给力，也可以适当调高，我用的是4060 -8G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd13a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flash_AttentionTriton(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionTriton.apply(Q, K, V)\n",
    "        loss = O.sum()\n",
    "        loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'FlashAttentionTriton',run = run)\n",
    "    t1 = profile(description ='FlashAttentionTriton' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttentionTriton] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9c3b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：50.40081342061361ms\n",
      "正在使用cuda\n",
      "[FlashAttentionTriton] B=4, N=4096, d=64, time=50400.813 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                    FlashAttentionTritonBackward         0.15%     381.197us         0.22%     567.242us     567.242us     213.809ms        85.45%     214.310ms     214.310ms             1  \n",
      "                                                                flash_bwd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us     213.809ms        85.45%     213.809ms     213.809ms             1  \n",
      "                                                            FlashAttentionTriton         0.50%       1.274ms         1.97%       5.004ms       5.004ms      35.735ms        14.28%      35.735ms      35.735ms             1  \n",
      "                                                      flash_attention_fwd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us      35.735ms        14.28%      35.735ms      35.735ms             1  \n",
      "                                                                       aten::sum         0.59%       1.494ms         0.62%       1.585ms     792.545us     227.213us         0.09%     227.213us     113.606us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native...         0.00%       0.000us         0.00%       0.000us       0.000us     225.517us         0.09%     225.517us     112.758us             2  \n",
      "                                                                       aten::mul         0.01%      25.141us         0.01%      36.475us      36.475us     188.079us         0.08%     188.079us     188.079us             1  \n",
      "void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocas...         0.00%       0.000us         0.00%       0.000us       0.000us     188.079us         0.08%     188.079us     188.079us             1  \n",
      "                                                                     aten::fill_         0.02%      41.243us         0.05%     133.270us      33.318us     154.035us         0.06%     154.035us      38.509us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<flo...         0.00%       0.000us         0.00%       0.000us       0.000us     154.035us         0.06%     154.035us      38.509us             4  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 254.464ms\n",
      "Self CUDA time total: 250.218ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_flash_AttentionTriton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc070d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flash_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionAutograd.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_flash_attention',run = run)\n",
    "    t1 = profile(description ='test_flash_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttention] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cdd5f",
   "metadata": {},
   "source": [
    "\n",
    "# Triton 一般语法与编程模型说明\n",
    "\n",
    "Triton 是一种面向深度学习算子的 DSL（Domain-Specific Language），其设计目标并不是替代 PyTorch，而是 **在不直接编写 CUDA 的前提下，精确控制 GPU 上的并行计算与内存访问**。\n",
    "\n",
    "理解 Triton，需要从它的编程模型开始，而不是从 API 细节入手。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、Triton 的核心抽象：program\n",
    "\n",
    "在 Triton 中，**program 是最小的并行执行单元**。\n",
    "\n",
    "```python\n",
    "pid = tl.program_id(axis)\n",
    "```\n",
    "\n",
    "这一概念对应于 CUDA 中的：\n",
    "\n",
    "| Triton  | CUDA                    |\n",
    "| ------- | ----------------------- |\n",
    "| program | thread block            |\n",
    "| grid    | grid                    |\n",
    "| axis    | blockIdx.x / blockIdx.y |\n",
    "\n",
    "关键区别在于：\n",
    "\n",
    "* Triton 的 program **天然是 SIMD / 向量化的**\n",
    "* 一个 program 内部，操作的是一个 **tile（块）**，而不是单个标量\n",
    "\n",
    "因此，Triton 的思维方式是：\n",
    "\n",
    "> **“一个 program 负责一块数据”**\n",
    "\n",
    "而不是：\n",
    "\n",
    "> “一个 thread 负责一个元素”\n",
    "\n",
    "---\n",
    "\n",
    "## 二、grid 的含义：并行维度的显式声明\n",
    "\n",
    "Triton kernel 的调用形式：\n",
    "\n",
    "```python\n",
    "kernel[grid](...)\n",
    "```\n",
    "\n",
    "其中：\n",
    "\n",
    "```python\n",
    "grid = (grid_dim_0, grid_dim_1, ...)\n",
    "```\n",
    "\n",
    "对应：\n",
    "\n",
    "```python\n",
    "tl.program_id(0)  # 第 0 个并行维度\n",
    "tl.program_id(1)  # 第 1 个并行维度\n",
    "```\n",
    "\n",
    "常见用法：\n",
    "\n",
    "| grid 维度 | 语义           |\n",
    "| ------- | ------------ |\n",
    "| axis=0  | 序列 / 行 block |\n",
    "| axis=1  | batch        |\n",
    "| axis=2  | head         |\n",
    "\n",
    "Triton 不关心“线程数量”，它只关心：\n",
    "\n",
    "> **一共启动多少个 program，以及每个 program 处理哪一块数据**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、Triton 的内存访问模型\n",
    "\n",
    "### 1. 指针是“裸指针”，但带 shape 和 stride\n",
    "\n",
    "在 Triton kernel 中，所有张量参数本质上都是 **指针**：\n",
    "\n",
    "```python\n",
    "Q_ptr, K_ptr, V_ptr\n",
    "```\n",
    "\n",
    "它们没有 shape、没有 stride，只有地址。\n",
    "\n",
    "因此，Triton 需要你**显式告诉它**张量的逻辑形状、张量在内存中的步长、当前 program 访问哪一块。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. make_block_ptr：结构化内存访问的核心\n",
    "\n",
    "```python\n",
    "block_ptr = tl.make_block_ptr(\n",
    "    base,\n",
    "    shape,\n",
    "    strides,\n",
    "    offsets,\n",
    "    block_shape,\n",
    "    order\n",
    ")\n",
    "```\n",
    "\n",
    "这不是“语法糖”，而是 Triton 最重要的抽象之一。\n",
    "\n",
    "它的含义是**定义一个“可被向量化 load/store 的二维 tile”**。\n",
    "\n",
    "各参数的含义：\n",
    "\n",
    "| 参数          | 说明                      |\n",
    "| ----------- | ----------------------- |\n",
    "| base        | 数据起始指针                  |\n",
    "| shape       | 整个张量的逻辑 shape           |\n",
    "| strides     | 每个维度的步长                 |\n",
    "| offsets     | 当前 program 的起始偏移        |\n",
    "| block_shape | 一次 load/store 的 tile 大小 |\n",
    "| order       | 内存访问的主序（影响 coalescing）  |\n",
    "\n",
    "一旦定义了 `block_ptr`：\n",
    "\n",
    "```python\n",
    "x = tl.load(block_ptr)\n",
    "tl.store(block_ptr, x)\n",
    "```\n",
    "\n",
    "就等价于一次 **矢量化、对齐的内存操作**。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、Triton 中的“张量”是什么\n",
    "\n",
    "Triton 中的张量并不是 PyTorch Tensor，而是：\n",
    "\n",
    "> **编译期已知 shape 的寄存器向量**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "```\n",
    "\n",
    "意味着shape 在编译期是常量，并且存储在寄存器 / SRAM 中，所有运算是 element-wise 或 block-wise 的。这也是为什么 Triton 要求很多参数是 `tl.constexpr`。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、tl.constexpr：编译期常量，而不是运行时参数\n",
    "\n",
    "```python\n",
    "BLOCK_SIZE: tl.constexpr\n",
    "```\n",
    "\n",
    "含义是**该值在 kernel 编译时已知**\n",
    "\n",
    "这对 Triton 非常重要，因为lock_shape、loop unrolling、向量化宽度、内存布局、都依赖这些值。\n",
    "\n",
    "经验规则：**影响 shape / 循环次数 / 内存布局的参数 → 必须是 constexpr**，纯数值缩放（如 scale）可以是运行时参数。\n",
    "\n",
    "---\n",
    "\n",
    "## 六、算子语义：没有 Python 循环的“逐元素”\n",
    "\n",
    "Triton 的算子都是 **逐元素并行** 的。\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = tl.arange(0, BLOCK)\n",
    "```\n",
    "\n",
    "产生的是：\n",
    "\n",
    "```text\n",
    "[0, 1, 2, ..., BLOCK-1]\n",
    "```\n",
    "\n",
    "而不是单个值。\n",
    "\n",
    "再例如：\n",
    "\n",
    "```python\n",
    "mask = q_idx >= k_idx\n",
    "```\n",
    "\n",
    "这是一个 **逐元素比较，返回布尔矩阵**。\n",
    "\n",
    "`tl.where` 的语义是：\n",
    "\n",
    "```python\n",
    "out[i] = a[i] if cond[i] else b[i]\n",
    "```\n",
    "\n",
    "而不是 Python 分支。\n",
    "\n",
    "---\n",
    "\n",
    "## 七、控制流：为什么 Triton “不鼓励 if”\n",
    "\n",
    "Triton 中的 `if`：\n",
    "\n",
    "```python\n",
    "if is_causal:\n",
    "    ...\n",
    "```\n",
    "\n",
    "只有在 **`is_causal` 是 tl.constexpr** 时才安全。\n",
    "\n",
    "原因是在于Triton kernel 是 **单一静态程序**，所有 program 必须执行同一条指令流，因此**数据相关分支必须用 mask**，**配置相关分支可以用 constexpr if**。\n",
    "\n",
    "这也是为什么 causal mask 必须写成：\n",
    "\n",
    "```python\n",
    "S = tl.where(mask, S, -inf)\n",
    "```\n",
    "\n",
    "而不是 `if row > col`.\n",
    "\n",
    "---\n",
    "\n",
    "## 八、数学运算与矩阵乘法\n",
    "\n",
    "### 1. tl.dot\n",
    "\n",
    "```python\n",
    "C = tl.dot(A, B)\n",
    "```\n",
    "\n",
    "这是 Triton 提供的 **块级矩阵乘法原语**：输入是 `(M, K)` 和 `(K, N)`，输出是 `(M, N)`，自动映射到 tensor core（如果 dtype / shape 合适）。\n",
    "\n",
    "这是 Triton 实现 FlashAttention、GEMM、LayerNorm 融合的基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 广播规则\n",
    "\n",
    "Triton 的广播行为和 NumPy / PyTorch 基本一致，但前提是**shape 必须在编译期可推导**\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "M = tl.max(S, axis=1, keep_dims=True)\n",
    "```\n",
    "\n",
    "这是合法的，因为 axis 和 shape 都是常量。\n",
    "\n",
    "---\n",
    "\n",
    "## 九、Triton kernel 与 PyTorch Autograd 的关系\n",
    "\n",
    "Triton kernel 本身不关心梯度，不保存中间状态，因此常见结构是：\n",
    "\n",
    "```python\n",
    "class MyOp(torch.autograd.Function):\n",
    "    def forward(...):\n",
    "        launch triton kernel\n",
    "        save tensors\n",
    "    def backward(...):\n",
    "        launch triton kernel\n",
    "```\n",
    "\n",
    "Triton 负责 **高性能数值计算**\n",
    "PyTorch 负责 **计算图与调度**\n",
    "\n",
    "这是一个明确的职责划分。\n",
    "\n",
    "---\n",
    "\n",
    "## 十、总结\n",
    "\n",
    "\n",
    "\n",
    "> **Triton 是一种“显式控制内存与并行，但不暴露线程”的 GPU 编程模型**\n",
    "\n",
    "理解 Triton 的关键不是 API，而是以下三点：\n",
    "\n",
    "1. 一个 program 对应一个 tile\n",
    "2. 所有 shape 在编译期已知\n",
    "3. 控制流必须数据并行\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2_System",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
