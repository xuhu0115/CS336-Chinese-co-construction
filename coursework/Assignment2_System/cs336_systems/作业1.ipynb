{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192270c9",
   "metadata": {},
   "source": [
    "# 前期准备工作（环境准备）\n",
    "\n",
    "## 本次作业只需要看 作业1.ipynb 和 作业2.ipynb 即可，其他的py文件都是由这两个文件衍生的。\n",
    "\n",
    "## 在windows上安装Ubuntu-22.04（使用triton需要使用在linux上运行）\n",
    "\n",
    "由于triton在windows没有可用的版本，考虑到大家都是使用的windows系统的比较多，这里附一个安装教程，有linux系统或者不想运行triton模块的同学可以跳过，如果在wsl的Linux环境准备有疑问可以参考视频[【Linux虚拟机】wsl+ubuntu+vscode配置教程](https://www.bilibili.com/video/BV17X3tehEAS/?spm_id_from=333.337.search-card.all.click&vd_source=048a80e0e2397a4a86eb91880a41bd02)\n",
    "\n",
    "整个作业使用**linux或者ubuntu**进行，读者可以使用**windows的wsl**来在windows中跑通作本次作业是关于GPU和分布式训练的，如果有多个GPU就可以跑通，只有一个GPU也可以，但是就没有分布式训练的速度优势，但是作为教学学习是足够的。\n",
    "\n",
    "如果您想使用自己作业1的实现，请将`cs336-basics`目录替换为您自己的实现，或编辑外层`pyproject.toml`文件以指向您自己的实现。\n",
    "\n",
    "### 安装步骤\n",
    "\n",
    "#### 1. 版本限制\n",
    "\n",
    "我们需要在windows上安装wsl2，限制在Windows 10 2004 (19041) 及以上 或者 Windows 11 任意版本\n",
    "\n",
    "#### 2.手动启用 WSL 相关功能（关键）\n",
    "\n",
    "按住win键搜索**启用或者关闭windows功能**，找到并勾选 **于Linux的Windows子系统**和 **虚拟机平台**，点击确定\n",
    "\n",
    "\n",
    "#### 3. 确认 CPU 虚拟化已开启\n",
    "\n",
    "按住Ctrl + Shift + Esc打开任务管理器，性能 -> CPU ，右侧应显示：虚拟化：已启用\n",
    "\n",
    "如果是“未启用”\n",
    "\n",
    "需要进 BIOS / UEFI，开启：\n",
    "\n",
    "Intel：Intel Virtualization Technology (VT-x)\n",
    "\n",
    "AMD：SVM Mode\n",
    "\n",
    "##### 4. 安装 Ubuntu\n",
    "\n",
    "在windows搜索页面中输入CMD，并且以**管理员身份**运行\n",
    "\n",
    "```bash\n",
    "wsl --install -d Ubuntu\n",
    "```\n",
    "\n",
    "之后设置账号密码，**密码在输入的时候是不可见的**，看不到是正常的。\n",
    "\n",
    "要打开ubuntu的命令行只需要在CMD命令行中输入\n",
    "\n",
    "```bash\n",
    "wsl\n",
    "```\n",
    "\n",
    "##### 5. 安装相关的工具 pip uv 等\n",
    "\n",
    "依次执行\n",
    "先使用管理员身份打开CMD\n",
    "\n",
    "输入\n",
    "```bash\n",
    "\n",
    "cd $env:USERPROFILE\n",
    "@\"\n",
    "[wsl2]\n",
    "networkingMode=mirrored\n",
    "dnsTunneling=true\n",
    "autoProxy=true\n",
    "firewall=true\n",
    "\"@ | Out-File -Encoding UTF8 .wslconfig\n",
    "\n",
    "\n",
    "wsl --shutdown     # 彻底关掉虚拟机\n",
    "wsl                # 再进子系统\n",
    "```\n",
    "\n",
    "```bash\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install python3-pip\n",
    "\n",
    "sudo apt install python3.12-venv\n",
    "\n",
    "source ~/myenv/bin/activate\n",
    "pip install uv -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "sudo ln -s /home/kangkang/myenv/bin/uv /usr/local/bin/uv\n",
    "\n",
    "echo 'export UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple' >> ~/.bashrc\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "```\n",
    "\n",
    "#### 6. 设置vscode的远程连接\n",
    "\n",
    "打开vscode ，先搜索插件wsl。安装完成后打开左侧一列的远程连接就可以连接到wsl的ubuntu系统中\n",
    "\n",
    "\n",
    "运行在CS336-Chinese-co-construction\\coursework\\Assignment2_System下\n",
    "```bash\n",
    "cd CS336-Chinese-co-construction\\coursework\\Assignment2_System\n",
    "uv sync\n",
    "\n",
    ". .venv/bin/activate\n",
    "```\n",
    "在vscode远程连接ubuntu，可以挂载在C盘或着D盘的项目文件，目录输入。\n",
    "\n",
    "```bash\n",
    "/mnt/c/  #后面接C盘目录\n",
    "\n",
    "/mnt/d/ #后面接D盘目录\n",
    "\n",
    "# 比如在本机中目录为\"D:\\code\"，在ubuntu中就变成\"/mnt/d/code\"\n",
    "```\n",
    "\n",
    "也可以在Ubuntu的目录下克隆项目进行使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583d935",
   "metadata": {},
   "source": [
    "# 1. 基准测试和性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f745648",
   "metadata": {},
   "source": [
    "# 作业一\n",
    "\n",
    "（a）编写一个脚本，对模型的前向与反向传播进行基本的端到端基准测试。具体而言，你的脚本应支持以下功能：\n",
    "- 给定超参数（例如层数），初始化一个模型。\n",
    "- 生成一个随机批次的数据。\n",
    "- 先运行 w 个预热步骤（开始计时之前），然后计时执行 n 个步骤（仅前向或同时包含前向与反向，取决于参数）。计时可使用 Python 的 timeit 模块（例如直接调用 timeit 函数，或使用 timeit.default_timer()，后者提供系统最高分辨率的时钟，比 time.time() 更适合基准测试）。\n",
    "- 每一步之后调用 torch.cuda.synchronize()。\n",
    "\n",
    "交付物：一个脚本，能够根据给定超参数初始化基础 Transformer 模型，创建随机数据批次，并对前向与反向传播进行计时。\n",
    "\n",
    "（b）对 §1.2 中描述的模型规模，分别测试前向与反向传播耗时。  \n",
    "使用 5 个预热步骤，并在 10 次测量步骤上计算平均耗时与标准差。  \n",
    "一次前向传播耗时多久？反向传播呢？测量结果波动大吗，还是标准差很小？  \n",
    "\n",
    "交付物：1–2 句话给出你的计时结果。\n",
    "\n",
    "（c）基准测试的一个常见陷阱是跳过预热步骤。请在没有预热的情况下重复上述分析。结果如何变化？你认为原因是什么？再尝试仅使用 1 或 2 个预热步骤，为何结果仍可能不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9210e8",
   "metadata": {},
   "source": [
    "## 首先检查GPU是否可用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea2358",
   "metadata": {},
   "source": [
    "这边通过pyproject.toml下载的torch是cpu版本\n",
    "如果要安装GPU版本：\n",
    "```bash\n",
    "uv pip uninstall torch \n",
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "cu121是我的cuda版本为12.1，可以改为自己的cuda版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036a681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "MPS（苹果的 MPS） available: False\n"
     ]
    }
   ],
   "source": [
    "# 基础 Transformer 语言模型实现\n",
    "from cs336_basics.model import BasicsTransformerLM \n",
    "# 获取 cosine learning rate schedule 的工具函数\n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "# 自定义实现的 AdamW 优化器（可能用于教学或实验）\n",
    "from cs336_basics.optimizer import AdamW\n",
    "# 从数据集中取 batch 的工具函数\n",
    "from cs336_basics.data import get_batch\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 打印当前 PyTorch 版本\n",
    "# 不同版本在 API、性能和后端支持上可能存在差异\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "# 检查是否可以使用 CUDA（NVIDIA GPU）\n",
    "# 这是大多数深度学习训练中最关键的加速方式\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # 当前机器上可用的 CUDA 设备数量\n",
    "    # 多卡训练（DDP / FSDP）时非常重要\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "\n",
    "    # 当前进程正在使用的 CUDA 设备 ID\n",
    "    # 通常在多进程训练中由 local_rank 决定\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "\n",
    "    # 打印第 0 张 GPU 的名称\n",
    "    # 有助于确认运行环境是否符合预期\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    # 如果没有检测到 CUDA GPU\n",
    "    # 训练将只能在 CPU 或其他后端上进行，性能会显著下降\n",
    "    print(\"No CUDA GPU detected\")\n",
    "\n",
    "# 检查是否支持 Apple 的 MPS（Metal Performance Shaders）后端\n",
    "# 适用于 Apple Silicon（M1 / M2 / M3）设备\n",
    "# 注意：MPS 的算子覆盖和数值稳定性可能与 CUDA 不完全一致\n",
    "print(\n",
    "    \"MPS（苹果的 MPS） available:\",\n",
    "    hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2625f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(values: list[float]) -> float:\n",
    "    if not values:\n",
    "        raise ValueError(\"mean() requires at least one value\")\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):\n",
    "    \"\"\"Benchmark `func` by running it `num_trials`, and return all the times.\"\"\"\n",
    "    # 热身：第一次运行可能较慢,因为要编译和缓存\n",
    "    # 我们将多次要运行内核，因为重要的是稳态的运行时间。\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # 等待 CUDA 线程完成（非常重要！）\n",
    "    print('现在真正计时!')\n",
    "    times: list[float] = [] # @inspect times, @inspect description\n",
    "\n",
    "    for trial in range(num_trials):  # 多次重复\n",
    "        start_time = time.time()\n",
    "        run()  # 实际执行计算\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待 CUDA 线程 完成同步\n",
    "        end_time = time.time()\n",
    "        times.append((end_time - start_time) * 1000) # @inspect times\n",
    "    mean_time = mean(times) # 多次测量取平均\n",
    "\n",
    "    print(f'单次耗时：{mean_time  }ms')\n",
    "    return mean_time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b8414",
   "metadata": {},
   "source": [
    "现在来测试一下sleep函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7986b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：50.158023834228516ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.158023834228516"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(\"sleep\", lambda : time.sleep(50 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdfb5b",
   "metadata": {},
   "source": [
    "在误差范围之内"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6048",
   "metadata": {},
   "source": [
    "我们来实测一下transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8295b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备： cuda\n",
      "step      0 | loss 10.8277 | lr 0.00e+00\n",
      "step      2 | loss 10.8366 | lr 3.00e-06\n",
      "step      4 | loss 10.8351 | lr 6.00e-06\n",
      "step      6 | loss 10.8371 | lr 9.00e-06\n",
      "step      8 | loss 10.8293 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8265 | lr 0.00e+00\n",
      "step      2 | loss 10.8293 | lr 3.00e-06\n",
      "step      4 | loss 10.8335 | lr 6.00e-06\n",
      "step      6 | loss 10.8317 | lr 9.00e-06\n",
      "step      8 | loss 10.8278 | lr 1.20e-05\n",
      "现在真正计时!\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8305 | lr 0.00e+00\n",
      "step      2 | loss 10.8322 | lr 3.00e-06\n",
      "step      4 | loss 10.8288 | lr 6.00e-06\n",
      "step      6 | loss 10.8298 | lr 9.00e-06\n",
      "step      8 | loss 10.8292 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8262 | lr 0.00e+00\n",
      "step      2 | loss 10.8234 | lr 3.00e-06\n",
      "step      4 | loss 10.8329 | lr 6.00e-06\n",
      "step      6 | loss 10.8365 | lr 9.00e-06\n",
      "step      8 | loss 10.8304 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8266 | lr 0.00e+00\n",
      "step      2 | loss 10.8336 | lr 3.00e-06\n",
      "step      4 | loss 10.8242 | lr 6.00e-06\n",
      "step      6 | loss 10.8343 | lr 9.00e-06\n",
      "step      8 | loss 10.8255 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8266 | lr 0.00e+00\n",
      "step      2 | loss 10.8271 | lr 3.00e-06\n",
      "step      4 | loss 10.8285 | lr 6.00e-06\n",
      "step      6 | loss 10.8252 | lr 9.00e-06\n",
      "step      8 | loss 10.8293 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8256 | lr 0.00e+00\n",
      "step      2 | loss 10.8239 | lr 3.00e-06\n",
      "step      4 | loss 10.8333 | lr 6.00e-06\n",
      "step      6 | loss 10.8293 | lr 9.00e-06\n",
      "step      8 | loss 10.8351 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8339 | lr 0.00e+00\n",
      "step      2 | loss 10.8278 | lr 3.00e-06\n",
      "step      4 | loss 10.8360 | lr 6.00e-06\n",
      "step      6 | loss 10.8213 | lr 9.00e-06\n",
      "step      8 | loss 10.8372 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8356 | lr 0.00e+00\n",
      "step      2 | loss 10.8261 | lr 3.00e-06\n",
      "step      4 | loss 10.8308 | lr 6.00e-06\n",
      "step      6 | loss 10.8288 | lr 9.00e-06\n",
      "step      8 | loss 10.8305 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8215 | lr 0.00e+00\n",
      "step      2 | loss 10.8387 | lr 3.00e-06\n",
      "step      4 | loss 10.8306 | lr 6.00e-06\n",
      "step      6 | loss 10.8297 | lr 9.00e-06\n",
      "step      8 | loss 10.8338 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8290 | lr 0.00e+00\n",
      "step      2 | loss 10.8311 | lr 3.00e-06\n",
      "step      4 | loss 10.8255 | lr 6.00e-06\n",
      "step      6 | loss 10.8390 | lr 9.00e-06\n",
      "step      8 | loss 10.8257 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8315 | lr 0.00e+00\n",
      "step      2 | loss 10.8243 | lr 3.00e-06\n",
      "step      4 | loss 10.8332 | lr 6.00e-06\n",
      "step      6 | loss 10.8376 | lr 9.00e-06\n",
      "step      8 | loss 10.8276 | lr 1.20e-05\n",
      "单次耗时：7694.176483154297ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7694.176483154297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    语言模型（Language Model）的标准 Cross Entropy Loss\n",
    "\n",
    "    参数说明：\n",
    "    - logits: 模型输出，形状为 (B, T, V)\n",
    "        B: batch size\n",
    "        T: 序列长度（context length）\n",
    "        V: 词表大小（vocab size）\n",
    "    - targets: 目标 token id，形状为 (B, T)\n",
    "\n",
    "    训练目标：\n",
    "    - 对每一个时间步 t，预测下一个 token 的概率分布\n",
    "    \"\"\"\n",
    "\n",
    "    # 从 logits 中解析出 batch、序列长度、词表大小\n",
    "    B, T, V = logits.shape\n",
    "\n",
    "    # CrossEntropyLoss 要求输入为：\n",
    "    # - logits: (N, C)\n",
    "    # - targets: (N,)\n",
    "    # 因此需要将 (B, T, V) 展平为 (B*T, V)\n",
    "    return F.cross_entropy(\n",
    "        logits.view(B * T, V),\n",
    "        targets.view(B * T),\n",
    "    )\n",
    "\n",
    "\n",
    "def run_model(num_steps=1):\n",
    "    \"\"\"\n",
    "    Transformer 语言模型的完整训练 + benchmark 单次运行函数\n",
    "\n",
    "    包含：\n",
    "    - 模型构建\n",
    "    - 学习率调度\n",
    "    - 前向 / 反向传播\n",
    "    - 参数更新\n",
    "    - 显存清理\n",
    "\n",
    "    该函数会被 benchmark 工具多次调用\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================\n",
    "    # 学习率调度相关超参数\n",
    "    # =========================\n",
    "\n",
    "    min_lr = 3e-5                    # 最小学习率（cosine 衰减下限）\n",
    "    max_lr = 3e-4                    # 最大学习率（warmup 后峰值）\n",
    "    warmup_iters = 200               # warmup 步数\n",
    "    cosine_cycle_iters = 10_000      # 一个 cosine 周期的长度\n",
    "\n",
    "    # =========================\n",
    "    # 训练流程相关参数\n",
    "    # =========================\n",
    "\n",
    "    num_steps = 10                   # 总训练 step 数（benchmark 用，较小）\n",
    "    batch_size = 32                  # batch size\n",
    "    vocab_size = 50_000              # 词表大小\n",
    "    context_length = 128             # 上下文长度（序列长度）\n",
    "\n",
    "    # =========================\n",
    "    # Transformer 模型结构参数\n",
    "    # =========================\n",
    "\n",
    "    d_model = 512                    # embedding / hidden size\n",
    "    num_layers = 6                   # Transformer block 数量\n",
    "    num_heads = 8                    # 多头注意力 head 数\n",
    "    d_ff = 2048                      # FFN 中间层维度\n",
    "    rope_theta = 10000.0             # RoPE（旋转位置编码）的 theta 参数\n",
    "\n",
    "    # =========================\n",
    "    # 设备选择\n",
    "    # =========================\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('使用设备：', device)\n",
    "    # =========================\n",
    "    # 主训练循环\n",
    "    # =========================\n",
    "\n",
    "    for it in range(num_steps):\n",
    "\n",
    "        # 构造一个简单的 toy dataset（token id 序列）\n",
    "        # 实际项目中通常来自 tokenizer + 语料\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "\n",
    "        # =========================\n",
    "        # 模型构建\n",
    "        # =========================\n",
    "\n",
    "        model = BasicsTransformerLM(\n",
    "            vocab_size=vocab_size,\n",
    "            context_length=context_length,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            rope_theta=rope_theta,\n",
    "        ).to(device)\n",
    "\n",
    "        # =========================\n",
    "        # 优化器构建\n",
    "        # =========================\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=max_lr,                # 初始 lr，会在后面被 scheduler 覆盖\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # 1 更新学习率（每 step）\n",
    "        # =========================\n",
    "\n",
    "        # 使用 cosine + warmup 的学习率调度策略\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "\n",
    "        # 手动更新 optimizer 中的 lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        # =========================\n",
    "        # 2 取一个 batch 的训练数据\n",
    "        # =========================\n",
    "\n",
    "        # x: 输入 token id，形状 (B, T)\n",
    "        # y: 目标 token id，形状 (B, T)\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # 3 前向传播\n",
    "        # =========================\n",
    "\n",
    "        # logits 形状为 (B, T, V)\n",
    "        logits = model(x)\n",
    "\n",
    "        # =========================\n",
    "        # 4 计算 loss\n",
    "        # =========================\n",
    "\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # =========================\n",
    "        # 5 反向传播\n",
    "        # =========================\n",
    "\n",
    "        # 清空梯度（set_to_none=True 可略微节省显存）\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # （可选但强烈推荐）\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # =========================\n",
    "        # 6 更新参数\n",
    "        # =========================\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # =========================\n",
    "    # 7 日志打印\n",
    "    # =========================\n",
    "\n",
    "        if it % 2 == 0:\n",
    "            print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n",
    "    # =========================\n",
    "    # 显存 / 内存清理（benchmark 非常重要）\n",
    "    # =========================\n",
    "\n",
    "    try:\n",
    "        # 主动删除大对象引用，避免显存堆积\n",
    "        del model, x, y, logits, loss, lr, optimizer\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # 清空 CUDA 缓存（不等价于释放显存，但可缓解碎片）\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 触发 Python 垃圾回收\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 基准测试（benchmark）\n",
    "# =========================\n",
    "\n",
    "benchmark(\n",
    "    description='transformer模型的基准测试',\n",
    "    run=run_model,\n",
    "    num_warmups=2,       # 预热次数（避免首次 CUDA 编译影响）\n",
    "    num_trials=10,       # 正式测试次数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ad903",
   "metadata": {},
   "source": [
    "### 性能分析工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db37a8a",
   "metadata": {},
   "source": [
    "使用wsl后无法测量cuda时间，这是正常现象，可以使用正常的linux系统或者windows系统，原因在于wsl的linux无法使用torch的一些接口(CUPTI)来检测cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88bb1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda : 12.8\n",
      "CUPTI: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "import torch, ctypes, os\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda :', torch.version.cuda)\n",
    "try:\n",
    "    ctypes.CDLL('libcupti.so')\n",
    "    print('CUPTI: OK')\n",
    "except:\n",
    "    print('CUPTI: NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc51db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备： cuda\n",
      "step      0 | loss 10.8249 | lr 0.00e+00\n",
      "step      2 | loss 10.8265 | lr 3.00e-06\n",
      "step      4 | loss 10.8285 | lr 6.00e-06\n",
      "step      6 | loss 10.8351 | lr 9.00e-06\n",
      "step      8 | loss 10.8334 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8362 | lr 0.00e+00\n",
      "step      2 | loss 10.8311 | lr 3.00e-06\n",
      "step      4 | loss 10.8292 | lr 6.00e-06\n",
      "step      6 | loss 10.8286 | lr 9.00e-06\n",
      "step      8 | loss 10.8280 | lr 1.20e-05\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8305 | lr 0.00e+00\n",
      "step      2 | loss 10.8216 | lr 3.00e-06\n",
      "step      4 | loss 10.8385 | lr 6.00e-06\n",
      "step      6 | loss 10.8316 | lr 9.00e-06\n",
      "step      8 | loss 10.8317 | lr 1.20e-05\n",
      "正在使用 cuda\n",
      "使用设备： cuda\n",
      "step      0 | loss 10.8321 | lr 0.00e+00\n",
      "step      2 | loss 10.8295 | lr 3.00e-06\n",
      "step      4 | loss 10.8299 | lr 6.00e-06\n",
      "step      6 | loss 10.8306 | lr 9.00e-06\n",
      "step      8 | loss 10.8322 | lr 1.20e-05\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         0.53%      43.682ms         1.01%      83.546ms      50.634us        1.653s        48.58%        1.653s       1.002ms          1650  \n",
      "                                                           ampere_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     559.690ms        16.45%     559.690ms       1.302ms           430  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us     477.611ms        14.04%     477.611ms      47.761ms            10  \n",
      "                                                                       aten::mul         0.92%      75.671ms         8.77%     723.692ms     116.913us     452.807ms        13.31%     468.979ms      75.764us          6190  \n",
      "                                                                     aten::copy_         0.27%      22.020ms         6.23%     513.981ms     199.217us     446.889ms        13.13%     462.089ms     179.104us          2580  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_nt_align1>(cutlass_80...         0.00%       0.000us         0.00%       0.000us       0.000us     409.865ms        12.05%     409.865ms       2.157ms           190  \n",
      "                                                Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     385.230ms        11.32%     385.230ms     664.190us           580  \n",
      "                                                           ampere_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     286.811ms         8.43%     286.811ms     682.882us           420  \n",
      "                                                          ampere_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     269.506ms         7.92%     269.506ms      26.951ms            10  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us     249.419ms         7.33%     249.419ms     249.419us          1000  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.253s\n",
      "Self CUDA time total: 3.403s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import os\n",
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def profile(\n",
    "    description: str,\n",
    "    run: Callable,\n",
    "    num_warmups: int = 1,\n",
    "    with_stack: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    使用 PyTorch Profiler 对指定函数进行性能分析（CPU + CUDA）\n",
    "\n",
    "    参数说明：\n",
    "    - description:\n",
    "        本次 profiling 的描述信息，用于区分不同实验\n",
    "    - run:\n",
    "        被 profiling 的函数（通常是一次完整的训练 step / forward-backward）\n",
    "    - num_warmups:\n",
    "        预热次数，用于消除首次运行带来的额外开销\n",
    "    - with_stack:\n",
    "        是否记录 CUDA 调用的 Python/C++ 堆栈信息（用于火焰图分析）\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================\n",
    "    # 1 预热阶段（Warmup）\n",
    "    # =========================\n",
    "\n",
    "    # 预热的目的：\n",
    "    # - 触发 CUDA kernel 的 JIT / lazy init\n",
    "    # - 初始化 cuDNN / allocator\n",
    "    # - 避免首次运行严重拉高 profiling 数据\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "\n",
    "    # =========================\n",
    "    # 2 确认运行设备并同步\n",
    "    # =========================\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用 cuda\")\n",
    "\n",
    "        # CUDA 操作是异步的\n",
    "        # 在 profiling 或计时前必须 synchronize，\n",
    "        # 否则会统计到尚未完成的 kernel\n",
    "        torch.cuda.synchronize()\n",
    "    else:\n",
    "        print(\"正在使用 cpu\")\n",
    "\n",
    "    # =========================\n",
    "    # 3 使用 PyTorch Profiler 进行性能分析\n",
    "    # =========================\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        # 指定需要分析的设备类型\n",
    "        # 同时开启 CPU 和 CUDA\n",
    "        activities=[\n",
    "            ProfilerActivity.CPU,\n",
    "            ProfilerActivity.CUDA,\n",
    "        ],\n",
    "\n",
    "        # 是否记录 Python / C++ 的调用栈\n",
    "        # 用于后续生成火焰图或 stack trace\n",
    "        with_stack=with_stack,\n",
    "\n",
    "        # 实验性配置：\n",
    "        # verbose=True 会输出更详细的 profiling 信息\n",
    "        experimental_config=torch._C._profiler._ExperimentalConfig(\n",
    "            verbose=True\n",
    "        ),\n",
    "    ) as prof:\n",
    "\n",
    "        # 在 profiler 作用域内运行被测函数\n",
    "        run()\n",
    "\n",
    "        # 再次同步，确保所有 CUDA kernel 完全执行完毕\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # =========================\n",
    "    # 4 生成并格式化 profiling 结果表格\n",
    "    # =========================\n",
    "\n",
    "    table = prof.key_averages().table(\n",
    "        # 按 CUDA 自身耗时排序（不包含子算子）\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "\n",
    "        # 算子名称列的最大宽度\n",
    "        max_name_column_width=80,\n",
    "\n",
    "        # 最多显示的行数\n",
    "        row_limit=10,\n",
    "\n",
    "        # 是否只显示顶层算子\n",
    "        # False 表示展开所有子算子\n",
    "        top_level_events_only=False,\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # 5 导出 CUDA 调用栈（用于可视化）\n",
    "    # =========================\n",
    "\n",
    "    if with_stack:\n",
    "        # 创建输出目录\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "\n",
    "        # 文本形式的 stack trace\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "\n",
    "        # 火焰图（可用于 speedscope / flamegraph）\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "\n",
    "        # 按 self_cuda_time_total 导出 stack\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "\n",
    "    # 返回 profiling 表格（字符串）\n",
    "    return table\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 实际执行 profiling\n",
    "# =========================\n",
    "\n",
    "print(\n",
    "    profile(\n",
    "        description='transformer',\n",
    "        run=run_model,\n",
    "        num_warmups=3,\n",
    "        with_stack=True,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e633611",
   "metadata": {},
   "source": [
    "# 作业二\n",
    "\n",
    "然而，简单地将模型转换为低精度格式可能会导致模型精度降低。例如，许多梯度值在实践中太小，无法用 FP16 表示，因此在用 FP16 精度进行简单训练时会变成零。为了解决这个问题，通常在用 FP16 训练时使用损失缩放（loss is simply multiplied by a scaling factor, increasing gradient magnitudes so they don't flush to zero）。此外，FP16 的动态范围比 FP32 低，这可能导致溢出，表现为 NaN 损失。全精度（Full bfloat16）训练通常更稳定（因为 BF16 具有与 FP32 相同的动态范围），但与 FP32 相比仍可能影响最终模型性能。\n",
    "\n",
    "为了利用低精度数据类型的加速优势，通常使用混合精度训练。在 PyTorch 中，这通过 torch.autocast 上下文管理器实现。在这种情况下，某些操作（例如矩阵乘法）在低精度数据类型上执行，而需要 FP32 全动态范围的操作（例如累积和归约）保持不变。例如，以下代码将自动识别在前向传播期间需要在低精度上执行的操作，并将这些操作转换为指定的数据类型：\n",
    "\n",
    "```python\n",
    "model : torch.nn.Module = ...  # 例如，你的 Transformer 模型\n",
    "dtype : torch.dtype = ...  # 例如，torch.float16\n",
    "x : torch.Tensor = ...  # 输入数据\n",
    "\n",
    "with torch.autocast(device=\"cuda\", dtype=dtype):\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "如上所述，即使张量本身已被降级，通常最好在高精度下保持累积。以下练习将帮助你建立直觉，了解为什么会这样。\n",
    "\n",
    "问题（混合精度累积）：1 分\n",
    "\n",
    "运行以下代码并评论结果的准确性。\n",
    "\n",
    "```python\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "```\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "我们现在将混合精度应用于一个玩具模型进行直观理解."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20ec484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)\n",
    "\n",
    "\n",
    "#  清理显存引用\n",
    "try:\n",
    "    del s, x,i\n",
    "except NameError:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31646db",
   "metadata": {},
   "source": [
    "可以看到全精度最准确，半进度最不准确，混合精度介于二者之间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270f15e",
   "metadata": {},
   "source": [
    "# 作业三\n",
    "\n",
    "问题（benchmarking_mixed_precision）：2 分\n",
    "\n",
    "（a）考虑以下模型：\n",
    "\n",
    "```python\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "假设我们在 GPU 上训练模型，并且模型参数最初是 FP32。我们希望使用 FP16 的自动混合精度。上述列出的每个组件的数据类型是什么？\n",
    "\n",
    "- autocast 上下文内的模型参数，\n",
    "- 第一个前馈层（ToyModel.fc1）的输出，\n",
    "- 层归一化（ToyModel.ln）的输出，\n",
    "- 模型的预测 logits，\n",
    "- 损失，\n",
    "- 以及模型的梯度？\n",
    "\n",
    "交付物：上述列出的每个组件的数据类型。\n",
    "\n",
    "（b）你应该已经看到，FP16 混合精度自动转换对层归一化层的处理与前馈层不同。层归一化的哪些部分对混合精度敏感？如果我们使用 BF16 而不是 FP16，是否仍需要对层归一化进行不同处理？为什么或为什么不？\n",
    "\n",
    "交付物：2–3 句话的回应。\n",
    "\n",
    "（c）修改你的基准测试脚本，以便可以选择使用 BF16 混合精度运行模型。对 §1.1.2 中描述的每种语言模型规模，分别计时使用和不使用混合精度的前向和反向传播。比较使用全精度与混合精度的结果，并评论模型大小变化时的任何趋势。你可能会发现无上下文的 no-op 上下文管理器很有用。\n",
    "\n",
    "交付物：2–3 句话的回应，包含你的计时结果和评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1eb9b",
   "metadata": {},
   "source": [
    "## 一个简单的解答\n",
    "\n",
    "首先模型参数 **存储 dtype 不变**（仍是 FP32），autocast 只影响 **算子内部计算与中间张量**，反向传播中梯度通常以 **FP32 累积**。\n",
    "\n",
    "**`autocast`** 是 PyTorch 中用于 **自动混合精度（Automatic Mixed Precision, AMP）** 的上下文管理器，用来在 **不改模型代码的情况下**，自动为不同算子选择合适的数据类型（FP32 / FP16 / BF16），从而 **加速计算并减少显存占用，同时尽量保持数值稳定性**。\n",
    "\n",
    "\n",
    "当你写：\n",
    "\n",
    "```python\n",
    "with torch.cuda.amp.autocast():\n",
    "    y = model(x)\n",
    "    loss = criterion(y, target)\n",
    "```\n",
    "\n",
    "`autocast` 会在 **前向传播阶段** 完成以下：\n",
    "\n",
    "不同算子用不同精度计算：**线性层 / 卷积 / matmul** 会用 **FP16 / BF16**（速度快、Tensor Core 友好），而**LayerNorm / Softmax / 指数、除法等数值敏感算子**会使用 **FP32**（保证数值稳定）。\n",
    "\n",
    "但参数本身不被永久转换，`model.parameters()` 仍是 **FP32**，autocast 只在 **算子执行时临时 cast**。\n",
    "\n",
    "中间激活自动使用低精度，激活张量通常是 FP16 / BF16，显存占用会有明显降低。\n",
    "\n",
    "---\n",
    "\n",
    "常见使用：\n",
    "\n",
    "```python\n",
    "with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    ...\n",
    "```\n",
    "\n",
    "速度快，但是需要 `GradScaler`，对 LayerNorm / Softmax 非常敏感。GradScaler 是 PyTorch 自动混合精度（AMP）中 专门为 FP16 训练服务的梯度缩放器，作用只有一个：防止 FP16 反向传播时梯度下溢（underflow）。torch.cuda.amp.GradScaler 在 反向传播前 把 loss 放大一个比例因子（scale），使梯度落在 FP16 能表示的数值范围内，\n",
    "\n",
    "```\n",
    "loss        → loss × scale\n",
    "grad        → grad × scale\n",
    "optimizer   → 自动除以 scale\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    ...\n",
    "```\n",
    "\n",
    "指数位与 FP32 相同，**数值稳定**。通常 **不需要 GradScaler**，在 Ampere+ GPU 上支持良好。\n",
    "\n",
    "\n",
    "### 问题 (a)\n",
    "\n",
    "在自动混合精度（Autocast）下，使用 FP16 训练时，各组件的数据类型如下：\n",
    "\n",
    "- **模型参数（model parameters）**：由于模型参数最初是 FP32，它们在 autocast 上下文内保持为 FP32。\n",
    "- **第一个前馈层的输出（output of the first feed-forward layerToyModel.fc1）**：在 autocast 上下文中，该层的输出将被转换为 FP16。\n",
    "- **层归一化的输出（output of layer norm ToyModel.ln）**：层归一化层的输出通常保持为 FP32，因为层归一化需要较高的数值稳定性，FP16 的动态范围较小，可能导致数值不稳定。\n",
    "- **模型的预测 logits**：在 autocast 上下文中，这些 logits 将被转换为 FP16。\n",
    "- **损失（loss）**：损失计算通常在 FP32 中进行，以保持数值稳定性。\n",
    "- **模型的梯度（model's gradients）**：梯度通常也保持为 FP32，以确保优化过程的稳定性。\n",
    "\n",
    "### 问题 (b)\n",
    "\n",
    "层归一化对混合精度的敏感性主要体现在其计算过程中需要较高的数值精度。层归一化涉及到均值和方差的计算，这些计算在 FP16 中可能会因为数值范围的限制而导致不准确，从而影响模型的训练效果。因此，即使在 FP16 混合精度下，层归一化通常也需要保持在 FP32 中进行，以确保数值稳定性。\n",
    "\n",
    "如果我们使用 BF16 而不是 FP16，我们可能仍然需要对层归一化进行特殊处理。BF16 虽然在动态范围上与 FP32 相同，但其精度较低，可能仍然不足以保证层归一化计算的稳定性。因此，为了保持数值稳定性，层归一化可能仍然需要在 FP32 中进行。\n",
    "\n",
    "### 问题c\n",
    "\n",
    "运行以下代码\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5c1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 time: 0.3536s\n",
      "BF16 time: 0.1905s\n",
      "正在使用 cuda\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm        16.75%      24.307ms        37.50%      54.431ms      26.946us     154.984ms        95.61%     154.984ms      76.725us          2020  \n",
      "                                                          ampere_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     153.229ms        94.52%     153.229ms      76.006us          2016  \n",
      "                                                         aten::native_layer_norm         8.04%      11.664ms        21.05%      30.556ms      30.254us       5.060ms         3.12%       5.060ms       5.010us          1010  \n",
      "void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.060ms         3.12%       5.060ms       5.020us          1008  \n",
      "                                                                 aten::clamp_min         5.91%       8.582ms        11.14%      16.166ms      16.005us       2.061ms         1.27%       2.061ms       2.041us          1010  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous name...         0.00%       0.000us         0.00%       0.000us       0.000us       2.061ms         1.27%       2.061ms       2.045us          1008  \n",
      "                                                                 Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.755ms         1.08%       1.755ms       0.871us          2015  \n",
      "                                                                     aten::empty         6.07%       8.816ms         6.07%       8.816ms       2.905us       0.000us         0.00%       0.000us       0.000us          3035  \n",
      "                                                                    aten::detach         0.00%       6.620us         0.02%      25.553us       6.388us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                                                          detach         0.01%      18.933us         0.01%      18.933us       4.733us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 145.143ms\n",
      "Self CUDA time total: 162.105ms\n",
      "\n",
      "正在使用 cuda\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm        12.49%      29.597ms        23.68%      56.140ms      27.792us      50.606ms        71.38%      50.606ms      25.052us          2020  \n",
      "                      ampere_bf16_s1688gemm_bf16_128x128_ldg8_f2f_stages_32x1_tn         0.00%       0.000us         0.00%       0.000us       0.000us      49.863ms        70.33%      49.863ms      24.709us          2018  \n",
      "                                                                     aten::copy_        10.00%      23.698ms        26.37%      62.507ms      12.368us      14.133ms        19.93%      14.133ms       2.796us          5054  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::bfloat16_copy_k...         0.00%       0.000us         0.00%       0.000us       0.000us      10.642ms        15.01%      10.642ms       2.638us          4034  \n",
      "                                                         aten::native_layer_norm         4.40%      10.438ms        11.82%      28.016ms      27.739us       4.673ms         6.59%       4.673ms       4.627us          1010  \n",
      "void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, f...         0.00%       0.000us         0.00%       0.000us       0.000us       4.673ms         6.59%       4.673ms       4.632us          1009  \n",
      "void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_c...         0.00%       0.000us         0.00%       0.000us       0.000us       3.491ms         4.92%       3.491ms       3.460us          1009  \n",
      "                                                                 aten::clamp_min         3.17%       7.509ms         6.34%      15.026ms      14.877us       1.485ms         2.09%       1.485ms       1.470us          1010  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous name...         0.00%       0.000us         0.00%       0.000us       0.000us       1.485ms         2.09%       1.485ms       1.472us          1009  \n",
      "                                                                 Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     742.830us         1.05%     742.830us       0.368us          2018  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 237.044ms\n",
      "Self CUDA time total: 70.897ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from my_profile import profile\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    用于性能测试的简单前馈网络（Toy Model）\n",
    "\n",
    "    结构：\n",
    "    Linear (无 bias)\n",
    "      → ReLU\n",
    "      → LayerNorm\n",
    "      → Linear (无 bias)\n",
    "\n",
    "    设计目的：\n",
    "    - 结构足够简单，便于观察算子性能\n",
    "    - 同时包含：\n",
    "        - GEMM（Linear）\n",
    "        - 非线性（ReLU）\n",
    "        - 归一化（LayerNorm）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一层全连接：in_features → 1024\n",
    "        self.fc1 = nn.Linear(in_features, 1024, bias=False)\n",
    "\n",
    "        # LayerNorm：对最后一维做归一化\n",
    "        self.ln = nn.LayerNorm(1024)\n",
    "\n",
    "        # 第二层全连接：1024 → out_features\n",
    "        self.fc2 = nn.Linear(1024, out_features, bias=False)\n",
    "\n",
    "        # ReLU 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        张量形状：\n",
    "        - 输入 x: (B, in_features)\n",
    "        - 输出: (B, out_features)\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def benchmark_dtype_fp32(use_bf16=False, iters=1000):\n",
    "    \"\"\"\n",
    "    FP32 精度下的前向推理性能 benchmark\n",
    "\n",
    "    参数说明：\n",
    "    - use_bf16:\n",
    "        是否开启 bfloat16 autocast（此函数默认 False）\n",
    "    - iters:\n",
    "        实际测量的迭代次数\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # 构建模型并移动到 GPU\n",
    "    model = ToyModel(1024, 1024).to(device)\n",
    "\n",
    "    # 构造输入张量（batch=256）\n",
    "    x = torch.randn(256, 1024, device=device)\n",
    "\n",
    "    # 根据 use_bf16 决定是否启用 autocast\n",
    "    # - FP32：nullcontext（什么都不做）\n",
    "    # - BF16：torch.autocast\n",
    "    autocast_ctx = (\n",
    "        torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "        if use_bf16 else nullcontext()\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # warmup 阶段\n",
    "    # =========================\n",
    "\n",
    "    # 触发 CUDA kernel 初始化、cudnn autotune 等\n",
    "    for _ in range(10):\n",
    "        with autocast_ctx, torch.no_grad():\n",
    "            _ = model(x)\n",
    "\n",
    "    # 同步，确保 warmup 完成\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # =========================\n",
    "    # 正式计时\n",
    "    # =========================\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        with autocast_ctx, torch.no_grad():\n",
    "            _ = model(x)\n",
    "\n",
    "    # 等待所有 CUDA kernel 执行完成\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    return end - start\n",
    "\n",
    "\n",
    "def benchmark_dtype_bf16(use_bf16=True, iters=1000):\n",
    "    \"\"\"\n",
    "    BF16 混合精度下的前向推理性能 benchmark\n",
    "\n",
    "    与 FP32 版本逻辑完全一致\n",
    "    唯一区别：默认开启 bfloat16 autocast\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    model = ToyModel(1024, 1024).to(device)\n",
    "    x = torch.randn(256, 1024, device=device)\n",
    "\n",
    "    autocast_ctx = (\n",
    "        torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
    "        if use_bf16 else nullcontext()\n",
    "    )\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(10):\n",
    "        with autocast_ctx, torch.no_grad():\n",
    "            _ = model(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        with autocast_ctx, torch.no_grad():\n",
    "            _ = model(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    return end - start\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 实际运行 benchmark\n",
    "# =========================\n",
    "\n",
    "fp32_time = benchmark_dtype_fp32(use_bf16=False)\n",
    "bf16_time = benchmark_dtype_bf16(use_bf16=True)\n",
    "\n",
    "print(f\"FP32 time: {fp32_time:.4f}s\")\n",
    "print(f\"BF16 time: {bf16_time:.4f}s\")\n",
    "\n",
    "# =========================\n",
    "# 使用 PyTorch Profiler 进行算子级性能分析\n",
    "# =========================\n",
    "\n",
    "print(\n",
    "    profile(\n",
    "        description='benchmark_dtype_fp32',\n",
    "        run=benchmark_dtype_fp32,\n",
    "        num_warmups=3,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    profile(\n",
    "        description='benchmark_dtype_bf16',\n",
    "        run=benchmark_dtype_bf16,\n",
    "        num_warmups=3,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81311fb",
   "metadata": {},
   "source": [
    "可以看到不同精度下，精度越低则速度越快，在比较使用全精度（FP32）与混合精度（BF16）的结果时，我们可能会观察到 BF16 在某些情况下可以提供与 FP32 相似的精度，同时显著减少内存使用和加速计算。然而，对于需要高精度计算的层（如层归一化），BF16 可能不如 FP32 稳定。随着模型大小的变化，我们可能会看到不同规模的模型对混合精度的敏感性不同，这需要在实际应用中进行具体分析和调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eed308",
   "metadata": {},
   "source": [
    "# 作业四\n",
    "\n",
    "\n",
    "到目前为止，我们一直在关注计算性能。现在我们将把注意力转向**内存**，这是语言模型训练和推理中的另一项关键资源。\n",
    "PyTorch 也提供了一个强大的**内存分析器（memory profiler）**，可以随时间跟踪内存分配情况。\n",
    "\n",
    "要使用内存分析器，你可以按如下方式修改你的 benchmark 脚本：\n",
    "\n",
    "```\n",
    "7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ... 在你的 benchmarking 脚本中的 warm-up 阶段\n",
    "\n",
    "# 开始记录内存历史\n",
    "torch.cuda.memory._record_memory_history(max_entries=1000000)\n",
    "\n",
    "# ... 在你的 benchmarking 脚本中你想要分析的部分\n",
    "\n",
    "# 保存一个 pickle 文件，用于 PyTorch 的在线工具加载\n",
    "torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "\n",
    "# 停止记录内存历史\n",
    "torch.cuda.memory._record_memory_history(enabled=None)\n",
    "```\n",
    "\n",
    "这将输出一个名为 `memory_snapshot.pickle` 的文件，你可以将其加载到如下在线工具中：\n",
    "[https://pytorch.org/memory_viz](https://pytorch.org/memory_viz)\n",
    "该工具可以让你查看整体内存使用时间线，以及每一次独立的内存分配是如何发生的，包括其大小以及指向分配来源代码位置的调用栈追踪。\n",
    "\n",
    "要使用该工具，你应当在浏览器中打开上述链接，然后将你的 pickle 文件拖拽到页面中。\n",
    "\n",
    "现在你将使用 PyTorch profiler 来分析你模型的内存使用情况。\n",
    "\n",
    "---\n",
    "\n",
    "### Problem（memory profiling）：4 分\n",
    "\n",
    "| Size | d_model | d_ff | num_layers | num_heads |\n",
    "|------|---------|------|------------|-----------|\n",
    "| small | 768 | 3072 | 12 | 12 |\n",
    "| medium | 1024 | 4096 | 24 | 16 |\n",
    "| large | 1280 | 5120 | 36 | 20 |\n",
    "| xl | 1600 | 6400 | 48 | 25 |\n",
    "| 2.7B | 2560 | 10240 | 32 | 32 |\n",
    "对 **表 1 中的 2.7B 模型**进行 profiling，分析其 **前向传播（forward pass）**、**反向传播（backward pass）** 以及 **优化器步骤（optimizer step）**，上下文长度分别为 **128、256 和 512**。\n",
    "\n",
    "---\n",
    "\n",
    "**(a)**\n",
    "在你的 profiling 脚本中加入一个选项，使模型运行时能够经过内存分析器。\n",
    "你可能需要复用之前的一些基础设施（例如：激活 mixed-precision、加载特定的模型大小等）。\n",
    "\n",
    "然后，在以下两种情况下，为 2.7B 模型获取内存 profile：\n",
    "\n",
    "* 只做推理（仅前向传播）\n",
    "* 完整训练一步（前向 + 反向 + 优化器步骤）\n",
    "\n",
    "你的内存时间线看起来如何？你能否根据看到的峰值判断当前运行的是哪个阶段？\n",
    "\n",
    "**Deliverable：**\n",
    "来自 `pytorch.org/memory_viz` 工具中 **“Active Memory Timeline”** 的两张截图（针对 context length = 512）：\n",
    "\n",
    "* 一张是仅前向传播\n",
    "* 一张是完整训练步骤（前向和反向传播，不含 optimizer step），\n",
    "  并附上一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(b)**\n",
    "在仅进行前向传播时，不同 context length 下的峰值内存使用是多少？\n",
    "在执行完整训练步骤时又是多少？\n",
    "\n",
    "**Deliverable：**\n",
    "一张表格，每个 context length 对应两个数值。\n",
    "\n",
    "---\n",
    "\n",
    "**(c)**\n",
    "在使用 **mixed-precision** 的情况下，找出 2.7B 模型在以下两种情形下的峰值内存使用：\n",
    "\n",
    "* 前向传播\n",
    "* 完整优化器步骤\n",
    "\n",
    "Mixed-precision 是否显著影响了内存使用？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **2–3 句**的文字说明。\n",
    "\n",
    "---\n",
    "\n",
    "**(d)**\n",
    "考虑 2.7B 模型。给定我们的参考超参数，在 **single-precision** 下，\n",
    "Transformer 残差流（residual stream）中 **激活张量（activations）** 的大小是多少？\n",
    "\n",
    "请以 **MB** 为单位给出结果（即用字节数除以 (1024^2)）。\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的回答，并给出你的推导过程。\n",
    "\n",
    "---\n",
    "\n",
    "**(e)**\n",
    "仔细查看 `pytorch.org/memory_viz` 中，2.7B 模型在前向传播时的 **“Active Memory Timeline”**。\n",
    "\n",
    "当你将 **“Detail”** 级别调低时，工具会显示最小分配的快照\n",
    "（例如，将 “Detail” 调到 **10%**，表示展示 **10% 最大的分配**）。\n",
    "\n",
    "最大的分配大小是多少？\n",
    "查看其调用栈（stack trace），你能判断这些分配来自哪里吗？\n",
    "\n",
    "**Deliverable：**\n",
    "一段 **1–2 句**的文字说明。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import ProfilerActivity\n",
    "import torch\n",
    "import os\n",
    "from typing import Callable\n",
    "def memory_profile(description: str, run: Callable, num_warmups: int = 1, with_stack: bool = False):\n",
    "    # 预热\n",
    "    for _ in range(num_warmups):\n",
    "        run()\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"正在使用cuda\")\n",
    "        torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "    else:\n",
    "        print('正在使用cpu')\n",
    "    # 使用性能分析器运行代码\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            # 输出堆栈跟踪以进行可视化\n",
    "            with_stack=with_stack,\n",
    "            #  需要导出堆栈跟踪以进行可视化\n",
    "            experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        run()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # 等待CUDA线程结束\n",
    "\n",
    "    # warm-up（不记录）\n",
    "    for _ in range(10):\n",
    "        run()\n",
    "    torch.cuda.synchronize()\n",
    "    # 开始记录 CUDA 内存历史\n",
    "    torch.cuda.memory._record_memory_history(\n",
    "        max_entries=1000000\n",
    "    )\n",
    "    # ===== 被分析代码 =====\n",
    "    run()\n",
    "    torch.cuda.synchronize()\n",
    "    # =====================\n",
    "    # 导出快照\n",
    "    torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "    # 停止记录\n",
    "    torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "\n",
    "    # 打印表格\n",
    "    table = prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\",\n",
    "        max_name_column_width=80,\n",
    "        row_limit=10,\n",
    "        top_level_events_only=False) \n",
    "    #text(f\"## {description}\")\n",
    "    #text(table, verbatim=True)\n",
    "    # Write stack trace visualization\n",
    "    if with_stack:\n",
    "        os.makedirs(\"var\", exist_ok=True)\n",
    "        text_path = f\"var/stacks_{description}.txt\"\n",
    "        svg_path = f\"var/stacks_{description}.svg\"\n",
    "        prof.export_stacks(text_path, \"self_cuda_time_total\")\n",
    "    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e7b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.optimizer import AdamW, get_cosine_lr\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.data import get_batch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_model(num_steps = 1):\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    batch_size = 32\n",
    "    \n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    \n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    warmup_iters = 200\n",
    "    cosine_cycle_iters = 10_000\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('运行设备',device)\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "    for it in range(num_steps):\n",
    "        #print(f'{it}/{num_steps}')\n",
    "        # 1️更新学习率（每 step）\n",
    "        lr = get_cosine_lr(\n",
    "            it=it,\n",
    "            max_learning_rate=max_lr,\n",
    "            min_learning_rate=min_lr,\n",
    "            warmup_iters=warmup_iters,\n",
    "            cosine_cycle_iters=cosine_cycle_iters,\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        dataset = np.arange(1000, dtype=np.int64)\n",
    "        # 2取 batch\n",
    "        x, y = get_batch(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            context_length=context_length,\n",
    "            device=device,\n",
    "        )\n",
    "        #print(x)\n",
    "\n",
    "        # 3前向\n",
    "        logits = model(x)\n",
    "\n",
    "        # 4loss\n",
    "        loss = lm_loss(logits, y)\n",
    "\n",
    "        # 5反向\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # （可选）梯度裁剪（强烈推荐）\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 6更新参数\n",
    "        optimizer.step()\n",
    "        #  清理显存引用\n",
    "        try:\n",
    "            del X, attention_fn, out, loss\n",
    "        except NameError:\n",
    "            pass\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    # 7日志\n",
    "    if it % 100 == 0:\n",
    "        print(\n",
    "            f\"step {it:6d} | \"\n",
    "            f\"loss {loss.item():.4f} | \"\n",
    "            f\"lr {lr:.2e}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee061ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step      0 | loss 10.8306 | lr 0.00e+00\n",
      "正在使用cuda\n",
      "step      0 | loss 10.8350 | lr 0.00e+00\n",
      "step      0 | loss 10.8312 | lr 0.00e+00\n",
      "step      0 | loss 10.8314 | lr 0.00e+00\n",
      "step      0 | loss 10.8308 | lr 0.00e+00\n",
      "step      0 | loss 10.8288 | lr 0.00e+00\n",
      "step      0 | loss 10.8306 | lr 0.00e+00\n",
      "step      0 | loss 10.8309 | lr 0.00e+00\n",
      "step      0 | loss 10.8286 | lr 0.00e+00\n",
      "step      0 | loss 10.8322 | lr 0.00e+00\n",
      "step      0 | loss 10.8256 | lr 0.00e+00\n",
      "step      0 | loss 10.8327 | lr 0.00e+00\n",
      "step      0 | loss 10.8319 | lr 0.00e+00\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         0.47%       3.367ms         0.89%       6.451ms      39.099us     151.224ms        54.40%     151.224ms     916.509us           165  \n",
      "                                                           ampere_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      49.839ms        17.93%      49.839ms       1.278ms            39  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us      43.669ms        15.71%      43.669ms      43.669ms             1  \n",
      "                                                                       aten::mul         0.89%       6.441ms         9.54%      68.927ms     111.352us      43.221ms        15.55%      46.482ms      75.092us           619  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_128x128_8x4_nt_align1>(cutlass_80...         0.00%       0.000us         0.00%       0.000us       0.000us      38.396ms        13.81%      38.396ms       2.021ms            19  \n",
      "                                                           ampere_sgemm_64x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us      25.907ms         9.32%      25.907ms     616.826us            42  \n",
      "                                                          ampere_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us      24.973ms         8.98%      24.973ms      24.973ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us      23.436ms         8.43%      23.436ms     234.360us           100  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      19.981ms         7.19%      19.981ms      58.942us           339  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us      14.283ms         5.14%      14.283ms      38.708us           369  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 722.500ms\n",
      "Self CUDA time total: 278.010ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memory_profile(description=\"前向传播\",run=run_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dbd6",
   "metadata": {},
   "source": [
    "# Flash Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4215642",
   "metadata": {},
   "source": [
    "\n",
    "# 作业 五\n",
    "\n",
    "你的性能分析结果很可能表明，在注意力层中，无论是**内存**还是**计算**方面，都存在优化空间。从高层次来看，注意力操作由一次矩阵乘法、一次 softmax，然后再一次矩阵乘法组成：\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\mathrm{mask}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\\right)V\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "朴素的注意力实现需要为每个 batch / head 元素保存形状为\n",
    "\n",
    "${seqlen} \\times {seqlen}$ 的注意力分数矩阵。随着序列长度变长，这个矩阵会迅速变得非常大，从而在长输入或长输出任务中导致 **out-of-memory (OOM)** 错误。\n",
    "我们将实现一个遵循 **FlashAttention-2** 论文的注意力 kernel，它通过 **按 tile 方式计算注意力**，避免显式地物化 ${seqlen} \\times {seqlen}$ 的注意力分数矩阵，从而支持更长的序列长度。\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem (pytorch_attention): 2 points**\n",
    "\n",
    "#### (a) 在不同规模下对你的注意力实现进行基准测试。编写一个脚本来完成以下任务：\n",
    "\n",
    "(a) 将 batch size 固定为 **8**，并且 **不使用多头注意力**（即移除 head 这一维）。\n",
    "\n",
    "(b) 遍历如下笛卡尔积配置：\n",
    "\n",
    "* head embedding 维度 $d_{\\text{model}} \\in {16, 32, 64, 128}$\n",
    "* 序列长度 ${seqlen} \\in {256, 1024, 4096, 8192, 16384}$\n",
    "\n",
    "(c) 为对应的尺寸生成随机输入 (Q, K, V)。\n",
    "\n",
    "(d) 使用这些输入，对注意力前向传播 **计时 100 次**。\n",
    "\n",
    "(e) 测量在 **反向传播开始之前** 的显存使用量，并对 **反向传播计时 100 次**。\n",
    "\n",
    "(f) 确保进行 **warm-up**，并在每次前向 / 反向传播之后调用\n",
    "`torch.cuda.synchronize()`。\n",
    "\n",
    "---\n",
    "\n",
    "请报告你在这些配置下得到的 **时间结果（或 out-of-memory 错误）**。\n",
    "**在什么规模下会出现 out-of-memory？**\n",
    "\n",
    "请选择你发现的 **最小的、仍然会 OOM 的配置之一**，对注意力的内存使用进行详细的**显存占用核算**（你可以使用 Assignment 1 中 Transformer 内存使用的公式）。\n",
    "\n",
    "* 反向传播中节省的显存会如何随着序列长度变化？\n",
    "* 你会如何 **消除反向传播中的这部分内存开销**？\n",
    "\n",
    "---\n",
    "\n",
    "### **Deliverable（提交内容）**\n",
    "\n",
    "* 一张包含 **时间测量结果** 的表格\n",
    "* 你对 **内存使用的推导过程**\n",
    "* 一段 **1–2 段的文字分析回答**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707a115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头、朴素自注意力实现\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 线性投影（Q, K, V）\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        return: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model\n",
    "\n",
    "        # 1. 线性投影\n",
    "        Q = self.q_proj(x)  # (B, T, D)\n",
    "        K = self.k_proj(x)  # (B, T, D)\n",
    "        V = self.v_proj(x)  # (B, T, D)\n",
    "\n",
    "        # 2. 注意力分数 (B, T, T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "\n",
    "        # 3. softmax\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # 4. 加权求和\n",
    "        out = torch.matmul(attn, V)  # (B, T, D)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c2f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing d_model=16\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.22ms | Bwd:    1.07ms | Mem: 25.0MB\n",
      "✓ T= 1024 | Fwd:   1.05ms | Bwd:    2.80ms | Mem: 147.3MB\n",
      "✓ T= 4096 | Fwd:  15.62ms | Bwd:   46.87ms | Mem: 2076.3MB\n",
      "✓ T= 8192 | Fwd: 352.39ms | Bwd: 1400.97ms | Mem: 8232.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "==================================================\n",
      "Testing d_model=32\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.19ms | Bwd:    1.50ms | Mem: 8240.3MB\n",
      "✓ T= 1024 | Fwd:   1.55ms | Bwd:    4.29ms | Mem: 8240.3MB\n",
      "✓ T= 4096 | Fwd:  15.57ms | Bwd:   43.96ms | Mem: 8240.3MB\n",
      "✓ T= 8192 | Fwd: 227.31ms | Bwd:  373.38ms | Mem: 8256.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "==================================================\n",
      "Testing d_model=64\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.30ms | Bwd:    1.36ms | Mem: 8272.3MB\n",
      "✓ T= 1024 | Fwd:   0.98ms | Bwd:    2.96ms | Mem: 8272.3MB\n",
      "✓ T= 4096 | Fwd:  15.02ms | Bwd:   44.85ms | Mem: 8272.3MB\n",
      "✓ T= 8192 | Fwd: 235.51ms | Bwd:  759.53ms | Mem: 8304.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "==================================================\n",
      "Testing d_model=128\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   1.01ms | Bwd:    2.03ms | Mem: 8336.3MB\n",
      "✓ T= 1024 | Fwd:   1.15ms | Bwd:    3.46ms | Mem: 8336.3MB\n",
      "✓ T= 4096 | Fwd:  16.20ms | Bwd:   48.54ms | Mem: 8336.3MB\n",
      "✓ T= 8192 | Fwd: 270.33ms | Bwd:  822.33ms | Mem: 8400.4MB\n",
      "✗ T=16384 | OOM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import torch._inductor.config as inductor_config\n",
    "import gc\n",
    "\n",
    "# ========== 禁用 CUDA Graphs 避免 backward 错误 ==========\n",
    "inductor_config.triton.cudagraphs = False\n",
    "inductor_config.triton.cudagraph_trees = False\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头、朴素自注意力实现\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d)) V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 线性投影（Q, K, V）\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        return: (B, T, D)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model\n",
    "\n",
    "        # 1. 线性投影\n",
    "        Q = self.q_proj(x)  # (B, T, D)\n",
    "        K = self.k_proj(x)  # (B, T, D)\n",
    "        V = self.v_proj(x)  # (B, T, D)\n",
    "\n",
    "        # 2. 注意力分数 (B, T, T)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "\n",
    "        # 3. softmax\n",
    "        attn = torch.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # 4. 加权求和\n",
    "        out = torch.matmul(attn, V)  # (B, T, D)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def benchmark_attention(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for d_model in d_models:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing d_model={d_model}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # 构造输入（无多头）\n",
    "                X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True)\n",
    "                \n",
    "                # 创建模型\n",
    "                attention_fn = NaiveSelfAttention(d_model=d_model).to(device).to(torch.float32)\n",
    "                \n",
    "                # warm-up：包含 forward + backward，并清零梯度\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    attention_fn.zero_grad(set_to_none=True)\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                # forward timing：需要创建计算图但不 backward\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    _ = out.sum()  # 创建标量，但不 backward\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "                # 记录 forward 后的显存（peak memory）\n",
    "                mem_allocated = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "                # backward timing：完整的 forward + backward\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(10):\n",
    "                    out = attention_fn(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    attention_fn.zero_grad(set_to_none=True)\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_time_ms\": forward_time * 1000,\n",
    "                    \"backward_time_ms\": backward_time * 1000,\n",
    "                    \"mem_MB\": mem_allocated,\n",
    "                })\n",
    "\n",
    "                print(f\"✓ T={seqlen:5d} | Fwd: {forward_time*1000:6.2f}ms | Bwd: {backward_time*1000:7.2f}ms | Mem: {mem_allocated:.1f}MB\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"✗ T={seqlen:5d} | OOM\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "          \n",
    "            finally:\n",
    "                # 清理显存引用\n",
    "                try:\n",
    "                    del X, attention_fn, out, loss\n",
    "                except NameError:\n",
    "                    pass\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行测试\n",
    "results = benchmark_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0e5d",
   "metadata": {},
   "source": [
    "**已知输入张量形状**\n",
    "\n",
    "```python\n",
    "x.shape = [8, 8192, 16]\n",
    "dtype = torch.float32  # 默认\n",
    "```\n",
    "\n",
    "* batch size (B = 8)\n",
    "* sequence length (T = 8192)\n",
    "* hidden dim (D = 16)\n",
    "* float32 = **4 bytes**\n",
    "\n",
    "---\n",
    "\n",
    "**元素总数**\n",
    "\n",
    "$$\n",
    "8 \\times 8192 \\times 16 = 1{,}048{,}576 \\text{ elements}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**显存占用（仅 input x）**\n",
    "\n",
    "$$\n",
    "1{,}048{,}576 \\times 4 \\text{ bytes}\n",
    "= 4{,}194{,}304 \\text{ bytes}\n",
    "\\approx 4.0 \\text{ MB}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> **输入 `torch.Size([8, 8192, 16])` 本身仅占约 `4 MB` 显存**\n",
    "\n",
    "\n",
    "真正爆显存的是注意力里的**attention score 矩阵**\n",
    "\n",
    "```python\n",
    "scores.shape = [8, 8192, 8192]\n",
    "```\n",
    "\n",
    "元素数：\n",
    "\n",
    "$$\n",
    "8 \\times 8192^2 = 536{,}870{,}912\n",
    "$$\n",
    "\n",
    "显存：\n",
    "\n",
    "\n",
    "$$\n",
    "536{,}870{,}912 \\times 4 \\approx 2.0 \\text{ GB}\n",
    "$$\n",
    "\n",
    "而且，scores 要存，softmax 输出要存，backward 还要再存一份中间量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cbd65",
   "metadata": {},
   "source": [
    "我们将在这里实现一个flash attention，我们需要实现两个部分，一个是分块，第二个是数值稳定softmax的方式，思想上非常接近 FlashAttention v1，具体可以看第六章：GPU和GPU相关的优化，有详细介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f7859",
   "metadata": {},
   "source": [
    "# 作业六\n",
    "Problem (torch_compile): 2 points\n",
    "------------------------------------------------------------\n",
    "\n",
    "(a) 扩展你的 attention 基准测试脚本，\n",
    "    加入你使用 PyTorch 实现的 attention 的\n",
    "    **编译版本（compiled version）**，\n",
    "    并在与上一个 pytorch_attention 问题\n",
    "    相同的配置下，将其性能与未编译版本进行比较。\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比你 **编译后的 attention 模块**\n",
    "    与 **pytorch_attention 问题中未编译版本**\n",
    "    在前向传播和反向传播上的耗时。\n",
    "\n",
    "(b) 接下来，在端到端的基准测试脚本中，\n",
    "    将你的整个 Transformer 模型进行编译。\n",
    "    前向传播的性能会如何变化？\n",
    "    那么 **前向 + 反向传播 + 优化器更新步骤**\n",
    "    的整体性能又会如何变化？\n",
    "\n",
    "Deliverable：\n",
    "    一张表格，对比 **原始（vanilla）**\n",
    "    与 **编译后（compiled）** 的 Transformer 模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5b218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Testing d_model=16\n",
      "========================================\n",
      "✓ T=  256 | Fwd:   0.32ms | Bwd:    1.22ms | Mem: 16.6MB\n",
      "✓ T= 1024 | Fwd:   0.49ms | Bwd:    1.59ms | Mem: 17.8MB\n",
      "✓ T= 4096 | Fwd:  10.52ms | Bwd:   27.80ms | Mem: 22.3MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 11:32:40.445000 20094 torch/_inductor/utils.py:1613] [0/3] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ T= 8192 | Fwd:  39.36ms | Bwd:  106.26ms | Mem: 28.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "========================================\n",
      "Testing d_model=32\n",
      "========================================\n",
      "✓ T=  256 | Fwd:   0.33ms | Bwd:    1.14ms | Mem: 17.0MB\n",
      "✓ T= 1024 | Fwd:   0.58ms | Bwd:    1.55ms | Mem: 19.3MB\n",
      "✓ T= 4096 | Fwd:   9.62ms | Bwd:   27.95ms | Mem: 28.3MB\n",
      "✓ T= 8192 | Fwd:  40.97ms | Bwd:  112.11ms | Mem: 40.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "========================================\n",
      "Testing d_model=64\n",
      "========================================\n",
      "✓ T=  256 | Fwd:   0.36ms | Bwd:    1.28ms | Mem: 17.8MB\n",
      "✓ T= 1024 | Fwd:   0.60ms | Bwd:    1.97ms | Mem: 22.3MB\n",
      "✓ T= 4096 | Fwd:  12.52ms | Bwd:   29.71ms | Mem: 40.3MB\n",
      "✓ T= 8192 | Fwd:  42.21ms | Bwd:  114.92ms | Mem: 64.3MB\n",
      "✗ T=16384 | OOM\n",
      "\n",
      "========================================\n",
      "Testing d_model=128\n",
      "========================================\n",
      "✓ T=  256 | Fwd:   0.61ms | Bwd:    1.38ms | Mem: 19.4MB\n",
      "✓ T= 1024 | Fwd:   0.83ms | Bwd:    2.41ms | Mem: 28.4MB\n",
      "✓ T= 4096 | Fwd:  11.82ms | Bwd:   32.63ms | Mem: 64.4MB\n",
      "✓ T= 8192 | Fwd:  46.05ms | Bwd:  126.12ms | Mem: 112.4MB\n",
      "✗ T=16384 | OOM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import torch._inductor.config as inductor_config\n",
    "import gc\n",
    "inductor_config.triton.cudagraphs = False\n",
    "# 同时禁用 cudagraph 相关的 tree reduction 优化\n",
    "inductor_config.triton.cudagraph_trees = False\n",
    "\n",
    "\n",
    "import torch._dynamo.config as dynamo_config\n",
    "dynamo_config.recompile_limit = 32\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "TORCH_LOGS=\"recompiles\"\n",
    "\n",
    "class NaiveSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = x.shape\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return out\n",
    "\n",
    "def benchmark_attention_fixed(device=\"cuda\",d_models =[16, 32, 64, 128] ):\n",
    "    torch.manual_seed(0)\n",
    "    batch_size = 8\n",
    "  \n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for d_model in d_models:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Testing d_model={d_model}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # 关键修复：为每个 d_model 单独编译，避免跨 d_model 的重编译\n",
    "        model = NaiveSelfAttention(d_model).to(device).to(torch.float32)\n",
    "        \n",
    "        # 修复：使用 max-autotune 模式获得更好性能，或 default 但接受重编译\n",
    "        # 注意：d_model 变化必须重新编译，因为权重矩阵形状变了\n",
    "        compiled_model = torch.compile(model, mode=\"default\", dynamic=False)\n",
    "        \n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # 创建输入\n",
    "                X = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True)\n",
    "                \n",
    "                # Warm-up：固定 5 次，包含 forward + backward\n",
    "                for _ in range(10):\n",
    "                    out = compiled_model(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    # 修复：必须清零梯度，否则累积\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # Forward 计时（带梯度，与 backward 一致）\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(10):\n",
    "                    out = compiled_model(X)\n",
    "                    # 修复：需要 retain_graph 否则第二次迭代 backward 会失败\n",
    "                    # 或者每次重新创建计算图\n",
    "                    loss = out.sum()\n",
    "                torch.cuda.synchronize()\n",
    "                forward_time = (time.perf_counter() - start) / 10\n",
    "                \n",
    "                # Backward 计时\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(10):\n",
    "                    out = compiled_model(X)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.perf_counter() - start) / 10\n",
    "                \n",
    "                mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "                \n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_ms\": forward_time * 1000,\n",
    "                    \"backward_ms\": backward_time * 1000,\n",
    "                    \"total_ms\": (forward_time + backward_time) * 1000,\n",
    "                    \"mem_MB\": mem_allocated,\n",
    "                })\n",
    "                print(f\"✓ T={seqlen:5d} | Fwd: {forward_time*1000:6.2f}ms | Bwd: {backward_time*1000:7.2f}ms | Mem: {mem_allocated:.1f}MB\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\"d_model\": d_model, \"seqlen\": seqlen, \"OOM\": True})\n",
    "                    print(f\"✗ T={seqlen:5d} | OOM\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            del X\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        del model, compiled_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行\n",
    "\n",
    "results = benchmark_attention_fixed(d_models=[16])\n",
    "results = benchmark_attention_fixed(d_models=[32])\n",
    "results = benchmark_attention_fixed(d_models=[64])\n",
    "results = benchmark_attention_fixed(d_models=[128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062be29d",
   "metadata": {},
   "source": [
    "对transformer模型进行torch_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b503846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import gc\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.optimizer import AdamW,get_cosine_lr\n",
    "from cs336_basics.data import get_batch\n",
    "from my_profile import benchmark, profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca32dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "tensor([[969, 970, 971, 972, 973, 974, 975, 976],\n",
      "        [ 58,  59,  60,  61,  62,  63,  64,  65],\n",
      "        [289, 290, 291, 292, 293, 294, 295, 296],\n",
      "        [ 91,  92,  93,  94,  95,  96,  97,  98]], device='cuda:0')\n",
      "tensor([[970, 971, 972, 973, 974, 975, 976, 977],\n",
      "        [ 59,  60,  61,  62,  63,  64,  65,  66],\n",
      "        [290, 291, 292, 293, 294, 295, 296, 297],\n",
      "        [ 92,  93,  94,  95,  96,  97,  98,  99]], device='cuda:0')\n",
      "设备：cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_transformer():\n",
    "\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        d_model=d_model,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=rope_theta,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=max_lr,\n",
    "        weight_decay=0.1,\n",
    "    )\n",
    "\n",
    "    # 假设是 token 序列\n",
    "    dataset = np.arange(1000, dtype=np.int64)\n",
    "    x, y = get_batch(\n",
    "        dataset=dataset,\n",
    "        batch_size=4,\n",
    "        context_length=8,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(x.shape)  # (4, 8)\n",
    "    print(y.shape)  # (4, 8)\n",
    "    print(x)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    print(f'设备：{device}')\n",
    "    #  清理显存引用\n",
    "    try:\n",
    "        del model, x,y, optimizer,dataset\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf1eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step      0 | loss 10.8336 | lr 0.00e+00\n",
      "step      2 | loss 10.8325 | lr 3.00e-06\n",
      "step      4 | loss 10.8242 | lr 6.00e-06\n",
      "step      6 | loss 10.8147 | lr 9.00e-06\n",
      "step      8 | loss 10.7979 | lr 1.20e-05\n",
      "现在真正计时!\n",
      "step      0 | loss 10.7808 | lr 0.00e+00\n",
      "step      2 | loss 10.7776 | lr 3.00e-06\n",
      "step      4 | loss 10.7694 | lr 6.00e-06\n",
      "step      6 | loss 10.7614 | lr 9.00e-06\n",
      "step      8 | loss 10.7451 | lr 1.20e-05\n",
      "step      0 | loss 10.7275 | lr 0.00e+00\n",
      "step      2 | loss 10.7262 | lr 3.00e-06\n",
      "step      4 | loss 10.7236 | lr 6.00e-06\n",
      "step      6 | loss 10.7119 | lr 9.00e-06\n",
      "step      8 | loss 10.6955 | lr 1.20e-05\n",
      "step      0 | loss 10.6771 | lr 0.00e+00\n",
      "step      2 | loss 10.6776 | lr 3.00e-06\n",
      "step      4 | loss 10.6745 | lr 6.00e-06\n",
      "step      6 | loss 10.6596 | lr 9.00e-06\n",
      "step      8 | loss 10.6401 | lr 1.20e-05\n",
      "单次耗时：1982.1436405181885ms\n",
      "step      0 | loss 10.6245 | lr 0.00e+00\n",
      "step      2 | loss 10.6215 | lr 3.00e-06\n",
      "step      4 | loss 10.6149 | lr 6.00e-06\n",
      "step      6 | loss 10.6057 | lr 9.00e-06\n",
      "step      8 | loss 10.5835 | lr 1.20e-05\n",
      "正在使用 cuda\n",
      "step      0 | loss 10.5704 | lr 0.00e+00\n",
      "step      2 | loss 10.5721 | lr 3.00e-06\n",
      "step      4 | loss 10.5665 | lr 6.00e-06\n",
      "step      6 | loss 10.5606 | lr 9.00e-06\n",
      "step      8 | loss 10.5394 | lr 1.20e-05\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm         0.76%      24.664ms         3.29%     106.882ms      82.854us        1.496s        48.56%        1.521s       1.179ms          1290  \n",
      "## Call CompiledFxGraph fxwxve63vja4o47hsbiemjea7dkmmru4v6emhlpr7nfhghwsstk6 ...         0.00%       0.000us         0.00%       0.000us       0.000us        1.234s        40.04%        1.234s     123.354ms            10  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us     674.247ms        21.89%     674.247ms      67.425ms            10  \n",
      "## Call CompiledFxGraph fwqhmxtfk2e5xyvp7ejh4bcshmv4al3cnh3upoh23securxbcxwd ...         0.00%       0.000us         0.00%       0.000us       0.000us     648.035ms        21.04%     648.035ms      64.804ms            10  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x256_16x4_tn_align4>(cu...         0.00%       0.000us         0.00%       0.000us       0.000us     526.151ms        17.08%     526.151ms       1.224ms           430  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nn_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us     234.931ms         7.63%     234.931ms      23.493ms            10  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nt_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us     234.072ms         7.60%     234.072ms      23.407ms            10  \n",
      "                                                                       aten::mul         1.27%      41.173ms         9.89%     321.299ms      93.673us     217.914ms         7.07%     258.743ms      75.435us          3430  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us     217.914ms         7.07%     217.914ms      63.532us          3430  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x64_16x6_nn_align4>(cut...         0.00%       0.000us         0.00%       0.000us       0.000us     205.231ms         6.66%     205.231ms     427.565us           480  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.249s\n",
      "Self CUDA time total: 3.081s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##优化后\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_benchmack():\n",
    "    \n",
    "    batch_size = 32\n",
    "    warmup_iters = 200\n",
    "    DTYPES = torch.float32\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    cosine_cycle_iters = 10_000\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    context_length=context_length,\n",
    "                                    d_model=d_model,\n",
    "                                    num_layers=num_layers,\n",
    "                                    num_heads=num_heads,\n",
    "                                    d_ff=d_ff,\n",
    "                                    rope_theta=rope_theta,\n",
    "                                    ).to(device).to(DTYPES)\n",
    "    model_compile = torch.compile(\n",
    "\n",
    "                                                model,\n",
    "                                                mode=\"default\",\n",
    "                                               \n",
    "\n",
    "                                                )\n",
    "    def run_model_complile(num_steps = 10):\n",
    "       \n",
    "        \n",
    "        for it in range(num_steps):\n",
    "            #print(f'{it}/{num_steps}')\n",
    "            dataset = np.arange(1000, dtype=np.int64)\n",
    "        \n",
    "\n",
    "            \n",
    "            optimizer = AdamW(\n",
    "                                model.parameters(),\n",
    "                                lr=max_lr,\n",
    "                                weight_decay=0.1,\n",
    "                            )\n",
    "            # 1️更新学习率（每 step）\n",
    "            lr = get_cosine_lr(\n",
    "                it=it,\n",
    "                max_learning_rate=max_lr,\n",
    "                min_learning_rate=min_lr,\n",
    "                warmup_iters=warmup_iters,\n",
    "                cosine_cycle_iters=cosine_cycle_iters,\n",
    "            )\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            # 2取 batch\n",
    "            x, y = get_batch(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                context_length=context_length,\n",
    "                device=device,\n",
    "            )\n",
    "            # print(x)\n",
    "\n",
    "            # 3前向\n",
    "            logits = model_compile(x)\n",
    "\n",
    "            # 4loss\n",
    "            loss = lm_loss(logits, y)\n",
    "\n",
    "            # 5反向\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            # （可选）梯度裁剪（强烈推荐）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 6更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "        # 7日志\n",
    "            if it % 2 == 0:\n",
    "                print(\n",
    "                f\"step {it:6d} | \"\n",
    "                f\"loss {loss.item():.4f} | \"\n",
    "                f\"lr {lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        #  清理显存引用\n",
    "\n",
    "    benchmark(description = 'transformer模型的基准测试', run =  run_model_complile, num_warmups = 1, num_trials = 3)\n",
    "    t = profile(description='transformers的性能分析',run=run_model_complile)\n",
    "    print(t)\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer, model_compile\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "run_benchmack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c74a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step      0 | loss 10.8274 | lr 0.00e+00\n",
      "step      2 | loss 10.8249 | lr 3.00e-06\n",
      "step      4 | loss 10.8262 | lr 6.00e-06\n",
      "step      6 | loss 10.8093 | lr 9.00e-06\n",
      "step      8 | loss 10.7956 | lr 1.20e-05\n",
      "现在真正计时!\n",
      "step      0 | loss 10.7820 | lr 0.00e+00\n",
      "step      2 | loss 10.7752 | lr 3.00e-06\n",
      "step      4 | loss 10.7697 | lr 6.00e-06\n",
      "step      6 | loss 10.7573 | lr 9.00e-06\n",
      "step      8 | loss 10.7435 | lr 1.20e-05\n",
      "step      0 | loss 10.7277 | lr 0.00e+00\n",
      "step      2 | loss 10.7250 | lr 3.00e-06\n",
      "step      4 | loss 10.7196 | lr 6.00e-06\n",
      "step      6 | loss 10.7150 | lr 9.00e-06\n",
      "step      8 | loss 10.6957 | lr 1.20e-05\n",
      "step      0 | loss 10.6771 | lr 0.00e+00\n",
      "step      2 | loss 10.6724 | lr 3.00e-06\n",
      "step      4 | loss 10.6691 | lr 6.00e-06\n",
      "step      6 | loss 10.6611 | lr 9.00e-06\n",
      "step      8 | loss 10.6479 | lr 1.20e-05\n",
      "单次耗时：12359.17353630066ms\n",
      "step      0 | loss 10.6202 | lr 0.00e+00\n",
      "step      2 | loss 10.6259 | lr 3.00e-06\n",
      "step      4 | loss 10.6196 | lr 6.00e-06\n",
      "step      6 | loss 10.6035 | lr 9.00e-06\n",
      "step      8 | loss 10.5891 | lr 1.20e-05\n",
      "正在使用 cuda\n",
      "step      0 | loss 10.5739 | lr 0.00e+00\n",
      "step      2 | loss 10.5756 | lr 3.00e-06\n",
      "step      4 | loss 10.5660 | lr 6.00e-06\n",
      "step      6 | loss 10.5523 | lr 9.00e-06\n",
      "step      8 | loss 10.5411 | lr 1.20e-05\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm         0.31%      35.968ms         1.05%     122.861ms      74.461us        8.839s        75.05%        8.851s       5.364ms          1650  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nn_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us        5.081s        43.14%        5.081s     508.059ms            10  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_128x128_32x3_nt_align4>(c...         0.00%       0.000us         0.00%       0.000us       0.000us        3.063s        26.01%        3.063s     306.315ms            10  \n",
      "                                                aten::_log_softmax_backward_data         0.00%     112.242us         0.00%     320.263us      32.026us        1.164s         9.88%        1.164s     116.351ms            10  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMaxBackward<4, float, float,...         0.00%       0.000us         0.00%       0.000us       0.000us        1.164s         9.88%        1.164s     116.351ms            10  \n",
      "                                                       Optimizer.step#AdamW.step         0.00%       0.000us         0.00%       0.000us       0.000us     706.626ms         6.00%     706.626ms      70.663ms            10  \n",
      "                                                                       aten::mul         0.49%      57.873ms        30.02%        3.521s     569.784us     625.405ms         5.31%     688.192ms     111.358us          6180  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us     342.942ms         2.91%     342.942ms      99.983us          3430  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_s1688gemm_64x256_16x4_tn_align4>(cu...         0.00%       0.000us         0.00%       0.000us       0.000us     326.682ms         2.77%     326.682ms     759.726us           430  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us     319.556ms         2.71%     319.556ms     319.556us          1000  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.728s\n",
      "Self CUDA time total: 11.777s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 优化前\n",
    "def lm_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B, T, V)\n",
    "        # targets: (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        return F.cross_entropy(\n",
    "            logits.view(B * T, V),\n",
    "            targets.view(B * T),\n",
    "        )\n",
    "def run_benchmack():\n",
    "    \n",
    "    batch_size = 32\n",
    "    warmup_iters = 200\n",
    "    DTYPES = torch.float32\n",
    "    vocab_size = 50_000\n",
    "    context_length = 128\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    rope_theta = 10000.0\n",
    "    max_lr = 3e-4\n",
    "    min_lr = 3e-5\n",
    "    cosine_cycle_iters = 10_000\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = BasicsTransformerLM(\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    context_length=context_length,\n",
    "                                    d_model=d_model,\n",
    "                                    num_layers=num_layers,\n",
    "                                    num_heads=num_heads,\n",
    "                                    d_ff=d_ff,\n",
    "                                    rope_theta=rope_theta,\n",
    "                                    ).to(device).to(DTYPES)\n",
    "    def run_model(num_steps = 10):\n",
    "       \n",
    "        \n",
    "        for it in range(num_steps):\n",
    "            #print(f'{it}/{num_steps}')\n",
    "            dataset = np.arange(1000, dtype=np.int64)\n",
    "        \n",
    "\n",
    "            \n",
    "            optimizer = AdamW(\n",
    "                                model.parameters(),\n",
    "                                lr=max_lr,\n",
    "                                weight_decay=0.1,\n",
    "                            )\n",
    "            # 1️更新学习率（每 step）\n",
    "            lr = get_cosine_lr(\n",
    "                it=it,\n",
    "                max_learning_rate=max_lr,\n",
    "                min_learning_rate=min_lr,\n",
    "                warmup_iters=warmup_iters,\n",
    "                cosine_cycle_iters=cosine_cycle_iters,\n",
    "            )\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            # 2取 batch\n",
    "            x, y = get_batch(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                context_length=context_length,\n",
    "                device=device,\n",
    "            )\n",
    "            # print(x)\n",
    "\n",
    "            # 3前向\n",
    "            logits = model(x)\n",
    "\n",
    "            # 4loss\n",
    "            loss = lm_loss(logits, y)\n",
    "\n",
    "            # 5反向\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            \n",
    "            # （可选）梯度裁剪（强烈推荐）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # 6更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "        # 7日志\n",
    "            if it % 2 == 0:\n",
    "                print(\n",
    "                f\"step {it:6d} | \"\n",
    "                f\"loss {loss.item():.4f} | \"\n",
    "                f\"lr {lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        #  清理显存引用\n",
    "\n",
    "    benchmark(description = 'transformer模型的基准测试', run =  run_model, num_warmups = 1, num_trials = 3)\n",
    "    t = profile(description='transformers的性能分析',run=run_model)\n",
    "    print(t)\n",
    "    try:\n",
    "        del model, x, y, logits, loss, lr, optimizer, model_compile\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "run_benchmack()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAJ/CAIAAAAbKL5IAAAgAElEQVR4Ae29748c13moWX9EPgywQdIEtAsI0AfpmwgEe0ezEBYm4A8eLLEIsQJG4b0LCMQFNuDVhxvaSYN3ri7CjJWEpmOLGk3zV49IJkORkoeSKQ87NrMz/pWhHQcjI7an7dDIOBDgScLEg40+9LL6dJ+prqrzdvXp6q7qt58BQZ4+p07VeZ+3u+fhOfUjaPEDAQhAAAIQgAAEIDB9BILpC5mIIQABCEAAAhCAAARaWCBvAghAAAIQgAAEIDCNBLDAacw6MUMAAhCAAAQgAAEskPcABCAAAQhAAAIQmEYCWOA0Zp2YIQABCEAAAhCAABbIewACEIAABCAAAQhMIwEscBqzTswQgAAEIAABCEAAC+Q9AAEIQAACEIAABKaRABY4jVknZghAAAIQgAAEIIAF8h6AAAQgAAEIQAAC00gAC5zGrBMzBCAAAQhAAAIQwAJ5D0AAAhCAAAQgAIFpJIAFTmPWiRkCEIAABCAAAQhggbwHIAABCEAAAhCAwDQSwAKnMevEDAEIQAACEIAABLBA3gMQgAAEIAABCEBgGglggdOYdWKGAAQgAAEIQAACWCDvAQhAAAIQgAAEIDCNBLDAacw6MUMAAhCAAAQgAAEskPcABCAAAQhAAAIQmEYCWOA0Zp2YIQABCEAAAhCAABbIewACEIAABCAAAQhMIwEscBqzTswQgAAEIAABCEAAC+Q9AAEIQAACEIAABKaRABY4jVknZghAAAIQgAAEIIAF8h6AAAQgAAEIQAAC00gAC5zGrBMzBCAAAQhAAAIQwAJ5D0AAAhCAAAQgAIFpJIAFTmPWiRkCEIAABCAAAQhggbwHIAABCEAAAhCAwDQSwAKnMevEDAEIQAACEIAABLBA3gMQgAAEIAABCEBgGglggdOYdWKGAAQgAAEIQAACWCDvAQhAAAIQgAAEIDCNBLDAacw6MUMAAhCAAAQgAAEskPcABCAAAQhAAAIQmEYCWOA0Zp2YIQABCEAAAhCAABbIewACEIAABCAAAQhMIwEscBqzTswQgAAEIAABCEAAC+Q9AAEIQAACEIAABKaRABY4jVknZghAAAIQgAAEIIAF8h6AAAQgAAEIQAAC00gAC5zGrBMzBCAAAQhAAAIQwAJ5D0AAAhCAAAQgAIFpJIAFTmPWiRkCEIAABCAAAQhggbwHIAABCGghsLUYOH9mnn5hbv6VpfrW3oGWcIkDAhAYkgAWOCRAukMAAhAoDQHJAg/1cOaF0+vN0oyZgUAAAsURwAKLY8+RIQABCORLIJsFhj44c6KOCOYLn71BYAIJYIETmDSGDAEIQCCVQNcCF7cSzZ8c7O/trJ8/OTfTnRR8fnGbteEEJyogMFUEsMCpSjfBQgACqgkIFmjjbtZPdEXw2MquraYAAQhMIQEscAqTTsgQgIBSAlkssNU62Fo8aiYEn13aVkqCsCAAgSwEsMAslNgGAhCAwCQQyGaBrdb+2stGA+eWfzQJcTFGCEBgNASwwNFwZa8QgAAExk8gqwW29m6eMBp44ube+IfJESEAgZIQwAJLkgiGAQEIQGBoApktsPWdRXNy4MxrrAkPjZ0dQGBiCWCBE5s6Bg4BCEAgRiC7BT6qz5vJwLObsX3wEgIQmB4CWOD05JpIIQAB7QSwQO0ZJj4I5EsAC8yXJ3uDAAQgUBwBLLA49hwZApNIAAucxKwxZghAAAJpBLJbYHfL517nvMA0ktRBYDoIYIHTkWeihAAEpoFA1+1Snh3SG/7ulc5pgafe3e9t4RUEIDBFBLDAKUo2oUIAAsoJZLXA3eVPmWtDjnG/QOVvCcKDgEgACxTx0AgBCEBggghks8CDB2eeMxL4qWUeITdB6WWoEMidABaYO1J2CAEIQKAgAlks8PA5wjMsBxeUJw4LgbIQwALLkgnGAQEIQGBYAoIFfnKw39xeO39yztwtOgiC43UmAocFTn8ITDgBLHDCE8jwIQABCFgCXQs0673S38+fWm/abhQgAIEpJYAFTmniCRsCEFBIIJsFHjm+tPmxwugJCQIQGJQAFjgoMbaHAAQgUFYCkgUeOfri/KnPLa9/xK1hypo+xgWBsRPAAseOnANCAAIQgAAEIACBEhDAAkuQBIYAAQhAAAIQgAAExk4ACxw7cg4IAQhAAAIQgAAESkAACyxBEhgCBCAAAQhAAAIQGDsBLHDsyDkgBCAAAQhAAAIQKAEBLLAESWAIEIAABCAAAQhAYOwEsMCxI+eAEIAABCAAAQhAoAQEsMASJIEhQAACEBglgUajMcrds28IQGBSCWCBk5o5xg0BCEAgC4FarfbkUXLVajXLxmwDAQhMFQEscKrSTbAQgMDUEVhYWAiCYHZ2duoiJ2AIQKAfASywHyHaIQABCEwyAWOBlUplkoNg7BCAwEgIYIEjwcpOIQABCJSEQKVSebIiHAQBZweWJCMMAwLlIYAFlicXjAQCEIBAzgSazaZRwCAIarVazntndxCAwIQTwAInPIEMHwIQgICbwOzsbBAEZjqwUqk0m033trRAAAJTRwALnLqUEzAEIDAlBMzVweaMQHN24MLCwpTETpgQgEAWAlhgFkpsAwEIQGDCCNi1YHM6YLPZNDOCrAtPWCIZLgRGSQALHCVd9g0BCECgCAKNRsOcDhi9TaCpfOKC0coiRscxIQCBshDAAsuSCcYBAQhAYHgCzWbTnAuYeo/AarVq7JBzBIdHzR4goIAAFqggiYQAAQhMNYFms/lknq9arZqT/8zlIK77wtil4UqlMjs7u7CwUKvVXBtPNVaCh8AUEMACpyDJhKidwJMzwJ7M8VSrVTsJZG8OQmEKCfR9TIh5w0whGUKOEqi0f/hvgPbfD33iwwL7AKIZAiUnYNf47Pe7+XLn76ki8OQ/ANVqdaApPTODWKvVFhYWZmdnpwoXwdp7iUe/N7iRUMm/7UcxPCxwFFTZJwTGQaDRaNivcjMXOJAEjGOIHAMCECgxgWazaf4bYL5JuHKoxLka1dCwwFGRZb8QGCkBOwU4OzvL/+BHipqdQ0A9gehJAn3PKFBPY6oCxAKnKt0Eq4RA6n1AlMRGGBCAQEEE7JVD3F28oAwUcFgssADoHBICQxIwp/Kw/jskRrpDAAIxArGbjcdaeamPABaoL6dEpJyAuRCYVRvlaSY8CBREIPrgwYKGwGHHRwALHB9rjgSB4QnYteDhd8UeIAABCKQSMP/VZF04FY6ySixQWUIJRzkBc1tgHgWrPM2EB4FCCZh14UqlUugoOPg4CGCB46DMMSCQFwHzf3QuCs6LJ/uBAARSCZgn0KQ2UamJABaoKZvEop+Aua2X/jiJEAIQKJSA+Q8nl6AVmoRxHBwLHAdljgGBvAjwH/S8SLIfCEBAIGDuSMrJJwIiHU1YoI48EsVUEDCP/GI5eCqSTZAQKJQA3zaF4h/fwbHA8bHmSBCAAAQgAAEIQKA8BLDA8uSCkUAAAhCAAAQgAIHxEcACx8eaI0EAAhCAAAQgAIHyEMACy5MLRgIBCEAAAhCAAATGRwALHB9rjgQBCEAAAhCAAATKQwALLE8uGAkEIAABCEAAAhAYHwEscHysORIEIAABCEAAAhAoDwEssDy5YCQQgAAEIAABCEBgfASwwPGx5kgQmCoCm2eD1J8jz8+dfHV5/Uf7XjQ2F1N3mqicX91r799sP19/dHi0vdX5IAi6GxzWx0pms8SOeyuO181hYn15CQEIQGAiCGCBE5EmBgmBySPgssCuRs3MfW5jcBPEAifvncCIIQCB0hLAAkubGgYGgckmYCxwcSsexcH+7ubK6bmZ0AaPnt08iLcP+HqrPTnonJMbdi6w75ThgMNlcwhAAAIlIoAFligZDAUCmgi4LLATY7N+IhTB5xa3hvNALFDTm4ZYIACB8RLAAsfLm6NBYGoI9LHAVmv3Snh+XvDK+uDrwhGIWGAEBkUIQAACAxHAAgfCxcYQgEBWAn0tsLW/firUwNMbw8wGYoFZE8J2EIAABOIEsMA4EV5DAAK5EOhvga3d+mfCReGlh0McEAscAh5dIQCBKSeABU75G4DwITAqAhkscH/9lXAyMHkFyQBjGrEFhuNz/vTcgGaAMbMpBCAAgXIQwALLkQdGAQF1BDJYYCvLNn3AYIF9ANEMAQhAwEkAC3SioQECEBiGQAbD21t7KZxnK/NcIHeKGeY9QF8IQKDkBLDAkieI4UFgUglksMDd5ReDIJhb/tEQMY54LhALHCI3dIUABMpOAAsse4YYHwQmlEB/C/x47WQ4Fcg1whOaYYYNAQhMPAEscOJTSAAQKCeBvha4e/FYEAQz/3WoG8W0mAssZ/oZFQQgMAkEsMBJyBJjhMAEEuhjgZ1nhxwd6jYxrRYWOIFvDYYMAQiUhQAWWJZMMA4IKCPgsMCD/b2dDZ4jrCzZhAMBCEwmASxwMvPGqCFQegLGAp332gtm5j63MdSz4wwB3xVh98A6dwHcW20/4M69XbuFWwaW/o3IACEAATcBLNDNhhYIQGAIAg4LnHn6hflT5+qbj4Z5bFxkWFhgBAZFCEAAAgMRwAIHwsXGEIAABCAAAQhAQAkBLFBJIgkDAhCAAAQgAAEIDEQACxwIFxtDAAIQgAAEIAABJQSwQCWJJAwITDKBzcU+F2HY5sXNSY6TsUMAAhAoFQEssFTpYDAQmE4CWOB05p2oIQCBgglggQUngMNDAAIQgAAEIACBQghggYVg56AQgEDOBBYWFiqVSqPRyHm/7A4CEICAXgJYoN7cEhkEpolApVIJgqBWq01T0MQKAQhAYCgCWOBQ+OgMAQiUhIC5fqRarZZkPAwDAhCAQPkJYIHlzxEjhAAE+hBoNBrGAmdnZ/tsSjMEIAABCHQJYIFdEvwLAQhMLIFarWYssFKpTGwQDBwCEIDAuAlggeMmzvEgAIHcCZiTAlkUzh0sO4QABHQTwAJ155foIKCfwMLCQhAEs7OzzWYzCAKuFNafciKEAARyIoAF5gSS3UAAAkUQsGcENpvNVqtVrVaNCBYxFo4JAQhAYMIIYIETljCGCwEIWAJWAe0NYprN5uzsbBAECwsLxgvtxhQgAAEIQCBGAAuMAeElBCAwGQTMtJ9ZC46OuNlsmtMEn/zNjWOiZChDAAIQiBHAAmNAeAkBCJSXQLPZfDL/V61WZc97coKgdcSFhYVarcYzRcqbVEYGAQgURwALLI49R4ZArgSsIS0sLMxq/DGXANu/+94a0N4+xt5EplKpaAQzi+nm+kliZxCYIgJY4BQlm1C1ErCnx1lD0lowGletVjPO7TWbzVqtZrRYK5NoXCyCa/2MExcERkQACxwRWHYLgTERsEufUUNqaPwZHqiZLtXHJma6uODwbxX2AIEpIYAFTkmiCVMhAXsZxJPZIC6DUJjgwUOKnhDJW2JwfvSAwNQRwAKnLuUErIOAuUOyvVuyjqCIIhcCdnp4YWEhlx2yEwhAQCsBLFBrZolLOQFzV7y+V0gop0B4DgL2PwkZT6B07IZqCEBAOQEsUHmCCU8lAXP1a6VSURkdQeVCgDdJLhjZCQR0E8ACdeeX6HQSMJeFMs2jM7v5RWUfo5LfLtkTBCCgigAWqCqdBDMNBMwcD6d8TUOuh4zRrAszZzwkRrpDQDEBLFBxcglNJ4GFhYUgCOyTc3UGSVQ5ETDTgUwb54ST3UBAGwEsUFtGiUc9AfPwtGazqT5SAhyegPk/AxY4PEn2AAGVBLBAlWklKM0EzEmBmiMktvwImLvGcO/A/IiyJwioIoAFqkonwagnYG4LzHKw+kTnFSBvmLxIsh8IqCSABapMK0FBAAIQgAAEIACBPgSwwD6AaIYABCAAAQhAAAIqCWCBKtNKUBCAAAQgAAEIQKAPASywDyCaIQABCCgg8P+1f/7lX/7FFBRERAgQgMDwBLDA4RmyBwhAAAIQgAAEIDB5BLDAycsZI4YABCAAAQhAAALDE8ACh2fIHiAAgakk8Kg+HwSLW+WJfXMxmK8/OhzP5tlgfnXPvN48a2402f777ObhRq5SO7pIn7Bo9+bq1K7fqx+P9Tt8GeIK92zH2d44dTxpA2jvaLE9+rBjFP7e6nyQuh9xrDRCYMoJYIFT/gYgfAhAwJdAySww6nxhSFuLwfG6ccDQkAIjT0bCMvhcSnSbi0EwoGmFXeLuOIgFRj0vnqdwP92gwra4F8a35zUEIJAggAUmkFABAQhAIAuBFE/K0m0022wtHnpeeIRQv7oKFepRVMUyTZulRpdaKQU0SgtstQTxlQZFGwQg0CWABXZJ8C8EIACBDoH2MqVdxoyuM7Y1yLQsrnZXhEMDs0uc7V1EnSws2x+7mTGzzcPF0+68nRlCdAG3K3Mdt+vsK7l9dJzRAfQYYWJ4nZAT/ziELxxY76ETPaMVo7XAcL6zZzow6r7RYVCGAATSCWCB6VyohQAEppVAWwGtUbVlqDORFi231SrozLfFXSdUJbOHUFOs+YVzV11r6Yhmx/B69txusqZ1aGPhUeyCbGRXsZm/MG+HA2jFzsNrZ7V3VOmJPjxuT3vP4nJPS+qLOJlwo3DPlkkv7eg+HAOIbtI75Rm29ATeuymvIACBJAEsMMmEGghAYIoJ9DhK52wz416hYVg5MyfedVdde5oi+hKXkkP9itvP4R7iA+jkIqFfEcE63K3ZONIUt672BvHt09IdiaKnOewbPRuvpzHxonckprknwDiHwz20B2AnUQ8LVtDDTcPu8cXuaI4Od0cJAhBIIYAFpkChCgIQgEB7yqrrHqF5xIXDbNCZzIt4VULXQpah5HV+zDRYfG/WAlO7m6PbiUCTHauY8S4xgeuxrnbXyGidiY7txG43XguMrIbbEfQULIRO7WDD69kVLyAwhQSwwClMOiFDAAICgfbslDG2rvy19Ss5rRXWdDXFtra72/mqUErMT1v+DvVLtMCU2azIqLp7DP9tH2icFhg/lgAybLJYItv1WGkvrshWPZIdre8tW3vuVB8S7t2OVxCAQBoBLDCNCnUQgMDUEohrhNWUuLfFNKWjIz1TaLZvl+bhzuN7szbj0KzErrq7DJdFozeC6SwBWz3tqFjXVtvdwmH0W9XtCeTwYPG5t8OW1NJYLNA6d2eZvl9oqSOlEgJTSQALnMq0EzQEIOAgEBpVdCquLUN9zwsMd9ae4lo8G7kzX+Li3FChOhdGOC2wfd2rvXjicJRWE7tVkT0cyqVpjLlXZMt2ezzG7h57/k21wNTKnm6xF7GRtFvboLp3t3bbbaZjeYUWGyMvITDFBLDAKU4+oUMAAkkCoVHZibS2owTdi0LaXtK9FiH0m8iW3etIuqu07R23u1unbO+52yWuLxHJ6+11uKjaPmJ33qt3/i9sis72xWbs2ht3zbInimT83ZoUCesZQHc7+d+wS5dYd8twz93BtM+2jJ3v2NkuZQDdPRz+2yfwww0pQQACaQSwwDQq1EEAAlNFoMdLzAJr5+S7+dW99gRed5GxrSam7fB+gV1WbdnqsTFzYlxnX8HiZrt724oECwx31z7o4Ri6R2h7WKe6O6R2W0z7EjfSa5tldxxR64p37B7JrHfbHqYQ9zm7sbOQzQJjhzFGG0Edaz/03fjSdlwKneOiAQIQaBPAAnkjQAACEJh8Av4+tLnYnV+cOAo8O2TiUsaAy0YACyxbRhgPBCAAAR8Cnkq0tTj4DJ/P8PLvE04WRidEw/nOw2nC/I/HHiGgkAAWqDCphAQBCEwlgc3Fw/PtQgBxL0yDsnnWnqKX1uyq657jGFurbb+Mmpmr//D1cecLl+MndlJzeBzsAQJ+BLBAP270ggAEIAABCEAAApNNAAuc7PwxeghAoEACP//5z7/whS/8L92fL3zhCz//+c8LHM9EH9rAfOGFFwzOF1544Vvf+tZER8TgIVB+Alhg+XPECCEAgdIRaDab1WrVrodWq9Vms1m6UU7ggAzYSqVi2FYqlUajMYFxMGQITAYBLHAy8qRulPs79zc27u/sqwuMgNQTwP/GkGJccAyQOQQEWq0WFsjbIC8C5k5msRPDXbaXunFeI2E/EBgJgZj/zc7OMv83EtDdncaALywsMC/YZcO/EMiHABaYD0f20r07bswCXbbnqgckBMpIIKYj+N84kxSDv7CwgHyPkz/H0k0AC9Sd33FGlyp2qZWdx9v3f579OIfPsSDgIBA9/w//c0AaeXXUBZ+cNciJmCMnzgGmgwAWOB15HkeUqcKXWokFjiMfHGN4AlH/q1QqTEENj3TIPSRdcMgd0h0CU04AC5zyN0CO4acKX2olFpgjdnY1EgKNRoPLVEdCNo+d4oJ5UGQfEAgJYIG8D/IikCp8qZVYYF7M2U/+BJrN5uzsrL1NSa1Wy/8Y7DEPAs1mc2FhwWaqWq3msVf2AYHpIoAFTle+RxltqvClVmKBo8wD+/YlEPM/rMIX5Fj7kbWx4uZg6ghggepSWlhAqcKXWokFFpYkDpxKgFmlVCwTVFmr1aIzuNxQZoJyx1CLJYAFFstf09FThS+1EgvUlPfJjoUzzCY7f72jr9VqnM3Zi4RXEOhDAAvsA4jmzARShS+1EgvMDJUNR0Yg6n9Pzi1j/XdkpMe6Y5NW64LJmwtyi5mx5oODlZ4AFlj6FE3MAI3wHTn64tzc4Z+jR8KTt2OVc3MvmvrYLaYnJlQGOukEoreAQQsmPZvJ8UcVP3ZzQXP1Nzf9SUKjZjoJYIHTmfdRRG0s0Fyxl/FvLHAUiWCfEoGo/3ELaInU5LclXdDEVGn/IIKTn2EiyIEAFpgDRHZhCBzsD/pzADoIjI1AtVq1C4WVSoULCMZGvtgDJV3Q/E/g13/91xHBYlPD0ctAAAssQxYYAwQgMEICsVtAcwvAEbIu666jl4HPzMyY1QpEsKzpYlzjI4AFjo81R4IABMZMAP8bM/CSHy56c0FEsOTJYnjjIYAFjoczR4kRcF07HNuMlxDwJBBb/+USYE+OurpFbytoT15mRlBXkolmMAJY4GC82DonAlhgTiDZTS+B2I1CzPWhvZvwauoINBoN+6w5K3/RQqVS4RzBqXtbEHCbABbIG6EQAligJ/Z/ivwclODHDKcEA+kM4cSJE+a3+2/+5m9+9rOfLXZg5YHzr//6r/aNUywTe/Txw/nhD3/41fbPxYsXP9v+eemll/7X9s9v/MZv/NZv/dYPf/hDO7xiC+OH44rXvm3+6Z/+qcWPRgJYoMas5hFTo9Go1Woju45yHBZYa/8o+y/+jRs3VvhxEzh37tyv/dqvnTx50r0JLRCAwMAE7ty5k8cvFvZROgJYYOlSUuyAomdTPZlTGdnZVKOyQHMloL0hSBAEIxPZYhKFBQ7864sOEIDA0ASwwGK+8Ud/VCxw9Iwn5Aixq+cW2j8jU6iRWGCj0Yie6/PkTCB9j4WwFvil5fqFN98u/M+XluvLK5cLH4YdwMpK7YvLq/ZlsYU33rp68a2rxY7BHt1oQG3lrZsrf1SGP1dqb6yunC/DSG6u/NHbK398ufZmSQZzc+WPLtWWr6+8Xobx1Fe+YN45WOCE/CYfeJhY4MDIVHao1WrGn8Z1N938LXB2dtaEoPuBEPfv3zdfyr/z+R8ce+0Xhf/5nc//4I/fWi98GHYAb6xcP/6HTfuy2ML/8yf/7+998RvFjsEc/dP/4x/M26b+1vnWG0EZ/nz98u/++OLzZRhJ643gFxf/5/Ur/70kg2m9Ebxz+fO/vPg/lWE8P3zzP5h3zre+9S2Vv/sICgvkPdCqVqvGn0a2/puEnLMFWgUc2eRlMoRiarBAWaqwwFQ+WKBsVFigiw8WWMwX/RiPigWOEXYpD/XkzhpGAYf2p/2d+xsbWf9cOBkeNZ/nCBuLrVQq+QPeWjRwYn8vbuV/qIx7xAJTLcdWYoEWRbSABbosx9RjgS4+WGDGb+bJ3QwLnNzc5TNyM4uWxyygmd6L+ZL8MgcLzM9iozy7sZzdjNZ2y3v14yauHMbf3WfWf7HAqNwky1hgksmx136BBbosBwuUyWCBWb+aJ3Y7LHBiU5fHwHOdRdtdP7e0NNif9d2hozCXA+dhsXYom4tZJymzb2l3PmwBC0y1HFuJBVoU0QIWKLsOc4EuPljgsF/Zpe+PBZY+RaMcoJnRGnoteJRDFPdtLgoeyVqweNwCG7HAqNwky1hgkglzgS7FsfVYoEURK2CBBX7bj+fQWOB4OJfxKGYtdaIVysxl5joRWMZMRceEBaZajq3EAi2KaIG5wJjcxF5igTEg9iUWGP36VVnGAlWmNVNQ5u4wCwsLmbYu5UbmpMYRzmVydUi/m9Fwp5iobMXK3CnGykSywJ1ikkxsDXeKKeUvHJ2DwgJ15jVLVHlPpA10jbC5mnhnP8tA3duYFW13u3cLV4dkvRMhFhgzv+hLLNBqTbKABSaZ2Bos0Pu7m46DEsACByWmZ/u8J9K65iRfFtzTOtQ1tiNb0c5+zUf2LXN727AiHNWsZJkV4SQTzgu0duUqsCLsIsOKcG7f3WXdERZY1syMflx5W+D2hRfn5gb7c2F7iDBHZoFDjGn0XbHAVMuxlVigRREtcF6gy3JMPRbo4oMFjv5LveAjYIEFJ6DAw+dtgeMOBQuM/povqsyKsECeFWGXW7TeCFgRFuCwIjzuXydTfDwscHqTjwWKube3hu5Zw87reSfioaVG5gIF6zr22i+YC0zlw1ygoFw8R1iAw1yg9HWsog0LVJFGryDytkBzdciwF3xkD2V0c4F7q/NBEMyv7qUMpn3VcHpTytb5V2GBqZZjK7FAiyJawAIF0cECBThYYP5f4iXbIxZYsoSMcTh5W6C5OmSoCz4Gin5kFhjOAgqeFzri8XqaIQ40fM+NscCo3CTLWGCSCVeHCJZjmjgv0IUIC/T8pp6cbljg5OQq75FigQ6ioc4ubjkaW61WOB04PtmNjQMLTLUcW4kFWhTRAnOBLsvBAmUyWGDsG1jfS9RwCggAACAASURBVCxQX06zRoQFOkgxF5j1ZoHHXvsFV4dEZStW5uoQwTC4OkSAw9Uhji9nqvMngAXmz3RS9ogFujJlzgtMnw7kvMDep4lggTHzi77EAgXRwQIFOFig68uZ+twJYIG5I52YHWKBYqocN8Eu7oxAM1pWhKOalSyzIpxkwnmBgm+ZJs4LdCFiRVj8NaGhEQvUkEW/GEZjgUeODnDjaO4aPXDqsMBUy7GVWKBFES1wXqDLcrBAmQwWOPB39KR1wAInLWP5jXc0Fhi7u578cqhrLEZ2jXB+iEewJywwKjfJMhaYZMJcoCw63ClG4IMFjuBbvFy7xALLlY9xjmY0Fnjywv2Njax/hrq5IBaY+vt+zJWcFygA57xAQS84L1CAw3mB4/xVOOXHwgKn9w0wGgscanpvoGRggYJ/jK0JCxRQY4GC6GCBAhwscKDfBWw8DAEscBh6k90XC5zE/LEiLFgXT5BzweG8QEG5WBEW4LAiPIm/JgYaMxY4EC5VG2OBk5hOLNAlOqae8wJT+WCBguhggQIcLHASf00MNGYscCBcqjbGAicxnVhgquXYSizQoogWsEBBdLBAAQ4WOIm/JgYaMxY4EC5VG2OBfuk095RuX/w8X3/ktw//XlhgVG6SZSwwyYRrhAXLMU3cL9CFCAv0/7KekJ5Y4IQkagTDzNsChxni9oXwLoOD3T6Qq0NSf9+PuZKrQwTgXB3icovWGwFXhwhwuDpkmF8n9B2IABY4EC5VG5fJAs2DOga7vhgLFPxjbE1YoIAaCxREBwsU4GCBqn7XljsYLLDc+Rnl6LBAB91QSdMfImw6hI8SHkxYHQfyqWZFWLAurhF2weG8QEG5OC9QgMOKsM/X9ET1wQInKl25DhYLdOCMWeBe/Xgwv7p3uDEW+NovrG0wF2hRJAvMBQp6wVygAIe5wMPvW0ojJoAFjhhwiXePBTqSgwUeSl7SbGI1WGAMSPQlFiiIDhYowMECHV/OVOdPAAvMn+mk7BELdGQKC8QCByAQ1b5YGQsURAcLFOBggY4vZ6rzJ4AF5s90UvaIBToyhQUO4EDMBcbML/oSCxREBwsU4GCBji9nqvMngAXmz3RS9ogFOjKFBWKBAxCIal+sjAUKooMFCnCwQMeXM9X5E8AC82c6KXucdAtsNBpBECwsLOQNHAscwIGYC4yZX/QlFiiIDhYowMEC8/5WZ39OAligE436hkm3wGq1+uQBHtVqNe9MYYFY4AAEotoXK2OBguhggQIcLDDvb3X25ySABTrRqG+YdAsc2fhDC+z3w/0CO57EXGDM/KIvsUBBdLBAAQ4WqP73b3kCxALLk4txj2RkFuURiBGvwdTKiJrHwSa6C3eNjmpWssxzhJNMeI6w4FumiecIuxBx1+iJ/n2RZfBYYBZKOreZaAs0y8Gzs7M6c+OOCgtMtRxbiQVaFNECzw5xWQ4WKJPBAt1fxkpasEAlifQIY3It0FwXEgRBs9n0CHzYLjw7hGeHRAhEZStWZkVYMAxWhAU4rAgP+y1N/8wEsMDMqNRtWCYLbB3s7+/vH2Rh3Gw2zVrwCK4LyXL8VgsLjDgQ5wXGzC/6EgsURAcLFOBggdm+i9kqBwJYYA4QJ3QXI7PAg72Ha8vn1nejXPY2ln776Zm2vh351On6D/ajjdnLjUajUqkEQVDkWjAWiAVGCES1L1bGAgXRwQIFOFhg9l8KbDkkASxwSIAT3H0kFri/ufiCkb3IpR7N+glTd3jl7ZGTN3ssMQtHM+CCFbDFXGDPXVSYC4yZX/QlFiiIDhYowMECs/xGYJtcCGCBuWCcyJ2MwAJ3lz/VFr2n5k+f39jrUNlffyV0wJmXlrc/brU+2d9eOXkkfH1q/eP+3Gq12kL7x0wBBkHQaDT6dxvpFswFRmbCsMCo9sXKWKAgOligAAcLHOlXODuPEsACozSmq5y7BR7cPxPq3vOLm48jJB/V50MznK8/spUHm2efC4Jg7qI0HWgvAbETiLOzs8VcDmIHbgrFWeATA/6DP/iDlfbP+bdufX55vfA/59+69eW3rhU+DDuA5bcu/fFb79qXxRa++NbNP3vrRrFjMEd//a1187a5tPLmh5d/rwx/rtf+5Pal/1GGkXx4+fe+cum/1Ve+WJLBfHj5966ufOnupWoZxvPOpT8075w7d+7Evgh5qYMAFqgjjz5R5G2BBxuvhlN8Z+73XOSx/+6pUONeWe85E/BHy3NBELy01p0vTBl/s9mcnZ21U4DGBWu1WsqmOVeV967RhsbJkydXVlb+y/kH//H1vyn8z385/+DCyq3Ch2EH8OZK/dSffNe+LLbw+1+899+//H6xYzBH/79f/775XX515c/+/s1ny/Dn/Ut/8O03/88yjOTv33z2b948duvy50symL9/89mbl87vvPm/lWE8W8v/l3nnfO1rX8v5i5bdlYMAFliOPBQxirwtcHf5xSAITvXqnlHD5LTf5plQ6yLnDroJNNo/CwsL9rqQUswIugc8opZq+4l5MzMz5kv5dz7/g9j6YyEvWREWsLMiLCx6siIswGFFeETfouw2SQALTDKZlpq8LTD1+R/bS8+GE4SL34lRTd04tk38pb1AuBRnB8ZHN9rXdn18dXUVCxTEi7tGp8LhrtGCcrXeCHh2iIsPd40e7Td7CfaOBZYgCQUNYRwWuLd2IpzzO73Rs0rcau2vh+vEM4vbg8e+sLAQBEGlUpmeGcHoLRJ5dkiq5dhKLNCiiBawQJflmHos0MUHCxz8d9SE9cACJyxhOQ43bwvcrX8mCIIT0XP9Du6dDiXweD1+/t/WYngdiXheoBCpEcGR3TW6dOcFmkyZWyRigVG5SZaxwCQTniPsUhxbjwVaFLECFij8JtLRhAXqyKNPFHlbYGv79fDK3/lVq3z7ay+HEnhsJXYtcOfeMYn6rFE0m01zjuBYLhbJOqoRbWdOB6xUKmb/WGCq5dhKLNCiiBaYC4zJTewlFhgDYl9igSP6Yi/PbrHA8uRi3CPJ3QJbP1o+Fq7zHlu8v7u/v7e9eupoKIEn13ruC7i/c7F9D+ls9wt0QYm5kWuzSa+3pwPauyRigVG5SZaxwCQT5gKt07gKWKCLDBY46b9E+o4fC+yLSO0G+Vtg62D73FzvU0JmTqzaicDd9c+dOvaMaY/WexI204EjODswXBFe3PIcVY7doqcD2t1igamWYyuxQIsiWmAu0GU5ph4LdPHBAu13r9YCFqg1s/3jGoEFhgfdu3/h1KfDRwYfef7k0n27Otxqtbrn283Mnbkbre8/1NQtzNmBI1gULosFmgTFnpiMBUblJlnGApNMmAt0KY6txwItilgBC0z97aOpEgvUlM3BYhmRBboHsbt+bnlta2f/E/cmg7TUarUgCBYWFgbplGXbUliga8kbC0y1HFuJBVoU0QJzgTG5ib3EAmNA7EssMMvvjIneBguc6PQNNfi8LXB7+ZWl+v2d/dhNYYYao9TZnDMXmyqTOmRtK94Ck6cD2rFjgVG5SZaxwCQT5gKt07gKWKCLDBZov3u1FrBArZntH1feFthd8A1mnv70qaXVjZ2PR+uD5rQ5e/Fs/4CzbmEDCa9tcfxkeupJ1gP2bpd6OqDdBAtMtRxbiQVaFNECc4EuyzH1WKCLDxZov3u1FrBArZntH1feFniwu7W2fHj9R1ufnjp68tXltYe7BzmtAkejGqkFFnh1iMmLa44TC4zKTbKMBSaZMBfoUhxbjwVaFLECFhj9paOyjAWqTGumoPK2wMhBH+/t3K9fePXkXOeK4CAIjhx9+fTyre3dx5HNhiuqtEDX6YAWFRaYajm2Egu0KKIF5gJjchN7iQXGgNiXWKD97tVawAK1ZrZ/XCO0wMjBD/Z3t+/Wl16ZP/pUZ3F15pljp87VNz4a9gRCfRZo14Lt3QEjIDtFLDAqN8kyFphkwlygdRpXAQt0kcECk1/CymqwQGUJHSCc8VhgdEAHezsbq0unjh89YoTQ6znCdof6LNDcAVF+Mh4WmGo5thILtCiiBeYCXZZj6rFAFx8s0P7G0VrAArVmtn9c47bAx3s7W2vL506ffHEuvJ1g+DPUNRYjs8D+6Lpb7NWPz9cfdV8N92/ftWCzeywwKjfJMhaYZMJcoEtxbD0WaFHECljgcN/rE9AbC5yAJI1oiOOwwE8O9j7aqJ+zjwzpzAE+/elTZ1bWtptDXUSsyQLtrWH6PgoFC0y1HFuJBVoU0QJzgTG5ib3EAmNA7EsscES/f8uzWyywPLkY90hGZ4EHH+9s3lo+/XJ35bftfjPPzJnrhYc9H7DLSY0FmkCCIMjyHBQsMCo3yTIWmGTCXKB1GlcBC3SRwQK7v3DU/osFqk1t38Byt8C9h4k7xXQuDd4cxb0D1VigSYTr1jCxPGKBqZZjK7FAiyJaYC7QZTmmHgt08cECY9/A+l5igfpymjWivC3w8GbLR54/efp8DlcBy5HosMCMpwNaFFhgVG6SZSwwyYS5QJfi2Hos0KKIFbBA+92rtYAFas1s/7hGY4FPH/vPF9Ye7ua17CuEocAC7VqwcGuYGAEsMNVybCUWaFFEC8wFxuQm9hILjAGxL7HA2DewvpdYoL6cZo0obwvcXjq8R3QQBDNPv3Dy9Mra5tD3BXTFo8ACTQrkW8PEwscCo3KTLGOBSSbMBVqncRWwQBcZLDD2DazvJRaoL6dZI8rbAtvHfWzuEX2sey8Yc1Fw+OCQC6sbO3tDXRQcC2zSLXDQtWATPhaYajm2Egu0KKIF5gJdlmPqsUAXHyww9ntH30ssUF9Os0Y0EguMHPygub0evUe0EcKnjs6/slS/m8Oj5CbaAj3Wgg1aLDAqN8kyFphkwlygS3FsPRZoUcQKWGDkd5rOIhaoM69Zohq1BR6O4ZOD3Yfr9XOn5p/vPDSkc9vAZ5a2DzcauDRaC3xUnw+C+dW9zrC2Fs2YgyCf20R7rAWbkWCBqZZjK7FAiyJaYC4wJjexl1hgDIh9iQUO/Jtp0jpggZOWsfzGOz4LjI75YH/n/vLp541TlfXZIW0FXNzqjrutgJ2XsabuJgP9W6vVgiCoVCoD9TIbY4FRuUmWscAkE+YCrdO4CligiwwW6PEtPVldsMDJyleeox2nBR7sh+cLXnj15FzkCpKZss4Fbp4NgrOblnX4MvKwu1ir3SxjwXst2OwfC0y1HFuJBVoU0QJzgS7LMfVYoIsPFpjxi31yN8MCJzd3w4581BZ4sBc+QeTMK/NHn+qupob/hleKLN/K4T7SI1sRDm98eLgW3NqrH++RwlY4Neg/i2kuCsl4j+hkjrHAqNwky1hgkglzgS7FsfVYoEURK2CByS9hZTVYoLKEDhDOKCzQYX7BKO4jPVILPFwObsWksDWMBdqJwL7PC3YlEgtMtRxbiQVaFNECc4ExuYm9xAJjQOxLLND1VaymHgtUk8qBA8nbArcXZ6JzfkFgLge+v7P3eOCxZekwJgtMnAi4tzofHK93LxvJMtLDbcxE4EA3CDzs3C5hgVG5SZaxwCQT5gKt07gKWKCLDBYY+wbW9xIL1JfTrBHlbYHmCXIzT3/61NLq+nYzz1sDpoY0MgtsRc/8C52v57rgxAJx6uDSKu1EYFpj1josMNVybCUWaFFEC8wFuizH1GOBLj5YYNav5ondDguc2NQNPfC8LXB/5+HuwSdDDyvzDkZnge01X3NHmLbaRmb+YleKZB5suKEBPsxEYKvVwgKjcpMsY4FJJswFuhTH1mOBFkWsgAUO9CU/iRtjgZOYtXzGnLcFJkb1ycHeR5sb9zfMn82P9vJ1xBFaYMuc/Nde4LYK2F4a9l4Lzmu0WGCq5dhKLNCiiBaYC4zJTewlFhgDYl9igYlfbNoqsEBtGc0ezwgtcH9n7ex8zx2iO2cMHjn2an1nP/sYpS3z8irpGPm1LSwsPGEw5EQgc4FRs0ktY4GpWLBAqzWpBSwwFUvrjQALzO+XQEn3hAWWNDFjGNaILPDg4dKx7mUiR56fm3/lzNK5paVXT87ZB4fMzC0+yMEER2aB5gTHIHKZ8LDZMEMNghw+bswFplqOrcQCLYpoAQt0WY6pxwJdfLDAYb/9S98/h19LpY+RAaYTGIkFPlw62p72O/qflzeTl9F+vF3/3bm2Ih5d/E4Ol49UKpUgCLzvupLOpX1rmM7cpf3Hrgs7+sjVw18abPePBUblJlnGApNMOC/QpTi2Hgu0KGIFLNB+92otYIFaM9s/rhFY4PZS+9Fwx85tuxXvYPvcsVCunh/qIcImvNFYYAxd+6Jgq4PtwqDThKZ3LraKBaZajq3EAi2KaIG5wJjcxF5igTEg9iUWGPt9oO8lFqgvp1kjyt0CD+6fCef5jtd3+wxhN3waRzBz5r7bFfvsodNsTrar1WrZNs9nq0EvEzYTgX5PDU6OGAuMyk2yjAUmmTAXaJ3GVcACXWSwwOSXsLIaLFBZQgcIJ28LPNh4NZzzOn2vv9sd3DsdbvrqRv9NxYBqtVoQBAsLC+JWeTSGT43r/Aw6EZjjcjBXh6QqTrQSC4zSsGXmAl2WY+qxQBcfLDCP3x+l3gcWWOr0jHRweVvg7vKLQRCcWEueDpgMY2/tRBAELy73mzVM9uypaTQaRs1yWWzt2XX7RfuW0R35izxZOLmhVGP6S1sM0sZcoDWb1AIWmIoFC3RZDhYok8ECB/l6nshtscCJTFsug87bAs2ltYubmQY30MbSHs1M2+zsrLTRgG3tNd9h5c8cM/cLmbHAVMuxlVigRREtYIGy6zAX6OKDBQ7422PyNscCJy9neY1YhwW2Wi1zjcjwt+Lrgg0N1Xvmr7uTzr/5LgezIhw1m9QyFpiKBQt0WY6pxwJdfLDA2Fe6vpdYoL6cZo0obwvcrX8mCB+528wwgGZ9PgiCz/S9jiTDrlotuy6c12Ui0YXg4Gy2yU3HSPOGzBPkfpFqObYSC7QoogUs0GU5WKBMBgt0fLXrqcYC9eRy0EhyF5TNs+ElwsdW+p/st3/rZBAEz72+PeiYXdubKbcgCGZnZ/M8RzByUYjfs+PMPKVr2B71rAhH5SZZxgKTTLhGWBad1hsBc4EuRFigx7f0ZHXBAicrX3mONncLbD1cei4IgmfPbD4Wx/l488yzQRAcXXoobjZgY6PRMMr15IS+arXaaDTy1EHzEOHOuYIZz30MA8j30pBWq7W+vr7S/jl9/q9e/vwPCv9z+vxffeGtdwofhh3AxZX6K3/81/ZlsYXPXbi3+OUPih2DOfp//PzfmLfN1be++POLz5Thz/uXfv87b/4fZRjJzy8+84M3//d3Li+VZDA/v/jMzUvnf/jmfyjDeL65/NvmnbOxsTHgVzKbTwYBLHAy8jSKUeZvga399VfC6cCZl+q7nziG/Hhn+aX2NkPfJib1AOYOgh1bC4JKpdJoNFK39K7Mfr/A3C8NabVaN27cMF/K/A0BCEBgbATu3Lnj/Z1JxzITwALLnJ3Rjm0EFthqPd5cbD8+ZOaFkxfu7uxFJwUP9nfuLs0/1Ta05xf7zBcOEfqT0wSr1ers7KyZGhzaApPPDpmvP8o0PmOB+V6/jAWO7dceB4IABCwBLDDTl/4EboQFTmDSchrySCyw1Wrtb184fsTOxiULR45f2I7aYU7h5Lcbcxeb3oF7XSMy0rnAi19afeMLNwv/c/FLq7W3rhY+DDuA2sqli1+8YV8WW3jzy9feunit2DF0jn7hpvl1funSpY1y/Ny4cePdd98tx1g27t69W6/XSzKYjY2Na9euffDBB2UYz507d+rtHywwv18x5doTFliufIxzNKOywHYMe1v1xZeP9srgkaMvL9a3stxUepwYkscyFph1wi/ZP1pj5iNzPEPRXh1y/rUPXv/sg8L/nH/tg9qbf1H4MOwAam9d+5Nqw74stvDFP7rzxp/eKXYM5uh//PvfMBZYr9ej788Cy1//+td//OMfFziA6KF/8YtfrK+vR2uKLb/zzju//OUvix2DPfrOzs7W1pZ9SUEZASxQWUIHCGekFmjHcbBvfoZ8Vpzd34QVsMAxOxAWmAocC5S/OLBAgQ8WKMBR0IQFKkiiZwjjsUBpcHt7+1KzhjZztUpeNzKM3jWaucBU3cECU7FggfK3CRYo8MECBTgKmrBABUn0DGGEFvhJOAN44LpMuNVqfbK3cW7+SDDALVc8gxyqW+K6kOP1QdezscBUKRldJRaYyhYLlL8JsECBDxYowFHQhAUqSKJnCCOxwP3t5f80F94JJvw5cuzV+k7iQpC9+90rhUtsgebxIbHnyKVWyvRrtZq5l7W8WfZWzgtMtRxbiQVaFNECFih/xLBAgQ8WKMBR0IQFKkiiZwj5W2D3NjEdCWz/E9470A7w482lw8uHj8yf2yzpinD4yBDH1SHt20cvDnKqdL6nBmKBUblJlrHAJJPXP/sAC7RfQqkFLDAVi6nEAgU4CpqwQAVJ9Awhdwvcfj18dEjw/Jn19rrpwY/WTrXvHXjyVih7+99ZOmYnCY8vbQy6tuoZpUe39kKw+9Yw4V2jB1kaNovC1WrVYyjJLlhgquXYSizQoogWsMDkRylagwVGacTKWGAMiLKXWKCyhA4QTt4WaM6imzlz//By4IO7p0IvfGV9d2vxqJkhfGp+6X55BbCNL7xTjDTbF84UDnBG45O7WOe4KIwFRuUmWcYCk0yYC+z7tYgFCoiwQAGOgiYsUEESPUPI2wLNbfZOrUdXeffWToTyd/RoOCk4M/e7KacJeo5+hN1ytsBWq5XjojAWmGo5thILtCiiBeYC5S8MLFDggwUKcBQ0YYEKkugZwmgsMDZJZtQwFMFTtw7PD/Qc8Zi65bwi3Gq1clwUxgKjcpMsY4FJJswF9v3mwAIFRFigAEdBExaoIImeIYzTAuevTIoCtmHmenVIq9Uyi8JBEAz/EBEsMNVybCUWaFFEC8wFyt+SWKDABwsU4ChowgIVJNEzhDFa4OmNw3MFPUc75m6pN4VJrcw4sLymA7HAqNwky1hgkglzgX0/pFiggAgLFOAoaMICFSTRM4QxWmBsmdhzwGPvlsNdo+2Y85oOxAJTLcdWYoEWRbTAXKD9JKYWsMBULKYSCxTgKGjCAhUk0TMELNATXKu1t1rfHLxzLtOBWGBUbpJlLDDJhLnAvh9WLFBAhAUKcBQ0YYEKkugZAhYogDOLv+2b28QmMs0EYaxS2NNhUy7TgVhgquXYSizQoogWmAs8/BymlbDANCqdOixQgKOgCQtUkETPEEZjgUeOvjg3d/jn6JHQpGKVdoML255jH3G3yNUhPfeIDuvDn9iT5bKPZvjpQCwwKjfJMhaYZMJcYN9PKBYoIMICBTgKmrBABUn0DGE0Fmg0KePfPjNqntEO0C12p5jO7QO7s4OOJ8tl23+z2Rzy3oFYYKrl2Eos0KKIFpgLlD+gWKDABwsU4ChowgIVJNEzhLwtsHWwP+hPOa8cDrUvMtsXSuH88flQbN2PlcuegyGnA7HAqNwky1hgkglzgX0/nliggAgLFOAoaMICFSTRM4TcLdBzHKXrFn92SLgo3OOFQ43YTgc2Gg2PHWGBqZZjK7FAiyJaYC5Q/qxhgQIfLFCAo6AJC1SQRM8QsEAHuDQLPF7P8eHH1Wo1CIJKpeJxE2ksMCo3yTIWmGTCXKDjk35YjQUeskiUsMAEElUVWKCqdA4UDBbowJVmgXmsBUcPZ84OrFar0cosZSww1XJsJRZoUUQLzAXKHy4sUOCDBQpwFDRhgQqS6BkCFugANw4LtOvCg04HYoFRuUmWscAkE+YCHZ/0w2os8JBFooQFJpCoqsACVaVzoGCwQAeucVhgq9XyWxfGAlMtx1ZigRZFtMBcoOPD3qnGAgU+WKAAR0ETFqggiZ4hYIEOcKEF9vvJ5x43HuvCWGBUbpJlLDDJhLlAxyf9sBoLPGSRKGGBCSSqKrBAVekcKBgscCBco9jY42kiWGCq5dhKLNCiiBaYC5Q/v1igwAcLFOAoaMICFSTRMwQs0BNcrt3MuvDs7GzGEwSxwKjcJMtYYJIJc4F9P7JYoIAICxTgKGjCAhUk0TMELNABbnMxyLjgm31Lx6Ha1QOtC2OBqZZjK7FAiyJaYC5Q+gS2WligwAcLFOAoaMICFSTRMwQs0A2ue2pg+g1i2o+YC88czCiL7uO0WxqNRvbHymGBUblJlrHAJBPmAvt8ArFAERAWKOKZ+EYscOJT6B0AFtgf3Vb6hSKLW/27DrRF9uuFscBUy7GVWKBFES0wFyh/HpkLFPhggQIcBU1YoIIkeoaABXqCG023jOvCWGBUbpJlLDDJhLnAvh9ZLFBAhAUKcBQ0YYEKkugZAhboCW403ex9pOUHimCBqZZjK7FAiyJaYC5Q/tRigQIfLFCAo6AJC1SQRM8QsEAHuOzXfGTf0nGo3mqzLhwEgXC9MBYYlZtkGQtMMmEusPdzlvIKC0yB0q3CArskdP6LBerMa5aosEA3pbFeHRIdRt8TBLHAVMuxlVigRREtMBcY/ZQly1hgkomtwQItCpUFLFBlWjMFhQX2xzSuq0OiI0k9QdDODmKBUblJlrHAJBPmAqOfr9QyFpiKxVRigQIcBU1YoIIkeoaABXqCG3G31BMEZ2dnzWGxwFTLsZVYoEURLTAXKH9qsUCBDxYowFHQhAUqSKJnCFigJ7jRd4udIGi80BwWC4zKTbKMBSaZMBfY9yOLBQqIsEABjoImLFBBEj1DwAKzgeueI3i8vtdqbZ4N5lf3snUcaqvoCYK1Wi0Igkaj0Wq1sMBUy7GVWKBFES0wFyh/GrFAgQ8WKMBR0IQFKkiiZwhYYH9w4XmB8/VHrb3V+aBtga1WKIXjEUF7gqAxQnMHGSwwKjfJMhaYZMJcYN9POhYoIMICBTgKmrBABUn0DAEL7AcufFKcEb6IBbaNMKdnxyUH0Gw2a7WamfazJwg+++yz7e4YgwAAIABJREFUQRCYUwOxwFTLsZVYoEURLTAXmPysRWuwwCiNWBkLjAFR9hILVJbQAcLBAvvBCqf9zMPiohbYCicI83mCcOoAms2mSU2lUjH+Fz6yOAgqlQorwlGzSS1jgalYsMDUz5qtxAItimQBC0wy0VSDBWrK5mCxYIH9eKXPBW6eDbqrw/124NvebDbtBSJGAc3fjUaDucBUy7GVWKBFES1ggfJnEQsU+GCBAhwFTViggiR6hoAF9geXOC8wnBTsThD27z7cFnZF2IpgtVrFAqNykyxjgUkmnBfY94OIBQqIsEABjoImLFBBEj1DwAKzgeteI9xxsfBikbH9xCYFZ2dnscBUy7GVWKBFES0wFyh/ZrFAgQ8WKMBR0IQFKkiiZwhYoCe4sXezq8OVSgULjMpNsowFJpkwF9j3I4sFCoiwQAGOgiYsUEESPUPAAj3BFdHNXjLy+uuvr7R/zr/2Qerv+zFXnn/tg9qbfzHmgwqHwwJT4TAXKH9qsUCBDxYowFHQhAUqSKJnCFhgFnA914K0Hys8npsFJsdmVoePHz9uLLC2cqX21tXi/6xcWXmrVvwwuihW3lopC5m3rq6sXFpZuVQKOCtXzdtmZWXlejl+Ll++fO3atXKM5frq6uqlS5dKMpjr169funRpdXW1JOO5du3a7du3k99I1OgggAXqyKNPFFhgX2o9CtjZun2a4NnNvn1HtMGHH35ofp1f+MO7f1JtFP7nwh/erS3/eeHDsAOovXXt/H/7mn1ZbOHPPv/OxS+8U+wYzNH/9Ox987ap1+v/Vo6f+/fv7+zslGMs//bTn/70vffeK8lg/u3f/m1tbe0f/uEfSjKe733vew8ePBjRFxq7LZwAFlh4CgobABbYD/3h/QJ7thzx/QJ7jpV4wXmBqSuetpIVYYsiWmBFOPFJ6qlgRbgHR+8LVoR7eWh7hQVqy2j2eLDAfqwO7xcY3bJ9s5gR3jU6eqxkGQuMyk2yjAUmmXB1SPJzFKvBAmNAoi+xwCgNfWUscAJz+sn+zv36hXNLS+eW17Z2930jwAL7kku5O+Cj+vy4niOcOjwsMNVybCUWaFFEC8wFpn6abCUWaFEkC1hgkommGiywnNnc37m1ePL5I8knlR08vDD/lL2LcLvw1Mnljw48wsACM0Fra1+E+FjvF5gcIRYYlZtkGQtMMmEuMPk5itVggTEg0ZdYYJSGvjIWWL6cfrJbf/lIVzt6Vx6b9RMz3Zaef48ufmdgEcQCy5f7/iPCAlMtx1ZigRZFtMBcoPzRwgIFPligAEdBExZYtiQebJ492hG8p46dOr8ZWfDdX3u53TJzbGnLVO/vXOxq4fNL2wOGggUOCKwUm2OBUblJlrHAJBPmAvt+dLFAAREWKMBR0IQFliyJP1o+Zhzw2TMbEQEMR9lt6r1f3cHGq2Z6cObM/cGmA7HADLkPLxBJ++mdo82wo7w2wQJTLcdWYoEWRbTAXKD8AcQCBT5YoABHQRMWWK4k7l6cM85x8lbMAVvmSoUgOLUea9la7KwSv7oxkAZigX1zH94vMChM+FKHhwVG5SZZxgKTTJgLTP0oRSuxwCiNWBkLjAFR9hILLFVC99ZeMhI4X2/GBra//kq76Xh9L9ayt3bCdHpxeTfWJL7EAkU8rVYrvF9g78xrvx6jb8cCUy3HVmKBFkW0wFyg/NHEAgU+WKAAR0ETFliqJG53p/XOJJ5N0Wl67vXk6X/tp1mEIjjYrBUW2C/3jrtG9+s20nYsMCo3yTIWmGTCXGDfjyQWKCDCAgU4CpqwwFIl0e1zzfA2dUEQnEl5kI+7lxgcFijiabVa7ZMCi3tYXOrwsMBUy7GVWKBFES0wF5j6abKVWKBFkSxggUkmmmqwwFJl0+lzB/dOtyXwxFp8PbjVOtgwbcHMYnKeUAgPCxTgdJrCh8UFi1v9NxzbFlhgVG6SZSwwyYS5wL4fTyxQQIQFCnAUNGGBpUribv0zbdkL4ra3ebZ9BcizabeDebj0nOn0UooiCuFhgQKcdpOVcsM3+vdgi+/9DjRAOxaYajm2Egu0KKIF5gLlzxgWKPDBAgU4CpqwwHIlsWN7QXD6XvR63+2lZ0MFmUlbnbSXFaedMihFhwVKdMrahgVG5SZZxgKTTJgL7PtpxgIFRFigAEdBExZYsiTaib3nFzcfd8Z2cO90eybwuaWHidE+3jBtQZDWmtg8WoEFRmlMShkLTLUcW4kFWhTRAnOB8gccCxT4YIECHAVNWGDZkri39lLn9n8zL5xcWq0vvzrfeZxc8h4x+9tLn+5uPOBycKvVwgIz5N61KMyK8AMjGedf+6D25l9EhaPYMhaYyh8LlD/sWKDABwsU4ChowgLLl8SPN848Hz0FrV1+6uRazx0Ed5aPH7UPGw5mTiTuL9g/LiywL6PwrtFt+Q5v2W2W4x+FF2sXeL0Ic4GplmMrsUCLIlrAAuUPOxYo8MECBTgKmrDAUibxk72NcyePPmVc8MixV+s7seeFtG9obJpnXjiz8bFPFFhgP2rhRGDnrtFbi0YHw/vHrM7bcr895N+OBUblJlnGApNMOC+w7+cQCxQQYYECHAVNWOCEJnH7wotz868s1bf2oleRDBQMFtgPV2iBnWm/cAqwuwocLffbRe7tWGCq5dhKLNCiiBaYC5Q/iVigwAcLFOAoaMICFSTRMwQssB+48K7R3SfIbS4G8/VH7R5Y4Gc7JwW+/tkHnBcYla1Y+Yt/dOeNP70TqyzkJRYof9ixQIEPFijAUdCEBZYqifu7H+0dfDKmIWGBfUGHi7/dKcDNsx0jZEU46jFYYJRGrIwFCh+xr3/96z/+8Y+FDcbZhAUKtLFAAY6CJiywVEk0V6QeOXr81NLq+nbTe7E3U1BYYBZM4QUinds02uuFu0vDWfrnvQ0rwjHTir1kRTgGxLxkLlD+IGKBAh8sUICjoAkLLFUSrWd0rxF+6mh48t/d7d3uvQNzHC4WmCPMse0KC0y1HFuJBVoU0QIWKH9CsUCBDxYowFHQhAWWKon7O3frF149OfdM5y6AXRkM/5155tipzy2vPdzNa8kYCyxV7jMOBguMyk2yjAUmmXCNcN8PFxYoIMICBTgKmrDAsibx8d7O1tpyuhHOPP3pU2dW1rab+8OsGWOB/XLfnppNe2pfv44jbMcCUy3HVmKBFkW0wFyg/JnEAgU+WKAAR0ETFjgJSTRG+LlT888f3ii6M0048/Tcy6eXb23vDi6EWGDf3IcnBR7+dK8R7tttlBtggVG5SZaxwCQT5gL7fiKxQAERFijAUdCEBU5aEg/2dx+u18+fPvli5NkhbVOZeWZpe5BosMABaG0tHtpgYK8XGWAHeW2KBaZajq3EAi2KaIG5QPkDiAUKfLBAAY6CJixwopN4sPdwY/l357qCMti1q1igV+7Dmwja28d47WGoTlhgVG6SZSwwyYS5wL4fOSxQQIQFCnAUNGGBk5bETw72PtpcWzlz6vjc0/FrSLDAkWWzdy6weyvpkR3OvWMsMNVybCUWaFFEC8wFuj9SYQsWKPDBAgU4CpqwwMlI4sHHOxurF06/HF8FDoKZp184eXplbfOjgU8MZC6wX+7NtF93ptU+O6Rft5G2Y4FRuUmWscAkE+YC+34ksUABERYowFHQhAWWOInhKYDhZcJHn7Ii0inMPDN38tXlta2dgdUvEi4WGIGRWuQa4cMnxaW6BU+Qc2Ex9Tw7JPVzZSp5dogA55133vnlL38pbDDOJixwnLTHfywscPzM+xzxoLkdLvh+OrHeO8TlwKmHxAJTsbRaiXt3xyWc8wIP7ZAnyAkiiAU6PmJhNRYowMECBTg05UsAC8yX55B7216Mn+rXWfAd8taAqcPCAlOx9FS2TwfsOQswrCnyljGsCAvW9fpnH7AinMqH8wJ7PteJF6wIJ5AcVjAXeMhCYwkLLFVWO7NQnQXfhx43ARwgHCywH6z2eYGJu0bvrc4Hx+t7/TqPqB0LTLUcW4kFWhTRAhYofx6xQIEPFijAUdCEBZYqiYdrkUeenzfPixvmzD85NixQ5mOWhhe3EluF04GDXY6d2IV/BRYYlZtkGQtMMuHqkL6fNyxQQIQFCnAUNGGBpUri7vrnTh2LP0TYPi9umMfFpYRpznZLaaCqQyCcC+xZDm7Xhw8UYS7ws51TAzkvMNW6TCXnBQrfJZwXKMDhvEABDk35EsAC8+WZz94O9ne3by2ffjlxR8CZp4+9ciZ8XtzjYQ/UbDaDIKhUKsPuSHf/9nmB0enAcDmY8wK7Csg1woICvv7ZB1ig8PWABQpwsEABDk35EsAC8+WZ+94O9sN7RJ8+mXiC8Mwzx06dq68/3D34xOegjUYjCILZ2VmfztPV53CZPpw9LW4W0FBnRVgWL1aEU/lwXqD8pcWKsMCHFWEBjoImLHBykniwv7OVevvAI0ePn1pa3djZG2DJuFqtPnGaarU6OfEz0pAAFphqObYSC7QoogUsUP76wAIFPligAEdBExY4kUnsPkoktmSc9ZIFsxwcBEGz2ZzI+Kd40FhgVG6SZSwwyYSrQ/p+YWCBAiIsUICjoAkLnOQkfnKw94ON+n891r2rcVYLNFcHMxGYIfe9y8Fd0FwjbFWDq0MsimSB8wKFjxjnBQpwOC9QgENTvgSwwHx5jn5vj/fCdeHPnZp/If5wkZlnlrYzHN+sBXNdSAZULXs5cHhRiLlx4KP6fBBErxfJsp8ct2EuMClb0RrmAqM0bJkVYfkzyFygwIe5QAGOgiYscAKSGF4yfLd+4dWTc/GbyARBeNXwUv1u1quGjQIGQdBoNCYg8oKHGE4Edu4Us7VorwvhrtHWLbhGOIoiWWYuUPgEMxcowGEuUIBDU74EsMB8eea2t4O9nc1by2demT/61OEyZLdkLgdZ324OcDlIo9EwC8FBENRqtdwGqnlHoQV2pv3CKcDugnu0PPbwmQtMyla0hrnAKA1bZi5Q/qQyFyjwYS5QgKOgCQssVRL3t1cvpNwmsG1/4dNEztU3Ptob6NYwtVqtWq0uLCwYg5ydnWUWMHPKo3eN3ly0twks1ALfeeedlfbPl//0L/5s6U7hf778p39RW64XPgw7gNrK5S+9/o59WWzhjQvX3/zS9WLH0Dn65++Yt82VK1cyv/9HuyFzgQJf5gIFODTlSwALzJfnkHtLXIvw1NGTr16o39/ZG/w20XbmrzuDyH1hBs5O+x7RnSnAzbOd1eFiV4Rv3Lhhfp3zNwT8CDwsx8/t27c3NjbKMZaHDx48uHnzZkkG8/Dhw7fffntzc7Mk47l3795777038LcnHSaEABZYqkQZCzxy9OXTF1Y3dj4eYME3GUalUrH+ZwrVapVbwyRByTXhBSLmupCWdfTu0rDcczStWKCf+tDLEtgux88777yzsbFRjrFsP3jw4MaNGyUZzPb29urq6ubmZknGc+/evXfffXc032fstXgCWGDxOYiMYH/3o/2h1C+yr1ar1Ww2nzwjpFqtzs7OGimcnZ1FBHshTdir999//+bNm7dv3/7GN77xvRL8fOMb31hbWyvBQDpDuHbt2re//e2SjOeDDz64e/duGQazvb199+7dmzdvvv/++yV5x7MiLCSCFWEBDk35EsAC8+VZ3r01Gg07O8jVIeXNU4aRfe1rX/vZz36WYcNxbPLo0aN79+6N40jZjnHz5s3Hjwc/fyLbzgfd6vvf//53v/vdQXuNaPt///d/v3r16oh27rFbLFCAhgUKcGjKlwAWmC/PvPf2yf7O3fBK4eg9YmaemZt7+fTy3Z39wZ8gvLCwYFyQGcG8UzW+/WGBAmss0AUHC3SRabVaXCMswOEaYQGOgiYssLRJ3N9ZPT03Ezu1r/flzNzp1Z39ASOotp8gXKlUEMEByXU2j14y4reHIXthgQJALNAFBwt0kcECBTKtVgsLlPlMeisWWMoMPt5ZfvlIj/HNPD334pz5E7uD4Mynl7YHXAEz04E8Qa6Uue8/KCxQYIQFuuBggS4yWKBABguU4ShoxQJLmMTd+kt2DvDI/Nn65qP4FSMHH++sn5u3njjzUn13kDiazSbrwoMAK9e2WKCQDyzQBQcLdJHBAgUyWKAMR0ErFli6JO5ePNaZBZw5sfwDab334KPlE11dPHZxIA9smXVhLhMpXfozDAgLFCBhgS44WKCLDBYokMECZTgKWrHAkiXx4/VTHbE7urgVnwJMjvVga/GoccaZ0xuDrAs/uYNMEAQLCwvJfVJTcgJYoJAgLNAFBwt0kcECBTJYoAxHQSsWWK4k7t862ZG6Vzf6O2A49oONVzvaePKWNHEYi7PZbAZBMDs7G6vnZfkJYIFCjrBAFxws0EUGCxTIYIEyHAWtWGCpkri//kpHAhe3Mg9sa7GjgVnFsbNnTg3MjLhcG2KBQj6wQBccLNBFBgsUyGCBMhwFrVhgqZK4vfSsscBT69nn9fbXT5lOzy5tDxKNedBwo9EYpBPbFk8ACxRygAW64GCBLjJYoEAGC5ThKGjFAkuVRL8n1fr1amGBpcp99sFggQIrLNAFBwt0kcECBTJYoAxHQSsWWKok+vmcXy8ssFSpH2AwWKAACwt0wcECXWSwQIEMFijDUdCKBZYqibv1z5jF3fl6M/PAHtXnTafPDHbXQOYCMyMu14ZYoJAPLNAFBwt0kcECBTJYoAxHQSsWWK4kbn7OCF2Q/YLf/Xc7pwXOnN0cKBgscCBc5dkYCxRygQW64GCBLjJYoEAGC5ThKGjFAsuVxIP7ZzoX/H5qOdttoHeXP2XEcebM/Wz3lulGjAV2SUzYv1igkDAs0AUHC3SRwQIFMligDEdBKxZYtiTay4RnTqz298Dd1e7TQ54f7ALhVovzAsuW+qzjwQIFUligCw4W6CKDBQpksEAZjoJWLLB0STx4cOa5zrLw0VO3BBHc37nYVcAg04NGYqEyFxgDMikvsUAhU1igCw4W6CKDBQpksEAZjoJWLLCESTzYfG2usy4cBEc+derCrc2dvf39znrvwf7+7vatCydfsJvMnFjZHWwxuB00FljC3GcZEhYoUMICXXCwQBcZLFAggwXKcBS0YoHlTOLBzsrJI50ZQfmfIydXdjwUkBXhciY+y6iwQIESFuiCgwW6yGCBAhksUIajoBULLHES9zaWfvtpO+OXkMGZuf90YWPPf/zMBfqzK7QnFijgxwJdcLBAFxksUCCDBcpwFLRigaVP4uPd7bv1C+eWTr88N/fi3NyLJ0+fu1C/u737eNiRY4HDEiyoPxYogMcCXXCwQBcZLFAggwXKcBS0YoEKkugZAhboCa7obligkAEs0AUHC3SRwQIFMligDEdBKxaoIImdEA4+2hlofRgLnNDcY4FC4rBAFxws0EUGCxTIYIEyHAWtWKCCJLZarf3Nc/NHgsWBHh6CBU5o7rFAIXFYoAsOFugigwUKZLBAGY6CViywnEk82NuqL70y3z4RcG7+laX6lnuar7l2unPXGCywnNnMeVRYoAAUC3TBwQJdZLBAgQwWKMNR0IoFli+JH2+cObwX4OGVwUdeXt6JXxGyv31+PnJDGSywfNkcwYiwQAEqFuiCgwW6yGCBAhksUIajoBULLFkSD7YXnz80v1hp5pX1fTveXlmceeH0WtO2ZSqwIpwJU/k2wgKFnGCBLjhYoIsMFiiQwQJlOApascByJXH/1smu+R09c9esAh/s3jp1tFP73NLDcMAHHy3bh8cFwZH589uHdpg5ICwwM6pybYgFCvnAAl1wsEAXGSxQIIMFynAUtGKBpUri/vorHd07drHnCcLb5zrPFp45u9lq1q0Czrxwuv6R36NDWlhgqXKffTBYoMAKC3TBwQJdZLBAgQwWKMNR0IoFliqJu8svGgucr8eWd3+0PGdaXjyz+Ip5nsjMsdc2PaYAbcBYoEUxWQUsUMgXFuiCgwW6yGCBAhksUIajoBULLFUSNxc7U4GJ6zwONk53mtr/zMyduTeMAYZRY4Glyn32wWCBAiss0AUHC3SRwQIFMligDEdBKxZYqiS6LbBlm4Jg5sSy7ypwNFosMEpjgspYoJAsLNAFBwt0kcECBTJYoAxHQSsWWKokWtVLzAVGLHB+1X3vwEGiwQIHoVWibbFAIRlYoAsOFugigwUKZLBAGY6CViywVEnMYoGnNzyvBolHigXGiUzIayxQSBQW6IKDBbrIYIECGSxQhqOgFQssVRKzWGBymtAzBCzQE1zR3bBAIQNYoAsOFugigwUKZLBAGY6CViywVEnEAkuVjpIOBgsUEoMFuuBggS4yWKBABguU4ShoxQJLlUQssFTpKOlgsEAhMVigCw4W6CKDBQpksEAZjoJWLLBUScQCS5WOkg4GCxQSgwW64GCBLjJYoEAGC5ThKGjFAkuVRGuB0XsDZi8Pdsog5wWWKvfZB4MFCqywQBccLNBFBgsUyGCBMhwFrVhgqZKIBZYqHSUdDBYoJAYLdMHBAl1ksECBDBYow1HQigWWKom76+eWlvz/rPc8e7hfZMwF9iNU0nYsUEgMFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0t69+9as/+tGPflWOn5/85Cfvv/9+OcYSjuLGjRsff/xxScbz3e9+95vf/GZJBvPP//zPV65cKclgfvWrX92/f39nZ6ck4/nZz3723nvvlWQwv/rVr9bW1vb29koynu9///sPHjwo+muP44+KABY4KrLl3y8WWP4cpY7w+vXr165de7scP9euXbt8+XI5xhKOolarra6ulmQ8V69evXLlSkkGY+CUZzCXL18uz9u4Xq9funSpPHAuXbpUr9dLMp6rV6/evn079buISgUEsEAFSfQMAQv0BFd0N1aEhQywIuyCw4qwiwwrwgIZVoRlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQtUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXfDAoUMYIEuOFigiwwWKJDBAmU4ClqxQAVJ9AwBC/QEV3Q3LFDIABbogoMFushggQIZLFCGo6AVC1SQRM8QsEBPcEV3wwKFDGCBLjhYoIsMFiiQwQJlOApasUAFSfQMAQv0BFd0NyxQyAAW6IKDBbrIYIECGSxQhqOgFQshSWHHAAATGElEQVRUkETPELBAT3BFd8MChQxggS44WKCLDBYokMECZTgKWrFABUn0DAEL9ARXdDcsUMgAFuiCgwW6yGCBAhksUIajoBULVJBEzxCwQE9wRXe7fv36V77ylY1y/HzlK195++23yzGWcBRXrlz56le/WpLxvPPOO2trayUZzIcffnjp0qWSDGZjY+PmzZvvvvtuScZz9+7d1dXVkgxmY2Pj2rVrH3zwQUnGc7v9U/TXHscfFQEscFRky79fLLD8OUod4bvvvvud73znp+X4+eu//us7d+6UYyzhKFZXV3/4wx+WZDx/+Zd/ubGxUZLB/OQnP7l8+XJJBvPTn/50fX39m9/8ZknG873vfW9tba0kg/npT39648aNv/3bvy3JeP7qr/7q3r17qd9FVCoggAUqSKJnCFigJ7iiu7EiLGSAFWEXHFaEXWRYERbIsCIsw1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4oru9995729vbj8rx8/Dhw3fffbccYwlHsbq6+nd/93clGc83vvGNRqNRksE0m83Lly+XZDCPHj16//33v/3tb5dkPD/4wQ9u3bpVksE8evTo5s2bH330UUnGs7m5+eGHHxb9tcfxR0UACxwV2fLvFwssf45SR3j9+vU7d+7cK8fPu+++u7q6Wo6xhKO4cuXK+++/X5LxrK2t/fmf/3lJBvPVr3710qVLJRnMvXv3rl+/fvv27ZKM57333qvX6yUZzL17965evbq+vl6S8dy6dev27dup30VUKiCABSpIomcIWKAnuKK7sSIsZIAVYRccVoRdZFgRFsiwIizDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoaABXqCK7obFihkAAt0wcECXWSwQIEMFijDUdCKBSpIomcIWKAnuKK7YYFCBrBAFxws0EUGCxTIYIEyHAWtWKCCJHqGgAV6giu6GxYoZAALdMHBAl1ksECBDBYow1HQigUqSKJnCFigJ7iiu2GBQgawQBccLNBFBgsUyGCBMhwFrViggiR6hoAFeoIruhsWKGQAC3TBwQJdZLBAgQwWKMNR0IoFKkiiZwhYoCe4orthgUIGsEAXHCzQRQYLFMhggTIcBa1YoIIkeoawsLAQBEGtVvPsT7eCCGCBAngs0AUHC3SRwQIFMligDEdBKxaoIImeIdRqtSAIFhYWPPvTrSACWKAAHgt0wcECXWSwQIEMFijDUdCKBSpIomcIjUYjCILZ2VnP/nQriAAWKIDHAl1wsEAXGSxQIIMFynAUtGKBCpLoH0KlUgmCoNls+u+CnmMngAUKyLFAFxws0EUGCxTIYIEyHAWtWKCCJPqHUK1WgyCoVCqIoD/EsffEAgXkWKALDhboIoMFCmSwQBmOglYsUEEShwrBTAdWq9Wh9kLnMRLAAgXYWKALDhboIoMFCmSwQBmOglYsUEEShwqh2WwaEWw0GkPtiM7jIoAFCqSxQBccLNBFBgsUyGCBMhwFrViggiQOG4JZFw6CoFqtsjQ8LM3R98cCBcZYoAsOFugigwUKZLBAGY6CVixQQRJzCKFarZoZwdnZWUQwB6Cj3AUWKNDFAl1wsEAXGSxQIIMFynAUtGKBCpKYTwiNRsOI4JNJQXMHGaYG8yGb916wQIEoFuiCgwW6yGCBAhksUIajoBULVJDEPEMwk4JWB7lqJE+4Oe3r+vXr9Xr9Rjl+VldXL1++XI6xhKOo1WrXr18vyXiuXbt29erVkgzm+vXrKysrJRnMjRs3rly5Uqq38aVLl8oD59KlS2+//XZJxnPt2rXbt2/n9O3FbkpHAAssXUpKMqBGo1Gr1bhkpCTpiA7jH9s/j8vxU6rBPH78uFTjKdVg9vf3//Ef/7Ec75pwFKWCU6rBlBNO9CuIsiYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYCWKCmbBILBCAAAQhAAAIQyEoAC8xKiu0gAAEIQAACEICAJgJYoKZsEgsEIAABCEAAAhDISgALzEqK7SAAAQhAAAIQgIAmAligpmwSCwQgAAEIQAACEMhKAAvMSortIAABCEAAAhCAgCYC/z8/9jvEuLikdAAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "6a2aa563",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e02546",
   "metadata": {},
   "source": [
    "请参考图上图，了解 tile 的示意图以及如何推进块指针。上述的加权求和函数如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import rearrange, einsum\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,       # 输入指针\n",
    "    output_ptr,              # 输出指针\n",
    "    x_stride_row, x_stride_dim,  # strides 告诉我们如何在张量的每个轴上移动一个元素\n",
    "    weight_stride_dim,       # 很可能是 1\n",
    "    output_stride_row,       # 很可能是 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # tile 的形状必须在编译时已知\n",
    "):\n",
    "    # 每个实例将计算 x 的一块行的加权和\n",
    "    # `tl.program_id` 给我们一个方法来检查当前线程块的编号\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "\n",
    "    # block 指针给我们一个方法，从 ND 内存区域中选择数据\n",
    "    # 并移动选择位置\n",
    "    # block 指针必须知道：\n",
    "    # - 张量第一个元素的指针\n",
    "    # - 张量的整体形状以处理越界访问\n",
    "    # - 每个维度的步长，用于使用内存布局属性\n",
    "    # - 起始块的ND坐标，即“偏移量”\n",
    "    # - 一次使用/存储的块形状\n",
    "    # - 内存中主要到次要的维度顺序\n",
    "    # axes（=np.argsort(strides)）用于优化，特别是在H100上\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D,),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # 初始化一个缓冲区以写入到输出\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    for i in range(tl.div(D, D_TILE_SIZE)):\n",
    "        # 加载当前块指针\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，且D_TILE_SIZE可能不能整除D，\n",
    "        # 我们需要为两个维度进行边界检查\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "\n",
    "        # 计算行的加权和。\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "\n",
    "        # 移动到下一个块。\n",
    "        # 这些是（行，列）坐标的增量\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # 在最后一个维度移动D_TILE_SIZE\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # 移动D_TILE_SIZE\n",
    "\n",
    "        # 将输出写入输出块指针（每行一个标量）\n",
    "        # 由于ROWS_TILE_SIZE可能不能整除ROWS，我们需要边界检查\n",
    "        tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "\n",
    "# 现在我们将这个内核包装在一个PyTorch自动微分函数中，该函数将与PyTorch（即，接受张量作为输入，输出张量，并在反向传播期间与自动微分引擎一起工作）一起解释\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # 在反向传递中缓存x和weight，当我们\n",
    "        # 只接收到输出张量的梯度时，将使用它们\n",
    "        # 需要计算x和weight的梯度。\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # 将输入张量重塑为2D\n",
    "        input_shape = x.shape\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"维度不匹配\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"期望CUDA张量\"\n",
    "        assert x.is_contiguous(), \"我们的指针算术将假设x是连续的\"\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # 大约16次循环遍历嵌入维度\n",
    "        ctx.ROWS_TILE_SIZE = 16  # 每个线程同时处理16个批次元素\n",
    "        ctx.input_shape = input_shape\n",
    "\n",
    "        # 需要初始化一个空的结果张量。注意这些元素不一定是0！\n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "\n",
    "        # 在我们的1D网格中启动我们的内核，有n个实例。\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(input_shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c8f8",
   "metadata": {},
   "source": [
    "### FlashAttention 中分块与数值稳定 Softmax 为什么能够显著加速\n",
    "\n",
    "FlashAttention 的加速来源，**并不在于减少了计算量（FLOPs）**，而在于它彻底改变了 attention 的计算路径，使得大量中间结果不再落到显存中。通过分块和流式计算，FlashAttention 将原本以显存读写为主导的计算，转变为主要发生在片上存储中的连续计算过程。\n",
    "\n",
    "从数学形式上看，FlashAttention 仍然计算的是\n",
    "\n",
    "$$\n",
    "\\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "但实现方式与标准 Attention 完全不同。\n",
    "\n",
    "---\n",
    "\n",
    "### 标准 Attention 的真实开销在哪里\n",
    "\n",
    "在标准 Attention 中，假设序列长度为 $N$，每个 head 的维度为 $d$。首先需要计算 $QK^T$，这一步的计算复杂度是 $O(N^2 d)$，并且会生成一个大小为 $N \\times N$ 的 attention matrix。接下来对这个矩阵做 softmax，需要再次读写整个 $N \\times N$ 的结果，最后再与 $V$ 相乘。\n",
    "\n",
    "从 FLOPs 的角度看，这些操作都是 $O(N^2 d)$ 或 $O(N^2)$，看起来并不离谱。但真正的问题在于，这个 $N \\times N$ 的中间矩阵太大，**只能被写入显存（HBM）**。而在 GPU 上，显存带宽远远慢于算力，实际运行时，模型往往是在“等内存”，而不是在“等计算”。\n",
    "\n",
    "换句话说，标准 Attention 的瓶颈并不是算不动，而是**读写显存的次数和数据量过多**。\n",
    "\n",
    "\n",
    "\n",
    "### GPU 的存储层级决定了性能上限\n",
    "\n",
    "GPU 上的存储可以粗略分为三个层级：寄存器速度最快，但容量极小；shared memory 次之，容量有限；显存容量最大，但访问延迟和带宽最差。标准 Attention 中的 $N \\times N$ attention matrix 无论如何都放不进片上存储，只能反复写入和读取显存，这直接决定了性能上限。\n",
    "\n",
    "FlashAttention 的核心设计目标，就是**避免让这个矩阵出现**。\n",
    "\n",
    "\n",
    "\n",
    "### 分块（blocking）的本质：让数据留在片上\n",
    "\n",
    "FlashAttention 将 Attention 的计算拆成多个小块。对每一小块 $Q$，它只与当前的 $K/V$ 子块交互，计算局部的 $QK^T$，立刻参与 softmax 的更新，并马上与 $V$ 相乘。整个过程中，从不生成完整的 $N \\times N$ attention matrix。\n",
    "\n",
    "这样一来，局部的中间结果可以一直停留在寄存器或 shared memory 中，用完即丢，不再写入显存。显存中只需要读入 $Q$、$K$、$V$ 的分块数据，并写回最终的输出块。\n",
    "\n",
    "结果是，显存 I/O 从原来的 $O(N^2)$ 量级，下降到了与输入输出规模相当的 $O(N \\cdot d)$。在大模型场景下，这个变化直接决定了数量级上的性能差异。\n",
    "\n",
    "\n",
    "\n",
    "### 为什么需要“在线”的数值稳定 Softmax\n",
    "\n",
    "分块之后，新的问题出现了：标准 softmax 的定义要求在计算时看到一整行的 $QK^T$，而这恰恰与“流式计算”相冲突。也就是说，如果仍然使用传统 softmax，就必须先把整行 attention score 算完并存下来，这会把我们重新拉回显存瓶颈。\n",
    "\n",
    "FlashAttention 使用的是一种**可递推、数值稳定的 softmax 形式**。对于每一行 attention，它不一次性计算最大值和归一化因子，而是在处理每个 block 时，逐步更新当前的最大值 $m_i$ 和归一化累积量 $l_i$。每来一个新的 block，就根据新的局部最大值调整已有的统计量。\n",
    "\n",
    "对应的递推形式可以写成：\n",
    "\n",
    "$$\n",
    "m_i^{(t)} = \\max\\left(m_i^{(t-1)}, \\max_j S_{ij}^{(t)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "l_i^{(t)} =\n",
    "l_i^{(t-1)} e^{m_i^{(t-1)} - m_i^{(t)}} +\n",
    "\\sum_j e^{S_{ij}^{(t)} - m_i^{(t)}}\n",
    "$$\n",
    "\n",
    "这种写法的关键在于：**不需要完整的 $S = QK^T$**，只要当前 block 的分数，就可以安全地更新 softmax 所需的全部信息。这使得 softmax 自然地融入了分块和 streaming 的计算流程。\n",
    "\n",
    "\n",
    "### 反向传播同样不需要 Attention Matrix\n",
    "\n",
    "FlashAttention 的设计并不只考虑 forward。在 forward 过程中，它会额外保存每一行的 $\\log\\sum\\exp$（通常记为 $L$），以及最终的输出\n",
    "\n",
    "$$\n",
    "O = \\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "有了这些量，反向传播时就可以直接重建梯度公式，而不需要显式地保存或重算完整的 attention matrix。例如，对 score 的梯度可以写成：\n",
    "\n",
    "$$\n",
    "dS_{ij} = P_{ij}\\left(dP_{ij} - D_i\\right)\n",
    "$$\n",
    "\n",
    "其中 $P_{ij}$ 表示 softmax 概率，$D_i$ 是按行聚合的项。\n",
    "\n",
    "因此，无论是在 forward 还是 backward 阶段，FlashAttention 都避免了 $N \\times N$ 级别的中间结果落入显存。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cc2ba",
   "metadata": {},
   "source": [
    "# 作业七：使用pytorch实现flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a41b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FlashAttentionAutograd(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    FlashAttention（forward + backward）\n",
    "\n",
    "    输入：\n",
    "        Q: (B, Nq, d)\n",
    "        K: (B, Nk, d)\n",
    "        V: (B, Nk, d)\n",
    "\n",
    "    输出：\n",
    "        O: (B, Nq, d)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        # -----------------------------\n",
    "        # block size\n",
    "        # -----------------------------\n",
    "        Bq = 64  # Query block 的尺寸\n",
    "        Bk = 64  # Key / Value block 的尺寸\n",
    "\n",
    "        # 我们将KV矩阵分成一个个小矩阵\n",
    "\n",
    "        Tq = Nq // Bq  # Q block 数\n",
    "        Tk = Nk // Bk  # K/V block 数\n",
    "\n",
    "        scale = d ** -0.5\n",
    "\n",
    "        # -----------------------------\n",
    "        # 输出与中间量\n",
    "        # -----------------------------\n",
    "        O = torch.zeros_like(Q)              # Attention 输出\n",
    "        L = torch.zeros(B, Nq, device=Q.device)  # log-sum-exp（给 backward 用）\n",
    "\n",
    "        # =========================================================\n",
    "        # 对 batch 维度显式循环\n",
    "        # =========================================================\n",
    "        for b in range(B):\n",
    "            Q_b = Q[b]  # (Nq, d)\n",
    "            K_b = K[b]  # (Nk, d)\n",
    "            V_b = V[b]  # (Nk, d)\n",
    "\n",
    "            # =====================================================\n",
    "            # 外层循环：Query block\n",
    "            # =====================================================\n",
    "            for i in range(Tq):\n",
    "                q_i = Q_b[i * Bq:(i + 1) * Bq]  # (Bq, d)\n",
    "\n",
    "                # -----------------------------\n",
    "                # FlashAttention 核心状态\n",
    "                # -----------------------------\n",
    "                m = torch.full((Bq, 1), -float(\"inf\"), device=Q.device)  # running max\n",
    "                l = torch.zeros((Bq, 1), device=Q.device)               # running sum\n",
    "                O_acc = torch.zeros((Bq, d), device=Q.device)           # 输出累积（未归一化）\n",
    "\n",
    "                # =================================================\n",
    "                # 内层循环：Key / Value block\n",
    "                # =================================================\n",
    "                for j in range(Tk):\n",
    "                    k_j = K_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "                    v_j = V_b[j * Bk:(j + 1) * Bk]  # (Bk, d)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 计算局部 attention score\n",
    "                    # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale  # (Bq, Bk)\n",
    "\n",
    "                    # 当前 block 内，每一行的最大值\n",
    "                    row_max = S.max(dim=1, keepdim=True).values  # (Bq, 1)\n",
    "\n",
    "                    # 更新 running max\n",
    "                    m_new = torch.maximum(m, row_max)\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 数值稳定 softmax（FlashAttention 关键）\n",
    "                    # --------------------------------------------\n",
    "                    # 旧 block 贡献修正\n",
    "                    exp_old = torch.exp(m - m_new) * l\n",
    "\n",
    "                    # 当前 block 的 exp\n",
    "                    P = torch.exp(S - m_new)\n",
    "\n",
    "                    # 更新分母\n",
    "                    l = exp_old + P.sum(dim=1, keepdim=True)\n",
    "\n",
    "                    # 更新未归一化输出\n",
    "                    O_acc = (\n",
    "                        torch.exp(m - m_new) * O_acc +\n",
    "                        P @ v_j\n",
    "                    )\n",
    "\n",
    "                    # 写回 running max\n",
    "                    m = m_new\n",
    "\n",
    "                # =================================================\n",
    "                # block 内 softmax 归一化\n",
    "                # =================================================\n",
    "                O_i = O_acc / l  # (Bq, d)\n",
    "\n",
    "                # log-sum-exp：L = m + log(l)\n",
    "                L_i = m.squeeze(1) + torch.log(l.squeeze(1))\n",
    "\n",
    "                # 写回全局输出\n",
    "                O[b, i * Bq:(i + 1) * Bq] = O_i\n",
    "                L[b, i * Bq:(i + 1) * Bq] = L_i\n",
    "\n",
    "        # 保存 backward 所需变量\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.scale = scale\n",
    "        ctx.Bq = Bq\n",
    "        ctx.Bk = Bk\n",
    "\n",
    "        return O\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, L = ctx.saved_tensors\n",
    "        scale = ctx.scale\n",
    "        Bq = ctx.Bq\n",
    "        Bk = ctx.Bk\n",
    "\n",
    "        B, Nq, d = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Tq = Nq // Bq\n",
    "        Tk = Nk // Bk\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # FlashAttention backward 中的重要中间量\n",
    "        # D_i = sum_j O_ij * dO_ij\n",
    "        # --------------------------------------------\n",
    "        D = torch.sum(O * dO, dim=-1)  # (B, Nq)\n",
    "\n",
    "        dQ = torch.zeros_like(Q)\n",
    "        dK = torch.zeros_like(K)\n",
    "        dV = torch.zeros_like(V)\n",
    "\n",
    "        for b in range(B):\n",
    "            for i in range(Tq):\n",
    "                q_i = Q[b, i * Bq:(i + 1) * Bq]\n",
    "                dO_i = dO[b, i * Bq:(i + 1) * Bq]\n",
    "                L_i = L[b, i * Bq:(i + 1) * Bq]\n",
    "                D_i = D[b, i * Bq:(i + 1) * Bq]\n",
    "\n",
    "                for j in range(Tk):\n",
    "                    k_j = K[b, j * Bk:(j + 1) * Bk]\n",
    "                    v_j = V[b, j * Bk:(j + 1) * Bk]\n",
    "\n",
    "                    # --------------------------------------------\n",
    "                    # 重算局部 attention 概率 P_ij\n",
    "                    # P_ij = exp(S_ij - L_i)\n",
    "                    # --------------------------------------------\n",
    "                    S = q_i @ k_j.T * scale\n",
    "                    P = torch.exp(S - L_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dV --------\n",
    "                    dV[b, j * Bk:(j + 1) * Bk] += P.T @ dO_i\n",
    "\n",
    "                    # -------- dS --------\n",
    "                    dP = dO_i @ v_j.T\n",
    "                    dS = P * (dP - D_i.unsqueeze(1))\n",
    "\n",
    "                    # -------- dQ --------\n",
    "                    dQ[b, i * Bq:(i + 1) * Bq] += dS @ k_j * scale\n",
    "\n",
    "                    # -------- dK --------\n",
    "                    dK[b, j * Bk:(j + 1) * Bk] += dS.T @ q_i * scale\n",
    "\n",
    "        return dQ, dK, dV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0992c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：58.657169342041016ms\n",
      "正在使用 cuda\n",
      "[FlashAttention] B=1, N=1024, d=64, time=58657.169 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                        aten::mm         7.45%       9.324ms        14.19%      17.766ms      34.699us       2.847ms        37.21%       2.847ms       5.560us           512  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us       1.013ms        13.24%       1.013ms       0.974us          1040  \n",
      "                                                 ampere_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us       1.002ms        13.10%       1.002ms       3.915us           256  \n",
      "                                                 ampere_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us     941.643us        12.31%     941.643us       3.678us           256  \n",
      "void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, f...         0.00%       0.000us         0.00%       0.000us       0.000us     902.724us        11.80%     902.724us       1.763us           512  \n",
      "                                                                       aten::mul         5.71%       7.150ms        10.66%      13.340ms      17.370us     847.364us        11.08%     847.364us       1.103us           768  \n",
      "                                                                       aten::sub         5.32%       6.655ms        10.66%      13.342ms      17.372us     841.735us        11.00%     841.735us       1.096us           768  \n",
      "                                                                       aten::exp         4.46%       5.578ms         9.88%      12.362ms      16.096us     834.898us        10.91%     834.898us       1.087us           768  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::exp_kernel_cuda...         0.00%       0.000us         0.00%       0.000us       0.000us     834.898us        10.91%     834.898us       1.087us           768  \n",
      "                                                                       aten::max         3.28%       4.110ms         5.10%       6.388ms      24.954us     752.438us         9.84%     752.438us       2.939us           256  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 125.170ms\n",
      "Self CUDA time total: 7.650ms\n",
      "\n",
      "现在真正计时!\n",
      "单次耗时：0.24080276489257812ms\n",
      "正在使用 cuda\n",
      "[NaiveAttention]  B=1, N=1024, d=64, time=240.803 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                       aten::bmm        17.17%      95.025us        68.71%     380.186us     190.093us      50.253us        77.70%      76.486us      38.243us             2  \n",
      "                                                         Activity Buffer Request        42.67%     236.133us        42.67%     236.133us     236.133us      26.233us        40.56%      26.233us      26.233us             1  \n",
      "                                                          ampere_sgemm_128x32_tn         0.00%       0.000us         0.00%       0.000us       0.000us      26.233us        40.56%      26.233us      26.233us             1  \n",
      "                                                 ampere_sgemm_64x32_sliced1x4_nn         0.00%       0.000us         0.00%       0.000us       0.000us      23.712us        36.66%      23.712us      23.712us             1  \n",
      "                                                                  aten::_softmax         2.60%      14.386us         4.17%      23.080us      23.080us       8.427us        13.03%       8.427us       8.427us             1  \n",
      "void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, fal...         0.00%       0.000us         0.00%       0.000us       0.000us       8.427us        13.03%       8.427us       8.427us             1  \n",
      "                                                                       aten::div         2.97%      16.442us         5.32%      29.431us      29.431us       5.997us         9.27%       5.997us       5.997us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<f...         0.00%       0.000us         0.00%       0.000us       0.000us       5.997us         9.27%       5.997us       5.997us             1  \n",
      "                                                                 Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       0.308us         0.48%       0.308us       0.308us             1  \n",
      "                                                                 aten::transpose         2.62%      14.490us         3.14%      17.386us      17.386us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 553.337us\n",
      "Self CUDA time total: 64.677us\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from my_profile import benchmark,profile \n",
    "def test_flash_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionAutograd.apply(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_flash_attention',run = run)\n",
    "    t1 = profile(description ='test_flash_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttention] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "\n",
    "def naive_attention(Q, K, V):\n",
    "    d = Q.size(-1)\n",
    "    scores = Q @ K.transpose(-2, -1) / (d ** 0.5)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P @ V\n",
    "def test_naive_attention(B=1, N=1024, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = naive_attention(Q, K, V)\n",
    "        #loss = O.sum()\n",
    "        #loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'test_attention',run = run)\n",
    "    t1 = profile(description ='test_attention' , run = run, num_warmups = 3, with_stack = True)\n",
    "    print(f\"[NaiveAttention]  B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "for N in [1024]:\n",
    "    test_flash_attention(B=1, N=N, d=64, device=device)\n",
    "    test_naive_attention(B=1, N=N, d=64, device=device)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a110c30",
   "metadata": {},
   "source": [
    "# 2. Triton 内核实现flash attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d049031",
   "metadata": {},
   "source": [
    "Triton 是一个用 Python 写 GPU kernel 的 DSL，本质上是“显式控制数据分块与内存层级的编程模型”，它不是NumPy，也不是高层 PyTorch API，它是一个 CUDA kernel 的“可读版本”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12251ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sympy import Q\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def flash_attention_fwd_kernel(\n",
    "    # -------- 全局内存指针 --------\n",
    "    Q_ptr, K_ptr, V_ptr,           # 输入 Q, K, V\n",
    "    O_ptr, L_ptr,                  # 输出 O, logsumexp L\n",
    "\n",
    "    # -------- stride 信息（用于 block ptr）--------\n",
    "    stride_qb, stride_qn, stride_qd,\n",
    "    stride_kb, stride_kn, stride_kd,\n",
    "    stride_vb, stride_vn, stride_vd,\n",
    "    stride_ob, stride_on, stride_od,\n",
    "    stride_lb, stride_ln,\n",
    "\n",
    "    # -------- 序列长度 --------\n",
    "    Nq, Nk,\n",
    "\n",
    "    # -------- scale --------\n",
    "    scale,\n",
    "\n",
    "    # -------- 编译期常量 --------\n",
    "    D: tl.constexpr,\n",
    "    Q_BLOCK: tl.constexpr,\n",
    "    K_BLOCK: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    每个 Triton program 负责：\n",
    "        - 一个 batch\n",
    "        - 一个 Query block（Q_BLOCK 行）\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Triton 并行索引\n",
    "    # ------------------------------------------------\n",
    "    q_block_id = tl.program_id(0)     # 第几个 Q block\n",
    "    batch_id   = tl.program_id(1)     # 第几个 batch\n",
    "\n",
    "    # =================================================\n",
    "    # 构造 block pointer（这是 Triton 的关键）\n",
    "    # =================================================\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_qn, stride_qd),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    #  K / V 从第 0 行开始，后面在 loop 中 advance\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K_ptr + batch_id * stride_kb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_kn, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V_ptr + batch_id * stride_vb,\n",
    "        shape=(Nk, D),\n",
    "        strides=(stride_vn, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O_ptr + batch_id * stride_ob,\n",
    "        shape=(Nq, D),\n",
    "        strides=(stride_on, stride_od),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        base=L_ptr + batch_id * stride_lb,\n",
    "        shape=(Nq, 1),\n",
    "        strides=(stride_ln, 1),\n",
    "        offsets=(q_block_id * Q_BLOCK, 0),\n",
    "        block_shape=(Q_BLOCK, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # =================================================\n",
    "    # 加载 Query block\n",
    "    # =================================================\n",
    "    Q_i = tl.load(Q_block_ptr)  # (Q_BLOCK, D)\n",
    "\n",
    "    # =================================================\n",
    "    # FlashAttention 核心状态（每个 Q block 独立）\n",
    "    # =================================================\n",
    "    O_acc = tl.zeros((Q_BLOCK, D), dtype=tl.float32)       # 未归一化输出累积\n",
    "    L_acc = tl.zeros((Q_BLOCK, 1), dtype=tl.float32)      # softmax 分母\n",
    "    M_acc = tl.full((Q_BLOCK, 1), -float(\"inf\"), tl.float32)  # running max\n",
    "\n",
    "    # =================================================\n",
    "    # 内层循环：遍历所有 K/V block\n",
    "    # =================================================\n",
    "    for k_block_id in range(tl.cdiv(Nk, K_BLOCK)):\n",
    "        K_j = tl.load(K_block_ptr)   # (K_BLOCK, D)\n",
    "        V_j = tl.load(V_block_ptr)   # (K_BLOCK, D)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # S_ij = Q_i @ K_j^T / sqrt(d)\n",
    "        # ---------------------------------------------\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale  # (Q_BLOCK, K_BLOCK)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # Causal mask（编译期 if）\n",
    "        # ---------------------------------------------\n",
    "        if is_causal:\n",
    "            q_idx = q_block_id * Q_BLOCK + tl.arange(0, Q_BLOCK)[:, None]\n",
    "            k_idx = k_block_id * K_BLOCK + tl.arange(0, K_BLOCK)[None, :]\n",
    "            causal_mask = q_idx >= k_idx\n",
    "            S_ij = tl.where(causal_mask, S_ij, -1e6)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 数值稳定 softmax（FlashAttention 核心）\n",
    "        # ---------------------------------------------\n",
    "        M_block = tl.max(S_ij, axis=1, keep_dims=True)\n",
    "        M_new = tl.maximum(M_acc, M_block)\n",
    "\n",
    "        P_ij = tl.exp(S_ij - M_block)\n",
    "\n",
    "        L_new = (\n",
    "            tl.exp(M_acc - M_new) * L_acc +\n",
    "            tl.exp(M_block - M_new) * tl.sum(P_ij, axis=1, keep_dims=True)\n",
    "        )\n",
    "\n",
    "        # 类型对齐（Triton 细节）\n",
    "        P_cast = P_ij.to(V_block_ptr.type.element_ty)\n",
    "\n",
    "        O_new = (\n",
    "            tl.exp(M_acc - M_new) * O_acc +\n",
    "            tl.exp(M_block - M_new) * tl.dot(P_cast, V_j)\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 更新 running 状态\n",
    "        # ---------------------------------------------\n",
    "        M_acc = M_new\n",
    "        L_acc = L_new\n",
    "        O_acc = O_new\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 移动 K/V block 指针\n",
    "        # ---------------------------------------------\n",
    "        K_block_ptr = K_block_ptr.advance((K_BLOCK, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_BLOCK, 0))\n",
    "\n",
    "    # =================================================\n",
    "    # softmax 归一化\n",
    "    # =================================================\n",
    "    O_i = O_acc / L_acc\n",
    "    L_i = M_acc + tl.log(L_acc)\n",
    "\n",
    "    tl.store(O_block_ptr, O_i)\n",
    "    tl.store(L_block_ptr, L_i)\n",
    "\n",
    "@triton.jit\n",
    "def flash_bwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr, dO_ptr, D_ptr,\n",
    "    dQ_ptr, dK_ptr, dV_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    stride_dob, stride_doq, stride_dod,\n",
    "    stride_db, stride_dq_d,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # program ids\n",
    "    k_tile_id = tl.program_id(0)\n",
    "    batch_id = tl.program_id(1)\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (Q-side)\n",
    "    # -------------------------\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q_ptr + batch_id * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_id * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dO_block_ptr = tl.make_block_ptr(\n",
    "        dO_ptr + batch_id * stride_dob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_doq, stride_dod),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_id * stride_lb,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_lq, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    D_block_ptr = tl.make_block_ptr(\n",
    "        D_ptr + batch_id * stride_db,\n",
    "        shape=(N_QUERIES, 1),\n",
    "        strides=(stride_dq_d, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(Q_TILE_SIZE, 1),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # block pointers (K/V-side)\n",
    "    # -------------------------\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dK_block_ptr = tl.make_block_ptr(\n",
    "        dK_ptr + batch_id * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    dV_block_ptr = tl.make_block_ptr(\n",
    "        dV_ptr + batch_id * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(k_tile_id * K_TILE_SIZE, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # load K/V tile\n",
    "    # -------------------------\n",
    "    K_j = tl.load(K_block_ptr)  # (K_TILE_SIZE, D)\n",
    "    V_j = tl.load(V_block_ptr)\n",
    "\n",
    "    dK_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "    dV_j = tl.zeros((K_TILE_SIZE, D), dtype=tl.float32)\n",
    "\n",
    "    # -------------------------\n",
    "    # loop over Q tiles\n",
    "    # -------------------------\n",
    "    for q_tile_id in range(tl.cdiv(N_QUERIES, Q_TILE_SIZE)):\n",
    "        Q_i = tl.load(Q_block_ptr)\n",
    "        O_i = tl.load(O_block_ptr)\n",
    "        dO_i = tl.load(dO_block_ptr)\n",
    "        L_i = tl.load(L_block_ptr)\n",
    "        D_i = tl.load(D_block_ptr)\n",
    "\n",
    "        # S = QK^T\n",
    "        S_ij = tl.dot(Q_i, K_j.T) * scale\n",
    "\n",
    "        if is_causal:\n",
    "            q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "            k_idx = k_tile_id * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)[None, :]\n",
    "            S_ij = tl.where(q_idx >= k_idx, S_ij, -1e6)\n",
    "\n",
    "        # P = softmax\n",
    "        P_ij = tl.exp(S_ij - L_i)\n",
    "\n",
    "        # dV\n",
    "        dV_j += tl.dot(P_ij.to(V_j.dtype).T, dO_i)\n",
    "\n",
    "        # dP\n",
    "        dP_ij = tl.dot(dO_i, V_j.T)\n",
    "\n",
    "        # dS\n",
    "        dS_ij = P_ij * (dP_ij - D_i)\n",
    "        dS_ij = dS_ij * scale\n",
    "\n",
    "        # dQ (atomic add)\n",
    "        dQ_i = tl.dot(dS_ij, K_j)\n",
    "\n",
    "        q_idx = q_tile_id * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)[:, None]\n",
    "        d_idx = tl.arange(0, D)[None, :]\n",
    "        mask = (q_idx < N_QUERIES) & (d_idx < D)\n",
    "        dq_ptrs = dQ_ptr + batch_id * stride_qb + q_idx * stride_qq + d_idx * stride_qd\n",
    "        tl.atomic_add(dq_ptrs, dQ_i.to(dQ_ptr.dtype.element_ty), mask=mask)\n",
    "\n",
    "        # dK\n",
    "        dK_j += tl.dot(dS_ij.T, Q_i)\n",
    "\n",
    "        # advance Q-side blocks\n",
    "        Q_block_ptr = Q_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        O_block_ptr = O_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        dO_block_ptr = dO_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        L_block_ptr = L_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "        D_block_ptr = D_block_ptr.advance((Q_TILE_SIZE, 0))\n",
    "\n",
    "    # -------------------------\n",
    "    # store dK / dV\n",
    "    # -------------------------\n",
    "    tl.store(dK_block_ptr, dK_j.to(dK_ptr.dtype.element_ty))\n",
    "    tl.store(dV_block_ptr, dV_j.to(dV_ptr.dtype.element_ty))\n",
    "\n",
    "\n",
    "\n",
    "class FlashAttentionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        \"\"\"\n",
    "        Q, K, V: (B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Nq, D = Q.shape\n",
    "        Nk = K.shape[1]\n",
    "\n",
    "        Q_BLOCK = 16\n",
    "        K_BLOCK = 16\n",
    "\n",
    "        scale = D ** -0.5\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        L = torch.empty(B, Nq, device=Q.device)\n",
    "\n",
    "        grid = (Nq // Q_BLOCK, B)\n",
    "\n",
    "        flash_attention_fwd_kernel[grid](\n",
    "            Q, K, V, O, L,\n",
    "            Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "            K.stride(0), K.stride(1), K.stride(2),\n",
    "            V.stride(0), V.stride(1), V.stride(2),\n",
    "            O.stride(0), O.stride(1), O.stride(2),\n",
    "            L.stride(0), L.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_BLOCK=Q_BLOCK,\n",
    "            K_BLOCK=K_BLOCK,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, l = ctx.saved_tensors\n",
    "        is_causal = ctx.is_causal\n",
    "    \n",
    "        B, Nq, D = q.shape\n",
    "        Nk = k.shape[1]\n",
    "    \n",
    "        Bq = 16\n",
    "        Bk = 16\n",
    "        scale = D ** -0.5\n",
    "    \n",
    "        dq = torch.zeros_like(q)\n",
    "        dk = torch.zeros_like(k)\n",
    "        dv = torch.zeros_like(v)\n",
    "    \n",
    "        # D_i = sum_j dO_ij * O_ij\n",
    "        D_sum = torch.sum(o * do, dim=-1, keepdim=True)\n",
    "    \n",
    "        grid = (triton.cdiv(Nk, Bk), B)\n",
    "    \n",
    "        flash_bwd_kernel[grid](\n",
    "            q, k, v, o, l, do, D_sum,\n",
    "            dq, dk, dv,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            o.stride(0), o.stride(1), o.stride(2),\n",
    "            l.stride(0), l.stride(1),\n",
    "            do.stride(0), do.stride(1), do.stride(2),\n",
    "            D_sum.stride(0), D_sum.stride(1),\n",
    "            Nq, Nk,\n",
    "            scale,\n",
    "            D=D,\n",
    "            Q_TILE_SIZE=Bq,\n",
    "            K_TILE_SIZE=Bk,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "    \n",
    "        return dq, dk, dv, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444dbb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing d_model=16\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.04ms | Bwd:    0.55ms | Peak Mem: 1.0MB\n",
      "✓ T= 1024 | Fwd:   0.24ms | Bwd:    0.99ms | Peak Mem: 4.1MB\n",
      "✓ T= 4096 | Fwd:   3.30ms | Bwd:   13.12ms | Peak Mem: 16.3MB\n",
      "✓ T= 8192 | Fwd:  13.45ms | Bwd:   48.18ms | Peak Mem: 32.5MB\n",
      "✓ T=16384 | Fwd:  52.95ms | Bwd:  205.94ms | Peak Mem: 65.0MB\n",
      "\n",
      "==================================================\n",
      "Testing d_model=32\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.05ms | Bwd:    0.48ms | Peak Mem: 65.0MB\n",
      "✓ T= 1024 | Fwd:   0.29ms | Bwd:    1.20ms | Peak Mem: 65.0MB\n",
      "✓ T= 4096 | Fwd:   4.47ms | Bwd:   17.61ms | Peak Mem: 65.0MB\n",
      "✓ T= 8192 | Fwd:  16.74ms | Bwd:   66.03ms | Peak Mem: 65.0MB\n",
      "✓ T=16384 | Fwd:  65.42ms | Bwd:  258.39ms | Peak Mem: 129.0MB\n",
      "\n",
      "==================================================\n",
      "Testing d_model=64\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.06ms | Bwd:    0.51ms | Peak Mem: 129.0MB\n",
      "✓ T= 1024 | Fwd:   0.39ms | Bwd:    2.33ms | Peak Mem: 129.0MB\n",
      "✓ T= 4096 | Fwd:   5.85ms | Bwd:   32.14ms | Peak Mem: 129.0MB\n",
      "✓ T= 8192 | Fwd:  22.12ms | Bwd:  139.88ms | Peak Mem: 129.0MB\n",
      "✓ T=16384 | Fwd:  94.88ms | Bwd:  550.40ms | Peak Mem: 257.0MB\n",
      "\n",
      "==================================================\n",
      "Testing d_model=128\n",
      "==================================================\n",
      "✓ T=  256 | Fwd:   0.07ms | Bwd:    0.63ms | Peak Mem: 257.0MB\n",
      "✓ T= 1024 | Fwd:   0.71ms | Bwd:    8.85ms | Peak Mem: 257.0MB\n",
      "✓ T= 4096 | Fwd:  10.80ms | Bwd:  142.33ms | Peak Mem: 257.0MB\n",
      "✓ T= 8192 | Fwd:  41.82ms | Bwd:  541.16ms | Peak Mem: 257.0MB\n",
      "✓ T=16384 | Fwd: 167.07ms | Bwd: 2187.29ms | Peak Mem: 513.0MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def benchmark_attention_flash(device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    batch_size = 8\n",
    "    d_models = [16, 32, 64, 128]\n",
    "    seqlens = [256, 1024, 4096, 8192, 16384]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for d_model in d_models:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing d_model={d_model}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for seqlen in seqlens:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # 构造输入（无多头）\n",
    "                Q = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True)\n",
    "                K = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True)\n",
    "                V = torch.randn(batch_size, seqlen, d_model, device=device, requires_grad=True)\n",
    "                \n",
    "                # Warm-up：包含 forward + backward，确保缓存预热\n",
    "                for _ in range(10):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    # 清零梯度，避免累积\n",
    "                    Q.grad = None\n",
    "                    K.grad = None\n",
    "                    V.grad = None\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # Forward timing（无梯度）\n",
    "                with torch.no_grad():\n",
    "                    start = time.perf_counter()\n",
    "                    for _ in range(100):\n",
    "                        out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                    torch.cuda.synchronize()\n",
    "                    forward_time = (time.perf_counter() - start) / 100\n",
    "                \n",
    "                # 记录 forward 后的峰值内存（作为参考）\n",
    "                mem_allocated = torch.cuda.max_memory_allocated() / 1024**2\n",
    "                \n",
    "                # Backward timing\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(100):\n",
    "                    out = FlashAttentionTriton.apply(Q, K, V)\n",
    "                    loss = out.sum()\n",
    "                    loss.backward()\n",
    "                    # 清零梯度\n",
    "                    Q.grad = None\n",
    "                    K.grad = None\n",
    "                    V.grad = None\n",
    "                torch.cuda.synchronize()\n",
    "                backward_time = (time.perf_counter() - start) / 100\n",
    "                \n",
    "                results.append({\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seqlen\": seqlen,\n",
    "                    \"forward_ms\": forward_time * 1000,\n",
    "                    \"backward_ms\": backward_time * 1000,\n",
    "                    \"total_ms\": (forward_time + backward_time) * 1000,\n",
    "                    \"mem_MB\": mem_allocated,\n",
    "                })\n",
    "                \n",
    "                print(f\"✓ T={seqlen:5d} | Fwd: {forward_time*1000:6.2f}ms | Bwd: {backward_time*1000:7.2f}ms | Peak Mem: {mem_allocated:.1f}MB\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    results.append({\n",
    "                        \"d_model\": d_model,\n",
    "                        \"seqlen\": seqlen,\n",
    "                        \"OOM\": True\n",
    "                    })\n",
    "                    print(f\"✗ T={seqlen:5d} | OOM\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            # 清理显存\n",
    "            del Q, K, V\n",
    "            try:\n",
    "                del out, loss\n",
    "            except NameError:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行\n",
    "results = benchmark_attention_flash()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf1f8f",
   "metadata": {},
   "source": [
    "如果报错out of resource: shared memory, Required: 115200, Hardware limit: 101376. Reducing block sizes or num_stages may help.\n",
    "\n",
    "适当将Q_BLOCK = 16 和 K_BLOCK = 16还有Bq = 16 ，Bk = 16调低，如果你的GPU非常给力，也可以适当调高，我用的是4060 -8G（笔记本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd13a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_profile import benchmark,profile\n",
    "def test_flash_AttentionTriton(B=4, N=4096, d=64, device=\"cuda\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    Q = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    K = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "    V = torch.randn(B, N, d, device=device, requires_grad=True)\n",
    "\n",
    "    def run():\n",
    "        O = FlashAttentionTriton.apply(Q, K, V)\n",
    "        loss = O.sum()\n",
    "        loss.backward()\n",
    "\n",
    "    t = benchmark(description= 'FlashAttentionTriton',run = run)\n",
    "    t1 = profile(description ='FlashAttentionTriton' , run = run, num_warmups = 3, with_stack = True)\n",
    "    \n",
    "    print(f\"[FlashAttentionTriton] B={B}, N={N}, d={d}, time={t*1000:.3f} ms\")\n",
    "    print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9c3b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在真正计时!\n",
      "单次耗时：19.52393849690755ms\n",
      "正在使用 cuda\n",
      "[FlashAttentionTriton] B=4, N=4096, d=64, time=19523.938 ms\n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                    FlashAttentionTritonBackward         1.67%     307.833us         2.47%     456.115us     456.115us      13.734ms        82.53%      13.775ms      13.775ms             1  \n",
      "                                                                flash_bwd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us      13.734ms        82.53%      13.734ms      13.734ms             1  \n",
      "                                                            FlashAttentionTriton         1.80%     331.868us        13.19%       2.432ms       2.432ms       2.779ms        16.70%       2.779ms       2.779ms             1  \n",
      "                                                      flash_attention_fwd_kernel         0.00%       0.000us         0.00%       0.000us       0.000us       2.779ms        16.70%       2.779ms       2.779ms             1  \n",
      "                                                                      aten::add_         0.14%      26.263us         0.30%      54.856us      18.285us      79.269us         0.48%      79.269us      26.423us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add...         0.00%       0.000us         0.00%       0.000us       0.000us      79.269us         0.48%      79.269us      26.423us             3  \n",
      "                                                                       aten::sum         0.60%     110.950us         1.07%     197.058us      98.529us      23.707us         0.14%      23.707us      11.854us             2  \n",
      "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native...         0.00%       0.000us         0.00%       0.000us       0.000us      22.973us         0.14%      22.973us      11.486us             2  \n",
      "                                                                     aten::fill_         0.18%      32.299us         0.53%      97.181us      24.295us      15.622us         0.09%      15.622us       3.906us             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      15.622us         0.09%      15.622us       3.906us             4  \n",
      "--------------------------------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 18.441ms\n",
      "Self CUDA time total: 16.642ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_flash_AttentionTriton()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cdd5f",
   "metadata": {},
   "source": [
    "下面是按你要求改写后的版本：整体改为连贯叙述，弱化“讲义/条目式”结构，减少无序列表，强调概念之间的因果关系和工程语境，读起来更像一篇技术说明而不是 AI 教程。\n",
    "\n",
    "---\n",
    "\n",
    "# Triton 的一般语法与编程模型说明\n",
    "\n",
    "Triton 是一种面向深度学习算子的领域专用语言（DSL）。它的目标并不是取代 PyTorch，而是在不直接编写 CUDA 的情况下，让开发者依然能够对 GPU 上的并行方式和内存访问模式进行精细控制。可以把 Triton 理解为介于高层框架与底层 CUDA 之间的一层：它保留了 Python 的表达方式，但暴露了足够多的硬件语义，使性能优化成为可能。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、program：Triton 中最核心的并行抽象\n",
    "\n",
    "在 Triton 中，最基本的执行单元叫作 program。每个 program 都可以通过\n",
    "\n",
    "```python\n",
    "pid = tl.program_id(axis)\n",
    "```\n",
    "\n",
    "获取自己在某一个并行维度上的编号。从映射关系上看，program 大致对应 CUDA 中的 thread block，而 `tl.program_id(axis)` 则类似于 `blockIdx.x` 或 `blockIdx.y`。\n",
    "\n",
    "不过，Triton 的 program 与 CUDA block 有一个本质差异：它天然是向量化的。一个 program 并不是在处理单个标量元素，而是负责一整块数据，也就是一个 tile。也正因为如此，在 Triton 中进行思考时，基本单位不再是“线程”，而是“一个 program 对应一块数据”。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、grid：显式声明并行的组织方式\n",
    "\n",
    "在调用 Triton kernel 时，常见的形式是：\n",
    "\n",
    "```python\n",
    "kernel[grid](...)\n",
    "```\n",
    "\n",
    "这里的 `grid` 本质上定义了要启动多少个 program，以及这些 program 在多个并行维度上的排列方式。`grid` 是一个多维元组，对应的每一个维度都可以通过 `tl.program_id(0)`、`tl.program_id(1)` 等方式在 kernel 内部访问。\n",
    "\n",
    "在实际使用中，开发者通常会把不同的 axis 映射为不同的语义维度，比如行块、batch 或 attention head。Triton 并不关心底层启动了多少线程，它关心的是一共启动了多少个 program，以及每个 program 负责处理哪一块逻辑数据。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、Triton 的内存访问模型\n",
    "\n",
    "在 Triton kernel 内部，所有传入的张量参数本质上都是裸指针，例如 `Q_ptr`、`K_ptr`、`V_ptr`。这些指针只包含内存地址，本身并不携带 shape 或 stride 等信息。因此，Triton 要求开发者显式地描述张量的逻辑形状、内存布局以及当前 program 应该访问的区域。\n",
    "\n",
    "为了完成这一点，Triton 提供了 `tl.make_block_ptr` 这一关键抽象：\n",
    "\n",
    "```python\n",
    "block_ptr = tl.make_block_ptr(\n",
    "    base,\n",
    "    shape,\n",
    "    strides,\n",
    "    offsets,\n",
    "    block_shape,\n",
    "    order\n",
    ")\n",
    "```\n",
    "\n",
    "这一步并不是简单的语法封装，而是在定义一个“可以被向量化加载和存储的二维 tile”。通过 `shape` 和 `strides` 描述整个张量的逻辑结构，再结合 `offsets` 和 `block_shape`，Triton 就能够明确当前 program 对应的是哪一块数据。`order` 则会影响内存访问顺序，从而决定是否能够实现良好的内存合并访问。\n",
    "\n",
    "一旦构造了 `block_ptr`，后续的 `tl.load` 和 `tl.store` 就会变成一次对齐的、向量化的内存操作，这也是 Triton 能够写出高性能 kernel 的基础。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、Triton 中“张量”的真实含义\n",
    "\n",
    "需要注意的是，Triton 中的“张量”并不是 PyTorch 的 Tensor，而是编译期 shape 已知的寄存器向量。例如：\n",
    "\n",
    "```python\n",
    "x = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "```\n",
    "\n",
    "这里的 `BLOCK_M` 和 `BLOCK_N` 必须在编译期就确定下来。这样的张量通常存放在寄存器或片上 SRAM 中，所有计算都是逐元素或按块展开的。这也是为什么 Triton 对很多参数都有 `tl.constexpr` 的要求。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、tl.constexpr：为什么编译期常量如此重要\n",
    "\n",
    "当一个参数被标记为 `tl.constexpr` 时，意味着它在 kernel 编译阶段就是已知的常量，而不是运行时再传入的值。对于 Triton 来说，这一点至关重要，因为 block 的 shape、循环展开方式、向量化宽度以及内存布局，都依赖这些值进行静态推导。\n",
    "\n",
    "经验上可以这样理解：凡是会影响 shape、循环次数或内存访问模式的参数，都必须是 `constexpr`；而仅仅参与数值缩放或计算的参数，则可以作为运行时变量存在。\n",
    "\n",
    "---\n",
    "\n",
    "## 六、算子语义：没有 Python 循环的逐元素并行\n",
    "\n",
    "Triton 中的运算本质上都是逐元素并行的。比如：\n",
    "\n",
    "```python\n",
    "x = tl.arange(0, BLOCK)\n",
    "```\n",
    "\n",
    "生成的并不是一个标量，而是一个向量 `[0, 1, 2, ..., BLOCK-1]`。类似地，表达式\n",
    "\n",
    "```python\n",
    "mask = q_idx >= k_idx\n",
    "```\n",
    "\n",
    "产生的是一个布尔矩阵，而不是单个判断结果。\n",
    "\n",
    "`tl.where` 的语义也是完全向量化的：对每一个位置独立地选择结果，而不是像 Python 那样进行控制流分支。\n",
    "\n",
    "---\n",
    "\n",
    "## 七、控制流：为什么 Triton 不鼓励数据相关的 if\n",
    "\n",
    "在 Triton 中，`if` 语句只有在条件是 `tl.constexpr` 时才是安全的。原因在于 Triton kernel 在本质上是一段静态程序，所有 program 都必须执行相同的指令流。如果控制流依赖于数据，就会破坏这种一致性。\n",
    "\n",
    "因此，与配置相关的分支可以通过 `constexpr if` 在编译期解决，而与数据相关的分支则必须通过 mask 来表达。这也是为什么 causal mask 需要写成 `tl.where(mask, S, -inf)`，而不能直接使用基于索引关系的 `if` 判断。\n",
    "\n",
    "---\n",
    "\n",
    "## 八、数学运算与矩阵乘法支持\n",
    "\n",
    "Triton 提供了 `tl.dot` 作为块级矩阵乘法的原语。它接受 `(M, K)` 和 `(K, N)` 形状的输入，输出 `(M, N)` 的结果，并在数据类型和 shape 合适的情况下自动映射到 tensor core。这一能力是实现 FlashAttention、GEMM 以及多种融合算子的基础。\n",
    "\n",
    "在广播行为上，Triton 与 NumPy 或 PyTorch 的规则基本一致，但前提是所有相关的 shape 都必须在编译期可以推导出来。例如，在已知 axis 和维度的情况下，`tl.max(..., keep_dims=True)` 是完全合法的。\n",
    "\n",
    "---\n",
    "\n",
    "## 九、Triton kernel 与 PyTorch Autograd 的分工\n",
    "\n",
    "Triton kernel 本身并不关心梯度，也不会保存中间状态。因此，在实际工程中，通常会把 Triton kernel 包裹在一个自定义的 `torch.autograd.Function` 中。前向阶段负责调用 Triton kernel 并保存必要的张量，反向阶段再调用对应的 Triton kernel 计算梯度。\n",
    "\n",
    "在这种结构下，Triton 专注于高性能数值计算，而 PyTorch 负责计算图管理和调度，两者的职责边界非常清晰。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
