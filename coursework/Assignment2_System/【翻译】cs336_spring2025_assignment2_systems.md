---
AIGC:
  Label: "1"
  ContentProducer: "001191330110MADHPQQY7L10000"
  ProduceID: "ExportRequest(files=[ExportFile(docType=1, fileType=0), ExportFile(docType=2, fileType=3)])"
  ReservedCode1: "79882ccc63c4e09aa727b8fe0700931fe5591fc336b5859236e8c1e58bb88b11"
  ContentPropagator: "001191330110MADHPQQY7L10000"
  PropagateID: "ExportRequest(files=[ExportFile(docType=1, fileType=0), ExportFile(docType=2, fileType=3)])"
  ReservedCode2: ""
---
### CS336 作业2（系统）：系统与并行性 
版本 1.0.4 
2025年春季 
### 1  任务概述 
在本次任务中，你将获得一些实际经验，学习如何提升单GPU训练速度，并将训练扩展至多GPU环境。 
你将要实现的内容。 
1. 基准测试与性能分析工具链 
2. Flash Attention 2 Triton内核 
3. 分布式数据并行训练 
4. 优化器状态分片 
代码的外观如下……所有作业代码以及本文档均可在 GitHub 上找到：github.com/stanford-cs336/assignment2-systems 
请执行 `git clone` 命令克隆仓库。如有更新，我们将及时通知您，届时您可以运行 `git pull` 获取最新版本。 
1. cs336-basics/:: 在本次作业中，你将对我们在作业1中构建的一些组件进行性能分析。此文件夹包含作业1的教职员工解决方案代码，因此你会在这里找到一个cs336-basics/pyproject.toml文件，以及一个cs336-basics/cs336_basics/*模块。如果你希望使用自己实现的模型，可以修改基础目录中的pyproject.toml文件，使其指向你自己的软件包。 
2. /: cs336-systems 基础目录。我们创建了一个名为 cs336_systems 的空模块。请注意，这里没有任何代码，因此你可以从头开始自由地进行任何操作。 
3. tests/*.py：这里包含了所有你必须通过的测试。这些测试会调用在`tests/adapters.py`中定义的钩子。你需要实现这些适配器，以将你的代码与测试连接起来。编写更多测试和/或修改测试代码有助于调试你的代码，但你的实现仍需确保能顺利通过最初提供的测试套件。 
4. README.md：此文件包含有关预期目录结构的更多详细信息，以及一些关于如何设置开发环境的基本说明。 
如何提交……您将把以下文件提交至 Gradescope： 
●撰写报告.pdf：请回答所有书面问题，并务必使用电子文档形式提交您的答案。 
· code.zip：包含你所编写的所有代码。 
运行 test_and_make_submission.sh 中的脚本，以创建 code.zip 文件。 
在作业的第一部分，我们将探讨如何优化Transformer模型的性能，以实现对GPU的最高效利用。我们会对模型进行性能分析，了解其在前向和反向传播过程中分别消耗了哪些时间和内存；随后，我们将通过自定义GPU内核来优化自注意力操作，使其速度超过直接使用PyTorch的实现方式。在作业的后续部分，我们将进一步利用多块GPU。 
### 1.11 画像分析与基准测试 
在实施任何优化之前，先对程序进行性能分析非常有帮助，以便了解它将资源（如时间和内存）耗费在哪些地方。否则，我们可能会误将模型中那些实际并不占用大量时间和内存的环节加以优化，从而无法观察到可衡量的端到端性能提升。 
我们将实施三条性能评估路径：(a) 使用Python标准库进行简单、端到端的基准测试，以计时我们的前向和反向传播；(b) 利用NVIDIA Nsight Systems工具分析计算性能，了解时间在CPU和GPU上各操作间的具体分布情况；(c) 分析内存使用情况。 
### 1.1.11设置 - 导入您的基础变压器模型 
首先，确保你能成功加载上一节课的模型。在上一节课中，我们将模型设置在一个Python包内，以便日后能够轻松导入。我们已在./cs336-basics文件夹中添加了该模型的官方实现，并在pyproject.toml文件中进行了指向配置。只需像往常一样调用`uv run [命令]`，UV便会自动找到本地的cs336-basics包。如果你希望使用自己实现的模型，可以修改pyproject.toml文件，将其指向你自己的包。 
你可以通过以下方式测试是否能够导入你的模型： 
```
 ~$ 运行 python 
使用 CPython 3.12.10 
创建虚拟环境于：/path/to/uv/env/dir 
         构建 cs336-系统 @ file:///path/to/systems/dir 
         构建了 cs336-基础 @ file:///path/to/basics/dir 
安装了85个软件包，耗时711毫秒 
Python 3.12.10（主分支，2025年4月9日04:03:51）[Clang 20.1.0]，运行于Linux系统 
··· 
>>>>import cs336_basics 
>>> 
```

现在，作业1中的相关模块应已可用（例如，对于model.py，你可以通过import cs336_basics.model来导入）。 

### 1.1.21 模型尺寸

在本次作业中，我们将对模型进行基准测试和性能分析，以更深入地了解它们的表现。为了掌握规模变化下的各项指标，我们将参考并使用以下模型配置。对于所有模型，我们将采用10,000的词汇量和4的批大小，并尝试不同的上下文长度。此外，本次作业（以及后续作业）需要大量结果以表格形式呈现。我们强烈建议您通过代码自动化生成报告所需的表格，因为手动在LaTeX或Markdown中排版表格会非常繁琐。您可以参考`pandas.DataFrame.to_latex()`和`pandas.DataFrame.to_markdown()`，或者自行编写函数，直接从您偏好的表格格式中生成这些表格。 

![alt text](image.png)

### 1.1.3 端到端基准测试 

现在，我们将实现一个简单的性能评估脚本。由于我们将测试模型的多种变体（如调整精度、替换层等），因此通过命令行参数使脚本支持这些变化将非常有益，以便日后轻松运行。此外，我们强烈建议使用 Slurm 的 sbatch 或 submitit 工具，对模型规模、上下文长度等基准超参数进行快速迭代的扫描实验。 
首先，我们通过计时前向和反向传播来对模型进行最简单的性能分析。由于我们只关注速度和内存的测量，因此将使用随机权重和数据。 

衡量性能是一件微妙的事情——一些常见的陷阱可能会导致我们无法准确测量真正想要的结果。在对GPU代码进行基准测试时，一个需要注意的事项是：CUDA调用是异步的。当你调用一个CUDA内核，比如执行`torch.matmul`时，该函数调用会立即将控制权返回给你的代码，而不会等待矩阵乘法运算完成。这样一来，CPU可以在GPU执行矩阵乘法的同时继续运行。但另一方面，这意味着简单地测量`torch.matmul`调用的返回时间，并不能反映GPU实际执行矩阵乘法所需的时间。在PyTorch中，我们可以调用`torch.cuda.synchronize()`来确保所有GPU内核都已完成，从而获得更精确的CUDA内核运行时间测量结果。基于这一点，让我们来编写我们的基本性能分析基础设施吧。 

交付物：用1到2句话说明你的时间安排。

### 1.1.4 Nsight Systems 性能分析器 

端到端基准测试无法告诉我们模型在前向和反向传播过程中分别花费了哪些时间和内存，因此也无法揭示具体的优化机会。为了了解程序在各个组件（例如函数）上各耗时多少，我们可以使用性能分析工具——Profiler。执行Profiler会通过在函数开始和结束时插入检测代码来对目标代码进行插桩，从而提供详细的函数级执行统计信息，如调用次数、平均运行时间，以及该函数累计占用的总时间等。 
标准的 Python 性能分析工具（例如 CProfile）无法对 CUDA 内核进行性能分析，因为这些内核是在 GPU 上异步执行的。幸运的是，NVIDIA 提供了一款性能分析工具，我们可以通过命令行工具 nsys 来使用，而这款工具我们已为您安装好。在本次作业中，您将利用 nsys 来分析您的 Transformer 模型的运行时性能。使用 nsys 非常简单：只需在先前章节的 Python 脚本前加上 nsys profile 命令即可。例如，您可以对脚本 benchmark.py 进行性能分析，并将结果输出到文件 result.nsys.rep 中，具体命令如下： 
1 ~$1uv run nsys profile -o result python benchmark.py 
然后，您可以在本地机器上使用NVIDIA Nsight Systems桌面应用程序查看该性能分析报告。在性能分析报告的CUDA API行中，选择某个特定的CUDA API调用（位于CPU上），即可高亮显示CUDA HW行中所有对应的GPU内核执行情况。 
我们鼓励您尝试使用 nsys profile 的各种命令行选项，以便深入了解其功能。特别值得一提的是，通过添加 `--python-backtrace=cuda` 参数，您可以获取每个 CUDA API 调用的 Python 回溯信息，但请注意，这可能会带来一定的性能开销。此外，您还可以用 NVTX 区间为代码添加注释，这些区间会在捕获所有 CUDA API 调用及相应内核执行的 NVTX 行中显示为独立的区块。尤其建议您利用 NVTX 区间来忽略基准测试脚本中的预热步骤（只需在分析报告的 NVTX 行上应用过滤条件即可）。同时，您还能明确哪些内核负责模型的前向和反向传播过程；甚至可以通过如下方式为您的实现添加注释，从而进一步区分自注意力层中不同部分所对应的内核： 
```
...
import torch.cuda.nvtx as nvtx

@nvtx.range("scaled dot product attention")
def annotated_scaled_dot_product_attention(
    ...
):  # Q, K, V, mask（查询、键、值、掩码）
    ...

    with nvtx.range("computing attention scores"):
        ...  # 计算 Q 与 K 之间的注意力分数

    with nvtx.range("computing softmax"):
        ...  # 对注意力分数计算 softmax

    with nvtx.range("final matmul"):
        ...  # 计算最终的输出投影

    return ...


```
您可以通过以下方式，在基准测试脚本中用带注释的版本替换您的原始实现： 
1  cs336_basics.model.scaled_dot_product_attention = 注释化缩放点积注意力 
最后，你可以使用 `--pytorch` 命令行选项配合 nsys，自动为 PyTorch C++ API 的调用添加 NVTX 范围注释。 
### 1.1.5 混合精度 
到目前为止，在本次任务中，我们一直以FP32精度运行——模型的所有参数和激活函数都采用torch.float32数据类型。然而，现代NVIDIA GPU配备了专门的GPU核心（Tensor Cores），能够以更低的精度加速矩阵乘法运算。例如，NVIDIA A100的产品规格表显示，其FP32模式下的最大吞吐量为19.5 TFLOP/秒，而使用FP16（半精度浮点数）或BF16（脑浮点数）时，最大吞吐量则显著提升至312 TFLOP/秒。因此，采用低精度数据类型将有助于加快训练和推理的速度。 
然而，将模型简单地转换为较低精度格式可能会导致模型精度下降。例如，实际中许多梯度值往往小到无法用FP16表示，因此在使用FP16精度进行训练时会直接变为零。为应对这一问题，通常会在FP16训练中采用损失缩放技术——即简单地将损失乘以一个缩放因子，从而增大梯度的幅度，避免其被舍入为零。此外，FP16的动态范围低于FP32，这可能导致溢出现象，最终表现为损失值为NaN。相比之下，完全使用bfloat16进行训练通常更为稳定（因为BF16的动态范围与FP32相同），但即便如此，与FP32相比，最终的模型性能仍可能受到影响。 
为了利用低精度数据类型的加速优势，通常采用混合精度训练。在PyTorch中，这可以通过`torch.autocast`上下文管理器来实现。在这种情况下，某些操作（例如矩阵乘法）会以低精度数据类型执行，而其他需要FP32完整动态范围的操作（例如累加和约简）则保持原样。例如，以下代码将自动识别前向传播过程中哪些操作应以低精度执行，并将这些操作转换为指定的数据类型： 
模型：

```python
model: torch.nn.Module = ...  # 例如：你的 Transformer 模型
dtype: torch.dtype = ...      # 例如：torch.float16
x: torch.Tensor = ...         # 输入数据

with torch.autocast(device="cuda", dtype=dtype):
    y = model(x)
```

正如上文所述，即使待累积的张量已被降级类型，通常仍建议以更高的精度保存累积结果。以下练习将帮助你逐步理解其中的原因。 
现在，我们将首先将混合精度应用于一个玩具模型，以帮助理解，然后再应用到我们的基准测试脚本中。 

### 1.1.6 内存剖析 
到目前为止，我们一直在关注计算性能。接下来，我们将把注意力转向内存，这是语言模型训练和推理中另一个关键资源。此外，FPyTorch还配备了一个功能强大的内存分析器，能够实时跟踪内存分配情况。 
要使用内存分析器，您可以按如下方式修改您的基准测试脚本： 
```python
...  # 在你的基准测试脚本中的预热阶段

# 开始记录显存（内存）历史
torch.cuda.memory._record_memory_history(max_entries=1000000)

...  # 在你的基准测试脚本中你想要进行分析的部分

# 保存一个 pickle 文件，用于 PyTorch 的在线分析工具加载
torch.cuda.memory._dump_snapshot("memory_snapshot.pickle")

# 停止记录历史
torch.cuda.memory._record_memory_history(enabled=None)

```

这将输出一个文件 memory_snapshot.pickle，你可以将其加载到以下在线工具中：https://pytorch.org/memory_viz。该工具可帮助你查看整体的内存使用时间线，以及每次单独的内存分配情况——包括其大小和指向代码来源的堆栈跟踪信息。要使用此工具，你应将上述链接在网页浏览器中打开，然后将你的 Pickle 文件拖放到页面上。 

### 1.2 使用FlashAttention-2优化注意力机制 
### 1.2.1 PyTorch注意力基准测试 

你的分析很可能表明，你的注意力层在内存和计算方面都存在优化空间。从高层次来看，注意力操作包括一次矩阵乘法，接着是Softmax函数，然后再进行一次矩阵乘法： 
朴素的注意力实现需要为每个批次/头元素保存形状为`seq_len x seq_len`的注意力分数矩阵，而当序列长度较长时，这些矩阵会迅速膨胀，导致任何处理长输入或长输出的任务都可能引发内存不足的错误。为此，我们将按照FlashAttention-2论文中的方法实现一个注意力核，该核通过分块方式计算注意力，完全避免显式地生成`seq_len × seq_len`大小的注意力分数矩阵，从而支持扩展到更长的序列长度。 

### 1.3 基准测试即时编译注意力 

自2.0版本起，PyTorch还内置了一个功能强大的即时编译器，可自动尝试对PyTorch函数应用多种优化：详情请参阅https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html以了解入门介绍。特别是，它会通过动态分析你的计算图，自动生成融合的Triton内核。使用PyTorch编译器的接口非常简单。例如，若想将其应用于模型中的单个层，我们可以这样操作： 
11层 = 某个PyTorch模块（……） 
2  compiled_layer = torch.compile(layer) 
现在，compiled_layer 的功能与 layer 完全一致（例如，具备前向和反向传播功能）。我们还可以使用 `torch.compile(model)` 编译整个 PyTorch 模型，甚至可以直接编译调用 PyTorch 操作的 Python 函数。 
问题（torch_compile）：2分 
(a) 扩展你的注意力基准测试脚本，加入你用PyTorch实现的注意力机制的编译版本，并将其性能与未编译版本进行对比，两者均采用与上述pytorch_attention问题相同的配置。 
交付物：一张表格，对比您已编译的注意力模块与上述 PyTorch_attention 问题中未编译版本的正向和反向传递时间。 
(b)现在，在你的端到端基准测试脚本中编译整个Transformer模型。前向传播的性能有何变化？而前向与反向传播以及优化器步骤的联合表现又如何？ 
交付成果：一张对比您原始版与编译版Transformer模型的表格。 
鉴于我们观察到的序列长度相关的缩放行为，我们需要大幅改进，以有效处理长序列。即便使用了`torch.compile`，当前实现仍面临长序列时极差的内存访问模式问题。为此，我们将基于Triton编写FlashAttention-2的实现，在新版本中将能更灵活地控制内存访问方式，以及明确何时进行具体计算。 
### 1.3.1 示例 - 加权总和 
为了介绍你需要了解的关于Triton及其与PyTorch互操作性的内容，我们将通过一个“加权求和”操作的示例内核来逐步讲解。如需更多帮助以快速掌握Triton，可参阅Triton的教程。需要注意的是，这些教程并未采用我们接下来将详细介绍的全新、便捷的块指针抽象机制。 
给定输入矩阵X，我们将按列将其元素与权重向量w相乘，并对每一行求和，从而得到矩阵X与向量w的矩阵-向量乘积。首先，我们来详细推导这一操作的前向传播过程，随后再编写用于反向传播的Triton内核。 
前向传播 我们的核函数的前向传播步骤，正是以下经过广播的内积运算。 
1   定义加权和函数(x, 权重): 
2       # 假设c的形状为[...,D]，而weight的形状为[1D]3          return (weight * x).sum(axis=-1) 
在编写我们的Triton内核时，我们将让每个程序实例（可能并行运行）计算矩阵s中一行块的加权和，并将相应的标量输出写入输出张量。在Triton中，一个程序实例是一组线程块，这些线程块均执行相同的程序，并且这些线程块可在GPU上并行运行。此外，我们不直接以张量作为参数，而是传递指向它们第一个元素的指针，以及用于指示如何沿各轴移动的步长信息。 
我们可以利用步长加载与当前实例中正在求和的矩阵a的行块相对应的张量，并通过程序ID来分配工作（即，第i个实例将处理r的第i个行块）。在这一简单情况下，Triton与PyTorch前向传播的主要区别在于：前者需要进行指针算术运算，并显式地执行加载和存储操作。我们将使用块指针抽象方法，结合 

![image](https://damo-moshicloud-test.oss-cn-hangzhou.aliyuncs.com/document/testcase/dingding/zhiwen_cases/1407364148115021824/1407364148115021824_cut_Figure_1_0_.png)
 
tl.make_block_ptr 可大幅简化指针运算，尽管这意味着我们需要进行一些设置来准备块指针。 
请参阅图1，了解拼贴的示意图以及如何推进块指针。上述加权求和函数如下所示： 
1  导入 Triton 
2   导入 triton.language 作为 tl 
4   @triton.jit 
5   def 加权和前向计算 
6      x_ptr, weight_ptr,  # 输入指针 
7        输出指针，# 输出指针 
8     x_stride_row, x_stride_dim,  # 步长告诉我们如何在张量的每个维度上移动一个元素 
9        weight_stride_dim,  # 可能为1 
10        输出步长行，# 可能是1 
11 行，D， 
12     ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # 模板形状必须在编译时已知 
13）： 
14       # 每个实例将计算一行瓷砖的加权和。 
15       #tl.program_id 为我们提供了一种方法，用于检查当前正在运行的线程块 
16        行块索引=程序ID(0) 
17 
18       # 块指针为我们提供了一种从多维内存区域中进行选择的方式 
19        # 并移动我们的选择以调整位置。 
20        # 块指针必须知道： 
21        # - 指向张量第一个元素的指针 
22       #一处理越界访问的张量整体形状 
22 
25 
26 
27 
#一正确使用内存布局的各维度步长#二起始块的ND坐标，即“偏移量”——每次加载/存储时使用的块形状；内存中从主到次的维度顺序——为了优化，可使用ces（= np.argsort(strides）），尤其在H100上效果显著 
28 
x_block_ptr = tl.make_block_ptr( 
x指针， 
31            形状=(行数, D,), 
32 
33 
步幅=(x_行步幅, x_步长维度)，偏移量=(行块索引 * 行块大小, 0)， 
34           block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE), 
35            order=(1,0), 
36        ) 
37 
38 
39 
权重块指针 = tl.make_block_ptr(权重指针, 
40 
41 
42 
形状=(D,)，步幅=(权重步幅维度,)，偏移量=(0,)， 
43            block_shape=(D_TILE_SIZE,), 
44            order=(0,), 
45        ) 
46 
47       output_block_ptr = t1.make_block_ptr( 
48             输出指针， 
49            shape=(ROWS,), 
步长=(输出步长行,), 
偏移量=（行瓦片索引 * 行瓦片大小，） 
52            block_shape=(ROWS_TILE_SIZE,), 
53            order=(0,), 
54        ) 
55 
56       # 初始化一个用于写入的缓冲区 
57       output = tl.zeros((ROWS_TILE_SIZE,), dtype=t1.float32) 
58 
59        for i in range(tl.cdiv(D, D_TILE_SIZE))： 
60            #加载当前块指针 
61            # 由于ROWS_TILE_SIZE可能无法整除ROWS，而D_TILE_SIZE也可能无法整除D， 
# 我们需要对两个维度都进行边界检查 
row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option="zero")  # (ROWS_TILE_SIZE, D_TILE_SIZE) weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option="zero")  # (D_TILE_SIZE,) 
65 
66         # 计算该行的加权和。 
67             output += tl.sum(row * weight[None, :], axis=1) 
68 
69            # 将指针移至下一个方块。 
70           # 这些是（行、列）坐标增量 
71         x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # 沿最后一个维度移动 D_TILE_SIZE 尺度 
72        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # 按 D_TILE_SIZE 移动 
73 
74       #将输出写入输出块指针（每行一个标量）。 
# 由于 ROWS_TILE 大小可能无法整除 ROWS，因此我们需要进行边界检查 
tl.store(output_block_ptr, output, boundary_check=(0,)) 
现在，让我们将这个内核封装进一个 PyTorch Autograd 函数中，以便与 PyTorch 无缝互操作（即接受张量作为输入，输出一个张量，并在反向传播过程中也能顺利配合自动微分引擎工作）： 
class WeightedSumFunc(torch.autograd.Function): 
D，输出维度 = x.shape[-1]，x.shape[:-1] 
# 将输入张量重塑为2D 
输入形状 = x.shape 
x = 重排(x, d ->(...) d") 
ctx.save_for_backward(x, weight) 
确保权重的形状为一维，且其长度等于D，“维度不匹配”  
确保x和权重均为CUDA张量，“预期为CUDA张量” 
确保 `x.is_contiguous()`，否则“我们的指针算术将假定 `x` 是连续的” 
ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16 # 大致每轮处理嵌入维度中的16个元素 ctx.ROWS_TILE_SIZE = 16  # 每个线程一次处理16个批次元素 ctx.input_shape = input_shape 
# 需要初始化空的结果张量。请注意，这些元素不一定是0！ 
y = torch.empty(output_dims, device=x.device) 
# 在我们的一维网格中启动具有n个实例的内核。 
行数 = y.numel() 
加权和前向传播[(cdiv(行数, 上下文.行块大小))] 
x，重量， 
y， 
x.strides(0), x.strides(1), 
权重的步长为0， 
y的步长(0)， 
行数=n_rowS，维度=D， 
ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE，D_TILE_SIZE=ctx.D_TILE_SIZE， 
返回 y 的视图，形状为 input_shape 的前缀部分。 
请注意，当我们调用带有 weighted_sum_fwd[(cdiv(n_rows, ctx.ROWS_TILE_SIZE),)] 的 Triton 内核时，通过传递元组 (cdiv(n_rows, ctx.ROWS_TILE_SIZE),)，我们定义了一个所谓的“启动网格”——即线程块的布局。随后，在内核中，我们可以使用 tl.program_id(0) 来访问当前的线程块索引。 
反向传播由于我们正在自定义内核，因此还需要编写自己的反向函数。 
在前向传播中，我们获得了层的输入，并需要计算其输出。而在反向传播中，回想一下，我们会得到目标函数关于输出的梯度，接着需要计算该梯度相对于每个输入的导数。在本例中，我们的操作输入是一个矩阵 \( z \in \mathbb{R}^{n \times h} \) 和一个权重向量 \( w \in \mathbb{R}^h \)。简而言之，我们将这一操作记为 \( f(a, w) \)，其输出范围为 \( \mathbb{R}^m \)。假设我们已知损失 \( L \) 关于该层输出的梯度 \( \nabla_{f(a,w)}L \)，那么我们可以应用多元链式法则，推导出关于 \( x \) 和 \( w \) 的梯度表达式如下： 
k=1 
这提供了一个计算反向传播的简单公式。要得到关于x的反向步骤，我们应用公式2，并取w与Vf(x,w)的外积。而要计算关于w的反向步骤（即(Vu_)），则需将输入梯度与对应的输出行相乘。 
我们的反向传播内核将首先定义所有块指针，然后计算VL： 
### @triton.jit 
2   def 加权和的反向传播 
3       x_ptr, weight_ptr, # 输入 
4       grad_output_ptr,,# 梯度输入 
5      grad_x_ptr，partial_grad_weight_ptr，# 梯度输出 
6         stride_xr, stride_xd, 
7          步幅_wd, 
8        步幅_gr, 
9         行距_gxr，行距_gxd， 
10        stride_gwb, stride_gwd, 
11 
12 
NUM_ROWS, D, ROWS_TILE_SIZE：tl. constexpr，D_TILE_SIZE：tl.constexpr， 
13）： 
14        行块索引 = tl.程序ID(0) 
15        n_row_tiles = tl.num_programs(0) 
16 
17        # 输入 
18 
grad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(NUM_ROWS,), strides=(stride_gr,), offsets=(row_tile_idx * ROWS_TILE_SIZE,), block_shape=(ROWS_TILE_SIZE,), order=(0,), 
x_block_ptr = tl.make_block_ptr(x_ptr, shape=(NUM_ROWS, D), strides=(stride_xr, stride_xd), offsets=(row_tile_idx * ROWS_TILE_SIZE, 0), block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE), order=(1, 0), 
weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(D,), strides=(stride_wd,), offsets=(0,), block_shape=(D_TILE_SIZE,), 
订单=(0,), 
grad_x_block_ptr = tl.make_block_ptr( 
梯度_x_指针， 
shape=(NUM_ROWS, D)，strides=(stride_gxr, stride_gxd)，offsets=(row_tile_idx * ROWS_TILE_SIZE, 0)，block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE)， 
订单=(1,0), 
部分梯度权重块指针 = t1.创建块指针 
部分梯度权重指针， 
shape=(n_row_tiles, D,), strides=(stride_gwb, stride_gwd), 
偏移量=(行瓦片索引, 0), 
53            block_shape=(1, D_TILE_SIZE), 
54            订单=(1,0), 
55        ) 
for i in range(tl.cdiv(D, D_TILE_SIZE)):  
    grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,), padding_option="zero")##(ROWS_TILE_SIZE,) 
60             # 用于grad_公的外积 
61       weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option="zero"))# (D_TILE_SIZE,) 
梯度_x_行 = 梯度输出[:, None] * 权重[None,:] 
tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1)) 
64 
65 
row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option="zero")  # (ROWS_TILE_SIZE, D_TILE_SIZE) grad_weight_row = t1.sum(row * grad_output[:, None], axis=0, keep_dims=True) 
tl.store(partial_grad_weight_block_ptr, grad_weight_row, boundary_check=(1,))  # 永远不会超出第0维的边界 
# 将指针沿D方向移动到下一个方块 
x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE)) 
权重块指针 = 权重块指针向前移动((D_TILE_SIZE,)) 
部分梯度权重块指针 = 部分梯度权重块指针向前移动 (O, D_TILE_SIZE) 
grad_x_block_ptr = grad_x_block_ptr.advance((0, D_TILE_SIZE)) 
计算梯度V非常简单，我们将结果写入输出张量的相应分块中。然而，计算Vw则稍显复杂。每个内核实例负责z的一个行分块，但我们现在需要对a的各行进行求和。为了不直接在反向传播过程中执行这一求和操作，我们将假设`partial_grad_weight_ptr`包含一个n_row_tiles × H的矩阵，其中第一维仅在o的行分块内被缩减。因此，我们先在当前行分块内完成缩减，再将结果写入该张量。而在内核外部，我们会使用PyTorch的`torch.sum`函数，对每个行分块的结果进行累加，最终完成Ww的计算。至此，自动微分函数（autograd.Function）的最后一部分便显得相当简单了： 
1  f_weightedsum = WeightedSumFunc.apply 
现在，对两个 PyTorch 张量 a 和 w 调用 f_weightedsum，将得到如下张量：tensor([ 90.8563, -93.6815, -80.8884, ..., 103.4840, -21.4634, -24.0192], 
2           device='cuda:0', grad_fn=<WeightedSumFuncBackward>) 
请注意该张量上附加的grad_fn——这表明当此张量出现在计算图中时，PyTorch知道在反向传播阶段该调用什么。至此，我们已完成对加权求和操作的Triton实现。 
### 1.3.2 FlashAttention-2 前向传播 
你将用显著改进的Triton实现替换现有的PyTorch注意力机制，该实现遵循FlashAttention-2 [Dao, 2023]。FlashAttention-2采用了一些技巧，以分块方式计算前向传播，从而实现高效的内存访问模式，并避免在全局内存中显式存储完整的注意力矩阵。 
在进入本节之前，我们强烈建议您至少阅读一下原始的FlashAttention论文[Dao et al., 2022]，这将帮助您深入理解FlashAttention实现高效注意力机制的核心技术：即以分块方式在线计算Softmax（这一技术由[Milakov和Gimelshein, 2018]提出）。此外，我们也推荐您参阅He [2022]，以进一步了解GPU如何实际执行PyTorch代码的运作原理。 
理解普通注意力机制中的效率低下问题。请注意，注意力机制的前向传播过程（暂不考虑掩码）可以表示为： 
### 标准的反向遍历是 
dV = Pdω 
dP = dOV' 
我们可以看到，反向传播依赖于前向传播中生成的一些非常庞大的激活值。例如，在公式（7）中计算dV时，需要用到P——即注意力分数，其形状为（batch_size, n_heads, seq_len, seq_len）。而这一激活矩阵的大小却与序列长度呈平方关系，这也正是我们在上述针对长序列长度的注意力机制进行基准测试时遇到内存问题的根本原因。在标准的Vanilla注意力机制的前向和反向传播过程中，我们都需要付出巨大的内存I/O开销，频繁地在片上SRAM与GPU HBM之间传输P及其他大型激活数据。在典型的实现中，这样的数据传输甚至多达数次：比如，在标准的反向传播实现中，P会在计算公式（7）和（9）时分别从HBM中读取。 
FlashAttention 的主要目标是避免将注意力矩阵频繁地读写到高带宽内存（HBM）中，从而降低I/O开销和峰值内存成本。我们通过三种技术实现这一目标：分块处理、重新计算以及算子融合。 
铺 tile……为避免将注意力矩阵反复读写到 HBM，我们采用一种无需访问完整输入即可计算 softmax 约减的方法。具体而言，我们将注意力计算过程重新组织，将输入分割成多个 tile，并分多次遍历这些 input tile，从而逐步完成 softmax 约减操作。 
重新计算。1. 我们避免在HBM中存储形状为(batch_size, n_heads, seq_len, seq_len)的大型中间注意力矩阵。相反，我们会在HBM中保存部分“激活检查点”，然后在反向传播过程中重新计算前向传播的一部分，以获取计算梯度所需的其他激活值。FlashAttention-2还存储了注意力分数的logsumexp值L，这将有助于后续计算。 
在我们的最终内核中，我们将以在线方式计算这一过程，但最终结果应保持一致。通过结合分块处理与重新计算，我们的内存I/O和峰值占用将不再依赖于序列长度的平方，因此我们可以使用更长的序列长度。 
算子融合……最后，我们通过在一个统一的内核中完成所有操作，避免了注意力矩阵及其他中间激活值的重复内存I/O访问——这被称为算子或内核融合。我们将为前向传播过程编写一个单独的Triton内核，该内核能在HBM与SRAM之间实现有限的数据传输，同时完成注意力机制涉及的所有计算任务。算子融合在一定程度上得益于重新计算技术，因为这样我们无需像传统方式那样将每个中间激活值都存储到HBM中，从而节省了宝贵的内存带宽。如需进一步理解这些技术的原理，可参阅FlashAttention相关论文：Dao et al., 2022，以及Dao, 2023。 
通过重新计算实现反向传播。1. 利用L，我们可以高效地进行适当重计算并完成反向传播。在开始反向传播之前，我们预先将值D = rowsum(0 o dO)（其中o表示逐元素乘法）存入全局内存——由于PdP儿 = (PV)do = 0do（且对于任意矩阵A和B，有rowsum(A o B) = diag(ABT)），这一值实际上等于rowsum(PodP)。有了L和D这两个向量，反向传播便无需再使用Softmax函数即可完成。目前，反向传播的完整计算过程如下： 
dP = dOV'                                (16) 
我们可以看到，这一系列操作并不需要我们在前向传播过程中将注意力分数P存储在HBM中——我们只需根据公式（13）和（14）中的激活值Q、K和L重新计算它们。 
现在，我们已经对FlashAttention-2中所用的技术有了大致了解，接下来将深入探讨你即将实现的FA2前向传播核函数的具体细节。为了避免频繁地在HBM中读写注意力矩阵，我们希望采用分块处理的方式，即独立计算输出矩阵的每个分块，互不干扰。为此，我们需要能够高效地计算出P矩阵的各个分块，理想情况下，这些分块应在查询和键两个维度上均采用分块化设计。 
然而，当我们对S应用softmax时，需要将S的整行数据归一化以计算softmax分母，这意味着我们无法直接按块计算P。FlashAttention-2通过在线softmax解决了这一问题。在接下来的文本中，我们将用下标索引i表示当前的查询块，用上标索引(j)表示当前的键块。其中，沿查询维度的块大小为Bq，而沿键维度的块大小为B。我们不会在隐藏维度d上进行分块处理。 
我们还保留了一些按行累积的值，其中m；i ∈ RBq，且l(j)i ∈ RB。这些按行计算的mi值是动态的最大值，我们会持续跟踪，以便以数值稳定的方式计算Softmax（还记得我们在作业1中实现Softmax时用到的技巧吗？）。每当我们处理S的新的按行分块时（即当j增加时），都会更新mi的值。通过使用这个动态最大值，我们便能计算出未归一化的Softmax值。 
🟥ij − m(j)（分子）作为 P() = exp(Sij - m)）。其中，1 是 softmax 分母的近似值，将在处理完所有关键块后，通过除以 1 来完成最终归一化，得到 l 的最终值。 
算法1 FlashAttention-2 前向传播 
返回输出 O 和对数求和指数 L。 
在我们开始在Triton中实现前向传播之前，这里先为大家整理了一些编写Triton内核的通用技巧与窍门。 
(a) 编写一个纯PyTorch（不使用Triton）的autograd.Function，以实现FlashAttention-2 
前向传播。这将比常规的 PyTorch 实现慢很多，但会有所帮助 
你正在调试你的Triton内核。 
你的实现应接收输入 Q、K 和 V，以及一个标志 is_causal，并生成 
输出 O 和 LogSumExp 值 L。在本任务中，您可以忽略 is_causal 标志。 
自动微分函数的前向传播应随后保存L、Q、K、V和O，以供反向传播使用，并且 
返回 O。请记住，这是 autograd.Function 的 forward 方法的实现。 
始终将上下文作为其第一个参数。任何 autograd.Function 类都需要 
实现一种反向方法，但目前你可以让它直接抛出 NotImplementedError。 
如果你需要作为对比的参考，可以实现式（4）至（6）以及式（12）。 
PyTorch，并比较你的输出。 
然后，接口会调用 def forward(ctx, Q, K, V, is_causal=False)。请自行确定。 
瓦片大小，但请确保其至少为16×16。我们将始终使用以下内容测试您的代码： 
尺寸必须是2的幂且至少为16，因此您无需担心。 
越界访问。 
交付物：一个继承自torch.autograd.Function的子类，用于实现FlashAttention-2。 
前向传播。要测试你的代码，请实现 
[adapters.get_flashattention_autograd_function_pytorch]。然后，使用uv运行测试。 
运行 `runTpytest -k test_flash_forward_pass_pytorch`，并确保你的实现。 
把它传递过去。 
(b) 按照算法1，为FlashAttention-2的前向传播编写一个Triton内核。然后， 
编写另一个 `torch.autograd.Function` 的子类，以调用这个（融合的）内核。 
直接进行前向传播，而非在PyTorch中计算结果。一些针对具体问题的技巧： 
·为进行调试，我们建议将您执行的每次Triton操作的结果与以下内容进行对比： 
你在第（a）部分编写的分层式PyTorch实现。 
你的启动网格应设置为（Tq，batch_size），这意味着每个 Triton 程序实例 
将仅加载单个批次索引中的元素，并且只对单个查询瓦片进行读写。 
关于Q、O和L。 
·内核应仅包含一个循环，该循环将迭代关键瓦片 1≤j≤Tk。 
·在循环末尾提前设置块指针。 
·请使用以下函数声明（利用我们提供的块指针，您应能够 
能够推断出其余指针的设置）： 
1   @triton.jit 
2   def flash_fwd_kernel( 
3       Q指针，K指针，V指针， 
4        O指针，L指针， 
5        行步_qb，行步_qq，行步_qd， 
6        步长_kB、步长_kK、步长_kD， 
7        stride_vb, stride_vk, stride_vd, 
8        步幅_ob，步幅_oq，步幅_od， 
9       步幅磅，步幅1夸脱， 
该标志是可选的，默认值为 False，因此之前的测试仍能通过。 
实施带重新计算的反向传播请注意，与式7至11中的标准反向传播不同，我们可以通过重新计算来避免式13至19所示反向传播中的Softmax操作。这意味着我们可以使用一个简单的内核来完成反向传播的计算，而无需任何在线技巧。因此，在这一部分，你可以通过在常规代码中调用torch.compile来实现反向传播。 
现在，让我们比较一下你所实现的（部分）Triton版FlashAttention-2，与你用PyTorch实现的普通Attention模块的性能。 
### 1.3.3 FlashAttention-2 排行榜 
第二项任务的排行榜将测试你实现FlashAttention-2的速度（包括前向和反向传播）。我们鼓励你充分发挥创意，进一步优化你的实现性能。但请注意，你不得更改函数的输入输出，并且必须使用Triton（很遗憾，不能使用CUDA）。测试时，我们将以BF16格式、因果掩码方式处理输入数据，并确保你的实现能够通过与常规实现相同的各项测试。此外，代码必须完全由你自己编写，严禁使用任何现成的实现方案。计时将基于H100 GPU，针对一批次大小为1、序列长度为16,384（用于查询、键和值）且模型维度d_model=1024、拥有16个注意力头的样本进行。最终，我们会对排名前5到10的提交作品进行正确性和性能的双重验证。我们用来计时的具体测试代码如下： 
def测试_定时_前后快进： 
n个头 = 16 
头 = 64 
序列长度 = 16384 
q, k, v = torch.randn( 
3, n_heads, sequence_length, d_head, device='cuda', dtype=torch.bfloat16, requires_grad=True) 
flash = torch.compile(FlashAttention2.apply) 
定义闪现前后： 
o = flash(q, k, v, True) 
损失 = o.sum( 
损失反向传播() 
results = triton.testing.do_bench(flash_forward_backward, rep=10000, warmup=1000) print(results) 
出于测试目的，您可以将重复和预热时间（以毫秒为单位）缩短至更短的时长。以下是一些改进建议： 
· 调整内核的瓦片大小（可使用 Triton 自动调优功能！） 
·调整额外的 Triton 配置参数 
在 Triton 中实现反向传播，而不仅仅是使用 torch.compile（参见下文 1.3.4 节）。 
·对输入执行两次反向传播：一次用于计算dQ，另一次用于计算dK和dV，以避免块之间的原子操作或同步问题。 
· 在执行因果掩码时，提前停止程序实例，跳过所有始终全为零的图块。 
·将未遮罩的方块与方块对角线分开，前者无需比较索引即可计算，后者只需一次比较即可完成。 
·在H100上使用TMA（张量内存加速器）功能，操作方式与本教程类似。 
将你的最佳成绩提交至排行榜，网址为： 
github.com/stanford-cs336/assignment2-systems-leaderboard 
### 1.3.4（可选：Triton反向传递） 
如果你希望进一步练习Triton，或想快速提交排行榜结果，我们提供了下面的分块FlashAttention-2反向传播实现，你可以在Triton中直接加以应用。算法2展示了应在Triton中实现的FlashAttention-2反向传播流程。这里的一个关键技巧是：我们将P计算两次——一次用于dQ的反向传播，另一次则用于dK和dV的计算。这样，我们便能避免线程块间的同步操作。 
算法2 分块FlashAttention-2反向传播 
要求：Q、0、dO ∈ ℝᴺ×ᴰ，K、V ∈ ℝᴷ×ᴰ，L ∈ ℝᴷ，块大小分别为Bg、Bk 
计算 D = 行和(d0o0) × eRN 
将Q、O、dO分割为Tg=Nq Bq  
s Q1,..., gTq 将K、V分割为Tk=Nk Bk瓷砖 K(1),...,K(Tk)，以及V(1)....,V(T.)，每个大小均为Bk ×d 
对于 j = 1,..., Tk 做 
从全局内存中加载K()、V() 
初始化 dK()=dv()=0eRBkxd 
对 i = 1,..., Tq 执行 
从全局内存中加载Qi、Oi、do、dQ 
i = ti(√(j))k，其中 d ∈ RBq×Bk 
计算注意力概率 P(j) = exp(P(S🟥(j) i − Li ∈ RBq×Bk)) 
核心           i)TdOi ∈ RBk×d 
i= doiVTj ∈ RBq×Bk 
√i = .(j)◦   i − Di/d ∈ RBq×Bk 
从全局内存加载dQ，然后更新dQ；+=dSK() × e RBxd，并将其写回全局内存。为确保正确性，操作必须是原子的！ 
i) TQi ∈ RBk×d。 
### 结束于 
将dK()和dv()分别写入全局内存，作为dK和dV的第j个分块。 
### 结束于 
返回 dQ、dK、dV。 
### 2Distributed 数据并行训练 
在本次作业的下一环节，我们将探讨如何利用多块GPU来训练我们的语言模型，重点聚焦于数据并行方法。首先，我们会简要介绍PyTorch中的分布式通信机制；随后，我们将研究一种简单的分布式数据并行训练实现方案，并在此基础上逐步实施和评估多种优化措施，以提升通信效率。 
### 2.1 PyTorch 中的单节点分布式通信 
让我们从一个简单的 PyTorch 分布式应用开始，目标是生成四个随机整数张量，并计算它们的和。 
在下面的分布式场景中，我们将启动四个工作进程，每个进程生成一个随机整数张量。为了在各工作进程间对这些张量求和，我们将调用 all-reduce 集体通信操作，该操作会将每个进程上的原始数据张量替换为经过全归约处理后的结果（即求和值）。 
现在让我们来看一些代码。 
1   import os 
2  导入PyTorch 
3  import torch.distributed as dist 
4 导入 1torch.multiprocessinggasmp 
6 defsetup(rank, world_size): 
7      os.environ["MASTER_ADDR"] = "localhost" 
8         环境["MASTER_PORT"]="2950 
9       dist.init_process_group("gloo", rank=rank, world_size=world_size) 
10 
11   def_distributed_demo(rank, world_size): 
12       设置（rank, world_size） 
13       data = torch.randint(0, 10, (3,)) 
14       print(f"排名 {rank} 的数据（在全归约之前）：{data}") 
15        dist.all_reduce(data, async_op=False) 
16       print(f"排名 {rank}，数据（经全归约后）：{data}") 
17 
18   如果  名称    三三     主要一一· 
19         世界尺寸 = 420 
mp.spawn(fn=distributed_demo, args=(world_size,), nprocs=world_size, join=True) 
运行上述脚本后，我们得到以下输出。正如预期的那样，每个工作进程最初都持有不同的数据张量。在执行完全归约操作——该操作会将所有工作进程中的张量相加——之后，各工作进程的数据会被就地修改，以保存经过全归约处理后的结果。 
1   运行 python distributed_hello_world.py 
6 个排名为1的数据（经all-reduce后）：tensor([22,16,25]) 
7 号rank0数据（经all-reduce后）：tensor([22,16,25]) 
排名第三的数据（经All-Reduce后）：tensor([22,16,25]) 
排名2的数据（经All-Reduce后）：tensor([22,16,25]) 
现在让我们更仔细地回顾一下我们上面的脚本。命令 `mp.spawn` 会启动 `nprocs` 个进程，每个进程都会运行 `fn` 函数，并传入指定的参数。此外，函数 `fn` 的调用方式为 `fn(rank, *args)`，其中 `rank` 是工作进程的索引（取值范围为 0 到 `nprocs-1`）。因此，我们的分布式演示函数必须将这个整数类型的 `rank` 作为第一个位置参数接收。同时，我们还传递了 `world_size`，它表示所有工作进程的总数。 
这些工作进程各自属于一个进程组，该进程组通过`dist.init_process_group`初始化。进程组代表多个工作进程，它们将通过共享的主节点进行协调与通信。主节点由其IP地址和端口定义，且主节点运行着rank为0的进程。诸如all-reduce之类的集体通信操作会作用于进程组中的每个进程。 
在本例中，我们使用“gloo”后端初始化了进程组，但其他后端同样可用。尤其是，“nccl”后端将调用NVIDIA NCCL集体通信库，通常情况下，这对于CUDA张量的性能表现更为出色。不过，NCCL只能在配备GPU的机器上使用，而Gloo则可在仅支持CPU的设备上运行。一个实用的经验法则就是：分布式GPU训练建议使用NCCL，而分布式CPU训练和/或本地开发则推荐Gloo。在本示例中，我们选择了Gloo，因为它支持在仅CPU的机器上进行本地执行与开发。 
在运行多GPU任务时，请确保不同进程使用不同的GPU。一种实现方法是在设置函数中调用`torch.cuda.set_device(rank)`，这样当张量调用`tensor.to("cuda")`时，会自动被移动到指定的设备上。此外，你也可以显式地为每个进程创建一个设备字符串（例如，`device = f"cuda:{rank}"`），然后将该设备字符串作为目标设备，用于所有数据的移动操作（如`tensor.to(f"cuda:{rank}")`）。 
术语。在本次作业的其余部分（以及你可能在网上看到的各种其他资源中），你可能会遇到以下与PyTorch分布式通信相关的术语。尽管本作业将重点介绍单节点多进程的分布式训练，但这些术语对于理解分布式训练的整体概念同样很有帮助。请参阅图2以获取直观展示。 
节点：网络中的一个设备。 
工作者：参与分布式训练的程序实例。在本次作业中，每个工作者将拥有一个独立的进程，因此我们将“工作者”、“进程”和“工作者进程”三者交替使用。然而，一个工作者也可能使用多个进程（例如，用于加载训练数据），所以在实际应用中，这些术语并不总是完全等同。 
世界规模：一个进程组中总员工的数量。 
全局排名：一个整数ID（介于0和world_size-1之间），用于唯一标识进程组中的某个工作进程。例如，如果全局大小为2，其中一个进程的全局排名将是0（主进程），而另一个进程的排名则为1。 
本地世界大小：当在不同节点上运行应用程序时，本地世界大小是指特定节点上本地运行的工作进程数量。例如，如果我们有一个应用程序，在两个节点上各启动了4个工作进程，那么整个应用的世界大小将是8，而本地世界大小则为4。需要注意的是，当应用程序仅运行于单个节点时，某个工作进程的本地世界大小将等同于（全局）世界大小。 
本地排名：一个整数ID（介于0到local_world_size-1之间），用于唯一标识机器上本地工作进程的索引。例如，如果某个应用程序在2个节点上各启动4个进程，那么每个节点上的工作进程将分别拥有本地排名0、1、2和3。需要注意的是，当运行单节点多进程分布式应用时，进程的本地排名与其全局排名是等同的。 
机器1                                           机器2 
图2：一个分布式应用的示意图，该应用运行在2个节点上，世界规模为8。每个工作进程通过全局排名（从0到7）和本地排名（从0到3）进行标识。图片来源：lightning.ai/docs/fabric/stable/advanced/distributed_communication.html 
### 2.1.1 分布式应用基准测试的最佳实践 
在本次作业的这一部分，你将对分布式应用进行基准测试，以更深入地了解通信带来的开销。以下是一些最佳实践： 
·尽可能在同一台机器上运行基准测试，以方便进行受控比较。 
在开始计时感兴趣的运算之前，请执行若干个预热步骤。这一点对于NCCL通信调用尤为重要。通常，进行5次预热迭代即可满足需求。 
· 在使用GPU进行基准测试时，请调用torch.cuda.synchronize()以等待CUDA操作完成。请注意，即使在调用通信操作且将async_op设置为False的情况下，这一点也是必要的——因为此时该操作仅会在GPU上被排队，并不会等到实际通信结束才返回。 
· 不同rank之间的计时可能略有差异，因此通常会跨rank汇总测量结果，以提高估算的准确性。你可能会发现，使用all-gather集体通信（特别是dist.all_gather_object函数）有助于从所有rank收集结果。 
通常情况下，先在CPU上使用Gloo进行本地调试，然后根据具体问题的需求，再切换到GPU上的NCCL进行性能测试。在不同后端之间切换时，只需修改`init_process_group`调用，并调整张量的设备类型即可。 
### 2.22A 分布式数据并行训练的朴素实现 
现在我们已经了解了在PyTorch中编写分布式应用的基本知识，接下来让我们构建一个分布式数据并行（DDP）训练的最小化实现。 
数据并行将批次分割到多个设备（例如GPU）上，从而支持在超出单个设备容量的大批量训练中进行模型训练。例如，如果有四台设备，每台最多可处理32的批次大小，那么通过数据并行训练，实际的有效批次大小将达到128。 
以下是进行朴素分布式数据并行训练的步骤：首先，每台设备会构建一个（随机初始化的）模型。接着，我们使用广播集体通信操作，将模型参数从Rank 0节点发送至所有其他节点。在训练开始时，每台设备都持有模型参数和优化器状态的完全相同副本（例如，Adam优化器中累积的梯度统计信息）。 
1. 给定一个包含n个样本的批次，该批次会被分片，每个设备接收到n/d个互不重叠的样本（其中d为用于数据并行训练的设备数量）。由于训练时间受最慢进程的制约，n必须能被d整除。 
2. 每个设备使用其本地保存的模型参数，对自己的n/d个样本执行前向传播，并通过反向传播计算梯度。需要注意的是，此时每个设备已持有由其所接收的n/d个样本计算出的梯度。 
3. 接着，我们使用全归约集体通信操作，在不同设备间对梯度进行平均，从而使每个设备都保存了基于全部n个样本计算出的平均梯度。 
4. 接下来，每台设备都会运行一次优化器步骤，以更新其自身的参数副本——从优化器的角度来看，这实际上只是在优化本地模型。由于所有设备均基于相同的初始模型和优化器状态启动，并且每轮迭代都采用相同的平均梯度，因此各设备上的参数和优化器状态将始终保持同步。至此，我们已完成一次训练迭代，接下来可重复这一过程。 
### 问题（naive_ddp）：5分 
### 问题（naive_ddp_benchmarking）：3分 
在这个朴素的DDP实现中，参数会在每次反向传播后，由各个进程分别进行全归约操作。为了更清楚地了解数据并行训练的开销，你可以编写一个脚本来对之前已实现的语言模型进行基准测试，该模型采用这种朴素的DDP实现进行训练。具体测量每步训练的总耗时，以及其中用于梯度通信的时间占比。实验将在单节点设置下进行（1个节点 × 2块GPU），所使用的XL模型大小如$1.1.2节所述。 
交付成果：您的基准测试设置说明，以及每次训练的实测时间。 
### 2.3改进最小DDP实现 
我们在2.2节中看到的最小DDP实现存在几个关键局限性： 
1. 它对每个参数张量单独执行一次全归约操作。每次通信调用都会产生开销，因此，将通信调用分批进行可能更有利，以尽量减少这种开销。 
2. 它会等待反向传播完成后再进行梯度通信。然而，反向传播是逐步计算的。因此，当某个参数的梯度准备就绪时，即可立即进行通信，而无需等待其他参数的梯度。这使得我们能够将梯度通信与反向传播的计算过程重叠起来，从而降低分布式数据并行训练的开销。 
在本次任务的这一部分，我们将逐一解决这些局限性，并衡量其对训练速度的影响。 
### 2.3.1 减少通信呼叫次数 
与其为每个参数张量单独发出通信调用，不如尝试通过批量执行全归约来提升性能。具体来说，我们将要进行全归约的梯度拼接成一个单一的张量，然后在所有进程间对合并后的梯度执行全归约操作。在此过程中，使用torch._utils._flatten_dense_tensors和torch._utils._unflatten_dense_tensors可能会有所帮助。 
2.3.22（基于单个参数梯度的通信实现计算重叠） 
虽然批量处理通信调用有助于降低执行大量小型全归约操作所带来的开销，但整个通信时间仍会直接增加系统负担。为解决这一问题，我们可以利用这样一个观察：反向传播过程会逐层逐步计算各层的梯度（从损失函数开始，向输入端推进）——因此，一旦某一层的参数梯度准备就绪，即可立即对其进行全归约，从而通过将反向传播的计算与梯度通信重叠，有效减少数据并行训练的开销。 
我们将首先实现并测试一个分布式数据并行封装器，该封装器会在反向传播过程中，当各个参数张量准备就绪时，异步地执行全归约操作。以下几点可能对您有所帮助： 
为了在反向传播过程中自动对参数调用一个函数，以便在其梯度被累积后执行，你可以使用`register_post_accumulate_grad_hook`函数。 
PyTorch 的所有集合通信操作均支持同步（`async_op=False`）和异步执行（`async_op=True`）。同步调用会阻塞，直到集合操作已成功排队到 GPU 上。但这并不意味着 CUDA 操作已完成——毕竟，CUDA 操作本身是异步的。不过，后续使用该输出的函数调用仍会按预期正常工作。  

相比之下，异步调用会返回一个分布式请求句柄。因此，当函数返回时，并不能保证集合通信操作已成功排队到 GPU 上，更别提真正完成。若要确保操作已成功排队到 GPU（从而确保输出可在后续操作中使用），你可以对返回的通信句柄调用 `handle.wait()`。例如，以下两个示例分别对一组张量中的每个张量执行全归约操作，可以选择采用同步或异步方式： 
同步或异步调用： 
1  tensors = [torch.rand(5) for _ in range(10)] 
3   #同步，阻塞直到操作已排队到GPU。 
4 用于张量的张量： 
5        dist.all_reduce(tensor, async_op=False) 
7     异步，每次调用后立即返回，并且 
8   在26日等待结果。 
9 个手柄 
10  for 张量 in 张量列表： 
11       handle = dist.all_reduce(tensor, async_op=True) 
12       handles.append(handle) 
13141516 
可能还会执行不依赖于all_reduce结果的其他命令。 
17 
18 确保所有归约调用被排队，且 
19 因此，其他合作取决于 
20   # 全归约输出可被排队。 
21   for handle in handles: 
22      处理器等待， 
23  handles.clear() 
`deffinish_gradient_synchronization(self):` 调用时，等待异步通信完成。 
调用将被排队等待GPU处理。 
问题（ddp_overlap_individual_parameters_benchmarking）：1分 
2.3.33（带分桶参数梯度通信的重叠计算） 
在前一节（2.3.2）中，我们曾将反向传播计算与各参数梯度的通信重叠进行。然而，我们之前已发现，批量发送通信调用能够提升性能，尤其是在我们拥有大量参数张量时（这正是深度Transformer模型中的典型情况）。我们此前尝试的批量发送方式是一次性传送所有梯度，这就需要等待反向计算完成。 
以完成为目标进行传递。在这一部分，我们将尝试兼顾两全其美：首先将参数分组归类（从而减少总的通信次数），并在每个组内的张量都准备就绪时执行全局规约操作（使我们能够实现通信与计算的重叠）。 
### 2.44D 平行性 
尽管实现起来复杂得多，但我们仍可沿着更多维度并行化训练过程。最常见的是，我们讨论5种并行化方法： 
· 数据并行（DP）——数据批次被分割到多个设备上，每个设备为其所属的批次计算梯度。随后，这些梯度需在各设备间进行某种方式的平均。 
· 全量分片数据并行（FSDP）——优化器状态、梯度和权重在各个设备间进行拆分。如果仅使用数据并行（DP）和FSDP，每个设备需先从其他所有设备收集到权重分片，才能执行前向或反向传播操作。 
· 张量并行（TP）——激活值按新维度进行分片，每个设备仅计算其对应分片的输出结果。在张量并行中，我们可选择沿操作的输入或输出进行分片。若将权重与激活值分别按相应维度进行分片，则张量并行可与FSDP高效结合使用。 
· 流水并行（PP）——模型按层被分割成多个阶段，每个阶段在不同设备上运行。 
专家并行（EP）——我们将混合专家模型中的各个专家分配到不同设备上，每个设备独立计算各自专家的输出结果。 
通常，我们总是将FSDP和TP结合使用，因此可以将其视为单一的并行化维度。这样一来，我们就有了4个并行化维度：DP、FSDP/TP、PP和EP。此外，我们还将专注于稠密模型（而非MoE），因此不再深入讨论EP。 
在讨论分布式训练时，我们通常将集群描述为一个由多个设备组成的网格，而网格的各个轴则对应着我们定义并行化的方向。例如，如果我们有16块GPU，且模型规模远超单个设备的承载能力，我们可能会倾向于将网格组织成一个4×4的GPU阵列：其中，第一维代表数据并行（DP），第二维则结合了全模型分片（FSDP）与张量并行（TP）。 
更多关于这些方法的工作原理及其通信和内存开销的推导细节，请参阅《TPU扩展手册》第5部分（Austin 等人 [2025]），这将特别有助于解决以下问题。如需更深入的流水线并行讨论，请参考《超大规模实践手册》附录（Nouamane Tazi [2025]）。此外，本书的其他部分也包含大量您可能觉得有用的信息。 
### 问题（沟通与会计）：10分 
考虑一种新的模型配置XXL，其参数设置为：d_model=16384，d_ff=53248，num_blocks=126。由于对于超大规模模型而言，绝大部分浮点运算量都集中在前馈网络中，我们做出了一些简化假设。首先，我们省略了注意力机制、输入嵌入层以及输出线性层。其次，我们假设每个前馈网络仅由两个线性层构成（忽略激活函数），其中第一个线性层的输入维度为d_model、输出维度为d_ff，第二个线性层的输入维度为d_ff、输出维度为d_model。你的模型由num_blocks个这样的两层线性网络块组成。此外，无需进行任何激活检查点技术，并且保持激活值及梯度通信采用BF16格式；而累积梯度、主权重和优化器状态则应使用FP32格式。 
(a) 使用FP32格式，在单个设备上存储主模型权重、累积梯度和优化器状态各需要多少内存？其中，反向传播过程会使用BF16，这样能节省多少内存？这相当于多少块H100 80GB GPU的显存容量？ 
交付内容：您的计算结果及一句话的回复。 
(b) 现在假设你的主权重、优化器状态、梯度以及一半的激活值（实际上每第二个层）被分片存储在NrsDp个设备上。请写出每个设备所需内存的表达式。为了使总内存开销低于1个V5P TPU（每个设备95GB），NrsDp需要取何值？交付内容：你的计算过程及一句简要回答。 
（c）仅考虑前向传播过程。根据TPU扩展手册中的数据，采用Wici = 2.9 × 10¹⁰¹°的通信带宽，以及TPU v5p的4.6 × 10¹⁴ FLOPS/s浮点运算性能。按照扩展手册的记号，设置Mx=2、My=1（即三维网格），其中X=16为您的FSDP维度，Y=4则为您的TP维度。请问，在这种配置下，模型的计算瓶颈出现在每设备的批量大小是多少？同时，整个系统的总批量大小又是多少？ 
交付内容：您的计算结果及一句话的回复。 
（d）实际上，我们希望整体批量大小尽可能小，同时我们始终采用我们的 
高效计算（换句话说，我们希望永远不受通信瓶颈的限制）。还有哪些 
我们能采取哪些技巧来减少模型的批量大小，同时保持高吞吐量？ 
交付成果：一段简短的回应。请以参考文献和/或公式支持你的观点。 
条款。 
### 3优化器状态分片 
分布式数据并行训练在概念上简单，且通常非常有效，但要求每个进程都保存模型参数和优化器状态的独立副本。这种冗余会带来显著的内存开销。例如，AdamW优化器为每个参数维护两个浮点数，这意味着它的内存占用是模型权重的两倍。Rajbhandari等人[2020]描述了几种方法，通过将（1）优化器状态、（2）梯度，以及（3）参数在各进程间进行划分，并根据需要在不同工作节点之间通信，从而有效降低数据并行训练中的冗余问题。 
在本次任务中，我们将通过实现优化器状态分片的简化版本，来降低每个Rank的内存消耗。具体来说，我们不会为所有参数保存优化器状态，而是让每个Rank的优化器实例仅处理一部分参数（大约为1/世界规模）。当每个Rank的优化器执行一步更新时，它只会更新其分片中对应的那部分模型参数。随后，各Rank会将其更新后的参数广播给其他Rank，以确保在每一步优化器更新后，模型参数始终保持同步。 
现在我们已实现了优化器状态分片，接下来让我们分析它对训练期间峰值内存使用量以及运行时开销的影响。 
(a) 编写一个脚本，用于分析在启用与禁用优化器状态分片的情况下训练语言模型时的峰值内存使用情况。采用标准配置（1个节点，2块GPU，XL模型尺寸）， 
报告模型初始化后、优化器步骤执行前以及优化器步骤执行后的峰值内存占用情况。结果是否符合您的预期？请详细分析每种情况下的内存分配（例如：参数占用多少内存，优化器状态占用多少内存等）。 
交付物：包含峰值内存使用情况结果的2至3句话回复，以及内存如何在不同模型和优化器组件间分配的详细说明。 
(b) 我们的优化器状态分片实现如何影响训练速度？请测量在标准配置（1节点、2块GPU、XL模型尺寸）下，启用与禁用优化器状态分片时每轮迭代所需的时间。 
交付物：用2到3句话回答，并注明你的时间安排。 
（c）我们的优化器状态分片方法与ZeRO第一阶段（Rajbhandari等人，2020年中称为ZeRO-DP Pos）有何不同？ 
交付物：对各项差异的2-3句总结，尤其关注与内存和通信量相关的部分。 
### 第四章 后记 
恭喜你完成任务！我们希望你觉得它既有趣又有收获，并且学到了一些关于如何通过提升单GPU速度和/或利用多GPU来加速语言模型训练的技巧。 
参考文献 
崔道。FlashAttention-2：更高效的注意力机制，具备更优的并行性和工作划分，2023年3月。网址：https://arxiv.org/abs/2307.08691。 
崔道、丹尼尔·Y·傅、斯特法诺·埃尔蒙、阿特里·鲁德拉与克里斯托弗·雷。FlashAttention：一种兼具高效性和内存效率的精确注意力机制，同时具备I/O感知能力。载于Alice H. Oh、Alekh Agarwal、Danielle Belgrave及Kyunghyun Cho主编的《神经信息处理系统进展》，2022年。网址：https://openreview.net/forum?id=H4DqfPSibmx。 
马克西姆·米拉科夫与娜塔莉亚·吉梅尔谢因。用于Softmax的在线归一化计算，2018年。网址：https://arxiv.org/abs/1805.02867。 
霍勒斯·何：从基本原理出发，让深度学习“冷得发抖”。2022年。网址：https://horace.io/brrr_intro.html。 
雅各布·奥斯汀、肖尔托·道格拉斯、罗伊·弗罗斯蒂格、安塞尔姆·列夫斯卡娅、查理·陈、沙拉德·维克拉姆、费德里科·莱布龙、彼得·蔡、维奈·拉马塞什、阿尔伯特·韦布森以及雷纳尔·波普。如何扩展你的模型。2025年。取自：https://jax-ml.github.io/scaling-book/。 
赵浩俊  Phuc Nguyen 穆罕默德·梅库里 莱安德罗·韦拉 托马斯·沃尔夫 努阿曼·塔齐，费尔迪-南德·莫姆。《超大规模实践指南：在GPU集群上训练大模型》，2025年。 
萨米亚姆·拉吉班达里、杰夫·拉斯利、奥拉图尼吉·鲁瓦塞和何宇雄。ZeRO：面向训练万亿参数模型的内存优化技术，2020年。arXiv:1910.02054。 
