---
AIGC:
  Label: "1"
  ContentProducer: "001191330110MADHPQQY7L10000"
  ProduceID: "ExportRequest(files=[ExportFile(docType=7, fileType=3), ExportFile(docType=2, fileType=1), ExportFile(docType=8, fileType=6)])"
  ReservedCode1: "d9ed61ec9417da7c84ccba10ad0a95874bc4568fa2a367bba871f39eeb801a41"
  ContentPropagator: "001191330110MADHPQQY7L10000"
  PropagateID: "ExportRequest(files=[ExportFile(docType=7, fileType=3), ExportFile(docType=2, fileType=1), ExportFile(docType=8, fileType=6)])"
  ReservedCode2: ""
---
# cs336_spring2025_assignment4_data 
 ## 全文概述 
###  
本作业要求学生实践过滤互联网数据以构建语言模型训练集的过程。首先，学生需要从Common Crawl获取HTML数据并转换为文本格式。然后，通过各种方法对提取的文本进行过滤，包括去除有害内容、个人身份信息等，并进行数据去重。接下来，利用FastText库进行语言识别，以及应用预训练的模型来识别和过滤含有敏感信息（如电子邮件地址、电话号码和IP地址）的内容。此外，还需应用简单的规则和机器学习模型来评估文档的质量，并决定是否保留。最后，通过对多个WET文件的处理，生成用于训练语言模型的数据集，并使用GPT-2小规模模型进行训练与测试，目标是优化数据以减少验证集上的 perplexity。整个过程旨在通过高效的数据预处理提高语言模型的性能。 
## 关键要点 
1. 文本中提到的质量过滤器包括平均词长、行末尾标点符号数量和单词含字母数等。
2. 使用高质量链接来构建数据集，例如使用Reddit评论中的链接或者Wikipedia页面中的外部链接。
3. 使用爬虫工具（如wget）从WARC格式文件中提取URL并进行质量分类。
4. 实现一种简单的方法来删除文档中的重复行，可以使用哈希表来存储唯一行，并将输入文件重写为输出目录中的新文件。
5. 对于模糊去重，使用MinHash和LSH算法计算Jaccard相似度来进行模糊去重。
 
## 文档速读 
### 利用公共数据集训练语言模型的步骤和挑战 
这一章节介绍了斯坦福大学的一门自然语言处理课程的第四次作业，任务是将Common Crawl（一个免费的网页数据集）转换成可用于训练语言模型的数据。作业包括三个部分：从HTML中提取文本、过滤掉不良内容和重复数据以及使用优化后的PyTorch模型进行训练。作业代码可以在GitHub上找到，并且需要提交写作文档和代码文件。
 
##  
### 互联网页面文本提取与语言识别技术介绍 
这一章节主要介绍了如何从网页中提取出文本信息，并且提到了在提取过程中可能会遇到的挑战和需要注意的问题。其中提到使用Resiliparse库进行文本提取，并且需要考虑不同编码格式的情况。此外，还介绍了一个名为fastText的工具库，可以用于语言识别分类器的训练和应用。最后，要求实现一个基于fastText的语言识别过滤器，可以根据预测结果给出相应的得分。
 
##  
### 网页信息隐私保护与有害内容识别技术 
这一章节主要介绍了两个方面的内容。第一部分是关于个人身份信息的处理，包括了对电子邮件、电话号码和IP地址等敏感信息的屏蔽方法，并要求实现相应的函数来完成这个任务。第二部分则是关于有害内容的识别，使用了预训练模型来判断输入的文本是否包含不适宜的内容或毒性言论，并返回相应的标签和置信度得分。这两个方面都是在保护用户隐私和维护网络环境健康的前提下进行的。
 
##  
### 如何提高语言模型训练的数据质量？ 
这一章节主要讲述了如何通过过滤器和质量分类器来提高从网页上提取的文本的质量。在过滤器方面，作者提供了一些简单的规则，例如去除长度小于50或大于100,000个单词的文档、去除非文本内容等。而在质量分类器方面，作者建议使用高质量链接作为正例，将随机选择的网页作为负例，并训练一个fastText分类器来评估文本的质量。最后，作者提供了用于获取高质量链接的Wikipedia参考URL列表以及下载链接。
 
##  
### 精确行去重方法实现 
这一章节主要讲述了如何进行精确的重复行删除操作。在互联网上存在大量的重复内容，包括页面的完全复制和更细粒度的重复，如菜单、页脚等。通过统计每个行在语料库中的出现次数，并使用哈希值作为键来存储计数器，可以实现对重复行的快速查找和删除。最终目标是将输入文件中的重复行去除并输出到指定目录下。
 
##  
### MinHash和LSH技术在文档去重中的应用 
这一章节介绍了如何使用MinHash和LSH进行文档去重的方法。具体来说，它通过计算每个文档的n-gram并生成签名来表示文档，并使用多个随机哈希函数对签名进行分组，从而将相似的文档聚类到同一个桶中。这种方法可以高效地处理大型文档集合，并且可以在不存储整个文档的情况下对其进行比较。最后，它可以将候选重复文档进一步分类为真正的重复文档或非重复文档。
 
##  
### 使用Web爬取数据进行语言模型训练 
这一章节介绍了如何使用一些基本的工具来过滤网络爬取的数据，并生成语言模型训练数据。目标是通过构建最佳数据集，使经过训练的Transformer语言模型在C4 100领域子集上的验证困惑度最小化。你需要从CC WET文件中筛选出合适的样本，并将其转换成GPT-2小模型可以使用的格式。为了高效处理大量的数据，建议使用多进程技术。最后，你需要将处理后的数据用于训练GPT-2小模型，并评估其性能。
 
##  
### 使用SubmitIt和FastWARC处理大规模数据并进行语言模型训练 
这一章节介绍了如何使用Python中的submitit包来在Slurm集群上并行处理数据，并提供了详细的代码示例和解释。首先需要配置submitit的参数，然后使用executor.batch()将所有任务组合成一个大的作业数组，最后使用submitit.helpers.as_completed()收集结果。此外，还建议使用FastWARC和TLDExtract库来迭代WET文件和提取URL域名进行过滤。最后，使用GPT-2 tokenizer对过滤后的数据进行编码，以便训练语言模型。
 
##  
### 训练语言模型并优化数据以提高性能 
这一章节主要介绍了如何使用Python对数据进行分词和序列化，并且给出了具体的实现方法和交付要求。在完成分词后，可以使用训练脚本来训练语言模型并评估其性能。最后需要提交最佳的验证损失值到排行榜上。建议不要修改训练配置或脚本，而是优化数据以最小化验证损失。
 
##  
### 大规模语言模型训练与预训练研究综述 
这一章节列举了一些关于语言模型预训练的研究论文和数据集，包括Dolma、Gopher、LLAMA等，并提到了一些相关的技术和方法，如大规模语料库的构建、多任务学习等。这些研究和数据集对于自然语言处理领域的进展具有重要意义，可以帮助我们更好地理解和应用语言模型。此外，该章节还介绍了一些经典的文献，如《PageRank》和《Mining of Massive Datasets》，它们对搜索引擎和信息检索等领域的发展做出了重要贡献。
 
