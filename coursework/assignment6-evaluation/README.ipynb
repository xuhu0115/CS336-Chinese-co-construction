{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸ç”¨è¯„æµ‹æ¡†æ¶ä»‹ç»"

æœ¬æ¬¡ä½œä¸šæˆ‘ä»¬ä¸»è¦ä»‹ç»å¸¸ç”¨çš„å‡ ç§å¤§å‹è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼š

## æ¡†æ¶æ¦‚è§ˆ

| æ¡†æ¶åç§° | å¼€å‘æœºæ„ | ä¸»è¦ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) | EleutherAI | åŠŸèƒ½ä¸°å¯Œï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œä»»åŠ¡ï¼Œå­¦æœ¯ç•Œæ ‡å‡† | å­¦æœ¯ç ”ç©¶ã€åŸºå‡†æµ‹è¯• |
| [evalscope](https://github.com/modelscope/evalscope) | ModelScope | æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ç»„åˆï¼Œå¯è§†åŒ–åˆ†æï¼Œä¸­æ–‡å‹å¥½ | äº§ä¸šåº”ç”¨ã€æ¨¡å‹è¯„ä¼° |
| [Evalchemy](https://github.com/mlfoundations/evalchemy) | ML Foundations | è½»é‡çº§ï¼Œæ³¨é‡å¯å¤ç°æ€§å’Œæ‰©å±•æ€§ | ç ”ç©¶å®éªŒã€å¿«é€ŸåŸå‹ |
| [lighteval](https://github.com/huggingface/lighteval) | Hugging Face | é›†æˆTransformersç”Ÿæ€ï¼Œæ˜“äºä½¿ç”¨ | Hugging Faceç”¨æˆ· |

æˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»**lm-evaluation-harness**å’Œ**evalscope**çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚           
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27c40e",
   "metadata": {},
   "source": [
    "# 1. lm-evaluation-harness\n",
    "\n",
    "æˆ‘ä»¬ä»‹ç»ç¬¬ä¸€ç§å¸¸ç”¨çš„è¯„æµ‹æ¡†æ¶ `lm-evaluation-harness`ï¼Œlm-evalè¦ä» GitHub ä»“åº“å®‰è£…è½¯ä»¶åŒ…ï¼Œè¯·åœ¨å‘½ä»¤è¡Œè¿è¡Œï¼š\n",
    "\n",
    "```bash\n",
    "git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e .\n",
    "pip install -e .[math]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa697d3",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬é¦–å…ˆä»‹ç»å®ƒçš„ç®€å•ä½¿ç”¨ï¼Œè¿™é‡Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå°æ¨¡å‹ gpt2 æ¥æµ‹è¯•ä¸€ä¸‹ï¼Œè¯·åœ¨å‘½ä»¤è¡Œè¿è¡Œå¦‚ä¸‹ä»£ç ï¼š\n",
    "\n",
    "```bash\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=openai-community/gpt2 \\\n",
    "    --tasks hellaswag \\\n",
    "    --limit 10 \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size 8\n",
    "```\n",
    "\n",
    "å¤§æ¦‚ç­‰å¾…ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´ï¼Œæˆ‘ä»¬å³å¯å¾—åˆ°ç»“æœï¼š\n",
    "\n",
    "|  Tasks  |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n",
    "|---------|------:|------|-----:|--------|---|----:|---|-----:|\n",
    "|hellaswag|      1|none  |     0|acc     |â†‘  |  0.3|Â±  |0.1528|\n",
    "|         |       |none  |     0|acc_norm|â†‘  |  0.3|Â±  |0.1528|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2554408",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»ä¸€ç§æ›´å¤æ‚çš„ä½¿ç”¨æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œå¤šç»´çš„èƒ½åŠ›è¯„æµ‹ï¼Œæˆ‘ä»¬å°†å…¶åˆ’åˆ†ä¸ºé€šç”¨è¯­è¨€ç†è§£ã€å¸¸è¯†æ¨ç†ã€ä»£ç ã€æ•°å­¦æ¨ç†å››ç±»ä»»åŠ¡ã€‚\n",
    "\n",
    "| ç»´åº¦        | ä»»åŠ¡é›†åˆ                                               |\n",
    "| --------- | -------------------------------------------------- |\n",
    "| é€šç”¨ï¼ˆGenï¼‰   | LAMBADA(LAMBADA Standardã€LAMBADA OpenAI), TriviaQA |\n",
    "| å¸¸è¯†æ¨ç†ï¼ˆComï¼‰ | PIQA, ARC-easy                                     |\n",
    "| ä»£ç ï¼ˆCodeï¼‰  | HumanEval, MBPP                                    |\n",
    "| æ•°å­¦ï¼ˆMathï¼‰  | GSM8K, Minerva Math                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc791ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation_kwargs: {'max_gen_toks': 512} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot tasks: ['arc_easy', 'piqa', 'lambada', 'triviaqa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running generate_until requests:   9%|â–‰         | 576/6477 [16:36<2:50:13,  1.73s/it]\n",
      "Overwriting default num_fewshot of triviaqa from None to 0\n",
      "Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2325.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1021.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1199.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1608.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1261.07it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.40it/s]\n",
      "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 32.08it/s]\n",
      "generation_kwargs: {'max_gen_toks': 512} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 3-shot tasks: ['humaneval', 'mbpp', 'gsm8k', 'minerva_math']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/xuhu.6736/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--code_eval/78d307ea938083398db7d9815f03ed661e9c15f60d77880ce007a8a02648f176 (last modified on Mon Jan 19 21:27:53 2026) since it couldn't be found locally at evaluate-metric--code_eval, or remotely on the Hugging Face Hub.\n",
      "Overwriting default num_fewshot of minerva_math_algebra from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_geometry from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_num_theory from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 3\n",
      "Overwriting default num_fewshot of minerva_math_precalc from 4 to 3\n",
      "Overwriting default num_fewshot of gsm8k from 5 to 3\n",
      "Overwriting default num_fewshot of mbpp from 3 to 3\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 741.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 727.29it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 823.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 809.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 985.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1023.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 896.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 945.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 334.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2189.09it/s]\n",
      "Left truncation applied. Original sequence length was 641, truncating to last 512 tokens. Some content will be lost.\n",
      "Left truncation applied. Original sequence length was 553, truncating to last 512 tokens. Some content will be lost.\n",
      "Left truncation applied. Original sequence length was 660, truncating to last 512 tokens. Some content will be lost.\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:18<00:00,  7.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results keys: ['arc_easy', 'lambada_openai', 'lambada_standard', 'piqa', 'triviaqa', 'gsm8k', 'humaneval', 'mbpp', 'minerva_math', 'minerva_math_algebra', 'minerva_math_counting_and_prob', 'minerva_math_geometry', 'minerva_math_intermediate_algebra', 'minerva_math_num_theory', 'minerva_math_prealgebra', 'minerva_math_precalc']\n",
      "all_results: {'arc_easy': {'alias': 'arc_easy', 'acc,none': 0.0, 'acc_stderr,none': 'N/A', 'acc_norm,none': 1.0, 'acc_norm_stderr,none': 'N/A'}, 'lambada_openai': {'alias': 'lambada_openai', 'perplexity,none': 48895.074567592026, 'perplexity_stderr,none': 'N/A', 'acc,none': 0.0, 'acc_stderr,none': 'N/A'}, 'lambada_standard': {'alias': 'lambada_standard', 'perplexity,none': 26270.121698137566, 'perplexity_stderr,none': 'N/A', 'acc,none': 0.0, 'acc_stderr,none': 'N/A'}, 'piqa': {'alias': 'piqa', 'acc,none': 1.0, 'acc_stderr,none': 'N/A', 'acc_norm,none': 1.0, 'acc_norm_stderr,none': 'N/A'}, 'triviaqa': {'alias': 'triviaqa', 'exact_match,remove_whitespace': 0.0, 'exact_match_stderr,remove_whitespace': 'N/A'}, 'gsm8k': {'alias': 'gsm8k', 'exact_match,strict-match': np.float64(0.0), 'exact_match_stderr,strict-match': 'N/A', 'exact_match,flexible-extract': np.float64(0.0), 'exact_match_stderr,flexible-extract': 'N/A'}, 'humaneval': {'alias': 'humaneval', 'pass@1,create_test': np.float64(0.0), 'pass@1_stderr,create_test': 'N/A'}, 'mbpp': {'alias': 'mbpp', 'pass_at_1,none': np.float64(0.0), 'pass_at_1_stderr,none': 'N/A'}, 'minerva_math': {'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'alias': 'minerva_math'}, 'minerva_math_algebra': {'alias': ' - minerva_math_algebra', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_counting_and_prob': {'alias': ' - minerva_math_counting_and_prob', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_geometry': {'alias': ' - minerva_math_geometry', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_intermediate_algebra': {'alias': ' - minerva_math_intermediate_algebra', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_num_theory': {'alias': ' - minerva_math_num_theory', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_prealgebra': {'alias': ' - minerva_math_prealgebra', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}, 'minerva_math_precalc': {'alias': ' - minerva_math_precalc', 'exact_match,none': 0.0, 'exact_match_stderr,none': 'N/A', 'math_verify,none': 0.0, 'math_verify_stderr,none': 'N/A'}}\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "\n",
    "# eval_model_path = \"/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B\"\n",
    "eval_model_path = \"openai-community/gpt2\"\n",
    "zero_shot_tasks = [\"arc_easy\", \"piqa\", \"lambada\", \"triviaqa\"]\n",
    "few_shot_tasks = [\"humaneval\", \"mbpp\", \"gsm8k\", \"minerva_math\"]\n",
    "all_results = {}\n",
    "\n",
    "# ğŸ”¥ å…³é”®ï¼šåªåˆ›å»ºä¸€æ¬¡æ¨¡å‹å®ä¾‹\n",
    "lm = HFLM(\n",
    "    pretrained=eval_model_path,      # å¯ä»¥æ˜¯è·¯å¾„ï¼Œä¼šè‡ªåŠ¨åŠ è½½\n",
    "    tokenizer=eval_model_path,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "# ===== 1. Zero-shot evaluation =====\n",
    "print(f\"Evaluating zero-shot tasks: {zero_shot_tasks}\")\n",
    "results_zero = evaluator.simple_evaluate(\n",
    "    model=lm,  # â† å¤ç”¨åŒä¸€ä¸ª lm å®ä¾‹\n",
    "    tasks=zero_shot_tasks,\n",
    "    num_fewshot=0,\n",
    "    limit=1,\n",
    "    batch_size=32,\n",
    "    gen_kwargs={\"max_gen_toks\": 512}, \n",
    "    # device='cuda',\n",
    "    confirm_run_unsafe_code=True\n",
    ")\n",
    "all_results.update(results_zero['results'])\n",
    "\n",
    "# ===== 2. Few-shot evaluation =====\n",
    "print(f\"Evaluating 3-shot tasks: {few_shot_tasks}\")\n",
    "results_few = evaluator.simple_evaluate(\n",
    "    model=lm,  # â† åŒä¸€ä¸ª lm å®ä¾‹\n",
    "    tasks=few_shot_tasks,\n",
    "    num_fewshot=3,\n",
    "    limit=1,\n",
    "    batch_size=32,\n",
    "    gen_kwargs={\"max_gen_toks\": 512}, \n",
    "    # device='cuda',\n",
    "    confirm_run_unsafe_code=True\n",
    ")\n",
    "all_results.update(results_few['results'])\n",
    "\n",
    "print(\"Combined results keys:\", list(all_results.keys()))\n",
    "print(\"all_results:\",all_results)\n",
    "\n",
    "\n",
    "# os.makedirs(\n",
    "#     f'results/eval_result',\n",
    "#     exist_ok=True\n",
    "# )\n",
    "# save_filepath = os.path.join(\n",
    "#     f'results/eval_result',\n",
    "#     f\"multi-dimension-eval.json\"\n",
    "# )\n",
    "# with open(save_filepath, \"w\") as file:\n",
    "#     json.dump(all_results, file, indent=4)\n",
    "\n",
    "# print(f\"Combined evaluation results saved to {save_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b2753",
   "metadata": {},
   "source": [
    "# 2. evalscoep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1d25c",
   "metadata": {},
   "source": [
    "## 2.1 ç¯å¢ƒå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºcondaç¯å¢ƒ (å¯é€‰)\n",
    "\n",
    "# å»ºè®®ä½¿ç”¨ python 3.10\n",
    "conda create -n evalscope python=3.10\n",
    "\n",
    "# æ¿€æ´»condaç¯å¢ƒ\n",
    "conda activate evalscope\n",
    "\n",
    "# pipå®‰è£…ä¾èµ–\n",
    "pip install evalscope\n",
    "pip install 'evalscope[app]' -U  # å¯è§†åŒ–ä¾èµ–åŒ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae32ae2",
   "metadata": {},
   "source": [
    "## 2.2 ç¬¬ä¸€æ­¥ï¼šå®šä¹‰ Schema\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª CollectionSchemaï¼Œåœ¨å…¶ä¸­åˆ—å‡ºé€‰ç”¨çš„æ•°æ®é›†åŠå…¶æƒé‡ã€‚\n",
    "\n",
    "> æç¤ºï¼šweight å¯ä»¥æ˜¯ä»»æ„æ­£æ•°ï¼ŒEvalScope ä¼šè‡ªåŠ¨è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚é€šè¿‡ flatten() æ–¹æ³•å¯ä»¥é¢„è§ˆæœ€ç»ˆæ¯ä¸ªæ•°æ®é›†çš„å®é™…å æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0293c5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/evalscope/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetInfo(name='gsm8k', weight=0.25, task_type='', tags=['en'], args={}, hierarchy=['reasoning_index', 'math']), DatasetInfo(name='aime25', weight=0.25, task_type='', tags=['en'], args={}, hierarchy=['reasoning_index', 'math']), DatasetInfo(name='arc', weight=0.25, task_type='', tags=['en'], args={}, hierarchy=['reasoning_index', 'logic']), DatasetInfo(name='ceval', weight=0.25, task_type='', tags=['zh'], args={'subset_list': ['logic']}, hierarchy=['reasoning_index', 'logic'])]\n"
     ]
    }
   ],
   "source": [
    "from evalscope.collections import CollectionSchema, DatasetInfo\n",
    "\n",
    "# åµŒå¥—ç»“æ„ç¤ºä¾‹ï¼šæ•°å­¦ç»„ + æ¨ç†ç»„\n",
    "schema = CollectionSchema(name='reasoning_index', datasets=[\n",
    "    CollectionSchema(name='math', weight=0.5, datasets=[\n",
    "        DatasetInfo(name='gsm8k', weight=0.5, tags=['en']),\n",
    "        DatasetInfo(name='aime25', weight=0.5, tags=['en']),\n",
    "    ]),\n",
    "    CollectionSchema(name='logic', weight=0.5, datasets=[\n",
    "        DatasetInfo(name='arc', weight=0.5, tags=['en']),\n",
    "        DatasetInfo(name='ceval', weight=0.5, tags=['zh'], args={'subset_list': ['logic']}),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# æ‰“å°æŸ¥çœ‹å½’ä¸€åŒ–åçš„æƒé‡åˆ†å¸ƒ\n",
    "print(schema.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2401d",
   "metadata": {},
   "source": [
    "## 2.3 ç¬¬äºŒæ­¥ï¼šé‡‡æ ·æ•°æ®\n",
    "\n",
    "å®šä¹‰å¥½ Schema åï¼Œæˆ‘ä»¬éœ€è¦æŒ‰ç…§ç­–ç•¥æŠ½å–æ ·æœ¬ã€‚EvalScope æä¾›äº†å¤šç§é‡‡æ ·å™¨ï¼š\n",
    "\n",
    "- åŠ æƒé‡‡æ ·ï¼šæ ·æœ¬æ•°é‡ä¸ä½ è®¾ç½®çš„æƒé‡æˆæ­£æ¯”ã€‚æƒé‡è¶Šé«˜çš„ä»»åŠ¡ï¼Œåœ¨æµ‹è¯•é›†ä¸­å‡ºç°çš„é¢˜ç›®è¶Šå¤šï¼Œå¯¹æ€»åˆ†çš„å½±å“ä¹Ÿè¶Šå¤§ã€‚è¿™æœ€èƒ½ä½“ç°â€œä¸šåŠ¡å¯¼å‘â€ã€‚\n",
    "\n",
    "- StratifiedSamplerï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰ï¼šä¿æŒåŸæ•°æ®é›†çš„æ ·æœ¬è§„æ¨¡æ¯”ä¾‹ï¼ˆé€‚åˆä¸åšäººä¸ºå¹²é¢„çš„å®¢è§‚ç»Ÿè®¡ï¼‰ã€‚\n",
    "\n",
    "- UniformSamplerï¼ˆå‡åŒ€é‡‡æ ·ï¼‰ï¼šæ‰€æœ‰æ•°æ®é›†æ ·æœ¬æ•°ç›¸åŒï¼ˆé€‚åˆæ¨ªå‘å¯¹æ¯”å„èƒ½åŠ›çŸ­æ¿ï¼‰ã€‚\n",
    "\n",
    "é’ˆå¯¹â€œæ„å»ºæŒ‡æ•°â€åœºæ™¯ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ åŠ æƒé‡‡æ · (WeightedSampler)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4214b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling data:   0%|          | 0/4 [00:00<?, ?it/s]2026-01-20 11:59:44 - evalscope - \u001b[32mINFO\u001b[0m: No model is provided, using DummyCustomModel for testing.\u001b[0m\n",
      "2026-01-20 11:59:51 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset AI-ModelScope/gsm8k from modelscope > subset: main > split: test ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 131218.12 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 313888.62it/s]\n",
      "2026-01-20 12:00:03 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset AI-ModelScope/gsm8k from modelscope > subset: main > split: train ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 727797.01 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 61455.00it/s]\n",
      "Sampling data:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:23<01:10, 23.62s/it]2026-01-20 12:00:07 - evalscope - \u001b[32mINFO\u001b[0m: No model is provided, using DummyCustomModel for testing.\u001b[0m\n",
      "2026-01-20 12:00:07 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset opencompass/AIME2025 from modelscope > subset: AIME2025-I > split: test ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 2008.32 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 86659.17it/s]\n",
      "2026-01-20 12:00:16 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset opencompass/AIME2025 from modelscope > subset: AIME2025-II > split: test ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 6221.77 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 223101.28it/s]\n",
      "Sampling data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:38<00:36, 18.33s/it]2026-01-20 12:00:22 - evalscope - \u001b[32mINFO\u001b[0m: No model is provided, using DummyCustomModel for testing.\u001b[0m\n",
      "2026-01-20 12:00:22 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset allenai/ai2_arc from modelscope > subset: ARC-Easy > split: test ...\u001b[0m\n",
      "Downloading [README.md]: 9.00kB [00:00, 6.05MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331k/331k [00:00<00:00, 2.54MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 346k/346k [00:00<00:00, 4.52MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.1k/86.1k [00:00<00:00, 6.25MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2251/2251 [00:00<00:00, 374167.89 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 768481.36 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 298732.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 261504.27 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 526587.39it/s]\n",
      "2026-01-20 12:00:32 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset allenai/ai2_arc from modelscope > subset: ARC-Challenge > split: test ...\u001b[0m\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190k/190k [00:00<00:00, 6.10MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204k/204k [00:00<00:00, 8.29MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55.7k/55.7k [00:00<00:00, 3.48MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1119/1119 [00:00<00:00, 198655.13 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 123560.33 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 63376.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 103132.85 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 394204.03it/s]\n",
      "Sampling data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:57<00:18, 18.84s/it]2026-01-20 12:00:42 - evalscope - \u001b[32mINFO\u001b[0m: No model is provided, using DummyCustomModel for testing.\u001b[0m\n",
      "2026-01-20 12:00:42 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset evalscope/ceval from modelscope > subset: logic > split: val ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 2693.44 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 128695.52it/s]\n",
      "2026-01-20 12:00:49 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset evalscope/ceval from modelscope > subset: logic > split: dev ...\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 646.57 examples/s]\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 51527.08it/s]\n",
      "Sampling data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:09<00:00, 17.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from evalscope.collections import WeightedSampler\n",
    "from evalscope.utils.io_utils import dump_jsonl_data\n",
    "\n",
    "# åˆå§‹åŒ–åŠ æƒé‡‡æ ·å™¨\n",
    "sampler = WeightedSampler(schema)\n",
    "\n",
    "# é‡‡æ · 100 æ¡æ•°æ®ä½œä¸ºæœ€ç»ˆæµ‹è¯•é›†\n",
    "# æ ¹æ®æƒé‡ï¼ŒçŸ¥è¯†é—®ç­” 30 æ¡ï¼Œé•¿æ–‡æœ¬æ£€ç´¢ 30 æ¡ï¼ŒæŒ‡ä»¤éµå¾ª 40 æ¡\n",
    "# å®é™…é‡‡æ ·æ•°é‡å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "mixed_data = sampler.sample(count=10)\n",
    "\n",
    "# å°†æ··åˆå¥½çš„æ•°æ®ä¿å­˜ä¸º JSONL æ–‡ä»¶ï¼Œè¿™å°±æ˜¯ä½ çš„â€œæŒ‡æ•°è¯„æµ‹é›†â€\n",
    "dump_jsonl_data(mixed_data, 'data/index_testset.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f6a73",
   "metadata": {},
   "source": [
    "## 2.4 ç¬¬ä¸‰æ­¥ï¼šç»Ÿä¸€è¯„æµ‹\n",
    "\n",
    "ç°åœ¨ï¼Œä½ æ‹¥æœ‰äº†ä¸€ä¸ªåä¸º rag_index_testset.jsonl çš„æ–‡ä»¶ã€‚åœ¨ EvalScope çœ‹æ¥ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„æœ¬åœ°æ•°æ®é›†ã€‚æˆ‘ä»¬ç›´æ¥è°ƒç”¨ run_task å³å¯ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89459172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/evalscope/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: No eval_type is provided, setting eval_type to CHECKPOINT.\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Args: Task config is provided with TaskConfig type.\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Set resume from outputs/20260119_232050\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Running with native backend\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Dump task config to outputs/20260119_232050/configs/task_config_b42323.yaml\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: {\n",
      "    \"model\": \"openai-community/gpt2\",\n",
      "    \"model_id\": \"gpt2\",\n",
      "    \"model_args\": {\n",
      "        \"revision\": \"master\",\n",
      "        \"precision\": \"torch.float16\"\n",
      "    },\n",
      "    \"model_task\": \"text_generation\",\n",
      "    \"chat_template\": null,\n",
      "    \"datasets\": [\n",
      "        \"data_collection\"\n",
      "    ],\n",
      "    \"dataset_args\": {\n",
      "        \"data_collection\": {\n",
      "            \"local_path\": \"data/index_testset.jsonl\",\n",
      "            \"shuffle\": true\n",
      "        }\n",
      "    },\n",
      "    \"dataset_dir\": \"/Users/xuhu.6736/.cache/modelscope/hub/datasets\",\n",
      "    \"dataset_hub\": \"modelscope\",\n",
      "    \"repeats\": 1,\n",
      "    \"generation_config\": {\n",
      "        \"batch_size\": 5,\n",
      "        \"temperature\": 0.0\n",
      "    },\n",
      "    \"eval_type\": \"llm_ckpt\",\n",
      "    \"eval_backend\": \"Native\",\n",
      "    \"eval_config\": null,\n",
      "    \"limit\": null,\n",
      "    \"eval_batch_size\": 5,\n",
      "    \"use_cache\": \"outputs/20260119_232050\",\n",
      "    \"rerun_review\": false,\n",
      "    \"work_dir\": \"outputs/20260119_232050\",\n",
      "    \"no_timestamp\": false,\n",
      "    \"ignore_errors\": false,\n",
      "    \"debug\": false,\n",
      "    \"seed\": 42,\n",
      "    \"api_url\": null,\n",
      "    \"timeout\": null,\n",
      "    \"stream\": null,\n",
      "    \"judge_strategy\": \"auto\",\n",
      "    \"judge_worker_num\": 1,\n",
      "    \"judge_model_args\": {},\n",
      "    \"analysis_report\": false,\n",
      "    \"use_sandbox\": false,\n",
      "    \"sandbox_type\": \"docker\",\n",
      "    \"sandbox_manager_config\": {},\n",
      "    \"evalscope_version\": \"1.4.2\"\n",
      "}\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Start loading benchmark dataset: data_collection\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Loading dataset from data/index_testset.jsonl\u001b[0m\n",
      "Processing records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1786.03it/s]\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Start evaluating 1 subsets of the data_collection: ['default']\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Evaluating subset: default\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Getting predictions for subset: default\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Reusing predictions from outputs/20260119_232050/predictions/gpt2/data_collection_default.jsonl, got 5 predictions, remaining 5 samples\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Processing 5 samples, if data is large, it may take a while.\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Loading model for prediction...\u001b[0m\n",
      "2026-01-20 13:03:57 - evalscope - \u001b[32mINFO\u001b[0m: Creating model openai-community/gpt2 with eval_type=llm_ckpt base_url=None, config={'retries': 5, 'retry_interval': 10, 'batch_size': 5, 'temperature': 0.0}, model_args={'revision': 'master', 'precision': 'torch.float16'}\u001b[0m\n",
      "Evaluating [data_collection]:   0%|          | 0/1 [00:00<?, ?subset/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/xuhu.6736/.cache/modelscope/hub/models/openai-community/gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/xuhu.6736/.cache/modelscope/hub/models/openai-community/gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:04:02 - evalscope - \u001b[32mINFO\u001b[0m: Model loaded successfully.\u001b[0m\n",
      "Evaluating [data_collection]:   0%|          | 0/1 [00:04<?, ?subset/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4885 > 1024). Running this sequence through the model will result in indexing errors\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:04:58 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 01:00 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:05:58 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 02:00 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:06:58 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 03:00 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:07:58 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 04:01 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:08:59 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 05:01 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:09:59 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 06:01 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:10:59 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 07:01 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:12:00 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 08:02 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:13:00 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 09:02 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:14:01 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 10:03 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:15:01 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 11:03 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:16:02 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 12:04 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:17:02 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 13:04 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:18:02 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 14:04 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:19:02 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 15:04 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:20:03 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 16:05 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:21:03 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 17:05 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:22:03 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 18:05 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:23:04 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 19:06 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:24:04 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 20:06 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:25:05 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 21:07 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:26:05 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 22:07 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:27:05 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 23:08 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:28:06 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 24:08 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:29:06 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 25:08 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:30:06 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:    0%| 0/5 [Elapsed: 26:08 < Remaining: ?, ?it/s]\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                                 2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Predicting[data_collection@default]:  100%| 5/5 [Elapsed: 26:46 < Remaining: 00:00, 1606.14s/it]\u001b[0m\n",
      "Predicting[data_collection@default]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [26:46<00:00, 321.23s/it] \n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Finished getting predictions for subset: default.\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Getting reviews for subset: default\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Reviewing 10 samples, if data is large, it may take a while.\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:30:44 - evalscope - \u001b[33mWARNING\u001b[0m: No valid answer found in prediction: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:30:44 - evalscope - \u001b[33mWARNING\u001b[0m: No valid answer found in prediction: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:30:44 - evalscope - \u001b[33mWARNING\u001b[0m: No valid answer found in prediction: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                       2026-01-20 13:30:44 - evalscope - \u001b[33mWARNING\u001b[0m: No valid answer found in prediction: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "ï¿½\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\n",
      "\n",
      "ï¿½\n",
      "\n",
      "ï¿½\n",
      "\n",
      "\u001b[0m\n",
      "                                                                       \n",
      "\u001b[A                                                                                2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Reviewing[data_collection@default]:  100%| 10/10 [Elapsed: 00:00 < Remaining: 00:00, 91.25it/s]\u001b[0m\n",
      "Reviewing[data_collection@default]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 89.93it/s]\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Finished reviewing subset: default. Total reviewed: 10\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Aggregating scores for subset: default\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Evaluating [data_collection] 100%| 1/1 [Elapsed: 26:46 < Remaining: 00:00, 1606.30s/subset]\u001b[0m\n",
      "Evaluating [data_collection]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [26:46<00:00, 1606.30s/subset]\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Generating report...\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: subset_level Report:\n",
      "+-----------+--------------+-------------+------------+-------+\n",
      "| task_type | dataset_name | subset_name | micro_avg. | count |\n",
      "+-----------+--------------+-------------+------------+-------+\n",
      "|           |    ceval     |    logic    |    0.0     |   4   |\n",
      "|           |     arc      |  ARC-Easy   |    0.0     |   2   |\n",
      "|           |    gsm8k     |    main     |    0.0     |   2   |\n",
      "|           |    aime25    | AIME2025-I  |    0.0     |   1   |\n",
      "|           |    aime25    | AIME2025-II |    0.0     |   1   |\n",
      "+-----------+--------------+-------------+------------+-------+\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: dataset_level Report:\n",
      "+-----------+--------------+------------+------------+---------------+-------+\n",
      "| task_type | dataset_name | micro_avg. | macro_avg. | weighted_avg. | count |\n",
      "+-----------+--------------+------------+------------+---------------+-------+\n",
      "|           |    ceval     |    0.0     |    0.0     |      0.0      |   4   |\n",
      "|           |    aime25    |    0.0     |    0.0     |      0.0      |   2   |\n",
      "|           |     arc      |    0.0     |    0.0     |      0.0      |   2   |\n",
      "|           |    gsm8k     |    0.0     |    0.0     |      0.0      |   2   |\n",
      "+-----------+--------------+------------+------------+---------------+-------+\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: task_level Report:\n",
      "+-----------+------------+------------+---------------+-------+\n",
      "| task_type | micro_avg. | macro_avg. | weighted_avg. | count |\n",
      "+-----------+------------+------------+---------------+-------+\n",
      "|           |    0.0     |    0.0     |      0.0      |  10   |\n",
      "+-----------+------------+------------+---------------+-------+\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: tag_level Report:\n",
      "+------+------------+------------+---------------+-------+\n",
      "| tags | micro_avg. | macro_avg. | weighted_avg. | count |\n",
      "+------+------------+------------+---------------+-------+\n",
      "|  en  |    0.0     |    0.0     |      0.0      |   6   |\n",
      "|  zh  |    0.0     |    0.0     |      0.0      |   4   |\n",
      "+------+------------+------------+---------------+-------+\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: category_level Report:\n",
      "+-----------------+-----------+------------+------------+---------------+-------+\n",
      "|    category0    | category1 | micro_avg. | macro_avg. | weighted_avg. | count |\n",
      "+-----------------+-----------+------------+------------+---------------+-------+\n",
      "| reasoning_index |   logic   |    0.0     |    0.0     |      0.0      |   6   |\n",
      "| reasoning_index |   math    |    0.0     |    0.0     |      0.0      |   4   |\n",
      "+-----------------+-----------+------------+------------+---------------+-------+\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: \n",
      "data_collection report table:\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| Model   | Dataset         | Metric   | Subset             |   Num |   Score | Cat.0           | Cat.1   |\n",
      "+=========+=================+==========+====================+=======+=========+=================+=========+\n",
      "| gpt2    | data_collection | acc      | arc/ARC-Easy       |     2 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | ceval/logic        |     4 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | aime25/AIME2025-I  |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | aime25/AIME2025-II |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | gsm8k/main         |     2 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | OVERALL            |    10 |       0 | -               |         |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+ \n",
      "\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Skipping report analysis (`analysis_report=False`).\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Dump report to: outputs/20260119_232050/reports/gpt2/data_collection.json \n",
      "\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Benchmark data_collection evaluation finished.\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Overall report table: \n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| Model   | Dataset         | Metric   | Subset             |   Num |   Score | Cat.0           | Cat.1   |\n",
      "+=========+=================+==========+====================+=======+=========+=================+=========+\n",
      "| gpt2    | data_collection | acc      | arc/ARC-Easy       |     2 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | ceval/logic        |     4 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | aime25/AIME2025-I  |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | aime25/AIME2025-II |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | gsm8k/main         |     2 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | data_collection | acc      | OVERALL            |    10 |       0 | -               |         |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | arc/ARC-Challenge  |     1 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | arc/ARC-Easy       |     1 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | ceval/logic        |     4 |       0 | reasoning_index | logic   |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | aime25/AIME2025-I  |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | aime25/AIME2025-II |     1 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | gsm8k/main         |     2 |       0 | reasoning_index | math    |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+\n",
      "| gpt2    | index_testset   | Average  | OVERALL            |    10 |       0 | -               |         |\n",
      "+---------+-----------------+----------+--------------------+-------+---------+-----------------+---------+ \n",
      "\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Finished evaluation for gpt2 on ['data_collection']\u001b[0m\n",
      "2026-01-20 13:30:44 - evalscope - \u001b[32mINFO\u001b[0m: Output directory: outputs/20260119_232050\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_collection': Report(name='data_collection', dataset_name='data_collection', dataset_pretty_name='', dataset_description='', model_name='gpt2', score=0.0, metrics=[Metric(name='acc', num=10, score=0.0, macro_score=0.0, categories=[Category(name=('reasoning_index', 'logic'), num=6, score=0.0, macro_score=0.0, subsets=[Subset(name='arc/ARC-Easy', score=0.0, num=2), Subset(name='ceval/logic', score=0.0, num=4)]), Category(name=('reasoning_index', 'math'), num=4, score=0.0, macro_score=0.0, subsets=[Subset(name='aime25/AIME2025-I', score=0.0, num=1), Subset(name='aime25/AIME2025-II', score=0.0, num=1), Subset(name='gsm8k/main', score=0.0, num=2)])])], analysis='N/A')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from evalscope import TaskConfig, run_task\n",
    "\n",
    "task_cfg = TaskConfig(\n",
    "    # model='qwen2.5-14b-instruct', # è¯„æµ‹æ¨¡å‹\n",
    "    # # ä½¿ç”¨ä¸€ä¸ªæä¾› OpenAI å…¼å®¹æ¥å£çš„æ¨¡å‹è¿›è¡Œè¯„æµ‹\n",
    "    # # å¯ä»¥æ˜¯äº‘ä¸Šçš„APIï¼Œä¹Ÿå¯ä»¥æ˜¯é€šè¿‡vllmç­‰æ¡†æ¶éƒ¨ç½²çš„æœ¬åœ°æ¨¡å‹\n",
    "    # api_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "    # api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    # eval_type='openai_api',\n",
    "    model='openai-community/gpt2', # è¯„æµ‹æ¨¡å‹\n",
    "    \n",
    "    # å…³é”®é…ç½®ï¼šæŒ‡å®šæ•°æ®é›†ä¸º 'data_collection' æ¨¡å¼\n",
    "    datasets=['data_collection'],\n",
    "    dataset_args={\n",
    "        'data_collection': {\n",
    "            'local_path': 'data/index_testset.jsonl', # æŒ‡å‘åˆšæ‰ç”Ÿæˆçš„æ–‡ä»¶\n",
    "            'shuffle': True # æ‰“ä¹±é¡ºåº\n",
    "        }\n",
    "    },\n",
    "    eval_batch_size=5, # æ ¹æ®ä½ çš„ API å¹¶å‘é™é¢è°ƒæ•´\n",
    "    generation_config={\n",
    "        'temperature': 0.0 # è¯„æµ‹é€šå¸¸è®¾ä¸º 0 ä»¥ä¿è¯ç»“æœå¯å¤ç°\n",
    "    },\n",
    "    use_cache=\"outputs/20260119_232050\", # å¤ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„çš„æ¨ç†ç»“æœå’Œè¯„æµ‹ç»“æœ\n",
    ")\n",
    "\n",
    "run_task(task_cfg=task_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5470b7b",
   "metadata": {},
   "source": [
    "è¯„æµ‹å®Œæˆåï¼Œä½ ä¼šåœ¨æ—¥å¿—ä¸­çœ‹åˆ° Overall report tableï¼ŒåŒ…å«åœ¨æ¯ä¸ªè‡ªå·±ä¸Šçš„è¯„æµ‹ç»“æœï¼š\n",
    "\n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| Model   | Dataset       | Metric   | Subset             |   Num |   Score | Cat.0           | Cat.1   |  \n",
    "+=========+===============+==========+====================+=======+=========+=================+=========+    \n",
    "| gpt2    | index_testset | Average  | arc/ARC-Challenge  |     1 |       0 | reasoning_index | logic   |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | arc/ARC-Easy       |     1 |       0 | reasoning_index | logic   |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | ceval/logic        |     4 |       0 | reasoning_index | logic   |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | aime25/AIME2025-I  |     1 |       0 | reasoning_index | math    |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | aime25/AIME2025-II |     1 |       0 | reasoning_index | math    |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | gsm8k/main         |     2 |       0 | reasoning_index | math    |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+  \n",
    "| gpt2    | index_testset | Average  | OVERALL            |    10 |       0 | -               |         |  \n",
    "+---------+---------------+----------+--------------------+-------+---------+-----------------+---------+   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167aef8",
   "metadata": {},
   "source": [
    "## 2.5 ç¬¬å››æ­¥ï¼šCaseåˆ†æ\n",
    "\n",
    "æ‹¿åˆ° Index æ€»åˆ†å¹¶ä¸ä»£è¡¨å·¥ä½œç»“æŸã€‚ç›¸åï¼Œåˆ†ææ¨¡å‹ä¸ºä»€ä¹ˆå–å¾—è¿™ä¸ªåˆ†æ•°é€šå¸¸æ›´æœ‰ä»·å€¼ï¼šæ˜¯æ¨¡å‹çŸ¥è¯†å‚¨å¤‡ä¸è¶³ï¼Ÿè¿˜æ˜¯ä»…ä»…å› ä¸ºæ²¡æŒ‰æ ¼å¼è¾“å‡ºè¢«è¯¯åˆ¤ï¼Ÿ\n",
    "\n",
    "EvalScope å†…ç½®äº†å¯è§†åŒ–åˆ†æå·¥å…·ï¼Œè®©ä½ éå¸¸æ–¹ä¾¿æŸ¥çœ‹æ¯ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†æƒ…å†µã€‚æ— éœ€ç¼–å†™é¢å¤–ä»£ç ï¼Œåªéœ€åœ¨ç»ˆç«¯è¿è¡Œä¸€æ¡å‘½ä»¤ï¼Œå³å¯å¯åŠ¨æœ¬åœ°æœåŠ¡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscope app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee3347",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥è®¿é—® `http://localhost:7860` æˆ–è€… `http://0.0.0.0:7860` å³å¯çœ‹åˆ°å¯è§†åŒ–ç•Œé¢ã€‚\n",
    "  \n",
    "<center align=\"center\">\n",
    "    <img src=\"images/evalscope_panel.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evalchemy_intro",
   "metadata": {},
   "source": [
    "# 3. Evalchemy\n",
    "\n",
    "## 3.1 ç®€ä»‹\n",
    "\n",
    "[Evalchemy](https://github.com/mlfoundations/evalchemy) æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è¯„æµ‹æ¡†æ¶ï¼Œç”±ML Foundationså¼€å‘ã€‚å®ƒæ³¨é‡å¯å¤ç°æ€§ã€æ‰©å±•æ€§å’Œç®€æ´æ€§ï¼Œç‰¹åˆ«é€‚åˆç ”ç©¶äººå‘˜è¿›è¡Œå¿«é€Ÿå®éªŒå’ŒåŸå‹å¼€å‘ã€‚\n",
    "\n",
    "## 3.2 ä¸»è¦ç‰¹ç‚¹\n",
    "\n",
    "- **è½»é‡çº§è®¾è®¡**ï¼šæ ¸å¿ƒä»£ç ç®€æ´ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹\n",
    "- **é«˜åº¦å¯æ‰©å±•**ï¼šæ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡ã€æŒ‡æ ‡å’Œæ¨¡å‹\n",
    "- **å¯å¤ç°æ€§**ï¼šå†…ç½®å®éªŒè·Ÿè¸ªå’Œç‰ˆæœ¬æ§åˆ¶\n",
    "- **ç ”ç©¶å¯¼å‘**ï¼šé€‚åˆå­¦æœ¯ç ”ç©¶å’Œå¿«é€ŸåŸå‹å¼€å‘\n",
    "\n",
    "## 3.3 å®‰è£…å’Œä½¿ç”¨\n",
    "\n",
    "```bash\n",
    "pip install evalchemy\n",
    "```\n",
    "\n",
    "åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "import evalchemy as ec\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„è¯„æµ‹ä»»åŠ¡\n",
    "@ec.task\n",
    "def simple_qa(model, question):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    response = model.generate(prompt)\n",
    "    return response\n",
    "\n",
    "# åˆ›å»ºè¯„æµ‹é…ç½®\n",
    "config = ec.Config(\n",
    "    model=\"gpt2\",\n",
    "    tasks=[\"simple_qa\"],\n",
    "    metrics=[\"exact_match\", \"f1_score\"]\n",
    ")\n",
    "\n",
    "# è¿è¡Œè¯„æµ‹\n",
    "results = ec.run(config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lighteval_intro",
   "metadata": {},
   "source": [
    "# 4. LightEval\n",
    "\n",
    "## 4.1 ç®€ä»‹\n",
    "\n",
    "[LightEval](https://github.com/huggingface/lighteval) æ˜¯Hugging Faceå¼€å‘çš„è¯„æµ‹æ¡†æ¶ï¼Œæ·±åº¦é›†æˆTransformersç”Ÿæ€ç³»ç»Ÿã€‚å®ƒæä¾›äº†ç»Ÿä¸€çš„æ¥å£æ¥è¯„æµ‹Hugging Faceæ¨¡å‹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹å’Œè¯„æµ‹æŒ‡æ ‡ã€‚\n",
    "\n",
    "## 4.2 ä¸»è¦ç‰¹ç‚¹\n",
    "\n",
    "- **Transformersé›†æˆ**ï¼šæ— ç¼å¯¹æ¥Hugging Faceæ¨¡å‹å’Œæ•°æ®é›†\n",
    "- **ä»»åŠ¡å¤šæ ·æ€§**ï¼šæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€åˆ†ç±»ç­‰å¤šç§ä»»åŠ¡\n",
    "- **åˆ†å¸ƒå¼è¯„æµ‹**ï¼šæ”¯æŒå¤šGPUå¹¶è¡Œè¯„æµ‹\n",
    "- **æ˜“äºä½¿ç”¨**ï¼šç®€æ´çš„APIè®¾è®¡\n",
    "\n",
    "## 4.3 å®‰è£…å’Œä½¿ç”¨\n",
    "\n",
    "```bash\n",
    "pip install lighteval\n",
    "```\n",
    "\n",
    "åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from lighteval import LightEval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# åˆ›å»ºè¯„æµ‹å™¨\n",
    "evaluator = LightEval(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    tasks=[\"hellaswag\", \"winogrande\"],  # è¦è¯„æµ‹çš„ä»»åŠ¡\n",
    "    num_fewshot=0,  # few-shotç¤ºä¾‹æ•°é‡\n",
    "    batch_size=8,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# è¿è¡Œè¯„æµ‹\n",
    "results = evaluator.run()\n",
    "print(results)\n",
    "```\n",
    "\n",
    "## 4.4 å‘½ä»¤è¡Œä½¿ç”¨\n",
    "\n",
    "LightEvalä¹Ÿæ”¯æŒå‘½ä»¤è¡Œä½¿ç”¨ï¼š\n",
    "\n",
    "```bash\n",
    "lighteval evaluate \\\n",
    "    --model \"gpt2\" \\\n",
    "    --tasks \"hellaswag,winogrande\" \\\n",
    "    --num_fewshot 0 \\\n",
    "    --batch_size 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frameworks_summary",
   "metadata": {},
   "source": [
    "# 5. æ€»ç»“\n",
    "\n",
    "è¿™å››ä¸ªè¯„æµ‹æ¡†æ¶å„æœ‰ç‰¹è‰²ï¼š\n",
    "\n",
    "- **lm-evaluation-harness**ï¼šå­¦æœ¯ç•Œæ ‡å‡†ï¼ŒåŠŸèƒ½æœ€å…¨é¢\n",
    "- **evalscope**ï¼šäº§ä¸šåº”ç”¨å¯¼å‘ï¼Œæ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ç»„åˆå’Œå¯è§†åŒ–\n",
    "- **Evalchemy**ï¼šè½»é‡çº§ï¼Œé€‚åˆç ”ç©¶å’Œå¿«é€ŸåŸå‹\n",
    "- **LightEval**ï¼šHugging Faceç”Ÿæ€é›†æˆï¼Œæ˜“äºä½¿ç”¨\n",
    "\n",
    "é€‰æ‹©å“ªä¸ªæ¡†æ¶ä¸»è¦å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ï¼š\n",
    "- å¦‚æœè¿›è¡Œå­¦æœ¯ç ”ç©¶ï¼Œæ¨èä½¿ç”¨lm-evaluation-harness\n",
    "- å¦‚æœéœ€è¦æ„å»ºè‡ªå®šä¹‰è¯„æµ‹é›†å’Œå¯è§†åŒ–åˆ†æï¼Œæ¨èevalscope\n",
    "- å¦‚æœæ˜¯Hugging Faceç”¨æˆ·ï¼Œæ¨èLightEval\n",
    "- å¦‚æœéœ€è¦å¿«é€ŸåŸå‹å’Œå®éªŒï¼Œæ¨èEvalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e75be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
