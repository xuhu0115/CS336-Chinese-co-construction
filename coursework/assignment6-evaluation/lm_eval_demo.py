from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

import os
os.environ["HF_ALLOW_CODE_EVAL"] = "1"

# eval_model_path = "/home/magnus-share/xuhu/model/Qwen2___5-Math-1___5B"
eval_model_path = "openai-community/gpt2"
zero_shot_tasks = ["arc_easy", "piqa", "lambada", "triviaqa"]
few_shot_tasks = ["humaneval", "mbpp", "gsm8k", "minerva_math"]
all_results = {}

# ğŸ”¥ å…³é”®ï¼šåªåˆ›å»ºä¸€æ¬¡æ¨¡å‹å®ä¾‹
lm = HFLM(
    pretrained=eval_model_path,      # å¯ä»¥æ˜¯è·¯å¾„ï¼Œä¼šè‡ªåŠ¨åŠ è½½
    tokenizer=eval_model_path,
    batch_size=32,
    device="cpu",
    max_length=1024,
)

# ===== 1. Zero-shot evaluation =====
print(f"Evaluating zero-shot tasks: {zero_shot_tasks}")
results_zero = evaluator.simple_evaluate(
    model=lm,  # â† å¤ç”¨åŒä¸€ä¸ª lm å®ä¾‹
    tasks=zero_shot_tasks,
    num_fewshot=0,
    limit=1,
    batch_size=32,
    gen_kwargs={"max_gen_toks": 512}, 
    # device='cuda',
    confirm_run_unsafe_code=True
)
all_results.update(results_zero['results'])

# ===== 2. Few-shot evaluation =====
print(f"Evaluating 3-shot tasks: {few_shot_tasks}")
results_few = evaluator.simple_evaluate(
    model=lm,  # â† åŒä¸€ä¸ª lm å®ä¾‹
    tasks=few_shot_tasks,
    num_fewshot=3,
    limit=1,
    batch_size=32,
    gen_kwargs={"max_gen_toks": 512}, 
    # device='cuda',
    confirm_run_unsafe_code=True
)
all_results.update(results_few['results'])

print("Combined results keys:", list(all_results.keys()))
print("all_results:",all_results)