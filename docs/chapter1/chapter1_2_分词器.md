# 分词器
分词器是连接人类自然语言与机器计算的“关键桥梁”，负责将非结构化的文本高效地编码为模型可理解的数字序列（Tokens）。其地位至关重要，它决定了模型的输入形式。我们可以将大语言模型（LLM）视为一位阅读者，分词器则执行了文本的“认知拆解”即将连续的文本流切分为具备最小语义或功能意义的基本单元，类似于人类阅读时对词汇或音节的本能识别。本章节将剖析分词器实现文本“数字化”的核心原理，并探讨BPE等主流分词算法的实际应用。

<p align="center">
<img width="600" height="350" alt="0e717e4cf0df64875a271123bf962631" src="https://github.com/user-attachments/assets/a53dd727-8682-4314-92dc-3b3e18dfaf58" />
</p>

### 1分词器原理
在我们把海量数据喂给大模型之前，必须先经过一道关键工序——分词。分词器常被视为LLM的一部分，但它其实拥有独立的训练生命周期。我们需要利用正则表达式对原始文本进行预处理，并统计构建出一套高效的**词元————数字**映射表。这个映射过程决定了模型眼中的世界是由字、词还是更碎的片段组成的，直接影响后续模型对语义的理解效率。也正因如此，[分词器](https://tiktokenizer.vercel.app/?model=deepseek-ai%2FDeepSeek-R1)虽独立训练却与LLM保持着“强耦合”的关系。

分词器目的是把原始文本转换为词元（token）序列，这个预训练过程表示为:

```

第一步：准备语料
       |
       |  输入海量原始文本（如 100GB 文本）
       V
第二步：预分词
       |
       |  利用正则化表达式根据空格、标点等进行初步切分
       |  得到基础词单元序列（如将 "learning" 切分为 "l, e, a, r, n, i, n, g"）
       V
第三步：统计与迭代合并 <--- 核心步骤
       |
       |  1. 统计所有相邻字符对的【共现频率】
       |  2. 找到频率最高的一对（例如 "i" 和 "n" 出现最多）
       |  3. 将其合并为新token "in"，并加入词表
       |  4. 循环重复上述步骤，直到词表达到预定大小（如 32k 或 100k）
       V
第四步：输出产物
       |
       |
       V
得到:词表、合并规则表(Merges)

```

### 2常用的分词器

#### 2.1 字符分词器

#### 2.2 字节分词器

#### 2.3 词级分词器

#### 2.4 BPE分词器

