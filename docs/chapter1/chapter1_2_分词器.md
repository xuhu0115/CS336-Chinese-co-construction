# 分词器
分词器是连接人类自然语言与机器计算的“关键桥梁”，负责将非结构化的文本高效地编码为模型可理解的数字序列（Tokens）。其地位至关重要，它决定了模型的输入形式。我们可以将大语言模型（LLM）视为一位阅读者，分词器则执行了文本的“认知拆解”即将连续的文本流切分为具备最小语义或功能意义的基本单元，类似于人类阅读时对词汇或音节的本能识别。本章节将剖析分词器实现文本“数字化”的核心原理，并探讨BPE等主流分词算法的实际应用。

<p align="center">
<img width="600" height="350" alt="0e717e4cf0df64875a271123bf962631" src="https://github.com/user-attachments/assets/a53dd727-8682-4314-92dc-3b3e18dfaf58" />
</p>

### 1训练分词器
在我们把海量数据喂给大模型之前，必须先经过一道关键工序——分词。分词器常被视为LLM的一部分，但它其实拥有独立的训练生命周期。我们需要利用正则表达式对原始文本进行预处理，并统计构建出一套高效的**词元————数字**映射表。这个映射过程决定了模型眼中的世界是由字、词还是更碎的片段组成的，直接影响后续模型对语义的理解效率。也正因如此，[分词器](https://tiktokenizer.vercel.app/?model=deepseek-ai%2FDeepSeek-R1)虽独立训练却与LLM保持着“强耦合”的关系。

>训练一个用于现代大型语言模型的分词器以BPE为例可以拆成四步：准备语料 → 预分词/初始化基础单元 → 统计并迭代合并（训练）→ 输出产物并用于编码、解码。

#### 第一步：准备语料（收集与预处理）

目标：得到大规模、干净、代表性的数据，保证训练出的词表能覆盖目标任务、语言域。

1. **收集多样化语料**
   - 覆盖目标语言、领域（例如新闻、维基、社交、代码）、风格、字符集（例如数学符号、拉丁字母、中文、emoji等）。
   - 大小推荐：至少几GB到几十GB，常见LLM训练用百GB级别语料，小试验可用几百MB。
2. **去重与采样**
   - 由于重复会严重偏置频率统计，所以这里需要去掉完全重复的文档。
   - 如果corpus非常不均衡，可按域进行分层采样以保证代表性。
3. **文本正规化**
   - Unicode标准化，通常用NFKC或NFC=。
   - 统一换行、制表符为空格或保留特定符号，取决于是否需要保留原始格式。
   - 是否小写是可选的——现代LLM通常**不小写**以保留大小写信息。
4. **保留或显式标记空白**
   - 子词分词器通常需要知道单词边界（例如tokens前的空格）；很多实现通过在单词前保留一个显式空格符或特殊前缀符号来处理。不要盲目删除所有空格。
5. **分句、分词（可选）**
   - 对于 BPE 通常按“词”（word）或空格分片进行预分割（详见第二步）。对中文等无空格语言，按字符序列输入即可。
6. **保存原始/清洗日志**，并切分训练与验证集（比如99%训练、1%验证）用于监控合并质量和过拟合。

#### 注意事项

- 未去重导致少数样本被过度学习（重复句子出现很多次会把对应子词合并优先级抬高）。
- 误删空格或换行，导致无法正确学习前导空格的token。


#### 第二步：预分词与初始化基础单元（确定“原子”单位）

目标：定义训练时的“最小单元”并把文本映射成这些单元序列，作为合并的初始输入。

#### 核心概念

* **最小单元不是“词”也不是任意“基础词单元”**；现代做法常见两种：

  1. **Byte-level（字节级）单元**（Byte-level BPE, BBPE）：把文本通过 UTF-8 编码为字节（0–255），所以初始字典是 256 种字节，能无歧义处理任意 Unicode 字符。
  2. **Character-level（字符级）单元**：把 Unicode codepoints 作为初始单元（对某些语言可能更直观）。
* **“预分词”通常是按空格/标点将文本切成单词片段（tokenization pre-splitting）**，然后把每个“单词片段”拆成上述基础单元（字节或字符）。

  * 例如（使用 byte-level）：字符串 `" learning is fun"` → 明确保留**前导空格**：`" learning"`, `" is"`, `" fun"` → 每个分片再拆成 bytes：`[b' ', b'l', b'e', ...]`。
  * 注意前导空格会被看成字节（或一个特殊前缀符），这在 GPT 风格 tokenizer 中常用来区分词边界（像 `Ġ` 的效果）。

#### 为什么选字节级（推荐）

* 字节级能覆盖所有 Unicode，不会产生未知字符（<UNK>），对表情/特殊符号/非拉丁文字支持更健壮。
* 很多开源实现（如 GPT / tiktoken 概念）采用字节级 BPE。

#### 实用参数与建议

* **初始 vocab**：如果是 byte-level，初始是 256（所有字节）＋保留的特殊 token（例如 `<PAD>, <BOS>, <EOS>, <UNK>`）。
* **是否保留大小写和空格**：通常保留大小写；把空格作为 token 的一部分（或在前面加特殊字符）以保留单词边界信息。


#### 第三步：统计与迭代合并（训练核心）

目标：从初始基础单元出发，贪婪地合并频繁的相邻单元对，形成子词（subword）直到词表大小或合并次数达到目标。

> 统计对象是当前token序列中相邻对的共现频率——不是始终固定的“字符对”，合并是**贪婪**、按频率最高对逐步进行，合并操作改变语料中token的分布并影响后续统计。

#### 算法流程

```
"""
统计与合并
"""
输入：预分词后的语料（每个单词拆成基础单元序列），目标vocab_size或max_merges
初始化vocab = set(所有基础单元)
merges = []  #记录合并操作的顺序

while len(vocab) < target_vocab_size and iterations < max_merges:

    # 1.统计所有相邻token对（pair）在整个语料中的出现次数
    pair_counts = count_adjacent_pairs(corpus_as_token_seqs)

    # 2.找到出现次数最高的 pair p = (a, b)
    p = argmax(pair_counts)
    if pair_counts[p] < min_frequency_threshold: break  #可选阈值

    # 3.将p合并为新token ab
    vocab.add(ab)
    merges.append(p)

    # 4.在语料中把每处(a, b)替换为(ab)
    corpus_as_token_seqs = replace_pair_with_token(corpus_as_token_seqs, p)
```

#### 细节解释

- **为什么是“相邻token对”**：随着合并，原来的两元组可能变成一个新的token，下次统计会以新的token参与共现统计。
- **合并是“贪婪的”**：每步只选择当前频率最高的一对并合并，顺序决定最终的tokens结构，因此合并顺序需要保存（merges文件）。
- **终止条件**：

  - 达到目标vocab大小（例如30k、50k、100k），或
  - 达到最大合并次数
  - 到达最低合并频次阈值（避免噪声驱动的少样本合并）。
- **性能优化**：

  - 直接在完整语料上重复替换非常耗时、耗内存；生产级实现使用更高效的数据结构（优先队列/字典增量更新、并行化、streaming）。
  - 对超大语料可以用采样而非全部数据，每次迭代统计时用缓存和增量更新。

#### 实例分析

预分词并拆字节示例（以字符展示便于理解）：

* `["l","e","a","r","n","i","n","g"]`
* `["i","s"]`
* `["f","u","n"]`
  统计相邻对（初始）可能：`("n","g"): 1000`, `("i","n"): 1200`, `("l","e"): 900` 等。
  合并频率最高的 `("i","n")` → 新 token `"in"`，把语料中每处 `["i","n"]` 替换为 `["in"]`，下一轮继续统计更新后的相邻对（注意 `"in"` 可能与 `g` 连成 `("in","g")`）。

#### 其他合并原理

- **Unigram**：不是贪婪合并，而是基于概率模型选词表（另一种常见方法）。
- **BPE-dropout、subword regularization**：训练或编码时随机丢弃合并以获得更鲁棒的分割。

#### 第四步：输出产物与如何使用（存储、编码、解码、版本管理）

目标：把训练结果保存为可重复使用的 tokenizer（包括 vocab、merges、预处理规则），并说明如何用它对新文本进行编码/解码。

#### 必要产物

1. 词表
   - 列出所有token（基础单元 + 新生成的合并token）及对应的token id。
   - 通常以JSON、TSV、二进制格式保存例如`vocab.json`。
2. 合并规则表（merges），按执行顺序记录每一步合并对（例如 `('i','n')`），常见格式为每行一对（例如`i n`）。合并的顺序很重要，tokenizer使用该顺序进行编码时需一致。
3. 预处理与正规化规则，说明是否做 Unicode normalize、是否保留前导空格、是否小写、哪些特殊 token（`<PAD>`, `<BOS>`, `<EOS>`, `<UNK>`, `<MASK>`）被保留及其 id。
4. 编码、解码逻辑
   - **编码**：文本 → 规范化 → 按空格、预分割切片 → 将每片拆成基础单元 → 使用 merges（最长匹配/贪婪）或其他算法合并出token列表 → map为token ids并加上 special tokens。
   - **解码**：token ids → token string → 合并 token 字符串 → 恢复空格、前缀 → 反正规化（若需要）。
5. 版本与元数据，训练时的超参数即目标vocab 大小、合并次数、语料描述、预处理规则、训练日期等应一起保存，便于复现和回滚。

>编码时的关键实现点（“最长匹配”、贪婪拆分）
> 文本编码（运行时）通常使用贪婪最长匹配对输入片段从最长可能token开始匹配，匹配不到则回退。合并规则文件和 vocab 支持这种行为，例如vocab有 `["l", "le", "ear", "learn"]`，编码 `"learn"` 时会优先匹配 `"learn"` 而非 `["l","e","a","r","n"]`。

#### 测试与校验

- 用小验证集检查**编码→解码**是否可逆（在容忍正规化差异下）。
- 用验证集监控 token 长度分布、rare token 比例、平均 token 长度，确保没有异常（如太多单字符 token 表示合并不够）。



### 2常用的分词器

#### 2.1 字符分词器

#### 2.2 字节分词器

#### 2.3 词级分词器

#### 2.4 BPE分词器

