# 第七章 GPU高性能编程

## 7.1 回顾GPU架构与执行模型

### 7.1.1 GPU硬件架构（以A100/H100为例）

以A100或H100为例，GPU包含大量**流式多处理器**（SM），每个SM内部有众多执行计算的处理单元比如**Int32或FP32单元**。每个SM会启动大量线程，内存层级结构包括**容量大但速度慢的DRAM（全局内存）**，以及更高速的缓存。特别要注意**寄存器**，这是每个线程都能访问的极高速存储器，在今天我们编写高性能GPU代码时将大量使用这些寄存器。

**执行模型的基本结构**

上一章讲过块（block）、线程束（warp）和线程（thread），这是粒度逐级细化的顺序。块是大型线程组，每个块会被分配给一个SM处理。可以把每个SM想象成独立工作的单元，而块就是分配给它的处理单元。在每个块内部包含大量线程，每个线程代表待执行的任务单元。这些线程在执行时会分组运行，这种分组称为线程束。每个线程束由32个连续编号的线程组成，从块中提取出来同步执行。

warp本质上是一组共同执行的线程。warp存在的意义在于这些线程是同时执行的，不需要为每个线程单独配置控制单元，**只需每32个线程块配置**一个。所以计算单元的数量远多于warp调度器。这样就能在无需担心控制开销的情况下执行更多并行工作。这也是GPU与CPU的权衡之一：CPU会将更多硅片面积用于控制单元和分支预测等功能，而GPU则更侧重计算能力并采用更简化的控制机制。

**线程块**集合会被调度到单个SM上执行，这是我们在**Triton**等编程中需要重点考虑的基础单元。每个块内包含大量实际执行计算的**线程**。当需要对向量进行操作时让每个线程同时处理向量中多个元素的代码，**所有线程协同完成整个向量的处理**。

这可以实现**高效通信**，因为线程块内可通过**SM中高速的共享内存进行数据交换**。例如矩阵乘法运算需要线程间传递数据时，**块内通信极快，而跨块通信则代价高昂**。因此应尽量将数据保持在同个线程块（或同个计算单元）内，这样就能获得高速性能。虽然可以通过线程块实现线程间同步，但无法跨块同步，也无法精确控制执行流程。因此我们要尽量避免跨块通信。

实际运行时，**线程会被分组为连续的32线程单元（warp）**，在SM中批量执行。因此**应尽量确保**所有Warp的计算负载均衡，理想情况下**线程块数量应远多于SM数量**，且最好能被SM数量整除，这样才能**保证每个Warp的工作量均衡**（每个sm都有自己的块要处理，而不是闲置）。

上一章中我们讨论过芯片的计算能力扩展速度远远快于内存扩展速度，所以计算最终会受限于内存带宽，导致无法充分发挥计算性能，一般来说，矩阵运算时计算受限的，其他所有操作都是内存受限的。**所以在GPU相关的编程中我们要尽可能的降低计算对内存的依赖程度**。

---

## 7.2 性能分析方法

编写高性能代码的高层原则是必须先对代码进行**基准测试和性能分析**。因为常常会有学生或开发者主观认定**某个部分是瓶颈**，然后花三个小时去优化，结果发现根本不是瓶颈所在，这就浪费了许多时间。

如果使用高性能或精细的性能分析器，就能准确**定位瓶颈所在和机器的实际运行状态**。掌握这些信息后我们就可以集中精力优化代码执行中最关键的部分，这就是这个高层理念重要性，因为关于GPU执行细节或如何编写softmax内核的具体方法可能会不断演变，甚至我们可以直接依赖Torch编译器的自动即时编译功能。**但性能分析的重要性永远不会随工具改变而改变**。

我们希望大家能内化这个观念：**要编写高性能代码，就必须持续进行性能分析**。

### 7.2.1 基准测试（Benchmarking）

```pyhon

class MLP(nn.Module):
    """Simple MLP: linear -> GeLU -> linear -> GeLU -> ... -> linear -> GeLU"""
    def __init__(self, dim: int, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(dim, dim) for _ in range(num_layers)])
    def forward(self, x: torch.Tensor):
        for layer in self.layers:
            x = layer(x)
            x = torch.nn.functional.gelu(x)
        return x
def run_mlp(dim: int, num_layers: int, batch_size: int, num_steps: int) -> Callable:
    # Define a model (with random weights)
    model = MLP(dim, num_layers).to(get_device())
    # Define an input (random)
    x = torch.randn(batch_size, dim, device=get_device())
    def run():
        # Run the model `num_steps` times (note: no optimizer updates)
        for step in range(num_steps):
            # Forward
            y = model(x).mean()
            # Backward
            y.backward()
    return run
def run_operation1(dim: int, operation: Callable) -> Callable:
    # Setup: create one random dim x dim matrices
    x = torch.randn(dim, dim, device=get_device())
    # Return a function to perform the operation
    return lambda : operation(x)
def run_operation2(dim: int, operation: Callable) -> Callable:
    # Setup: create two random dim x dim matrices
    x = torch.randn(dim, dim, device=get_device())
    y = torch.randn(dim, dim, device=get_device())
    # Return a function to perform the operation
    return lambda : operation(x, y)
def benchmarking():
```


我们运行简单的多层感知机，设置128个维度，16个网络层，指定批次大小，进行5个训练步长。这里仅执行前向传播和反向传播各5次。

首先定义MLP模型，然后生成随机高斯分布输入，最后在5个步长中运行，每次计算前向传播后执行反向传播，最终返回MLP输出的平均值。这里甚至没有损失函数，简单到只是运行MLP前向传播并在最后进行平均池化。

接下来我要做两件事：**进行基准测试（测量运行时间）和性能分析（探查函数内部的时间消耗分布）**。

我们先从基准测试开始。基准测试就是测量执行这些操作的实际耗时，这里只需要关注多层感知机函数的端到端执行时间。

我们进行基准测试的目的是要比较不同实现的性能：将Triton实现与手写C++、PyyTorch实现以及Torch编译进行对比。我们需要评估编写CUDA内核是否值得，同时还想了解当矩阵乘法规模增大时，性能会下降多少。因此我们需要进行实证基准测试。

本章会一直使用这个基准测试函数，基准测试函数包含以下部分：需要测试的运行函数、若干预热迭代次数、以及多次正式测试次数。

**预热**

关键在于首次运行PyTorch代码时（比如调度到GPU执行），虽然表面看起来很快，但背后其实在编译机器代码，向GPU发送指令，存在各种初始化开销。预热迭代能确保我们测量的是稳定状态下的速度，而不是启动速度。当运行成千上万次迭代时，我们关心的正是稳定性能，而不是即时编译CUDA代码的速度。另一个重要点是调用torch.cuda.synchronize()。这是因为GPU和CPU本质上是独立的计算单元，可以并行运行。我的Python代码在CPU上执行，当运行计算时会向GPU分发CUDA内核，此时CPU会继续执行后续代码而不等待GPU完成。这种特性虽然有利于编写高性能代码，但会给基准测试带来问题： 如果GPU在异步执行而CPU在运行其他任务，就无法准确测量GPU执行时间。torch.cuda.synchronize()能确保GPU和CPU达到同步状态，清空所有队列任务，使两者处于代码执行的同一节点。这样我们就能在真实同步状态下进行多次计时测量。接下来我将执行计算，在这个例子中就是sleep命令。我会重复执行三次。由于我设置的休眠时间是50毫秒，最终得到的就是这个时长。因此我对这个时长测量了三次。当然，这里我在运行结束时还调用了torch.cuda.synchronize，以确保GPU和CPU状态同步。这样如果CPU运行较快，它会等待GPU执行实际完成，反之亦然。现在测量完成，我将取平均值——因为单次测量可能会因GPU的热特性等因素产生波动，所以需要多次重复测量，取平均值后返回结果。这就是我们的基准测试代码，非常简单。但请记住两个关键点：始终进行预热操作，务必调用CUDA同步。遵循这两点操作就很简便，如果忘记执行，可能会得到极其异常的数据（比如显示大型矩阵乘法瞬间完成，这显然不符合事实）。现在我们可以进行矩阵乘法的基准测试。我将逐步演示部分结果，虽然只是用数据验证已知结论，但希望通过具体演示确保理解一致。我在课程使用的A100GPU上运行测试，针对不同尺寸的矩阵乘法进行测量，系统收集了各个维度的矩阵乘法耗时数据。正如预期，随着矩阵尺寸增大，运行时间呈现超线性增长。不过在最小尺寸（如1024和2048）时，耗时基本没有增长，因为执行矩阵乘法存在固定开销——需要将数据从CPU传输到GPU，启动内核也有开销，所以并非从零开始就一直保持超线性增长。但当矩阵足够大时，确实观察到了预期的缩放规律。现在让我们尝试对MLP进行基准测试。具体操作是：将MLP扩展至256维，设置四层网络，批处理大小为256，执行两个训练步。测得耗时6.2秒。接着可以进行基础扩展测试：将训练步数从2逐步增加到5，分别进行基准测试。与矩阵乘法不同，当增加MLP的前向传播和反向传播次数时，运行时间应该呈线性增长——实际数据也印证了这一点，每次MLP执行约5秒，总体运行时间基本符合n×5秒的规律。同样地，当网络层数从2、3、4增加到5层时，运行时间也随之递增。这次仍然呈现线性增长趋势：单层运行约5秒（略少于5秒），总体耗时约为层数的四倍。再次验证了线性缩放规律。这完全符合预期——无论是训练步数还是网络层数，都与运行时间存在线性关系。关于批处理规模的测试就此跳过，因为当前追踪的数据量已经有些过于庞杂。那么基准测试这部分就到此为止。我们可以创建这个实用的函数，它进行少量预热操作，执行CUDA同步，并且能够测量我们所需的任何代码的运行时间。这很有用。你应该在代码中经常这样做，可以测量新潮架构的运行耗时。但我觉得如果想要解决某些问题，基准测试是个相当粗粒度的工具。它能告诉你代码运行缓慢，但无法指出时间具体消耗在何处。因此我们更倾向于使用性能分析。这将是我们需要进行的更精细化的操作。性能分析非常实用，因为它不仅能帮你查明时间消耗在哪个函数，更重要的是——当你查看调用栈时，通常你接触的是PyTorch接口层，即你直接调用的PyTorch组件。但在PyTorch底层，存在着完整的CUDA调用体系。运行分析器时，你实际上可以一直追踪到底层调用，看清实际执行的代码路径。这样你就能更直观地理解程序在硬件上的真实执行过程。接下来我们将逐步分析几个简单函数，建立对运行机制的直观认知。值得称道的是，如果你需要基础性能分析，PyTorch内置了非常便捷的分析器工具。这让你无需离开Python/PyTorch环境就能获得相当清晰的输出结果。这里我分析了一些函数，你们也可以看到对应的输出。我沿用之前的sleep函数示例，这就是那个休眠函数。当我们分析该函数时，分析函数的结构大致如下：同样进行预热，执行TorchCUDA同步，然后调用分析器同时追踪CPU和GPU时间。接着运行目标代码，再次同步后输出所有时间的平均值表格。现在回到演示——我将分析这个sleep函数。观察运行结果会发现什么？100%的时间都消耗在名为CUDA设备同步的操作上，因为实际上没有GPU计算任务。这就像是在分析空操作，有点滑稽。现在让我们看个有实际意义的例子。以这个矩阵加法基础运算为例：我定义了接收参数A和B的加法函数执行矩阵相加。这个辅助函数会实例化两个随机高斯分布矩阵，然后调用操作参数中的内容。这里是将两个2048维矩阵相加。现在开始分析并调用分析器，会得到类似右侧区块的输出结果。这就是分析器返回的内容（需要缩小界面显示，当前显示不全）后排观众能看清吗？能看清的请竖大拇指？好的很好。看不清的请倒竖拇指。行。当我们在Python中调用加法函数时，我们接触的只有这个add函数——A加B，这就是我们的全部认知。但实际上，就像冰山理论，底层发生的远不止这些。这个操作会被分派到GPU执行：首先经过aten（PyTorch的C++接口层），这个封装器被调用后确认执行加法运算——这是最外层的调用封装。接着会分派到具体的内核函数vectorized_elementwise_kernel4，在nativeCUDA中执行向量加法等等...这才是实际执行加法的部分。同时还有CUDA启动内核操作也耗费时间——这实际上是CPU接收指令并发送给GPU的过程，即内核启动耗时。最后CUDA设备同步需要等待GPU完成计算并传回数据，这个阶段同样耗时。光是同步屏障这个操作本身就会消耗一定时间。最终我们得到的总时间是：CPU上1.4毫秒，CUDA上17微秒。可见GPU运行极快，而CPU较慢。观察CPU耗时（即自占CPU时间），发现C++接口或C接口实际消耗了大量CPU时间，这些是向GPU传输数据时产生的开销。这就是加法函数的内部运行机制。矩阵乘法也是类似情况。这里我对A乘以B进行矩阵乘法运算，再次使用2048维矩阵并进行性能分析。此时看到810次matmul调用，这说明底层接口执行矩阵乘法的过程。该操作会调用Cutlass——英伟达的高性能矩阵乘法CUDA库，随后分派到特定的Cutlass内核，其中包含分块尺寸参数（此处名称被截断，稍后展示完整版本）。这实际上指向特定的分块尺寸和线程块数量等参数化配置，正是这些在执行矩阵乘法。同样在底部看到两个熟悉项：内核启动和CUDA设备同步。可以再次观察到CPU时间与CUDA时间的分配情况。由于矩阵乘法比向量加法更耗时，CUDA部分占用时间显著增加。

**必须遵循的原则**：
- **预热（Warmup）**：首次运行PyTorch代码会触发编译和初始化，测量稳定状态而非启动速度
- **CUDA同步**：必须调用`torch.cuda.synchronize()`确保CPU和GPU同步，否则测量结果不准确
- **多次测量取平均**：减少GPU热特性等因素导致的波动

**基准测试目的**：
- 比较不同实现方案性能
- 了解运算规模增大时的性能缩放行为
- 评估手写CUDA内核的价值

### 7.2.2性能分析（Profiling）工具
**PyTorch内置分析器**：
- 可追踪CPU和GPU时间
- 显示高层PyTorch操作到底层CUDA内核的完整调用栈
- 不同矩阵尺寸会调用不同内核（如cutlass vs cublas）
- 复杂操作（如`torch.cdist`）会分解为多个基本操作（matmul, pow, sum等）

**NVIDIA Nsight Systems**：
- 提供硬件级详细分析
- 可清晰展示CPU与GPU的异步执行特性
- CPU可超前GPU多个step，但打印损失值等操作会强制同步，降低性能
- 显示GPU利用率、内存分配、内核启动开销等
- 可添加NVTX标注追踪特定代码段

---

## 四、内核融合（Kernel Fusion）的重要性
**核心概念**：避免每次运算都在DRAM和SM之间往返数据传输，将多个操作合并为单个内核

**GELU函数案例**：
- **朴素实现**：多次启动CUDA内核，总耗时**8.1秒**
- **PyTorch优化版**：内核融合后仅需**1.1毫秒**（约**8000倍**性能提升）
- **关键教训**：逐元素操作的内核融合能带来巨大性能收益

---

## 五、CUDA编程实践

### 1. 基础实现步骤
**CPU端包装函数（Launcher）**：
- 检查设备：`x.device().is_cuda()`
- 检查连续性：`x.is_contiguous()`（确保内存连续，便于索引）
- 分配输出：`torch::empty_like(x)`（用empty而非zeros避免多余操作）
- 计算网格参数：块大小和块数量（`cdiv`向上取整）
- 启动内核：`<<<num_blocks, block_size>>>`语法

**GPU端内核函数**：
- 使用`__global__`标识符
- 计算全局索引：`int i = blockIdx.x * blockDim.x + threadIdx.x`
- 必须手动进行边界检查：`if (i < n_elements)`
- 指针操作直接访问内存

### 2. 调试技巧
- 设置环境变量`CUDA_LAUNCH_BLOCKING=1`以获得错误信息（牺牲性能换取可调试性）

---

## 六、Triton编程实践

### 1. Triton核心优势
- **OpenAI 2021年开发的DSL**
- **Python语法**：更易编写和调试（可单步调试）
- **自动管理细节**：内存合并、共享内存分配、线程同步
- **以SM为中心思考**：无需管理单个线程，关注块级操作
- **向量化处理**：一次操作整个块的数据

### 2. 编程模型对比
**CUDA**：线程级思考，需手动计算`threadIdx.x`

**Triton**：块级思考，偏移量是**向量**而非标量
```python
offsets = BLOCK_SIZE * pid + tl.arange(0, BLOCK_SIZE)
# 处理整个块范围
```

### 3. 内存管理
- 使用`tl.load`和`tl.store`并配合掩码（mask）处理边界
- 自动合并内存访问（一次加载4个相邻值）
- 寄存器作为临时存储（极高速）

---

## 七、PyTorch JIT编译器（torch.compile）

### 1. 核心能力
- 自动优化和融合内核
- 接收未优化代码，生成Triton/CUDA优化版本
- 对简单操作和矩阵乘法特别有效

### 2. 性能表现
- GELU案例：`torch.compile`达到**1.47ms**，优于手写CUDA（1.8ms）
- Softmax案例：`torch.compile`达到**1.3ms**，优于PyTorch原生（1.5ms）
- **关键洞察**：现代JIT编译器常能匹配或超越手动优化

### 3. 适用边界
- **适合**：简单算子融合、标准矩阵乘法
- **不适合**：利用底层硬件特性（如FlashAttention-3在H100上的优化）、复杂算法创新

---

## 八、Softmax实现案例

### 1. 设计考虑
- 涉及**规约操作**（求和），比逐元素操作复杂
- **合理设计**：每个SM处理矩阵的一行
- 块大小应为列数的下一个2的幂（`next_power_of_2`）
- 利用共享内存在SM内完成行内通信

### 2. Triton实现特点
- 网格维度：`grid = (n_rows,)`（每行一个块）
- 行偏移简单：`row_start = pid * n_cols`
- 列偏移：`cols = tl.arange(0, BLOCK_SIZE)`
- 完整融合：加载→减最大值→指数→求和→除法→存储

---

## 九、核心原则与最佳实践

### 1. 黄金法则
- **始终进行基准测试和性能分析**：不要主观猜测瓶颈
- **理论分析有局限**：实际测量至关重要（microarchitectural细节、库版本、硬件配置影响大）
- **端到端测试**：系统设计相对可推理，体系结构需实际测量

### 2. 常见陷阱
- 忘记CUDA同步导致测量错误（显示运算"瞬间完成"）
- 忽略预热导致测量启动开销
- 内存不连续（如转置后）导致内核失败
- CPU操作（如打印）强制同步，破坏CPU-GPU并行性

### 3. 开发策略
- **优先使用torch.compile**：除非证明其不足
- **Triton作为中间选择**：平衡性能与开发效率
- **手写CUDA作为最后手段**：仅当需要极致优化或底层控制时

---

## 十、性能数据总结

| 实现方式 | GELU耗时 | Softmax耗时 | 核心特征 |
|---------|---------|------------|---------|
| 朴素手动实现 | 8.1秒 | 3.7秒 | 多内核启动，内存密集型 |
| PyTorch优化版 | 1.1毫秒 | 1.5毫秒 | 内核融合，高度优化 |
| 手写CUDA | 1.8毫秒 | - | 单次内核启动，接近PyTorch |
| Triton | 1.848毫秒 | 1.9毫秒 | Python开发，自动优化 |
| torch.compile | **1.47毫秒** | **1.3毫秒** | **自动融合，性能最佳** |

**关键结论**：现代自动化工具（torch.compile）在手写优化之上仍能提供额外性能提升，反映了编译器技术的强大能力。