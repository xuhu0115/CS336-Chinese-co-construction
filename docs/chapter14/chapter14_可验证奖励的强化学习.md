# ç¬¬åå››ç« ï¼šå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RLVR)

åœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº† RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚è™½ç„¶ RLHF æ˜¯ä½¿æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„å…³é”®ï¼Œä½†å®ƒé¢ä¸´ç€å·¨å¤§çš„æ‰©å±•æ€§æŒ‘æˆ˜ï¼šäººç±»åé¦ˆæ˜‚è´µã€ç¼“æ…¢ä¸”å®¹æ˜“è¢«â€œè¿‡åº¦ä¼˜åŒ–â€ï¼ˆGoodhart's Lawï¼‰ã€‚

æœ¬ç« æˆ‘ä»¬å°†ç›®å…‰è½¬å‘ **o1** å’Œ **DeepSeek R1** ç­‰æ¨ç†æ¨¡å‹èƒŒåçš„æ ¸å¿ƒæŠ€æœ¯â€”â€”**RLVR (Reinforcement Learning from Verifiable Rewards)**ã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**
1.  **ç®—æ³•æ¼”è¿›**ï¼šç†è§£ä» PPO åˆ° GRPO çš„æ¼”å˜é€»è¾‘ï¼Œä»¥åŠä¸ºä»€ä¹ˆ GRPO æ›´é€‚åˆå¤§æ¨¡å‹æ¨ç†è®­ç»ƒã€‚
2.  **å·¥ç¨‹å®ç°**ï¼šæ·±å…¥ PPO å’Œ GRPO çš„ä»£ç å®ç°ç»†èŠ‚ï¼ŒæŒæ¡ Advantage è®¡ç®—ä¸ Loss è®¾è®¡ã€‚
3.  **å‰æ²¿æ¡ˆä¾‹**ï¼šè§£æ„ DeepSeek R1ã€Kimi k1.5 å’Œ Qwen 3 çš„è®­ç»ƒæµæ°´çº¿ï¼Œç†è§£â€œå†·å¯åŠ¨æ•°æ®â€ã€â€œæ€ç»´é“¾ï¼ˆCoTï¼‰â€ä¸â€œé•¿åº¦æ§åˆ¶â€çš„å…³é”®ä½œç”¨ã€‚

---

## 14.1 ä¸ºä»€ä¹ˆéœ€è¦ RLVRï¼Ÿ

åœ¨ AlphaGo æˆ– AlphaFold ç­‰é¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå› ä¸ºå®ƒä»¬æ‹¥æœ‰**å®Œç¾çš„æ¨¡æ‹Ÿå™¨**å’Œ**æ˜ç¡®çš„å¥–åŠ±å‡½æ•°**ï¼ˆèµ¢/è¾“ï¼Œè›‹ç™½è´¨æŠ˜å èƒ½çº§ï¼‰ã€‚

åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¦‚æœæˆ‘ä»¬èƒ½æ‰¾åˆ°ç±»ä¼¼çš„é¢†åŸŸâ€”â€”**ç­”æ¡ˆå®¢è§‚ã€å¯éªŒè¯**ï¼ˆå¦‚æ•°å­¦é¢˜ã€ä»£ç ç”Ÿæˆï¼‰ï¼Œæˆ‘ä»¬å°±èƒ½åˆ©ç”¨å¤§è§„æ¨¡çš„è®¡ç®—èµ„æºæ¥æ›¿ä»£æ˜‚è´µçš„äººç±»æ ‡æ³¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ã€‚è¿™å°±æ˜¯ RLVR çš„æ ¸å¿ƒæ„¿æ™¯ã€‚

### 14.1.1 RLHF çš„å›°å¢ƒ

ä¼ ç»Ÿçš„ RLHF ä¾èµ–äººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„æˆå¯¹åå¥½åˆ¤æ–­ï¼ˆå¦‚ â€œA æ¯” B å¥½â€ï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸‰å¤§æ ¹æœ¬æ€§é—®é¢˜ï¼š

- å¥–åŠ±å™ªå£°é«˜ï¼šäººç±»åˆ¤æ–­ä¸»è§‚ã€ä¸ä¸€è‡´ï¼Œä¸”æ˜“è¢«è¡¨é¢ä¿®è¾è¿·æƒ‘ï¼›
- éš¾ä»¥è§„æ¨¡åŒ–ï¼šé«˜è´¨é‡åå¥½æ•°æ®æ ‡æ³¨æˆæœ¬æé«˜ï¼Œæ— æ³•æ”¯æ’‘ä¸‡äº¿ token çº§è®­ç»ƒï¼›
- è¿‡ä¼˜åŒ–ï¼ˆOver-optimizationï¼‰ï¼šæ¨¡å‹å­¦ä¼šâ€œè®¨å¥½â€å¥–åŠ±æ¨¡å‹ï¼Œç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å†…å®¹ç©ºæ´ã€å†—é•¿ç”šè‡³å¹»è§‰çš„è¾“å‡ºã€‚

> RLHF ä¼˜åŒ–çš„æ˜¯ä»£ç†ç›®æ ‡ï¼ˆäººç±»åå¥½ï¼‰ï¼Œè€ŒéçœŸå®ç›®æ ‡ï¼ˆä»»åŠ¡æ­£ç¡®æ€§ï¼‰ã€‚


### 14.1.2 æˆåŠŸæ¡ˆä¾‹çš„å¯ç¤º

å›é¡¾ AlphaGoã€AlphaFold ç­‰ RL æˆåŠŸæ¡ˆä¾‹ï¼Œå…¶å…±åŒç‚¹æ˜¯ï¼š**å¥–åŠ±å‡½æ•°æ˜¯æ˜ç¡®ã€å¯éªŒè¯ã€å¯è‡ªåŠ¨è®¡ç®—çš„**ã€‚ä¾‹å¦‚ï¼š
- å›´æ£‹ï¼šæœ€ç»ˆæ˜¯å¦è·èƒœï¼ˆ0/1ï¼‰ï¼›
- è›‹ç™½è´¨æŠ˜å ï¼šé¢„æµ‹è›‹ç™½è´¨ç»“æ„ä¸çœŸå®ç»“æ„çš„ RMSDï¼ˆRoot Mean Square Deviationï¼Œå‡æ–¹æ ¹åå·®ï¼‰ è·ç¦»ã€‚

è¿™ç±»ä»»åŠ¡ä¸­ï¼ŒRL ç®—æ³•å¯ç›´æ¥ä¼˜åŒ–**çœŸå®ç›®æ ‡**ï¼Œæ— éœ€äººç±»ä¸­ä»‹ã€‚è¿™å¯å‘æˆ‘ä»¬ï¼š**èƒ½å¦å°† RL å¼•å…¥è¯­è¨€æ¨¡å‹çš„â€œå¯éªŒè¯ä»»åŠ¡â€ä¸­ï¼Ÿ**

### 14.1.3 RLVR çš„å®šä½

RLVR èšç„¦äºä¸€ç±»ç‰¹æ®Šä»»åŠ¡ï¼š**è¾“å‡ºå¯è¢«ç¨‹åºè‡ªåŠ¨åˆ¤åˆ†**ã€‚å…¸å‹åœºæ™¯åŒ…æ‹¬ï¼š
- **æ•°å­¦æ¨ç†**ï¼šç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†è§£ä¸€è‡´ï¼ˆå¦‚ GSM8Kã€MATHï¼‰ï¼›
- **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆçš„ç¨‹åºæ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼›
- **å½¢å¼åŒ–è¯æ˜**ï¼šè¯æ˜æ­¥éª¤æ˜¯å¦é€»è¾‘è‡ªæ´½ã€‚

åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå¥–åŠ±å‡½æ•° $R(z)$ å¯å®šä¹‰ä¸ºï¼š

$$
R(z) = 
\begin{cases}
1 & \text{è‹¥ } z \text{ æ­£ç¡®} \\
0 & \text{å¦åˆ™}
\end{cases}
$$

æˆ–æ›´ç²¾ç»†çš„**è¿‡ç¨‹å¥–åŠ±**ï¼ˆå¦‚æ¯æ­¥æ¨ç†å¾—åˆ†ï¼‰ã€‚è¿™ç§**é«˜ä¿¡å™ªæ¯”ã€å¯è§„æ¨¡åŒ–**çš„å¥–åŠ±ï¼Œæ­£æ˜¯ RL å¤§å±•èº«æ‰‹çš„èˆå°ã€‚

> âœ… **RLVR çš„æœ¬è´¨**ï¼šåœ¨é‚£äº›â€œå¯¹é”™å¯è¢«è‡ªåŠ¨åˆ¤å®šâ€çš„çª„åŸŸä»»åŠ¡ä¸­ï¼Œç»•è¿‡äººç±»åå¥½ï¼Œç›´æ¥ç”¨å½¢å¼åŒ–éªŒè¯æœºåˆ¶æä¾›å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°æ›´å¯é ã€å¯æ‰©å±•ã€å¯éªŒè¯çš„æ™ºèƒ½ä½“è®­ç»ƒ

ä¸‹é¢æ˜¯å¯¹ RLHF å’Œ RLVR çš„ç®€å•å¯¹æ¯”ï¼š

| ç»´åº¦ | RLHF | RLVR |
|------|------|------|
| å¥–åŠ±æ¥æº | äººç±»åå¥½ï¼ˆå¦‚ rankingï¼‰ | è‡ªåŠ¨éªŒè¯ï¼ˆå¦‚æµ‹è¯•ã€è¯æ˜ã€è§„åˆ™ï¼‰ |
| ä»»åŠ¡é¢†åŸŸ | é€šç”¨ã€å¼€æ”¾åŸŸï¼ˆå¦‚èŠå¤©ï¼‰ | çª„åŸŸã€ç»“æ„åŒ–ï¼ˆå¦‚ç¼–ç¨‹ã€æ•°å­¦ï¼‰ |
| å¥–åŠ±è´¨é‡ | ä¸»è§‚ã€æœ‰å™ªå£°ã€æˆæœ¬é«˜ | å®¢è§‚ã€ç²¾ç¡®ã€å¯æ‰©å±• |
| å¯¹é½ç›®æ ‡ | â€œè®©äººè§‰å¾—å¥½â€ | â€œåœ¨å½¢å¼æ„ä¹‰ä¸Šæ­£ç¡®â€ |


---

## 14.2 ç®—æ³•æ¼”è¿›ï¼šä» PPO åˆ° GRPO

è¦ç†è§£ç°åœ¨çš„ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹èƒŒåçš„ GRPO ç®—æ³•ï¼Œæˆ‘ä»¬å¿…é¡»å…ˆå›é¡¾å®ƒçš„å‰èº« PPOï¼Œå¹¶æ˜ç™½ä¸ºä»€ä¹ˆè¦æŠ›å¼ƒå®ƒã€‚

### 14.2.1 PPO

#### å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥ä¼˜åŒ–æ–¹æ³•çš„å‘å±•è„‰ç»œ

ä»åŸå§‹çš„ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰ â†’ åˆ°æ›´ç¨³å®šçš„TRPOï¼ˆTrust Region Policy Optimizationï¼‰ â†’ å†åˆ°æ›´å®ç”¨çš„PPOï¼ˆProximal Policy Optimizationï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª**ç­–ç•¥**ï¼ˆpolicyï¼‰$\pi_\theta(a|s)$ï¼Œå®ƒç”¨å‚æ•° $\theta$ æ§åˆ¶æ™ºèƒ½ä½“å¦‚ä½•æ ¹æ®çŠ¶æ€ $s$ é€‰æ‹©åŠ¨ä½œ $a$ã€‚  
ç›®æ ‡æ˜¯ï¼š**æœ€å¤§åŒ–æœŸæœ›å›æŠ¥**ï¼ˆexpected returnï¼‰ï¼š

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
$$

å…¶ä¸­ $\tau = (s_1, a_1, s_2, a_2, ..., s_T)$ æ˜¯ä¸€æ¡è½¨è¿¹ï¼ˆtrajectoryï¼‰ï¼Œ $R(\tau)$ æ˜¯æ€»å¥–åŠ±ã€‚

æˆ‘ä»¬éœ€è¦è®¡ç®— $\nabla_\theta J(\theta)$ æ¥ç”¨æ¢¯åº¦ä¸Šå‡æ›´æ–° $\theta$ã€‚


ğŸ”¹ å°è¯• 1: ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰

åˆ©ç”¨**ä¼¼ç„¶æ¯”æŠ€å·§**ï¼ˆlikelihood ratio trickï¼‰ï¼Œå¯ä»¥æ¨å‡ºï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(\tau) \right]
$$

è€Œ $\pi_\theta(\tau) = p(s_1) \prod_{t=1}^T \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)$ ï¼Œæ‰€ä»¥ $\nabla_\theta \log \pi_\theta(\tau) = \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t|s_t)$

äºæ˜¯å¾—åˆ°**REINFORCE**ç®—æ³•ï¼ˆæœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦ï¼‰ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=1}^T R_t \right) \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \right]
$$

å…¶ä¸­ $R_t = \sum_{k=t}^T \gamma^{k-t} r_k$ æ˜¯ä»æ—¶é—´ $t$ å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥ã€‚

ç­–ç•¥æ¢¯åº¦å­˜åœ¨å“ªäº›é—®é¢˜ï¼š
- **é«˜æ–¹å·®ï¼ˆhigh varianceï¼‰**ï¼šå› ä¸ºæ•´ä¸ªè½¨è¿¹çš„æ€»å¥–åŠ± $R(\tau)$ è¢«ç”¨ä½œæ¯ä¸ªåŠ¨ä½œçš„â€œä¿¡å·â€ï¼Œä½†å¾ˆå¤šåŠ¨ä½œå…¶å®å’Œæœ€ç»ˆç»“æœæ— å…³ã€‚
- **æ›´æ–°ä¸ç¨³å®š**ï¼šä¸€æ¬¡æ›´æ–°å¯èƒ½å¤ªå¤§ï¼Œå¯¼è‡´ç­–ç•¥å´©æºƒï¼ˆâ€œcatastrophic collapseâ€ï¼‰ã€‚

> âœ… æ‰€ä»¥ç­–ç•¥æ¢¯åº¦**ç†è®ºä¸Šæ­£ç¡®ï¼Œä½†å®è·µä¸­éš¾ç”¨**ã€‚

ğŸ”¹ å°è¯• 2: TRPOï¼ˆTrust Region Policy Optimizationï¼‰

æ ¸å¿ƒæ€æƒ³ï¼šä¸è¦ç›´æ¥ç”¨åŸå§‹æ¢¯åº¦æ›´æ–°ï¼Œè€Œæ˜¯**æ¯æ¬¡åªå…è®¸ç­–ç•¥å˜åŠ¨ä¸€ç‚¹ç‚¹**ï¼Œç¡®ä¿æ–°ç­–ç•¥ $\pi_{\theta_{\text{new}}}$ å’Œæ—§ç­–ç•¥ $\pi_{\theta_{\text{old}}}$ è¶³å¤Ÿæ¥è¿‘ã€‚

å…·ä½“åšæ³•ï¼šè§£ä¸€ä¸ª**å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜**ï¼š

$$
\max_\theta \quad \mathbb{E}_{s,a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\text{old}}}(s,a) \right] \\
\text{subject to} \quad \mathbb{E}_s \left[ D_{\text{KL}} \left( \pi_{\theta_{\text{old}}}(\cdot|s) \,\|\, \pi_\theta(\cdot|s) \right) \right] \leq \delta
$$

- è¿™ä¸ªç›®æ ‡æ˜¯**è¿‘ä¼¼**ç­–ç•¥æ”¹è¿›ï¼ˆä½¿ç”¨é‡è¦æ€§é‡‡æ · + ä¼˜åŠ¿å‡½æ•° $A$ï¼‰
- çº¦æŸé¡¹é™åˆ¶ KL æ•£åº¦ä¸è¶…è¿‡ä¸€ä¸ªå°å¸¸æ•° $\delta$


TRPO ç‰¹ç‚¹ï¼š
- âœ… ç¨³å®šï¼Œç†è®ºä¿è¯å•è°ƒæ”¹è¿›
- âŒ å®ç°æå…¶å¤æ‚ï¼šéœ€è¦ç”¨å…±è½­æ¢¯åº¦ï¼ˆconjugate gradientï¼‰æˆ–äºŒé˜¶ä¼˜åŒ–ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰

> æ‰€ä»¥ TRPO æ˜¯â€œç†æƒ³ä½†ç¬¨é‡â€çš„æ–¹æ³•ã€‚


ğŸ”¹ å°è¯• 3: PPOï¼ˆProximal Policy Optimizationï¼‰

åŠ¨æœºï¼šèƒ½ä¸èƒ½**ç”¨ä¸€ä¸ªç®€å•çš„æ–¹æ³•ï¼Œè¿‘ä¼¼ TRPO çš„â€œå°æ­¥æ›´æ–°â€æ€æƒ³**ï¼Œè€Œä¸ç”¨è§£å¤æ‚çš„çº¦æŸä¼˜åŒ–ï¼Ÿ

PPO çš„æ ¸å¿ƒåˆ›æ–°ï¼š**è£å‰ªæ¦‚ç‡æ¯”ï¼ˆClipped Probability Ratioï¼‰**

å®šä¹‰**æ¦‚ç‡æ¯”**ï¼ˆlikelihood ratioï¼‰ï¼š

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

åœ¨ TRPO ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ› $r_t(\theta) \approx 1$ï¼ˆå³æ–°æ—§ç­–ç•¥è¾“å‡ºæ¦‚ç‡æ¥è¿‘ï¼‰ã€‚

PPO çš„æƒ³æ³•æ˜¯ï¼š**å¦‚æœ $r_t(\theta)$ å¤ªå¤§æˆ–å¤ªå°ï¼Œå°±æŠŠå®ƒâ€œè£å‰ªâ€æ‰**ï¼

äºæ˜¯æå‡º**è£å‰ªç›®æ ‡å‡½æ•°**ï¼ˆClipped Surrogate Objectiveï¼‰ï¼š

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) A_t, \ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t \right) \right]
$$

ç›´è§‚è§£é‡Šï¼š
- å¦‚æœ $A_t > 0$ï¼ˆè¿™ä¸ªåŠ¨ä½œå¥½ï¼‰ï¼š
  - æˆ‘ä»¬å¸Œæœ›å¢å¤§ $\pi_\theta(a_t|s_t)$ï¼Œå³è®© $r_t > 1$
  - ä½†å¦‚æœ $r_t > 1+\epsilon$ï¼Œè¯´æ˜æ›´æ–°å¤ªå¤§ â†’ è£å‰ªæ‰ï¼Œåªå– $1+\epsilon$
- å¦‚æœ $A_t < 0$ï¼ˆè¿™ä¸ªåŠ¨ä½œå·®ï¼‰ï¼š
  - æˆ‘ä»¬å¸Œæœ›å‡å° $\pi_\theta(a_t|s_t)$ï¼Œå³è®© $r_t < 1$
  - ä½†å¦‚æœ $r_t < 1-\epsilon$ï¼Œè¯´æ˜æƒ©ç½šå¤ªçŒ› â†’ è£å‰ªä¸º $1-\epsilon$

> ğŸ¯ è¿™æ ·ï¼ŒPPO **è‡ªåŠ¨é™åˆ¶äº†ç­–ç•¥æ›´æ–°çš„æ­¥é•¿**ï¼Œæ— éœ€æ˜¾å¼ KL çº¦æŸï¼


ğŸ” ä¸‰è€…å…³ç³»æ€»ç»“

| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | æ˜¯å¦çº¦æŸæ›´æ–°æ­¥é•¿ï¼Ÿ | å®ç°éš¾åº¦ | é€‚åˆ LLM å—ï¼Ÿ |
|------|--------|------------------|--------|-------------|
| **Policy Gradient** | ç›´æ¥æ¢¯åº¦ä¸Šå‡ | âŒ å¦ | ç®€å• | âŒï¼ˆæ–¹å·®å¤§ï¼‰ |
| **TRPO** | ç”¨ KL æ•£åº¦çº¦æŸæ›´æ–° | âœ… æ˜¯ï¼ˆç¡¬çº¦æŸï¼‰ | æéš¾ | âŒï¼ˆå†…å­˜/è®¡ç®—é«˜ï¼‰ |
| **PPO** | ç”¨è£å‰ªè¿‘ä¼¼å°æ­¥æ›´æ–° | âœ… æ˜¯ï¼ˆè½¯çº¦æŸï¼‰ | ä¸­ç­‰ | âœ…ï¼ˆä¸»æµé€‰æ‹©ï¼‰ |


####  PPO çš„ç—›ç‚¹

ä¸‹å›¾å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ•´ä½“æµç¨‹ã€‚

<div align="center">
   <img src="images/14-0-ppoç®—æ³•æµç¨‹å›¾.png" />
   <p>å›¾14.1 ppoç®—æ³•æµç¨‹å›¾</p>
</div>

æµç¨‹ä»¥ä¸€ä¸ªç”¨æˆ·æŸ¥è¯¢ x å¼€å§‹ï¼Œæ—§ç­–ç•¥æ¨¡å‹ï¼ˆ**Policy LM**ï¼‰ï¼Œæ ¹æ®è¾“å…¥ x ç”Ÿæˆå“åº”åºåˆ— $y_1, y_2, ..., y_{t-1}$ã€‚å°†(x,y)åˆ†è§£æˆ**çŠ¶æ€-åŠ¨ä½œå¯¹$(s_t, a_t)$**ã€‚åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼š
- çŠ¶æ€ $s_t$ = å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå¦‚å·²ç”Ÿæˆçš„éƒ¨åˆ† tokenï¼‰
- åŠ¨ä½œ $a_t$ = ä¸‹ä¸€ä¸ªè¦ç”Ÿæˆçš„ token

å°†`ç”¨æˆ·é—®é¢˜+æ¨¡å‹ç”Ÿæˆçš„å›ç­” (x, y)` è¾“å…¥ç»™**å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰**ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å¥–åŠ±å€¼r(x, y)ï¼Œè¡¨ç¤ºè¯¥å›ç­”çš„è´¨é‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ã€‚

**ä»·å€¼æ¨¡å‹ï¼ˆValue Modelï¼‰** çš„è¾“å…¥æ˜¯å½“å‰çŠ¶æ€ $s_t$ï¼Œè¾“å‡º $V(s_t)$ æ˜¯ä¼°è®¡ä»è¯¥çŠ¶æ€å¼€å§‹æœªæ¥èƒ½è·å¾—çš„æ€»å›æŠ¥ï¼ˆReturnï¼‰ ã€‚

**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAE, Generalized Advantage Estimationï¼‰** æ¨¡å—è®¡ç®—å¾—åˆ°çš„ä¼˜åŠ¿ $A(s_t, a_t)$ï¼Œä»¥åŠä¼°è®¡è¿”å› $RÌ‚_t$ã€‚

**Return**ï¼š $RÌ‚_t = Ã‚(s_t, a_t) + V(s_t)$

**ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰**ï¼š$Ã‚(s_t, a_t) = Î£(Î³Î»)^{l} Î´_{t+l}$ï¼Œå¯¹æœªæ¥å¤šä¸ªæ—¶é—´æ­¥çš„ TD error åŠ æƒæ±‚å’Œï¼ŒÎ» æ˜¯ GAE å‚æ•°ï¼ˆæ§åˆ¶åå·®-æ–¹å·®æƒè¡¡ï¼‰ã€‚

**æ—¶åºå·®åˆ†è¯¯å·®ï¼ˆTD Errorï¼ŒTemporal Difference Errorï¼‰**ï¼š$Î´_t = r(s_t, a_t) + Î³V(s_{t+1}) - V(s_t)$ï¼Œè¡¡é‡çš„æ˜¯â€œå®é™…å›æŠ¥â€ä¸â€œå½“å‰ä»·å€¼ä¼°è®¡â€ä¹‹é—´çš„å·®è·ã€‚

- \( r_t \)ï¼šåœ¨çŠ¶æ€ \( s_t \) ä¸‹æ‰§è¡ŒåŠ¨ä½œ \( a_t \) åè·å¾—çš„**å³æ—¶å¥–åŠ±**
- \( \gamma \in [0,1] \)ï¼šæŠ˜æ‰£å› å­ï¼ˆdiscount factorï¼‰ï¼Œé€šå¸¸å– 0.95~1.0
- \( V(s_t) \)ï¼šä»·å€¼ç½‘ç»œå¯¹çŠ¶æ€ \( s_t \) çš„ä¼°å€¼

**ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆExperience Bufferï¼‰** ç”¨æ¥å­˜å‚¨æ¯æ¬¡ rollout çš„æ•°æ®ï¼ŒåŒ…æ‹¬çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ˆ$s_t, a_t$ï¼‰ã€ä¼˜åŠ¿å‡½æ•°ä¼°è®¡å€¼ï¼ˆ$Ã‚(s_t, a_t)$ï¼‰ã€ä¼°è®¡å›æŠ¥ï¼ˆ$RÌ‚_t$ï¼‰å’Œæ—§ç­–ç•¥ä¸‹è¯¥åŠ¨ä½œçš„æ¦‚ç‡ï¼ˆ$Ï€_Î¸^old(a_t|s_t)$ï¼‰ã€‚

**ç­–ç•¥æ›´æ–°æ¨¡å—** Policy LM $Ï€_Î¸^RL(a_t|s_t)$ æ˜¯å½“å‰æ­£åœ¨ä¼˜åŒ–çš„ç­–ç•¥æ¨¡å‹ã€‚å®ƒæ¥æ”¶çŠ¶æ€ $s_t$ï¼Œè¾“å‡ºåŠ¨ä½œ $a_t$ çš„æ¦‚ç‡åˆ†å¸ƒã€‚

**PPO-clip Loss** æ˜¯ PPO çš„æ ¸å¿ƒæŸå¤±å‡½æ•°ï¼Œç›®æ ‡æ˜¯åœ¨ä¿è¯ç­–ç•¥æ›´æ–°ç¨³å®šçš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼ˆå³ Reward Model ç»™å‡ºçš„åˆ†æ•°ï¼‰ã€‚

$$
\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( 
r_t(\theta) \cdot \hat{A}_t,\ 
\text{clip}\big(r_t(\theta), 1-\epsilon, 1+\epsilon\big) \cdot \hat{A}_t 
\right) \right]
$$

å…¶ä¸­ï¼š
- $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$ï¼š**æ–°æ—§ç­–ç•¥æ¦‚ç‡æ¯”**
- \( \hat{A}_t \)ï¼šGAE è®¡ç®—å‡ºçš„**ä¼˜åŠ¿å‡½æ•°**ï¼ˆæ¥è‡ª TD Errorï¼‰
- \( \epsilon \)ï¼šè¶…å‚æ•°ï¼ˆé€šå¸¸ 0.1~0.2ï¼‰ï¼Œæ§åˆ¶æ›´æ–°æ­¥é•¿
- `clip`ï¼šå°†æ¯”ç‡è£å‰ªåˆ° \([1-\epsilon, 1+\epsilon]\) åŒºé—´

**LM Loss** æ˜¯æ ‡å‡†çš„ è‡ªå›å½’è¯­è¨€å»ºæ¨¡äº¤å‰ç†µæŸå¤±ï¼Œç›®æ ‡æ˜¯é˜²æ­¢ç­–ç•¥åœ¨ä¼˜åŒ–å¥–åŠ±æ—¶â€œå¿˜è®°â€å¦‚ä½•è¯´äººè¯ï¼ˆç¾éš¾æ€§é—å¿˜ï¼‰ã€‚

**MSE Loss** æ˜¯ä»·å€¼å‡½æ•°çš„å­¦ä¹ ç›®æ ‡ï¼Œè®©ä»·å€¼ç½‘ç»œ $V_\phi(s_t)$ **å‡†ç¡®é¢„æµ‹**ä»çŠ¶æ€ \( s_t \) å¼€å§‹çš„**æœŸæœ›æ€»å›æŠ¥**ã€‚

> PPO-clip Loss å†³å®šâ€œå¾€å“ªé‡Œèµ°â€ï¼ˆåå¥½æ–¹å‘ï¼‰ï¼ŒLM Loss ç¡®ä¿â€œä¸èµ°åâ€ï¼ˆè¯­è¨€åˆç†ï¼‰ï¼ŒMSE Loss æä¾›â€œåœ°å›¾â€ï¼ˆä»·å€¼ä¼°è®¡ï¼‰â€”â€”ä¸‰è€…åˆåŠ›è®© LLM åœ¨äººç±»åå¥½ç©ºé—´ä¸­ç¨³å¥èˆªè¡Œã€‚

ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæµç¨‹åº”è¯¥æ˜¯ï¼š
- **é‡‡æ ·é˜¶æ®µ**ï¼šç”¨ $Ï€_Î¸^{old}$ æ ¹æ®ç”¨æˆ·è¾“å…¥ x ç”Ÿæˆå›ç­” y ---> ç”¨ Reward Model ç»™ $(x,y)$ æ‰“åˆ† $r(x,y)$ ---> ç”¨ Value Model å’Œ GAE è®¡ç®—æ¯ä¸ª token çš„ä¼˜åŠ¿å‡½æ•° $Ã‚(s_t, a_t)$ å’Œå›æŠ¥ $RÌ‚_t$ ---> å­˜å…¥ Experience Bufferã€‚
- **æ›´æ–°é˜¶æ®µ**ï¼šä» Buffer ä¸­é‡‡æ · mini-batch æ•°æ® ---> è®¡ç®— PPO-clip Lossã€LM Lossã€MSE Loss ---> åå‘ä¼ æ’­æ›´æ–° Policy LM å’Œ Value Model --->  æ›´æ–°åçš„æ–°ç­–ç•¥æˆä¸ºä¸‹ä¸€è½®çš„ $Ï€_Î¸^old$
- **è¿­ä»£å¾ªç¯**ï¼šé‡å¤é‡‡æ · â†’ è®¡ç®—å¥–åŠ±ä¸ä¼˜åŠ¿ â†’ æ›´æ–°ç­–ç•¥ â†’ æ–°ç­–ç•¥é‡‡æ ·...

å½“æˆ‘ä»¬çœ‹ OPENAI å…³äº [PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html) ç®—æ³•çš„æ–‡æ¡£ï¼Œä»–çœ‹èµ·æ¥å¾ˆç®€å•ï¼š

<div align="center">
   <img src="images/14-1-ppoç®—æ³•ä¼ªä»£ç .png" />
   <p>å›¾14.1 ppoç®—æ³•ä¼ªä»£ç </p>
</div>

ä½†åœ¨å®è·µä¸Šï¼ŒPPO çš„ç†è®ºå’Œå®ç°å®Œå…¨æ˜¯ä¸¤å›äº‹ã€‚PPO ç†è®ºç®€æ´ï¼Œä½†å®é™…è°ƒå‚å’Œå®ç°é™·é˜±æå¤šï¼ˆå¦‚ä»·å€¼å‡½æ•°è®­ç»ƒã€ä¼˜åŠ¿ä¼°è®¡ã€KL æ§åˆ¶ã€å¥–åŠ±å½’ä¸€åŒ–ç­‰ï¼‰ï¼Œæœ‰ç¯‡åšå®¢ç”šè‡³åˆ—å‡ºäº†[37ä¸ªPPOå®ç°ç»†èŠ‚](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)ï¼Œå‘ç°ä¸åŒçš„ PPO å˜ä½“åœ¨ RL çš„ benchmarks è¡¨ç°å‡ºäº†ä¸åŒçš„å¾—åˆ†ã€‚

<div align="center">
   <img src="images/14-2-ppoå®ç°ç»†èŠ‚å¯¹æ€§èƒ½çš„å½±å“.png" />
   <p>å›¾14.2 ppoå®ç°ç»†èŠ‚å¯¹æ€§èƒ½çš„å½±å“.png

</p>
 </div>

è€Œä¸”è¿˜æœ‰ä¸€ç¯‡è®ºæ–‡ï¼Œä¸“é—¨æ¢è®¨ä¸ºä»€ä¹ˆç»†èŠ‚å¯¹äº PPO å¦‚æ­¤é‡è¦ï¼Œè¯·å‚è€ƒ [Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO](https://arxiv.org/abs/2005.12729)ã€‚ä»¥åŠå¦‚æœä½ çœŸçš„æŠŠä»–ä»¬æç ¸äº†ï¼Œç”šè‡³æ²¡æœ‰æ­£ç¡®è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œä½†æ•ˆæœåè€Œæ›´å¥½ã€‚å¦‚æœä½ å»çœ‹ PPO çš„å®ç°ç»†èŠ‚ï¼Œä¼šå‘ç°æƒ…å†µéå¸¸å¤æ‚ï¼Œæ‰€ä»¥æˆ‘ä»¬ç¡®å®éœ€è¦é€šè¿‡ä»£ç çœ‹ä¸‹ PPO çš„å…·ä½“å®ç°ï¼š

å‚è€ƒ [alpaca_farm ä¸­å…³äº PPO çš„å®ç°](https://github.com/tatsu-lab/alpaca_farm/blob/30717ddae735365de756ee2085191b491a71788d/src/alpaca_farm/rl/ppo_trainer.py)ï¼Œè¯¥å®ç°éµå¾ªå…¸å‹çš„ on-policy RL å¾ªç¯ï¼Œå®ç°äº† PPO ç®—æ³•åœ¨ è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ ä¸Šçš„å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ŒåŒ…å«ï¼š

- Rolloutï¼ˆé‡‡æ ·ï¼‰ï¼šç”¨å½“å‰ç­–ç•¥ç”Ÿæˆ responses
- å¥–åŠ±è®¡ç®—ä¸å¡‘å½¢ï¼ˆReward Shapingï¼‰ï¼šç»“åˆä»»åŠ¡å¥–åŠ± + KL æƒ©ç½š
- ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
- æŸå¤±è®¡ç®—ï¼ˆPolicy + Value Loss with Clippingï¼‰ï¼šç”¨ PPO æŸå¤±å‡½æ•°ä¼˜åŒ–ç­–ç•¥ï¼ˆActorï¼‰å’Œä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
- æ—¥å¿—è®°å½•ä¸æ¨¡å‹ä¿å­˜

**å¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰ï¼š** å°†ç¨€ç–çš„ä»»åŠ¡å¥–åŠ±ï¼ˆåªåœ¨åºåˆ—æœ«å°¾ï¼‰ä¸å¯†é›†çš„KL æƒ©ç½šï¼ˆæ¯ä¸ª tokenï¼‰ç»“åˆèµ·æ¥ï¼Œå½¢æˆå¯è®­ç»ƒçš„ reward signal

```
def _shape_reward(self, rewards, responses, logprobs, ref_logprobs):
    # è®¡ç®— KL æ•£åº¦ï¼šç”¨ (logp - ref_logp) çš„æ­£å€¼éƒ¨åˆ†ï¼ˆå³æ–°ç­–ç•¥æ¯”å‚è€ƒç­–ç•¥æ›´â€œè‡ªä¿¡â€æ‰æƒ©ç½šï¼‰
    kl = torch.clamp(logprobs - ref_logprobs, min=0.0)

    # éä»»åŠ¡å¥–åŠ± = -Î² * KLï¼ˆÎ² ç”± self.kl_ctl æ§åˆ¶ï¼Œå¯åŠ¨æ€è°ƒæ•´ï¼‰
    non_score_rewards = -self.kl_ctl.value * kl

    # åˆå§‹åŒ–å¡‘å½¢å¥–åŠ±ï¼šå…ˆå¡«å…¥ KL æƒ©ç½šï¼ˆæ¯ä¸ª token éƒ½æœ‰ï¼‰
    shaped_rewards = non_score_rewards.clone()

    # æ‰¾åˆ°æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªé padding token çš„ä½ç½®ï¼ˆå³ EOS æˆ–çœŸå®ç»“å°¾ï¼‰
    terminal_positions = (responses != self.tokenizer.pad_token_id).sum(dim=1) - 1

    # åœ¨æœ€åä¸€ä¸ª token å¤„åŠ ä¸Šä»»åŠ¡å¥–åŠ±ï¼ˆå¦‚æ•°å­¦é¢˜æ˜¯å¦ç­”å¯¹ï¼‰
    shaped_rewards[list(range(rewards.size(0))), terminal_positions] += rewards

    return dict(shaped_rewards=shaped_rewards, non_score_rewards=non_score_rewards, kl=kl)
```


**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼š** ç”¨ GAE ä¼°è®¡æ¯ä¸ª token çš„ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰ï¼Œæ›¿ä»£åŸå§‹å¥–åŠ±ï¼Œå¤§å¹…é™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®ã€‚

```
def _estimate_advantage(self, rewards, values):
    if self.args.whiten_rewards:
        rewards = torch_ops.whiten(rewards, shift_mean=False)  # å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰

    lastgaelam = 0
    advantages_reversed = []
    gen_length = self.args.response_len  # ç”Ÿæˆé•¿åº¦ï¼ˆå¦‚ 128ï¼‰

    # ä»åå¾€å‰è®¡ç®— GAEï¼ˆåå‘éå† tokenï¼‰
    for t in reversed(range(gen_length)):
        nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
        # TD error: Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
        delta = rewards[:, t] + self.args.gamma * nextvalues - values[:, t]
        # GAE: A_t = Î´_t + Î³Î» A_{t+1}
        lastgaelam = delta + self.args.gamma * self.args.lam * lastgaelam
        advantages_reversed.append(lastgaelam)

    advantages = torch.stack(advantages_reversed[::-1], dim=1)  # åè½¬å›æ­£å¸¸é¡ºåº
    returns = advantages + values  # Q(s,a) â‰ˆ A(s,a) + V(s)

    # ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ–ï¼ˆå‡å‡å€¼ã€é™¤æ ‡å‡†å·®ï¼‰â†’ é™ä½æ–¹å·®
    advantages = torch_ops.whiten(advantages, shift_mean=True)

    return dict(returns=returns, advantages=advantages)
```

**rolloutï¼ˆé‡‡æ ·è½¨è¿¹ï¼‰ï¼š** å®Œæˆä¸€æ¬¡å®Œæ•´çš„ é‡‡æ · â†’ è¯„ä¼° â†’ å¥–åŠ±è®¡ç®— â†’ ä¼˜åŠ¿ä¼°è®¡ æµç¨‹ï¼Œä¸ºåç»­ PPO æ›´æ–°å‡†å¤‡æ•°æ®

```
@torch.inference_mode()
def rollout(self, queries_data):
    self.policy.eval()
    unwrapped_policy = self.accelerator.unwrap_model(self.policy, keep_fp32_wrapper=True)
    self.ref_policy.eval()
    self.reward_model.eval()

    rollouts = []
    for batch in tqdm.tqdm(queries_data, desc="rollout"):
        # 1. ä»å½“å‰ç­–ç•¥ç”Ÿæˆ responses
        queries, masks = batch['queries'], batch['query_attn_masks']
        responses = unwrapped_policy.respond(queries, masks, temperature=...)  # ç”Ÿæˆ

        # 2. ç”¨å½“å‰ç­–ç•¥è®¡ç®— logprobs å’Œ valuesï¼ˆcritic è¾“å‡ºï¼‰
        policy_outputs = self.policy(queries, masks, responses, ...)  # forward

        # 3. ç”¨å‚è€ƒç­–ç•¥ï¼ˆSFT æ¨¡å‹ï¼‰è®¡ç®— ref_logprobsï¼ˆç”¨äº KLï¼‰
        ref_outputs = self.ref_policy(queries, masks, responses, ...)

        # 4. å°† response è½¬ä¸ºæ–‡æœ¬ï¼Œå†ç”¨ reward tokenizer é‡æ–° tokenize
        #    ï¼ˆå› ä¸º policy å’Œ reward model çš„ tokenizer å¯èƒ½ä¸åŒï¼‰
        text_queries = decode(queries); text_responses = decode(responses)
        text_sequences = [q + r for q, r in zip(text_queries, text_responses)]
        sequences = reward_tokenizer(text_sequences, ...)  # é‡æ–° tokenize

        # 5. ç”¨ reward model è®¡ç®—ä»»åŠ¡å¥–åŠ±
        reward_outputs = self.reward_model(**sequences)
        reward_outputs = self.post_reward(reward_outputs, responses)  # å¤„ç†æœªæ­£å¸¸ç»“æŸçš„åºåˆ—

        # 6. å¥–åŠ±å¡‘å½¢ï¼šåŠ å…¥ KL æƒ©ç½š
        shaped = self._shape_reward(rewards=reward_outputs['rewards'], ...)

        # 7. ä¿å­˜æ‰€æœ‰æ•°æ®åˆ° rollouts
        rollouts_batch.update(policy_outputs, ref_outputs, reward_outputs, shaped)
        rollouts.append(rollouts_batch.cpu())

    # åˆå¹¶æ‰€æœ‰ batch
    rollouts = common.merge_dict(rollouts, merge_fn=torch.cat)

    # 8. ç»Ÿä¸€è®¡ç®— GAEï¼ˆç”¨æ•´ä¸ª rollout æ•°æ®é›†ï¼Œæ›´ç¨³å®šï¼‰
    advantages = self._estimate_advantage(
        rewards=rollouts["shaped_rewards"].to(device),
        values=rollouts["values"].to(device),
    )

    return {**rollouts, **advantages}
```

**PPO æŸå¤±è®¡ç®—ï¼š** ä½¿ç”¨è£å‰ªæœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§

```
def compute_loss(self, rollouts):
    # æå–æ—§ç­–ç•¥æ•°æ®ï¼ˆfrom rolloutï¼‰
    values, old_logprob, returns, advantages, ... = rollouts

    # ç”¨å½“å‰ç­–ç•¥é‡æ–°è®¡ç®— logprobs å’Œ values
    outputs = self.policy(queries, masks, responses, ...)
    vpred = outputs["values"]      # æ–°çš„ value é¢„æµ‹
    logprob = outputs["logprobs"]  # æ–°çš„ log prob

    # --- Value Loss (Critic) ---
    # è£å‰ª value é¢„æµ‹ï¼ˆç±»ä¼¼ PPO è£å‰ªï¼‰
    vpredclipped = torch.clamp(vpred, values Â± cliprange_value)
    vf_losses1 = (vpred - returns) ** 2
    vf_losses2 = (vpredclipped - returns) ** 2
    vf_loss = 0.5 * max(vf_losses1, vf_losses2).mean()  # PPO-style value loss

    # --- Policy Loss (Actor) ---
    ratio = exp(logprob - old_logprob)  # æ–°æ—§ç­–ç•¥æ¦‚ç‡æ¯”
    pg_losses = -advantages * ratio
    pg_losses2 = -advantages * clip(ratio, 1-Îµ, 1+Îµ)
    pg_loss = max(pg_losses, pg_losses2).mean()  # PPO è£å‰ªç›®æ ‡

    # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + vf_coef * ä»·å€¼æŸå¤±
    loss = pg_loss + self.args.vf_coef * vf_loss

    # è®°å½•ç»Ÿè®¡é‡
    approxkl = 0.5 * (logprob - old_logprob)^2 çš„å‡å€¼
    entropy = outputs["entropies"].mean()

    return loss, stats
```


åœ¨è¯­è¨€æ¨¡å‹çš„ PPO è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå¥åº·ã€åˆç†çš„è®­ç»ƒæ›²çº¿æ˜¯ä»€ä¹ˆæ ·ï¼Ÿ


<div align="center">
   <img src="images/14-3-ppoè®­ç»ƒè¿‡ç¨‹æ›²çº¿.png" />
   <p>å›¾14.3 ppoè®­ç»ƒè¿‡ç¨‹æ›²çº¿</p>
 </div>

- Increasing overall rewardsï¼šæ€»å¥–åŠ±ä¸Šå‡ã€‚`kl_sum_seq` è¿™ä¸ªåå­—æœ‰ç‚¹è¯¯å¯¼æ€§ï¼Œå®ƒå®é™…ä¸Šä»£è¡¨çš„æ˜¯æ¯ä¸ªåºåˆ—çš„å¡‘å½¢å¥–åŠ±ï¼ˆshaped rewardsï¼‰çš„æ€»å’Œï¼Œè¿™ä¸ªâ€œå¡‘å½¢å¥–åŠ±â€ = ä»»åŠ¡å¥–åŠ±ï¼ˆå¦‚æ•°å­¦é¢˜ç­”å¯¹å¾—é«˜åˆ†ï¼‰ + KLæƒ©ç½šé¡¹ï¼ˆè´Ÿå€¼ï¼‰ï¼Œè¡¡é‡æ¨¡å‹çš„æ•´ä½“è¡¨ç°æ˜¯å¦åœ¨å˜å¥½ã€‚
- Incl. reward modelï¼š ä»»åŠ¡å¥–åŠ±ä¸Šå‡ã€‚è¿™ä¸ªæŒ‡æ ‡è¡¡é‡çš„æ˜¯æ¨¡å‹ç›´æ¥ä»å¥–åŠ±æ¨¡å‹é‚£é‡Œè·å¾—çš„ä»»åŠ¡å¥–åŠ±ï¼Œä¸åŒ…æ‹¬KLæƒ©ç½šã€‚å®ƒåæ˜ äº†æ¨¡å‹åœ¨æ ¸å¿ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜çš„å‡†ç¡®æ€§ã€éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ç­‰ã€‚
- Negative KL rewardsï¼š è´Ÿçš„KLå¥–åŠ±ã€‚è¿™ä¸ªæŒ‡æ ‡å°±æ˜¯ KL æƒ©ç½šé¡¹ï¼Œä¹Ÿå°±æ˜¯å‰é¢æåˆ°çš„ -Î² * KLã€‚è¯´æ˜æ¨¡å‹ç¡®å®åœ¨æ¢ç´¢å’Œæ”¹è¿›ï¼Œä½†æ²¡æœ‰å¤±æ§ã€‚è¿™æ˜¯PPOç®—æ³•â€œè¿‘ç«¯â€æ€æƒ³çš„ä½“ç°â€”â€”å…è®¸ä¸€å®šç¨‹åº¦çš„åç¦»ï¼Œä½†é™åˆ¶å…¶å¹…åº¦ã€‚

è¿™ä¸‰æ¡æ›²çº¿å…±åŒæç»˜äº†ä¸€ä¸ªå¥åº·çš„PPOè®­ç»ƒè¿‡ç¨‹ï¼šæ¨¡å‹åœ¨å¥–åŠ±æ¨¡å‹çš„å¼•å¯¼ä¸‹ï¼Œé€æ­¥å­¦ä¼šç”Ÿæˆæ›´å¥½çš„å›å¤ï¼ŒåŒæ—¶é€šè¿‡KLæƒ©ç½šä¿æŒä¸€å®šçš„ç¨³å®šæ€§ï¼Œé¿å…è¿‡åº¦åç¦»åˆå§‹çš„è‰¯å¥½è¡Œä¸ºã€‚

### 14.2.2 ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¦ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ï¼Ÿ

**1. ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ PPOï¼Ÿ**
PPO æ˜¯ç›®å‰æœ€æˆåŠŸçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œå°¤å…¶åœ¨è¯­è¨€æ¨¡å‹çš„å¯¹é½ï¼ˆAlignmentï¼‰ä¸­è¢«å¹¿æ³›åº”ç”¨ã€‚ä½†å®ƒæœ‰ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼š

- **å®ç°å¤æ‚ (complicated implementation)**ï¼šPPO ä¸æ˜¯ä¸€ä¸ªç®€å•çš„â€œå¼€ç®±å³ç”¨â€çš„ç®—æ³•ã€‚å®ƒåŒ…å«å¤šä¸ªå¤æ‚çš„ç»„ä»¶ï¼Œå¦‚ï¼š**Rollout é‡‡æ ·**ã€**å¥–åŠ±å¡‘å½¢**ã€ **ä¼˜åŠ¿ä¼°è®¡**ã€**æŸå¤±è®¡ç®—**ç­‰ã€‚è¿™äº›æ­¥éª¤éœ€è¦ç²¾å¿ƒè®¾è®¡å’Œè°ƒè¯•ï¼Œå¯¹äºæ–°æ‰‹æˆ–è¿½æ±‚å¿«é€Ÿè¿­ä»£çš„ç ”ç©¶è€…æ¥è¯´ï¼Œé—¨æ§›å¾ˆé«˜ã€‚

- **ä»·å€¼æ¨¡å‹ (Value model) çš„è´Ÿæ‹…**ï¼šPPO éœ€è¦ä¸€ä¸ªé¢å¤–çš„ **ä»·å€¼ç½‘ç»œï¼ˆValue Modelï¼‰** æ¥ä¼°è®¡çŠ¶æ€çš„ä»·å€¼ï¼ˆ`V(s)`ï¼‰ï¼Œä»è€Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆ`A = Q - V`ï¼‰ã€‚
    - **å†…å­˜æ¶ˆè€—å¤§ (memory hungry)**ï¼šä»·å€¼ç½‘ç»œä¸ç­–ç•¥ç½‘ç»œå…±äº«ä¸»å¹²ç»“æ„ï¼Œä½†éœ€è¦é¢å¤–çš„å‚æ•°å’Œè®¡ç®—èµ„æºã€‚
    - **é¢å¤–çš„è°ƒå‚ (additional tuning)**ï¼šä»·å€¼ç½‘ç»œæœ¬èº«ä¹Ÿéœ€è¦è®­ç»ƒå’Œä¼˜åŒ–ï¼Œè¿™å¢åŠ äº†æ•´ä¸ªç³»ç»Ÿçš„å¤æ‚æ€§å’Œè¶…å‚æ•°æœç´¢ç©ºé—´ã€‚ä½ éœ€è¦åŒæ—¶è°ƒä¼˜ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œï¼Œç¡®ä¿å®ƒä»¬ååŒå·¥ä½œã€‚

> âœ… **æ€»ç»“**ï¼šPPO è™½ç„¶å¼ºå¤§ä¸”æœ‰æ•ˆï¼Œä½†å…¶**å·¥ç¨‹å¤æ‚åº¦é«˜ã€èµ„æºæ¶ˆè€—å¤§ã€è°ƒå‚å›°éš¾**ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºæˆ–è¿½æ±‚é«˜æ•ˆå¼€å‘çš„åœºæ™¯ä¸‹ï¼Œæ˜¾å¾—ä¸å¤Ÿè½»ä¾¿ã€‚

**2. ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ DPOï¼Ÿ**

DPOï¼ˆDirect Preference Optimizationï¼‰æ˜¯è¿‘å¹´æ¥å…´èµ·çš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒç»•è¿‡äº†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ä»äººç±»åå¥½æ•°æ®ä¸­è¿›è¡Œä¼˜åŒ–ã€‚ä½†å…¶ä¹Ÿå­˜åœ¨ä»¥ä¸‹é™åˆ¶ï¼š

- **æ•°æ®å½¢å¼ä¸åŒ¹é… (Data not inherently pairwise)**ï¼šDPO çš„æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäº**æˆå¯¹æ¯”è¾ƒï¼ˆpairwise comparisonsï¼‰** æ•°æ®ï¼Œå³ç»™å®šä¸€ä¸ªæç¤ºï¼ˆpromptï¼‰ï¼Œæœ‰ä¸¤ä¸ªä¸åŒçš„å›å¤ï¼ˆresponse A å’Œ response Bï¼‰ï¼Œå¹¶æ ‡æ³¨å“ªä¸ªæ›´å¥½ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®å¹¶éå¤©ç„¶å°±æ˜¯æˆå¯¹çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œå¯éªŒè¯å¥–åŠ±â€ï¼ˆVerifiable Rewardsï¼‰é¢†åŸŸï¼Œæ•°æ®é€šå¸¸æ˜¯å•ä¸ªåºåˆ—åŠ ä¸Šä¸€ä¸ªå®¢è§‚çš„åˆ†æ•°ï¼ˆå¦‚æ•°å­¦é¢˜ç­”å¯¹å¾—1åˆ†ï¼Œç­”é”™å¾—0åˆ†ï¼‰ã€‚è¿™ç§**æ ‡é‡å¥–åŠ±ä¿¡å·**æ— æ³•ç›´æ¥ç”¨äº DPOã€‚

- **ç¦»çº¿ç®—æ³• (Offline)**ï¼šDPO æ˜¯ä¸€ç§**ç¦»çº¿ç®—æ³•**ã€‚å®ƒåœ¨å›ºå®šçš„ã€é¢„å…ˆæ”¶é›†å¥½çš„åå¥½æ•°æ®é›†ä¸Šè¿›è¡Œä¸€æ¬¡æ€§è®­ç»ƒã€‚è¿™ä¸ PPO çš„**åœ¨çº¿å­¦ä¹ **ç‰¹æ€§ä¸åŒã€‚PPO å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­ç”Ÿæˆæ–°çš„æ ·æœ¬ã€è·å¾—æ–°çš„åé¦ˆï¼Œå¹¶æ®æ­¤è¿­ä»£æ›´æ–°ç­–ç•¥ã€‚DPO â€œç†è®ºä¸Šå¯ä»¥é€šè¿‡è¿­ä»£ä½¿å…¶åœ¨çº¿åŒ–â€ï¼Œä½†è¿™ä¼šå¢åŠ å¤æ‚æ€§ï¼Œå¤±å»äº†å…¶ä½œä¸ºç®€å•ç¦»çº¿æ–¹æ³•çš„ä¼˜åŠ¿ã€‚

> âœ… **æ€»ç»“**ï¼šDPO åœ¨å¤„ç†**äººç±»åå¥½æ•°æ®**æ—¶éå¸¸ä¼˜é›…å’Œé«˜æ•ˆï¼Œä½†å®ƒ**ä¸é€‚ç”¨äºéæˆå¯¹çš„ã€å¯éªŒè¯çš„æ ‡é‡å¥–åŠ±åœºæ™¯**ï¼Œå¹¶ä¸”å…¶ç¦»çº¿ç‰¹æ€§é™åˆ¶äº†å®ƒåœ¨éœ€è¦æŒç»­æ¢ç´¢å’Œåœ¨çº¿å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚



### 14.2.3 GRPOï¼šå»æ‰äº†ä»·å€¼å‡½æ•°çš„ PPO

**GRPO (Group Relative Policy Optimization)** æ˜¯åœ¨ [DeepSeekMath](https://arxiv.org/pdf/2402.03300) è®ºæ–‡ä¸­æå‡ºå¹¶åœ¨ [Deepseek-R1](https://arxiv.org/abs/2501.12948) ä¸­å‘æ‰¬å…‰å¤§çš„ç®—æ³•ã€‚GRPO åœ¨ PPO çš„åŸºç¡€ä¸Šï¼Œç§»é™¤äº†ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰å’Œä¼˜åŠ¿è®¡ç®—ï¼ˆAdvantage Computationï¼‰ã€‚è¿™æ˜¯å¯¹ PPO æœ€å¤§çš„æ”¹åŠ¨ï¼Œä¹Ÿæ˜¯å…¶è½»é‡åŒ–çš„æ ¹æœ¬åŸå› ã€‚å¹¶ä¸”é‡‡ç”¨äº†ä¸€ç§å…¨æ–°çš„æ–¹å¼æ¥ä¼°ç®—â€œä¼˜åŠ¿â€â€”â€”å³ â€œç»„å†… z-scoreâ€ï¼ˆz-score within groupï¼‰ã€‚

<div align="center">
   <img src="images/14-4-ppoä¸grpoçš„å¯¹æ¯”.png" />
   <p>å›¾14.4 PPOä¸GRPOçš„å¯¹æ¯”</p>
 </div>

> âœ… ç®€å•æ¥è¯´ï¼ŒGRPO = PPO - Value Model + Group Z-Score Advantageã€‚


#### GRPO çš„ç›®æ ‡å‡½æ•°

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q) \right] \frac{1}{G} \sum_{i=1}^{G} \left( \min\left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} \left( \pi_\theta || \pi_{ref} \right) \right)
$$

- `min(...)`éƒ¨åˆ†ï¼šæ˜¯ PPO çš„ç»å…¸è£å‰ªç›®æ ‡å‡½æ•°ï¼Œç”¨äºæ›´æ–°ç­–ç•¥ $Ï€_Î¸$ã€‚
    - $\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}$ æ˜¯æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”ã€‚
    - $A_i$ æ˜¯ç¬¬ $i$ ä¸ªè¾“å‡º $o_i$ çš„â€œä¼˜åŠ¿â€ï¼Œè¿™æ˜¯ GRPO æœ€å¤§çš„åˆ›æ–°ç‚¹ã€‚
    - `clip(...)` æ˜¯ PPO çš„è£å‰ªæœºåˆ¶ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ã€‚
- $-Î² D_KL(...)$ éƒ¨åˆ†ï¼šè¿™æ˜¯ KL æ•£åº¦æƒ©ç½šé¡¹ï¼Œç”¨äºé˜²æ­¢æ–°ç­–ç•¥ `Ï€_Î¸` åç¦»å‚è€ƒç­–ç•¥ `Ï€_ref` å¤ªè¿œï¼Œä¿è¯ç”Ÿæˆç»“æœçš„ç¨³å®šæ€§ã€‚
    - `Î²` æ˜¯æ§åˆ¶ KL æƒ©ç½šå¼ºåº¦çš„è¶…å‚æ•°ã€‚

PPO çš„ç›®æ ‡å‡½æ•°ï¼š
$$
\min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), \text{ clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a) \right)
$$

GRPO ä¸ PPO ä¸¤è€…çš„ç›®æ ‡å‡½æ•°ç»“æ„éå¸¸ç›¸ä¼¼ï¼Œéƒ½åŒ…å«æ¦‚ç‡æ¯”å’Œè£å‰ªã€‚æ ¸å¿ƒåŒºåˆ«åœ¨äº `A` çš„æ¥æºï¼š
- **PPO**: `A` æ˜¯é€šè¿‡ä»·å€¼ç½‘ç»œ `V(s)` å’Œ GAE è®¡ç®—å‡ºæ¥çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚ä¸”èµ„æºå¯†é›†çš„è¿‡ç¨‹ã€‚
- **GRPO**: `A` æ˜¯é€šè¿‡ç»„å†… z-score è®¡ç®—å‡ºæ¥çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€å¿«é€Ÿã€æ— éœ€é¢å¤–æ¨¡å‹çš„è¿‡ç¨‹ã€‚


#### KL æ•£åº¦çš„è®¡ç®—

$$
\mathbb{D}_{KL} \left( \pi_\theta || \pi_{ref} \right) = \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1
$$

è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼çš„ KL æ•£åº¦å…¬å¼ã€‚å®ƒä¸æ˜¯æ ‡å‡†çš„ç§¯åˆ†å½¢å¼ï¼Œè€Œæ˜¯åœ¨æ¯ä¸ªé‡‡æ ·ç‚¹ $o_i$ ä¸Šè¿›è¡Œè¿‘ä¼¼ã€‚å®ƒçš„ä½œç”¨æ˜¯è¡¡é‡å½“å‰ç­–ç•¥ $Ï€_Î¸$ å’Œå‚è€ƒç­–ç•¥ $Ï€_{ref}$ åœ¨ç”Ÿæˆç‰¹å®šè¾“å‡º $o_i$ æ—¶çš„æ¦‚ç‡å·®å¼‚ã€‚

#### ç»„å†… z-score ä¼˜åŠ¿

$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}
$$

è¿™æ˜¯ GRPO çš„çµé­‚æ‰€åœ¨ï¼å®ƒå®Œå…¨æŠ›å¼ƒäº† PPO ä¸­å¤æ‚çš„ GAE è®¡ç®—ã€‚

**å¦‚ä½•è®¡ç®—ï¼Ÿ**
- å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ $q$ï¼Œä»æ—§ç­–ç•¥ $Ï€_{old}$ ä¸­**é‡‡æ ·ä¸€ç»„ï¼ˆGä¸ªï¼‰ä¸åŒçš„å›å¤** ${o_1, o_2, ..., o_G}$ã€‚
- ç”¨å¥–åŠ±æ¨¡å‹æˆ–å¯éªŒè¯è§„åˆ™ï¼Œä¸ºè¿™ G ä¸ªå›å¤**åˆ†åˆ«æ‰“åˆ†**ï¼Œå¾—åˆ°ä¸€ç»„å¥–åŠ± ${r_1, r_2, ..., r_G}$ã€‚
- è®¡ç®—è¿™ç»„å¥–åŠ±çš„**å‡å€¼ï¼ˆmeanï¼‰å’Œæ ‡å‡†å·®ï¼ˆstdï¼‰**ã€‚
- å°†æ¯ä¸ªå›å¤ $o_i$ çš„å¥–åŠ± $r_i$ å‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ ‡å‡†å·®ï¼Œå¾—åˆ°å®ƒçš„ $A_i$ã€‚

**ä¸ºä»€ä¹ˆå« â€œz-scoreâ€ï¼Ÿ**

åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œz-score è¡¨ç¤ºä¸€ä¸ªæ•°æ®ç‚¹è·ç¦»å¹³å‡å€¼æœ‰å¤šå°‘ä¸ªæ ‡å‡†å·®ã€‚è¿™é‡Œï¼Œ$A_i$ è¡¨ç¤ºå›å¤ $o_i$ çš„å¥–åŠ±åœ¨æœ¬ç»„ä¸­çš„â€œç›¸å¯¹ä¼˜åŠ£ç¨‹åº¦â€ã€‚å¦‚æœ $A_i > 0$ï¼Œè¯´æ˜è¿™ä¸ªå›å¤æ¯”ç»„å†…å¹³å‡æ°´å¹³å¥½ï¼›å¦‚æœ $A_i < 0$ï¼Œåˆ™è¯´æ˜å®ƒæ¯”å¹³å‡æ°´å¹³å·®ã€‚

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

- **ç®€å•é«˜æ•ˆ**ï¼šä¸éœ€è¦è®­ç»ƒé¢å¤–çš„ä»·å€¼ç½‘ç»œï¼Œä¹Ÿä¸éœ€è¦å¤æ‚çš„ GAE è®¡ç®—ã€‚
- **è‡ªå½’ä¸€åŒ–**ï¼šé€šè¿‡ç»„å†…æ¯”è¾ƒï¼Œè‡ªåŠ¨æ¶ˆé™¤äº†ä¸åŒé—®é¢˜ä¹‹é—´å¥–åŠ±å°ºåº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ•°å­¦é¢˜å¯èƒ½æœ€é«˜å¾—10åˆ†ï¼Œå¦ä¸€ä¸ªå¯èƒ½æœ€é«˜å¾—5åˆ†ï¼Œä½†å®ƒä»¬åœ¨åŒä¸€ç»„å†…æ¯”è¾ƒæ—¶ï¼Œz-score èƒ½å…¬å¹³åœ°åæ˜ ç›¸å¯¹å¥½åã€‚
- **é€‚ç”¨äºå¯éªŒè¯å¥–åŠ±**ï¼šå¯¹äºä¸€ä¸ªæ•°å­¦é¢˜ï¼Œä½ å¯ä»¥è®©æ¨¡å‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œç„¶åç”¨ç¨‹åºè‡ªåŠ¨åˆ¤æ–­æ¯ä¸ªç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆå¾—åˆ†ä¸º1æˆ–0ï¼‰ï¼Œå†ç”¨ z-score æ¥åŒºåˆ†å“ªä¸ªç­”æ¡ˆâ€œæ›´å¥½â€ã€‚

> åœ¨åœ¨çº¿å­¦ä¹ ï¼ˆè¾¹é‡‡æ ·è¾¹æ›´æ–°ï¼‰çš„åœºæ™¯ä¸‹ï¼ŒGRPO æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç§ä½¿ç”¨äº†ç»„å†…æ ‡å‡†åŒ–å¥–åŠ±çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚

#### ğŸ’» ä»£ç è§£è¯»ï¼šä¸€ä¸ªæç®€çš„ GRPO å®ç°

GRPO çš„å®ç°éå¸¸ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„ GAE è®¡ç®—ã€‚ä¸‹é¢æˆ‘ä»¬åŸºäº [nano-aha-moment](https://github.com/McGill-NLP/nano-aha-moment/blob/main/nano_r1_script.py) é¡¹ç›®ä¸­å¯¹äº GRPO ç®—æ³•çš„å®ç°ï¼Œå¯¹å…³é”®ä»£ç è¿›è¡Œåˆ†æã€‚å¦‚ä¸‹compute_pg_loss æ˜¯ä¸€ä¸ªå…¸å‹çš„ GRPO æŸå¤±è®¡ç®—å‡½æ•°ï¼š

```python
def compute_pg_loss(
    policy_model: Union[DeepSpeedEngine, PreTrainedModel], # å½“å‰è¦è®­ç»ƒçš„è¯­è¨€æ¨¡å‹
    batch: Dict[str, torch.Tensor], # ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
    total_response_len: torch.Tensor, # ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
    TEMPERATURE: float, # ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ï¼ˆå½±å“ log-prob è®¡ç®—ï¼‰
    KL_COEFFICIENT: float, # æ§åˆ¶ KL æƒ©ç½šå¼ºåº¦çš„è¶…å‚æ•°
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute the policy gradient loss with KL penalty between policy and reference models.

    This function:
    1. Calculates KL divergence penalty between the models
    2. Computes policy gradient loss using advantages
    3. Combines the losses with KL coefficient

    Args:
        policy_model: The model being trained
        batch: Dictionary containing:
            - input_ids: Tensor of shape [batch_size, seq_len]
            - attention_mask: Tensor of shape [batch_size, seq_len]
            - labels: Tensor of shape [batch_size, seq_len] with -100 for ignored positions
            - advantages: Tensor of shape [batch_size, seq_len]
            - ref_log_probs: Tensor of shape [batch_size, seq_len-1]
        total_response_len: Total number of valid tokens in the batch. This is a scalar tensor.

    Returns:
        Tuple containing:
            - loss: Combined policy gradient and KL penalty loss (scalar tensor)
            - metrics: Dictionary with detailed loss components:
                - policy_loss: Pure policy gradient loss
                - kl_penalty: KL divergence penalty
                - entropy: Policy entropy
    """

    # 1. ä» batch ä¸­æå–å…³é”®å¼ é‡
    input_ids = batch["input_ids"]  # [batch_size, seq_len]ï¼Œå®Œæ•´åºåˆ—ï¼ˆprompt + responseï¼‰
    attention_mask = batch["attention_mask"]  # [batch_size, seq_len]ï¼ŒæŒ‡ç¤ºæœ‰æ•ˆ token
    labels = batch["labels"]  # [batch_size, seq_len]ï¼Œé€šå¸¸ä¸ input_ids ç›¸åŒæˆ–å³ç§»ä¸€ä½
    labels_mask = batch["labels_mask"]  # [batch_size, seq_len]ï¼Œ1 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ response tokenï¼Œ0 è¡¨ç¤º prompt æˆ– padding
    advantages = batch["advantages"]  # [batch_size, seq_len]ï¼Œæ¯ä¸ª token çš„â€œä¼˜åŠ¿â€å€¼ï¼ˆæ¥è‡ªç»„å†…å½’ä¸€åŒ–ï¼‰
    ref_logps = batch["ref_log_probs"]  # [batch_size, seq_len-1]ï¼Œå‚è€ƒæ¨¡å‹åœ¨ response token ä¸Šçš„ log-probï¼ˆé•¿åº¦æ¯” input_ids å°‘ 1ï¼‰

    # 2. æ„å»ºæ¨¡å‹è¾“å…¥
    model_inputs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "labels_mask": labels_mask,
    }

    # 3. è®¡ç®—å½“å‰ç­–ç•¥çš„ token log-probabilities
    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)  # è®© policy_model å¯¹ input_ids åšå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°æ¯ä¸ª token çš„ log-probabilityï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯ [batch_size, seq_len-1]ï¼Œå› ä¸ºæ¨¡å‹é¢„æµ‹çš„æ˜¯ input_ids[1:]

    # 4. å¯¹é½ mask å¹¶è®¡ç®— KL æƒ©ç½šé¡¹
    labels_mask = labels_mask[..., 1:].to(logps.dtype)  # å°† labels_mask ä¹Ÿå³ç§»ä¸€ä½ï¼Œä¸ logps å¯¹é½ï¼Œåªä¿ç•™ response token çš„ maskï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯ [batch_size, seq_len-1]ï¼Œ
    
    # ç›´æ¥è®¡ç®— KL éœ€è¦å¯¹æ•´ä¸ªè¯æ±‡è¡¨æ±‚å’Œï¼ˆsum(p * log(p/q))ï¼‰ï¼Œè®¡ç®—é‡æå¤§ã€‚ä½¿ç”¨äº† Bregman divergence çš„ä¸€ç§è¿‘ä¼¼ï¼Œè€Œè¿™ä¸ªè¿‘ä¼¼åªä¾èµ–äº logps å’Œ ref_logpsï¼ˆå³ token-level çš„ log-probï¼‰ï¼Œéå¸¸é«˜æ•ˆã€‚
    ref_logratio = ref_logps - logps
    kl_penalty = torch.exp(ref_logratio) - 1 - ref_logratio  # [batch_size, seq_len-1]
    kl_penalty = kl_penalty * labels_mask  # [batch_size, seq_len-1]ï¼Œåªå¯¹ response token è®¡ç®— KL æƒ©ç½šï¼Œå¿½ç•¥ prompt

    # 5. è®¡ç®—è¾…åŠ©ç»Ÿè®¡é‡ï¼ˆä¸å‚ä¸æ¢¯åº¦ï¼‰
    with torch.no_grad():
        entropy = -logps.sum() / labels_mask.sum()  # scalar
        zero_advantages = close_to_zero(advantages[..., 1:], labels_mask)  # scalar

    # 6. è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
    policy_loss = -logps * advantages[..., 1:]  # [batch_size, seq_len-1]ï¼Œadvantages[..., 1:]å– advantage ä»ç¬¬ 2 ä¸ª token å¼€å§‹ï¼Œä¸ logps å¯¹é½
    policy_loss = policy_loss * labels_mask  # [batch_size, seq_len-1]

    # 7. ç»„åˆæ€»æŸå¤±å¹¶å½’ä¸€åŒ–
    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len  # å°†ç­–ç•¥æŸå¤±å’Œ KL æƒ©ç½šåŠ æƒæ±‚å’Œï¼Œé™¤ä»¥ total_response_lenï¼Œå°†æ€»æŸå¤±å½’ä¸€åŒ–ä¸ºæ¯ä¸ªæœ‰æ•ˆ token çš„å¹³å‡æŸå¤±ï¼Œä½¿ loss å€¼åœ¨ä¸åŒ batch size ä¸‹å¯æ¯”ã€‚

    # 8. æ„å»ºè¿”å›çš„æŒ‡æ ‡å­—å…¸
    metrics = {
        "policy_loss": policy_loss.sum().item() / total_response_len.item(),
        "kl_penalty": kl_penalty.sum().item() / total_response_len.item(),
        "entropy": entropy.item() / total_response_len.item(),
        "zero_advantages_ratio": zero_advantages.item() / total_response_len.item(),
    }

    return loss, metrics

```

åœ¨ GRPO ä¸­ï¼Œä¼˜åŠ¿ï¼ˆAdvantageï¼‰çš„è®¡ç®—è¿‡ç¨‹æå…¶ç®€å•ï¼Œå…¶æ ¸å¿ƒå°±æ˜¯â€œç»„å†… z-score å½’ä¸€åŒ–â€ï¼Œå¹¶ä¸”ä¸ºäº†æ•°å€¼ç¨³å®šï¼Œæ·»åŠ äº†ä¸€ä¸ªå¾®å°çš„å¸¸æ•° 1e-4ã€‚ä¸‹é¢ä½¿å…¶å®ç°ä»£ç ï¼š

```python
# 1. æ•°æ®æ ¡éªŒä¸åˆ†ç»„

assert len(all_generations) == len(all_finish_reasons) # all_generations æ˜¯æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰å›å¤ï¼Œall_finish_reasons æ˜¯æ¯ä¸ªå›å¤çš„ç»“æŸåŸå› ï¼ˆå¦‚ "stop" æˆ– "length"ï¼‰ï¼Œsamples æ˜¯åŸå§‹è¾“å…¥æ ·æœ¬
assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE # GENERATIONS_PER_SAMPLE æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œè¡¨ç¤ºå¯¹æ¯ä¸ªè¾“å…¥æ ·æœ¬ sampleï¼Œè¦ç”Ÿæˆå¤šå°‘ä¸ªä¸åŒçš„å›å¤ï¼ˆä¾‹å¦‚ 3 ä¸ªï¼‰ã€‚æ‰€ä»¥æ€»å›å¤æ•° = æ ·æœ¬æ•° Ã— æ¯æ ·æœ¬ç”Ÿæˆæ•°ã€‚

# å°†æ‰€æœ‰ç”Ÿæˆçš„å›å¤æŒ‰æ ·æœ¬åˆ†ç»„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ GENERATIONS_PER_SAMPLE=3ï¼Œé‚£ä¹ˆ groups = [[0,1,2], [3,4,5], ...]ï¼Œå…¶ä¸­ [0,1,2] å¯¹åº”ç¬¬ä¸€ä¸ªæ ·æœ¬ç”Ÿæˆçš„ä¸‰ä¸ªå›å¤
groups = [
    list(range(i, i + GENERATIONS_PER_SAMPLE)) for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)
]

# 2. åˆå§‹åŒ–å­˜å‚¨å˜é‡

all_query_token_ids, all_responses_token_ids, all_samples, all_rewards = [], [], [], []
stats = { "response_lengths": [], "rewards": [], "non_stop_rate": [], }

# 3. æ ¸å¿ƒå¾ªç¯ï¼šå¯¹æ¯ä¸ªæ ·æœ¬åŠå…¶ç”Ÿæˆçš„å›å¤ç»„è¿›è¡Œå¤„ç†
# å¯¹äºå½“å‰æ ·æœ¬ sampleï¼Œè·å–å®ƒå¯¹åº”çš„ group_indicesï¼ˆå¦‚ [0,1,2]ï¼‰ï¼Œç„¶åæå–å‡ºè¯¥ç»„çš„ç»“æŸåŸå› ã€token ID å’Œè§£ç åçš„æ–‡æœ¬
for sample, group_indices in zip(samples, groups):
    finish_reasons = [all_finish_reasons[i] for i in group_indices]
    response_token_ids = [all_generations[i] for i in group_indices]
    responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)

    # å¯¹ç»„å†…çš„æ¯ä¸€ä¸ªå›å¤ respï¼Œè°ƒç”¨ compute_reward å‡½æ•°è®¡ç®—å…¶å¥–åŠ±åˆ†æ•°ã€‚compute_reward æ˜¯ä½ è‡ªå®šä¹‰çš„å‡½æ•°ï¼Œæ¯”å¦‚åˆ¤æ–­æ•°å­¦é¢˜æ˜¯å¦ç­”å¯¹ã€ä»£ç æ˜¯å¦èƒ½è¿è¡Œç­‰
    rewards_and_metrics = [compute_reward(resp, sample, EOS_TOKEN) for resp in responses]
    rewards, reward_metrics = zip(*rewards_and_metrics) # zip(*rewards_and_metrics) å°† (reward, metrics) å…ƒç»„è§£åŒ…æˆä¸¤ä¸ªåˆ—è¡¨ï¼šrewards å’Œ reward_metrics

    # 4. å…³é”®æ­¥éª¤ï¼šè®¡ç®—ç»„å†…å½’ä¸€åŒ–çš„â€œä¼˜åŠ¿â€ (Advantages)
    rewards = np.array(rewards)
    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4) # rewards - rewards.mean()ï¼šè®¡ç®—æ¯ä¸ªå›å¤çš„å¥–åŠ±ç›¸å¯¹äºç»„å†…å¹³å‡å€¼çš„åå·®; é™¤ä»¥ç»„å†…æ ‡å‡†å·®ï¼Œå¾—åˆ° z-score; å½“ç»„å†…æ‰€æœ‰å¥–åŠ±éƒ½ç›¸åŒæ—¶ï¼ˆæ ‡å‡†å·®ä¸º 0ï¼‰ï¼Œç›´æ¥é™¤ä»¥ 0 ä¼šå¯¼è‡´ NaN é”™è¯¯ã€‚åŠ ä¸Šä¸€ä¸ªæå°çš„å¸¸æ•° 1e-4 å¯ä»¥é¿å…è¿™ç§æƒ…å†µï¼Œä¿è¯è®¡ç®—ç¨³å®š

    # å°†æ¯ä¸ªå›å¤çš„æ ‡é‡ä¼˜åŠ¿å€¼ resp_adv æ‰©å±•ä¸ºä¸€ä¸ªåºåˆ—ï¼Œä½¿å…¶é•¿åº¦ä¸å›å¤çš„ token æ•°ç›¸åŒ,ç›®çš„æ˜¯ä¸ºäº†è®©ä¼˜åŠ¿ä¿¡å·å¯ä»¥ä¸æ¯ä¸ª token çš„ log-probability å¯¹é½ï¼Œä»è€Œè®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
    per_token_advantages = [[adv] * len(resp) for adv, resp in zip(advantages, response_token_ids)]

    # 5. æ”¶é›†æœ€ç»ˆæ•°æ®å¹¶è¿”å›
    # å°†å½“å‰ç»„çš„æ•°æ®ï¼ˆå¥–åŠ±ã€æ ·æœ¬ã€token IDï¼‰è¿½åŠ åˆ°å…¨å±€åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åç»­ç»Ÿä¸€å¤„ç†
    all_query_token_ids.extend([sample["input_ids"]] * GENERATIONS_PER_SAMPLE)
    all_responses_token_ids.extend(response_token_ids)
    all_advantages.extend(per_token_advantages)

    # è®°å½•ä¸€äº›æœ‰ç”¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚å¹³å‡å¥–åŠ±ã€æœªæ­£å¸¸ç»“æŸçš„æ¯”ä¾‹ã€å›å¤é•¿åº¦ç­‰
    stats["rewards"].extend(rewards)
    stats["non_stop_rate"].extend([fr != "stop" for fr in finish_reasons])
    stats["response_lengths"].extend([len(ids) for ids in response_token_ids])

    for rm in reward_metrics:
        for k, v in rm.items():
            stats.setdefault(f"reward_metrics/{k}", []).append(v)

# å°†æ‰€æœ‰æ•°æ®æ‰“åŒ…æˆä¸€ä¸ªå­—å…¸ episodes è¿”å›ï¼Œä¾›åç»­çš„ compute_pg_loss å‡½æ•°ä½¿ç”¨
episodes = {
        "all_query_token_ids": all_query_token_ids,
        "all_response_token_ids": all_responses_token_ids,
        "all_advantages": all_advantages,
    }

    return episodes, stats
```

#### GRPO çš„å®é™…æ•ˆæœ

GRPO çš„å®é™…æ•ˆæœå¦‚ä½•å‘¢ï¼Ÿä¸‹å›¾å±•ç¤ºäº†åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½ï¼š

<div align="center">
   <img src="images/14-5-grpoä¸å…¶ä»–è®­ç»ƒæ–¹æ³•åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.5 GRPOä¸å…¶ä»–è®­ç»ƒæ–¹æ³•åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”</p>
 </div>

å·¦å›¾ GSM8K æ˜¯ä¸€ä¸ªå°å­¦æ•°å­¦åº”ç”¨é¢˜æ•°æ®é›†ï¼Œå³å›¾ MATH è¿™æ˜¯ä¸€ä¸ªæ›´éš¾çš„é«˜ä¸­æ•°å­¦ç«èµ›é¢˜æ•°æ®é›†ã€‚Yè½´æ˜¯å‡†ç¡®ç‡ï¼ˆAcc %ï¼‰ï¼ŒXè½´æ˜¯è®­ç»ƒæ­¥æ•°ï¼ˆStepsï¼‰ã€‚å›¾ä¸­æœ‰å¤šæ¡æ›²çº¿ï¼Œä»£è¡¨ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼š

- RFT (Reinforcing Fine-Tuning)ï¼šè¿™æ˜¯æœ€åŸºç¡€çš„æ–¹æ³•ã€‚å®ƒåªå¥–åŠ±â€œæ­£ç¡®ç­”æ¡ˆâ€ï¼Œè€Œä¸è€ƒè™‘ç”Ÿæˆè¿‡ç¨‹ã€‚å¯ä»¥ç†è§£ä¸ºâ€œåªè¦ç»“æœå¯¹ï¼Œä¸ç®¡è¿‡ç¨‹â€ã€‚åœ¨å›¾ä¸­ç”¨ç´«è‰²çº¿è¡¨ç¤ºã€‚
- Online RFTï¼šè¿™æ˜¯ RFT çš„åœ¨çº¿ç‰ˆæœ¬ï¼Œå¯èƒ½æ„å‘³ç€å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šåŠ¨æ€åœ°é‡‡æ ·å’Œæ›´æ–°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„æ•°æ®é›†ã€‚åœ¨å›¾ä¸­ç”¨ç»¿è‰²çº¿è¡¨ç¤ºã€‚
- GRPO+OS (Group Relative Policy Optimization + Online Sampling)ï¼šè¿™æ˜¯æ ‡å‡†çš„ GRPO æ–¹æ³•ï¼Œå³æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„â€œç»„å†… z-score å½’ä¸€åŒ–â€ä¼˜åŠ¿è®¡ç®—ã€‚åœ¨å›¾ä¸­ç”¨æ©™è‰²çº¿è¡¨ç¤ºã€‚
- GRPO+PS (Group Relative Policy Optimization + Process Supervision)ï¼šè¿™æ˜¯åœ¨ GRPO åŸºç¡€ä¸Šï¼Œé¢å¤–åŠ å…¥äº†â€œè¿‡ç¨‹ç›‘ç£â€ï¼ˆProcess Supervisionï¼‰ã€‚è¿™æ„å‘³ç€ä¸ä»…å¥–åŠ±æœ€ç»ˆç­”æ¡ˆï¼Œè¿˜å¥–åŠ±æ­£ç¡®çš„è§£é¢˜æ­¥éª¤ã€‚åœ¨å›¾ä¸­ç”¨è“è‰²çº¿è¡¨ç¤ºã€‚

ä»ä¸¤å¼ å›¾ä¸­æˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

- GRPO æ˜¾è‘—ä¼˜äº RFTï¼šæ— è®ºæ˜¯ GSM8K è¿˜æ˜¯ MATHï¼Œæ©™è‰²çº¿ï¼ˆGRPO+OSï¼‰å’Œè“è‰²çº¿ï¼ˆGRPO+PSï¼‰éƒ½æ˜æ˜¾é«˜äºç´«è‰²çº¿ï¼ˆRFTï¼‰ã€‚è¿™è¯´æ˜ GRPO ç®—æ³•æœ¬èº«æ˜¯æœ‰æ•ˆçš„ï¼Œå®ƒèƒ½å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ï¼Œä»è€Œè·å¾—æ›´é«˜çš„å‡†ç¡®ç‡ã€‚
- è¿‡ç¨‹ç›‘ç£ï¼ˆPSï¼‰å¸¦æ¥é¢å¤–å¢ç›Šï¼šåœ¨å¤§å¤šæ•°è®­ç»ƒæ­¥æ•°ä¸‹ï¼Œè“è‰²çº¿ï¼ˆGRPO+PSï¼‰ç•¥é«˜äºæ©™è‰²çº¿ï¼ˆGRPO+OSï¼‰ã€‚è¿™è¡¨æ˜ï¼Œå¦‚æœèƒ½å¤Ÿæä¾›å…³äºâ€œè§£é¢˜è¿‡ç¨‹â€çš„ç›‘ç£ä¿¡å·ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯ä»¥å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚
- GRPO çš„ç¨³å®šæ€§ï¼šç›¸æ¯”äºæ³¢åŠ¨è¾ƒå¤§çš„ RFT å’Œ Online RFT æ›²çº¿ï¼ŒGRPO çš„æ›²çº¿ç›¸å¯¹æ›´å¹³æ»‘ï¼Œè¿™åæ˜ äº†å…¶ç®—æ³•è®¾è®¡çš„ç¨³å®šæ€§ã€‚

### 14.2.4 GRPO çš„æ½œåœ¨ç¼ºé™·ï¼šé•¿åº¦åå·® (Length Bias)

è™½ç„¶ GRPO æ•ˆæœæ‹”ç¾¤ï¼Œä½†å­¦æœ¯ç•Œï¼ˆå¦‚ "Dr. GRPO" è®ºæ–‡ï¼‰æŒ‡å‡ºå…¶æ•°å­¦ä¸Šå­˜åœ¨ç‘•ç–µï¼š
#### 1. **æœ‰åæ¢¯åº¦**

GRPO é€šè¿‡â€œç»„å†… z-scoreâ€æ¥å½’ä¸€åŒ–å¥–åŠ±æˆ–è®¡ç®—ä¼˜åŠ¿ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†åœ¨ä¸å¼•å…¥å€¼å‡½æ•°çš„æƒ…å†µä¸‹æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œè®¡ç®—è¿™ä¸ª z-score ä¸­æ¶‰åŠçš„æ ‡å‡†å·®ï¼ˆstdevï¼‰å¯èƒ½ä¾èµ–äºæ‰€è§‚å¯Ÿåˆ°çš„æ ·æœ¬ï¼ˆå¯èƒ½ä¸å½“å‰ç­–ç•¥çš„è¾“å‡ºæœ‰å…³ï¼‰ï¼Œä»è€Œä½¿å¾—æ•´ä¸ªè¿‡ç¨‹ä¸å†æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šæ— åçš„åŸºçº¿å‡æ³•ï¼Œå¯èƒ½ä¼šåœ¨ç†è®ºä¸Šå¼•å…¥å¾®å°çš„åå·®ã€‚

ä¸€ä¸ªæ— åæ¢¯åº¦ç‰ˆæœ¬çš„ GRPO æ˜¯æ€æ ·çš„ï¼Ÿ

<div align="center">
   <img src="images/14-6-Dr-GRPOä¸æ ‡å‡†çš„GRPOçš„æ•°å­¦å…¬å¼ä¸æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.6 Dr.GRPOä¸æ ‡å‡†çš„GRPOçš„æ•°å­¦å…¬å¼ä¸æ€§èƒ½å¯¹æ¯”</p>
 </div>

Dr. GRPO çš„æ ¸å¿ƒæ”¹åŠ¨åœ¨äºç§»é™¤äº† GRPO åŸæœ‰çš„ $\frac{1}{|o_i|}$â€‹ï¼ˆå“åº”é•¿åº¦å½’ä¸€åŒ–ï¼‰å’Œ $\frac{1}{\text{std}(\{R(q, o_1), \dots, R(q, o_G)\})}$â€‹ï¼ˆæ ‡å‡†å·®å½’ä¸€åŒ–ï¼‰ï¼Œä»è€Œä¿®æ­£äº† GRPO ä¸­å­˜åœ¨çš„å“åº”é•¿åº¦åè§å’Œé—®é¢˜éš¾åº¦åè§ã€‚

å³å›¾å±•ç¤ºäº† GRPO å’Œ Dr. GRPO åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆRewardï¼‰ä¸è¾“å‡ºé•¿åº¦ï¼ˆOutput lengthï¼‰ä¹‹é—´çš„å…³ç³»ã€‚Dr. GRPO é€šè¿‡ç§»é™¤åè§ï¼Œæœ‰æ•ˆåœ°é˜»æ­¢äº†æ¨¡å‹ç”Ÿæˆä¸å¿…è¦çš„å†—é•¿å“åº”ï¼ˆå°¤å…¶æ˜¯åœ¨å›ç­”é”™è¯¯æ—¶ï¼‰ï¼Œä»è€Œæé«˜äº† token æ•ˆç‡ã€‚


#### 2. **é•¿åº¦åè§**

æ ‡å‡†å·®è¢«ç”¨æ¥â€œåŠ æƒâ€ï¼ˆupweightsï¼‰é‚£äº›â€œå¤ªå®¹æ˜“â€æˆ–â€œå¤ªéš¾â€çš„é—®é¢˜ã€‚

è¿™ç§åè§æºäºåœ¨GRPOï¼ˆGroup Relative Policy Optimizationï¼‰çš„ç›®æ ‡å‡½æ•°ä¸­ï¼Œå°†ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰é™¤ä»¥å“åº”çš„é•¿åº¦ \(|o_i|\)ã€‚

GRPOçš„ç›®æ ‡å‡½æ•°ä¸­ï¼Œé’ˆå¯¹å•ä¸ªå“åº” \(o_i\) åœ¨æ—¶é—´æ­¥ \(t\) çš„æ¢¯åº¦æ›´æ–°éƒ¨åˆ†ä¼šæ¶‰åŠä»¥ä¸‹é¡¹ï¼š
\[ \dots \times \frac{\hat{A}_{i,t}}{|o_i|} \dots \]
å…¶ä¸­ï¼š
*   \(|o_i|\)ï¼šè¡¨ç¤ºå“åº” \(o_i\) çš„é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰ã€‚
*   \(\hat{A}_{i,t}\)ï¼šæ˜¯ä¼˜åŠ¿å‡½æ•°ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸º \(\hat{A}_{i,t} = R(q, o_i) - \text{mean}(\{R(q, o_1), \dots, R(q, o_G)\}) / \text{std}(\{R(q, o_1), \dots, R(q, o_G)\})\)ï¼Œå…¶ä¸­ \(R(q, o_i)\) æ˜¯å“åº” \(o_i\) çš„å›æŠ¥ï¼ˆrewardï¼‰ã€‚

**å¯¹æ­£ç¡®å›ç­”çš„å½±å“ï¼ˆæ­£ä¼˜åŠ¿ï¼‰ï¼š** å½“ä¼˜åŠ¿å‡½æ•° \(\hat{A}_{i,t}\) ä¸ºæ­£ï¼ˆ\(\hat{A}_{i,t} > 0\)ï¼Œè¡¨ç¤ºä¸€ä¸ªæ­£ç¡®çš„å›ç­”ï¼‰æ—¶ï¼Œå°† \(\hat{A}_{i,t}\) é™¤ä»¥è¾ƒå°çš„å“åº”é•¿åº¦ \(|o_i|\) ä¼šå¾—åˆ°ä¸€ä¸ªæ›´å¤§çš„å€¼ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šæ”¶åˆ°æ›´å¤§çš„æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæ¿€åŠ±æ¨¡å‹å€¾å‘äºç”Ÿæˆæ›´çŸ­çš„æ­£ç¡®ç­”æ¡ˆï¼Œå³åå¥½ç®€æ´æ€§ã€‚
**å¯¹é”™è¯¯å›ç­”çš„å½±å“ï¼ˆè´Ÿä¼˜åŠ¿ï¼‰ï¼š** å½“ä¼˜åŠ¿å‡½æ•° \(\hat{A}_{i,t}\) ä¸ºè´Ÿï¼ˆ\(\hat{A}_{i,t} < 0\)ï¼Œè¡¨ç¤ºä¸€ä¸ªé”™è¯¯çš„å›ç­”ï¼‰æ—¶ï¼Œå°† \(\hat{A}_{i,t}\) é™¤ä»¥è¾ƒå¤§çš„å“åº”é•¿åº¦ \(|o_i|\) ä¼šå¾—åˆ°ä¸€ä¸ªç›¸å¯¹è¾ƒå°ï¼ˆå³è´Ÿå¾—ä¸é‚£ä¹ˆå‰å®³ï¼‰çš„æƒ©ç½šã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯¹è¾ƒé•¿çš„é”™è¯¯å›ç­”çš„æƒ©ç½šä¼šå‡è½»ã€‚

è¿™ç§æœºåˆ¶å¯¼è‡´æ¨¡å‹åœ¨ç”Ÿæˆé”™è¯¯å›ç­”æ—¶ï¼Œæ›´å€¾å‘äºç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œè¿™æ˜¯ä¸€ç§â€œè¶Šé”™è¶Šé•¿â€çš„ç°è±¡ã€‚

## 14.3 æ¡ˆä¾‹ç ”ç©¶

è¿™é‡Œæˆ‘ä»¬ä»‹ç»ä¸‰ä¸ªå…³äº RLVR çš„å·¥ä½œï¼š
- Deepseek R1ï¼šæ˜¯è®¸å¤šè¿‘æœŸ RLVR å·¥ä½œçš„æ ¸å¿ƒï¼ŒåŒ…å«è®¸å¤šæœ‰è¶£çš„ç»†èŠ‚ã€‚ 
- Kimi K1.5ï¼šä¸ R1 åŒæ—¶æœŸï¼ŒRLVR æä¾›äº†ä¸ R1 äº’è¡¥çš„ç»†èŠ‚ã€‚ 
- Qwen 3ï¼šæœ€æ–°çš„å¼€æºæ¨ç†æ¨¡å‹å°è¯•ï¼Œä½æ•°æ®é‡ RLVR

### 14.3.1 DeepSeek R1

[DeepSeek R1](https://arxiv.org/pdf/2501.12948)è¿™ç¯‡è®ºæ–‡å¼•èµ·äº†ä¸å°çš„è½°åŠ¨ã€‚

<div align="center">
   <img src="images/14-7-DeepSeek-R1å¼•èµ·å¹¿æ³›çš„å…³æ³¨.png" />
   <p>å›¾14.7 DeepSeek-R1å¼•èµ·å¹¿æ³›çš„å…³æ³¨</p>
 </div>

R1 æœ‰ä½•ç‰¹åˆ«ä¹‹å¤„ï¼Ÿ

- æ€§èƒ½è¶…è¶Š OpenAI O1 
- å¼€æ”¾çš„ RL é…æ–¹ï¼ˆä¸”ç›¸å½“ç®€å•ï¼‰ 
    - ç»ˆç»“äº†å…³äº MCTS/PRM å¿…è¦æ€§çš„çŒœæµ‹ 
- SFT è§è§£ï¼ˆåŒ…æ‹¬ R1-zero å’Œ distil-r1ï¼‰

ä»–ä»¬æ²¿ç”¨æ¥è‡ª DeepSeekMath è¿™ç¯‡è®ºæ–‡é‡Œçš„ GRPO æˆæœã€‚

<div align="center">
   <img src="images/14-8-GRPOå’Œå…¶ä»–ç®—æ³•çš„å¯¹æ¯”.png" />
   <p>å›¾14.8 GRPOå’Œå…¶ä»–ç®—æ³•çš„å¯¹æ¯”</p>
</div>

è¯¥å›¾å±•ç¤ºäº†åœ¨ä¸¤ä¸ªæ•°æ®é›†â€”â€”GSM8Kï¼ˆå·¦å›¾ï¼‰å’Œ MATHï¼ˆå³å›¾ï¼‰â€”â€”ä¸Šï¼Œå››ç§ä¸åŒç®—æ³•ï¼ˆRFTã€Online RFTã€GRPO+OSã€GRPO+PSï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡†ç¡®ç‡ï¼ˆAcc %ï¼‰éšè®­ç»ƒæ­¥æ•°ï¼ˆStepsï¼‰çš„å˜åŒ–ã€‚

DeepSeek R1 çš„æˆåŠŸè¯æ˜äº†**çº¯å¼ºåŒ–å­¦ä¹ **åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚

#### R1-Zeroï¼šçº¯ç²¹çš„ RL
*   **è®¾ç½®**: ç›´æ¥åœ¨ Base æ¨¡å‹ï¼ˆDeepSeek-V3ï¼‰ä¸Šè¿è¡Œ GRPOã€‚
*   **å¥–åŠ±**:
    *   **å‡†ç¡®æ€§å¥–åŠ±**: ç­”æ¡ˆå¯¹ä¸å¯¹ï¼Ÿï¼ˆé€šè¿‡è§„åˆ™åŒ¹é…æˆ–ç¼–è¯‘å™¨éªŒè¯ï¼‰ã€‚
    *   **æ ¼å¼å¥–åŠ±**: å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨ `<think>` å’Œ `</think>` æ ‡ç­¾åŒ…è£¹æ€ç»´è¿‡ç¨‹ã€‚
    - æ•°æ®ï¼šæœªå…¬å¼€

<div align="center">
   <img src="images/14-9-Deepseek-R1-Zeroå’ŒOpenAI-o1åœ¨ç›¸å…³æ¨ç†åŸºå‡†ä¸Šçš„æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.9 Deepseek-R1-Zeroå’ŒOpenAI-o1åœ¨ç›¸å…³æ¨ç†åŸºå‡†ä¸Šçš„æ€§èƒ½å¯¹æ¯”</p>
</div>

åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒDeepSeek-R1 ä¸ o1-mini çš„è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ï¼Œå¹¶ä¸”åœ¨å‡ ä¸ªä»»åŠ¡ä¸Šä¸ o1-0912 çš„è¡¨ç°ç›¸å½“ã€‚ä½†åœ¨ä»£ç é¢†åŸŸ DeepSeek-R1 çš„è¡¨ç°ä¸å¦‚ o1 æ¨¡å‹ã€‚

**Deepseek-R1-Zero äº§ç”Ÿäº†æœ‰è¶£çš„ç°è±¡** **Aha Moment (é¡¿æ‚Ÿæ—¶åˆ»)**: æ¨¡å‹åœ¨è®­ç»ƒä¸­æœŸå¼€å§‹å­¦ä¼šè‡ªæˆ‘åæ€ï¼ˆSelf-correctionï¼‰ï¼Œä¾‹å¦‚â€œç­‰ç­‰ï¼Œæˆ‘ç®—é”™äº†ï¼Œåº”è¯¥é‡æ–°å°è¯•...â€ã€‚
    
<div align="center">
   <img src="images/14-10-DeepSeek-R1-Zeroåœ¨è®­ç»ƒæœŸé—´çš„AIMEå‡†ç¡®ç‡å’Œåœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡å“åº”é•¿åº¦.png" />
   <p>å›¾14.10 DeepSeek-R1-Zeroåœ¨è®­ç»ƒæœŸé—´çš„AIMEå‡†ç¡®ç‡å’Œåœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡å“åº”é•¿åº¦</p>
</div>

æ€ç»´æ—¶é—´çš„å¢åŠ ä¿ƒè¿›äº†å¤æ‚è¡Œä¸ºçš„è‡ªä¸»å‘å±•ã€‚ å…·ä½“è€Œè¨€ï¼ŒDeepSeek-R1-Zero è¶Šæ¥è¶Šå¤šåœ°å±•ç°å‡ºé«˜çº§æ¨ç†ç­–ç•¥ï¼Œä¾‹å¦‚åæ€æ€§æ¨ç†å’Œç³»ç»Ÿæ€§åœ°æ¢ç´¢æ›¿ä»£è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å…¶åœ¨æ•°å­¦å’Œç¼–ç ç­‰å¯éªŒè¯ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ 

<div align="center">
   <img src="images/14-11-aha-momentçš„å‘ç°.png" />
   <p>å›¾14.11 aha momentçš„å‘ç°</p>
</div>

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDeepSeek-R1-Zero è¡¨ç°å‡ºä¸€ä¸ªâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œå…¶ç‰¹å¾æ˜¯åœ¨åæ€è¿‡ç¨‹ä¸­ä½¿ç”¨â€œç­‰ç­‰â€ä¸€è¯çš„é¢‘ç‡çªç„¶å¢åŠ ã€‚è¿™ä¸€æ—¶åˆ»æ ‡å¿—ç€æ¨ç†æ¨¡å¼çš„æ˜¾è‘—å˜åŒ–ï¼Œå¹¶æ¸…æ™°åœ°å±•ç¤ºäº† DeepSeek-R1-Zero çš„è‡ªæˆ‘æ¼”åŒ–è¿‡ç¨‹ã€‚

##### ä½†ä¹Ÿè®¸æœ‰ç‚¹è¨€è¿‡å…¶å®ï¼Ÿ

GRPO ä½¿ç”¨çš„æ˜¯æœ‰åçš„ä¼˜åŒ–ç›®æ ‡ï¼Œå½“ä¼˜åŒ–ç›®æ ‡ï¼ˆæ— è®ºæ˜¯å¥–åŠ±æ¨¡å‹è¿˜æ˜¯ DPO çš„æŸå¤±å‡½æ•°ï¼‰æ— æ„ä¸­åå‘äºç‰¹å®šé•¿åº¦çš„è¾“å‡ºæ—¶ï¼Œæ¨¡å‹åœ¨è¿½æ±‚æœ€å¤§åŒ–è¯¥ç›®æ ‡çš„è¿‡ç¨‹ä¸­ï¼Œå°±ä¼šè¡¨ç°å‡ºâ€œé•¿åº¦åè§â€ã€‚

<div align="center">
   <img src="images/14-6-Dr-GRPOä¸æ ‡å‡†çš„GRPOçš„æ•°å­¦å…¬å¼ä¸æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.6 Dr.GRPOä¸æ ‡å‡†çš„GRPOçš„æ•°å­¦å…¬å¼ä¸æ€§èƒ½å¯¹æ¯”</p>
 </div>

åŸºç¡€æ¨¡å‹æ—©å·²è¡¨ç°å‡ºâ€œaha momentâ€ï¼š

<div align="center">
   <img src="images/14-12-DeepSeek-V3-Baseæ—©å·²å±•ç°å‡ºaha-momentç°è±¡æ¡ˆä¾‹.png" />
   <p>å›¾14.6 Dr.GRPOä¸æ ‡å‡†çš„GRPOçš„æ•°å­¦å…¬å¼ä¸æ€§èƒ½å¯¹æ¯”</p>
</div>

####  DeepSeek-R1

å°½ç®¡ DeepSeek-R1-Zero å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä¹Ÿé¢ä¸´ä¸€äº›é—®é¢˜ã€‚DeepSeek-R1-Zero åœ¨å¯è¯»æ€§å·®å’Œè¯­è¨€æ··åˆç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸º DeepSeek-V3-Base æ˜¯åœ¨å¤šç§è¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç‰¹åˆ«æ˜¯è‹±è¯­å’Œä¸­æ–‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼ŒDeepseek å›¢é˜Ÿå¼€å‘äº† DeepSeek-R1ï¼Œå…¶æµç¨‹å¦‚å›¾ 2 æ‰€ç¤ºã€‚

<div align="center">
   <img src="images/14-13-Deepseek-R1å¼€å‘æµç¨‹.png" />
   <p>å›¾14.13 Deepseek-R1å¼€å‘æµç¨‹</p>
</div>

##### é˜¶æ®µ 1ï¼šDeepSeek-R1-Zero

ä½¿ç”¨ DeepSeek-V3-Base ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œ å®Œå…¨ä¾èµ–äºå¼ºåŒ–å­¦ä¹ ï¼Œå¥–åŠ±ä¿¡å·ä¸»è¦æ¥è‡ªè§„åˆ™å‹å¥–åŠ±ï¼ˆRule-based Rewardï¼‰ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰å’Œæ ¼å¼ï¼ˆFormatï¼‰å¥–åŠ±ã€‚

##### é˜¶æ®µ 2ï¼šææ•°æ®

DeepSeek-V3-Base ä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨**å†·å¯åŠ¨é•¿æ€ç»´é“¾æ•°æ®**è¿›è¡Œ **SFT** è®­ç»ƒå¾—åˆ° **DeepSeek-R1-Dev1**ã€‚

å¯¹äºå†·å¯åŠ¨é•¿æ€ç»´é“¾æ•°æ®çš„æ”¶é›†ï¼Œå…·ä½“æ¥è¯´ï¼Œä»–ä»¬é¦–å…ˆæ”¶é›†äº†æ•°åƒä¸ªé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ¨ç†æç¤ºã€‚å¯¹äºæ¯ä¸ªæç¤ºï¼Œä½¿ç”¨ DeepSeek-R1-Zero ä»¥ 1.0 çš„ç›¸å¯¹è¾ƒé«˜ temperature ç”Ÿæˆå¤šä¸ªæ¨ç†è½¨è¿¹ã€‚æ¥ä¸‹æ¥ï¼Œè¿‡æ»¤è¿™äº›ç”Ÿæˆå†…å®¹ï¼Œåªä¿ç•™å…·æœ‰æ­£ç¡®æœ€ç»ˆç­”æ¡ˆå’Œå¯è¯»æ ¼å¼çš„ã€‚å¯¹äºæ•°å­¦è¾“å‡ºï¼Œæˆ‘ä»¬ä½¿ç”¨ sympy(https://www.sympy.org/) è¿›è¡Œè§£æå’Œè¡¨è¾¾å¼æ¯”è¾ƒï¼›å¯¹äºæ ¼å¼åŒ–ï¼Œæˆ‘ä»¬åº”ç”¨è¯¸å¦‚é‡å¤æ£€æµ‹å’Œè¯­è¨€æ··åˆè¿‡æ»¤ä¹‹ç±»çš„è§„åˆ™ã€‚æœ€åï¼Œæç¤º DeepSeek-V3 æ¥ç²¾ç‚¼æ¨ç†å’Œæ‘˜è¦ï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„æ ¼å¼å’Œäººç±»å‹å¥½çš„è¡¨è¾¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸ºäº†è§£å†³è¯­è¨€æ··åˆé—®é¢˜ï¼Œä»–ä»¬æŒ‡ç¤º DeepSeek-V3ï¼šâ€œTranslate the thinking process to the same language as the question.â€ã€‚ç”±äº DeepSeek-R1-Zero çš„æ‘˜è¦ä»…æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ Listing 1 ä¸­çš„æ‘˜è¦æç¤ºæ¥ç”Ÿæˆä¸€ä¸ªç®€æ´ã€æ˜“äºäººç±»é˜…è¯»çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥è§£å†³æ–¹æ¡ˆæ¦‚è¿°äº†æ¨ç†æ­¥éª¤å’Œæœ€ç»ˆç»“æœã€‚

<div align="center">
   <img src="images/14-14-äº§ç”Ÿäººç±»å¯è¯»å›ç­”çš„æç¤º.png" />
   <p>å›¾14.14 äº§ç”Ÿäººç±»å¯è¯»å›ç­”çš„æç¤º</p>
</div>

åœ¨ DeepSeek-R1-Dev1 çš„åŸºç¡€ä¸Šï¼Œå¥–åŠ±ä¿¡å·ä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆå‡†ç¡®æ€§+æ ¼å¼ï¼‰å’Œè¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¿›è¡Œ RL è®­ç»ƒå¾—åˆ° **DeepSeek-R1-Dev2**ã€‚

##### é˜¶æ®µ 3ï¼šåè®­ç»ƒ

å°† **DeepSeek-V3-Base** ä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨ **800k**ï¼ˆ600kæ¨ç†ç›¸å…³æ•°æ®+200kéæ¨ç†æ•°æ®ï¼‰ ç›‘ç£æ•°æ®è¿›è¡Œ **SFT** è®­ç»ƒå¾—åˆ° **DeepSeek-R1-Dev3**ã€‚

é€šè¿‡ä» DeepSeek-R1-Dev2 çš„æ£€æŸ¥ç‚¹è¿›è¡Œæ‹’ç»é‡‡æ ·æ¥ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œæ”¶é›†äº†å¤§çº¦ 600k ä¸ªä¸æ¨ç†ç›¸å…³çš„è®­ç»ƒæ ·æœ¬ã€‚

å¯¹äºéæ¨ç†æ•°æ®ï¼Œä¾‹å¦‚å†™ä½œã€äº‹å® QAã€è‡ªæˆ‘è®¤çŸ¥å’Œç¿»è¯‘ï¼Œé‡‡ç”¨ DeepSeek-V3 pipeline å¹¶é‡ç”¨ DeepSeek-V3 çš„ SFT æ•°æ®é›†çš„éƒ¨åˆ†å†…å®¹ã€‚è¿˜æ•´åˆäº†è½¯ä»¶å·¥ç¨‹ç›¸å…³æ•°æ®ï¼ŒåŒ…æ‹¬ç¨‹åºä¿®å¤å’Œå‰ç«¯ç½‘é¡µå¼€å‘ï¼Œä»¥å¢å¼ºæ¨¡å‹è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„èƒ½åŠ›ã€‚å¯¹äºæŸäº›éæ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨é€šè¿‡æç¤ºå›ç­”é—®é¢˜ä¹‹å‰ï¼Œä¼šè°ƒç”¨ DeepSeek-V3 æ¥ç”Ÿæˆæ½œåœ¨çš„ chain-of-thoughtã€‚ç„¶è€Œï¼Œå¯¹äºè¾ƒç®€å•çš„æŸ¥è¯¢ï¼Œä¾‹å¦‚â€œhelloâ€ï¼Œæˆ‘ä»¬ä¸ä¼šæä¾› CoT ä½œä¸ºå›åº”ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ”¶é›†äº†çº¦ 200k ä¸æ¨ç†æ— å…³çš„è®­ç»ƒæ ·æœ¬ã€‚

åœ¨ DeepSeek-R1-Dev3ä¸Šç»§ç»­è¿›è¡Œ RLï¼Œå¯¹äºæ¨ç†æ•°æ®ä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼›é€šç”¨æ•°æ®ç”±äºæ²¡æœ‰ä¸€ä¸ªæ˜ç¡®çš„å¯¹é”™è§„åˆ™å¥–åŠ±ä¿¡å·ï¼Œé‡‡ç”¨ Reward Models æ¥æ•æ‰å¤æ‚ä¸”ç»†è‡´åœºæ™¯ä¸­çš„äººç±»åå¥½ï¼Œä» helpful å’Œ safety ä¸¤ä¸ªè§’åº¦è®¡ç®—å¥–åŠ±ã€‚

##### DeepSeek-R1 æ•ˆæœå¦‚ä½•å‘¢ï¼Ÿ

<div align="center">
   <img src="images/14-15-DeepSeek-R1å’Œå…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ.png" />
   <p>å›¾14.15 DeepSeek-R1å’Œå…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ</p>
</div>

##### è’¸é¦ï¼šæˆ‘ä»¬å¯ä»¥å°†éæ¨ç†æ¨¡å‹è½¬æ¢ä¸ºæ¨ç†æ¨¡å‹å—ï¼Ÿ

R1 çš„å¦ä¸€ä¸ªå·¨å¤§è´¡çŒ®æ˜¯è¯æ˜äº†**å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯ä»¥è’¸é¦ç»™å°æ¨¡å‹**ã€‚ä½¿ç”¨ R1 ç”Ÿæˆçš„ 800k æ¡æ•°æ®å¾®è°ƒ Qwen2.5ï¼Œè®©å­¦ç”Ÿæ¨¡å‹ï¼ˆQwen2.5ï¼‰å­¦ä¼šæ•™å¸ˆæ¨¡å‹ï¼ˆR1ï¼‰çš„æ¨ç†èƒ½åŠ›ï¼

<div align="center">
   <img src="images/14-16-Deepseek-R1è’¸é¦æ¨¡å‹å’Œå…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ.png" />
   <p>å›¾14.16 Deepseek-R1è’¸é¦æ¨¡å‹å’Œå…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ</p>
</div>

##### ä½¿ç”¨å°‘é‡é«˜è´¨é‡ SFT æ ·æœ¬æå‡æ•°å­¦æ¨ç†èƒ½åŠ›

é™¤äº† Deepseek-R1 è¿™ç§èŒƒå¼å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¼ºå¤§çš„æ¨ç†æ¨¡å‹å¤–ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ Base+SFT ä¹Ÿå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ€§èƒ½ä¸é”™çš„æ¨ç†æ¨¡å‹ã€‚

<div align="center">
   <img src="images/14-17-s1ä½¿ç”¨1ké«˜è´¨é‡æ ·æœ¬æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›.png" />
   <p>å›¾14.17 s1ä½¿ç”¨1ké«˜è´¨é‡æ ·æœ¬æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›</p>
</div>

æé£é£å›¢é˜Ÿçš„æ–‡ç« [s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393)ä½¿ç”¨ 1k ä¸ªé«˜è´¨é‡ã€å¸¦æœ‰é•¿æ€ç»´é“¾çš„æ•°æ®ï¼Œåœ¨ Qwen2.5-32B-Instruct ä¸Šè¿›è¡Œ SFT ä»è€Œè·å¾—äº†å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

ä¸Šæµ·äº¤é€šå¤§å­¦çš„åˆ˜é¹é£å›¢é˜Ÿåœ¨[LIMO: Less is More for Reasoning](https://arxiv.org/pdf/2502.03387)ä¸­ä¹Ÿå¾—åˆ°äº†ç›¸ä¼¼çš„ç»“è®ºï¼Œä½¿ç”¨ 800 ä¸ªé«˜è´¨é‡ä¸ªé«˜è´¨é‡ã€å¸¦æœ‰é•¿æ€ç»´é“¾çš„æ•°æ®ï¼Œåœ¨ Qwen2.5-32B-Instruct ä¸Šè¿›è¡Œ SFT å¤§å¤§æé«˜äº†æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

<div align="center">
   <img src="images/14-18-limoä½¿ç”¨800é«˜è´¨é‡æ ·æœ¬æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›.png" />
   <p>å›¾14.18 s1ä½¿ç”¨1ké«˜è´¨é‡æ ·æœ¬æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›</p>
</div>

> ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡å°‘é‡æ ·æœ¬æ¥æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›å¯¹äºåŸºåº§æ¨¡å‹çš„èƒ½åŠ›è¦æ±‚æ¯”è¾ƒé«˜ï¼Œä¸Šè¿°ç»“æœåœ¨ Qwen2.5-32B-Instruct æ•ˆæœè¾ƒå¥½ï¼Œä½†æ˜¯åœ¨ Qwen2.5-7B-Instruct å’Œ Qwen2.5-3B-Instruct ä¸Šæ•ˆæœè¾ƒå·®ã€‚

##### ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ ·æœ¬è¿›è¡Œ RLæå‡æ•°å­¦æ¨ç†èƒ½åŠ›

é€šè¿‡ Base+RL è¿™ç§è·¯çº¿åŒæ ·å¯ä»¥è·å¾—æ¨ç†æ¨¡å‹ï¼Œé™¤äº† Deepseek-R1-Zeroå¤–ï¼Œ[LIMR](https://arxiv.org/abs/2502.11886)ï¼ˆQwen2.5-Math-7B+PPOï¼‰å’Œ[Less is More: Improving LLM Alignment via Preference Data Selection](https://arxiv.org/abs/2502.14560)ï¼ˆllama3-8B+DPOï¼‰ä¸¤ç¯‡å·¥ä½œä¹Ÿè¯æ˜äº†è¿™æ¡è·¯çº¿çš„å¯è¡Œæ€§ã€‚

<div align="center">
   <img src="images/14-19-limrä¸å…¶ä»–æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ.png" />
   <p>å›¾14.19 limrä¸å…¶ä»–æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ</p>
</div>

##### ä¸æˆåŠŸçš„å°è¯•

Deepseek-R1 å›¢é˜Ÿä¹Ÿåˆ†äº«äº†ä»–ä»¬åœ¨ DeepSeek-R1 å¼€å‘çš„æ—©æœŸé˜¶æ®µï¼Œåšçš„ä¸€äº›å¤±è´¥çš„å°è¯•ï¼š

**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess Reward Model, PRMï¼‰**ï¼šPRMè¯•å›¾é€šè¿‡å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¿›è¡Œè¯„ä¼°æ¥ rerankã€å¼•å¯¼æœç´¢æˆ–æ”¹è¿›æ€è·¯ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨è‹¥å¹²é—®é¢˜ã€‚
- éš¾ä»¥æ˜ç¡®ç•Œå®šç»†ç²’åº¦çš„ä¸­é—´æ­¥éª¤ã€‚å¾ˆéš¾ç»™å‡ºä¸€ä¸ªé€šç”¨ã€å¯è‡ªåŠ¨åŒ–è¯„ä¼°çš„â€œæ­£ç¡®ä¸­é—´æ­¥éª¤â€å®šä¹‰ï¼Œå¯¼è‡´å¯¹ä¸­é—´è¿‡ç¨‹çš„é€æ­¥æ³¨é‡Šå’Œè¯„ä¼°å›°éš¾ã€‚
- åˆ¤æ–­å½“å‰ä¸­é—´æ­¥éª¤æ˜¯å¦æ­£ç¡®çš„æŒ‘æˆ˜æ€§ã€‚è‡ªåŠ¨æ ‡æ³¨ä¸€ä¸ªä¸­é—´æ­¥éª¤çš„æ­£ç¡®æ€§å¾€å¾€ä¸å¯é ï¼Œäººå·¥æ ‡æ³¨è§„æ¨¡éš¾ä»¥æ‰©å±•ï¼Œéš¾ä»¥åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­è½åœ°ã€‚
- å¼•å…¥æ¨¡å‹åæ˜“äº§ç”Ÿå¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä¸”æˆæœ¬é«˜ã€‚ä¸€æ—¦å¼•å…¥åŸºäºæ¨¡å‹çš„å¥–åŠ±ï¼Œæ¨¡å‹å¯èƒ½æ‰¾åˆ°ä½œå¼Šè·¯å¾„æ¥æå‡å¥–åŠ±ï¼Œä»è€Œåç¦»çœŸå®ç›®æ ‡ï¼›å¦å¤–ï¼Œé‡æ–°è®­ç»ƒå¥–åŠ±æ¨¡å‹éœ€è¦é¢å¤–çš„ç®—åŠ›å’Œæ•°æ®ï¼Œå¢åŠ è®­ç»ƒç®¡çº¿çš„å¤æ‚åº¦å’Œæˆæœ¬ã€‚


 **è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰**ï¼šå— AlphaGo å’Œ AlphaZero çš„å¯å‘ï¼Œä»–ä»¬æ¢ç´¢äº†ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥å¢å¼ºæµ‹è¯•æ—¶è®¡ç®—çš„å¯æ‰©å±•æ€§ã€‚ è¿™ç§æ–¹æ³•æ¶‰åŠå°†ç­”æ¡ˆåˆ†è§£æˆæ›´å°çš„éƒ¨åˆ†ï¼Œä»¥å…è®¸æ¨¡å‹ç³»ç»Ÿåœ°æ¢ç´¢è§£ç©ºé—´ã€‚ ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæç¤ºæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¯¹åº”äºæœç´¢æ‰€éœ€çš„ç‰¹å®šæ¨ç†æ­¥éª¤ã€‚

- ä¸æœç´¢ç©ºé—´ç›¸å¯¹æ˜ç¡®çš„å›½é™…è±¡æ£‹ä¸åŒï¼Œtoken ç”Ÿæˆå‘ˆç°å‡ºæŒ‡æ•°çº§æ›´å¤§çš„æœç´¢ç©ºé—´ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾ç½®äº†æœ€å¤§æ‰©å±•é™åˆ¶ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚ 
- å…¶æ¬¡ï¼Œä»·å€¼æ¨¡å‹ç›´æ¥å½±å“ç”Ÿæˆè´¨é‡ï¼Œå› ä¸ºå®ƒæŒ‡å¯¼ç€æœç´¢è¿‡ç¨‹çš„æ¯ä¸€æ­¥ã€‚ è®­ç»ƒä¸€ä¸ªç»†ç²’åº¦çš„ä»·å€¼æ¨¡å‹æœ¬èº«å°±å¾ˆå›°éš¾ï¼Œè¿™ä½¿å¾—æ¨¡å‹éš¾ä»¥è¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚ è™½ç„¶ AlphaGo çš„æ ¸å¿ƒæˆåŠŸä¾èµ–äºè®­ç»ƒä¸€ä¸ªä»·å€¼æ¨¡å‹æ¥é€æ­¥æå‡å…¶æ€§èƒ½ï¼Œä½†ç”±äº token ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œè¿™ä¸€åŸç†åœ¨æˆ‘ä»¬å½“å‰çš„è®¾ç½®ä¸­éš¾ä»¥å¤åˆ¶ã€‚ 

æ€»ä¹‹ï¼Œè™½ç„¶ MCTS åœ¨ä¸é¢„è®­ç»ƒçš„ä»·å€¼æ¨¡å‹é…å¯¹æ—¶å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­æé«˜æ€§èƒ½ï¼Œä½†é€šè¿‡è‡ªæˆ‘æœç´¢è¿­ä»£åœ°æå‡æ¨¡å‹æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚


### 14.3.2 Kimi k1.5

#### é•¿æ€ç»´é“¾æ¨ç†ç­–ç•¥

<div align="center">
   <img src="images/14-20-Kimi-k1.5çš„é•¿æ€ç»´é“¾ç»“æœ.png" />
   <p>å›¾14.20 Kimi-k1.5çš„é•¿æ€ç»´é“¾ç»“æœ</p>
</div>

å…³é”®æ­¥éª¤ï¼š
- æ•°æ®æ„å»ºï¼ˆå›°éš¾åº¦è¿‡æ»¤ï¼‰
- Long-CoT SFT
- RLï¼ˆä½¿ç”¨å®ƒä»¬è‡ªå·±çš„ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼‰

##### æ•°æ®ç®¡ç†

RL æç¤ºé›†ï¼ˆPrompt Setï¼‰çš„è´¨é‡å’Œå¤šæ ·æ€§åœ¨ç¡®ä¿å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ä¸€ä¸ªç²¾å¿ƒæ„å»ºçš„æç¤ºé›†ä¸ä»…èƒ½å¼•å¯¼æ¨¡å‹è¿›è¡Œé²æ£’æ¨ç†ï¼Œè¿˜èƒ½å‡è½»å¥–åŠ±é»‘å®¢å’Œå¯¹è¡¨é¢æ¨¡å¼è¿‡æ‹Ÿåˆçš„é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œä¸‰ä¸ªå…³é”®å±æ€§å®šä¹‰äº†ä¸€ä¸ªé«˜è´¨é‡çš„ RL æç¤ºé›†ï¼š

- å¤šæ ·åŒ–è¦†ç›–ï¼šæç¤ºåº”æ¶µç›–å¹¿æ³›çš„å­¦ç§‘ï¼Œå¦‚ STEMã€ç¼–ç å’Œé€šç”¨æ¨ç†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§å¹¶ç¡®ä¿åœ¨ä¸åŒé¢†åŸŸçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚kimi å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªæ ‡ç­¾ç³»ç»Ÿï¼ŒæŒ‰é¢†åŸŸå’Œå­¦ç§‘å¯¹æç¤ºè¿›è¡Œåˆ†ç±»ï¼Œç¡®ä¿åœ¨ä¸åŒå­¦ç§‘é¢†åŸŸä¹‹é—´ä¿æŒå‡è¡¡çš„ä»£è¡¨æ€§
- å¹³è¡¡çš„éš¾åº¦ï¼šæç¤ºé›†åº”åŒ…å«æ˜“ã€ä¸­ã€éš¾ç­‰ä¸åŒéš¾åº¦é—®é¢˜çš„è‰¯å¥½åˆ†å¸ƒèŒƒå›´ï¼Œä»¥ä¿ƒè¿›æ¸è¿›å¼å­¦ä¹ å¹¶é˜²æ­¢å¯¹ç‰¹å®šå¤æ‚ç¨‹åº¦çš„è¿‡æ‹Ÿåˆã€‚é‡‡ç”¨ä¸€ç§åŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„èƒ½åŠ›æ¥é€‚åº”æ€§åœ°è¯„ä¼°æ¯ä¸ªæç¤ºçš„éš¾åº¦ã€‚é€šè¿‡åˆ©ç”¨è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥é¢„å…ˆè¿‡æ»¤æ‰å¤§å¤šæ•°éå¸¸ç®€å•çš„æ ·æœ¬ï¼Œå¹¶åœ¨ RL è®­ç»ƒæœŸé—´è½»æ¾æ¢ç´¢ä¸åŒçš„é‡‡æ ·ç­–ç•¥ã€‚
- ç²¾ç¡®çš„å¯è¯„ä¼°æ€§ï¼šæç¤ºåº”å…è®¸éªŒè¯è€…è¿›è¡Œå®¢è§‚å¯é çš„è¯„ä¼°ï¼Œç¡®ä¿æ¨¡å‹æ€§ã€‚ä¸ºäº†é¿å…æ½œåœ¨çš„å¥–åŠ±é»‘å®¢ï¼Œéœ€è¦ç¡®ä¿æ¯ä¸ªæç¤ºçš„æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆéƒ½å¯ä»¥è¢«å‡†ç¡®éªŒè¯ã€‚å®è¯è§‚å¯Ÿè¡¨æ˜ï¼Œä¸€äº›å¤æ‚çš„æ¨ç†é—®é¢˜å¯èƒ½å…·æœ‰ç›¸å¯¹ç®€å•ä¸”æ˜“äºçŒœæµ‹çš„ç­”æ¡ˆï¼Œè¿™ä¼šå¯¼è‡´é”™è¯¯çš„æ­£é¢éªŒè¯â€”â€”å³æ¨¡å‹é€šè¿‡é”™è¯¯çš„æ¨ç†è¿‡ç¨‹å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬æ’é™¤äº†å®¹æ˜“å‡ºç°æ­¤ç±»é”™è¯¯çš„é—®é¢˜ï¼Œä¾‹å¦‚é€‰æ‹©é¢˜ã€åˆ¤æ–­é¢˜å’Œè¯æ˜é¢˜ã€‚æ­¤å¤–ï¼Œå¯¹äºé€šç”¨é—®ç­”ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥è¯†åˆ«å’Œç§»é™¤æ˜“äºè¢«é»‘å®¢æ”»å‡»çš„æç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æç¤ºæ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½• CoT æ¨ç†æ­¥éª¤çš„æƒ…å†µä¸‹çŒœæµ‹æ½œåœ¨ç­”æ¡ˆã€‚å¦‚æœæ¨¡å‹åœ¨ N æ¬¡å°è¯•å†…é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œåˆ™è¯¥æç¤ºè¢«è®¤ä¸ºå¤ªå®¹æ˜“è¢«é»‘å®¢æ”»å‡»è€Œè¢«ç§»é™¤ã€‚ä»–ä»¬å‘ç°è®¾ç½® N = 8 å¯ä»¥ç§»é™¤å¤§å¤šæ•°æ˜“äºè¢«é»‘å®¢æ”»å‡»çš„æç¤ºã€‚

##### é•¿æ€ç»´é“¾ï¼ˆLong-CoTï¼‰SFT

åœ¨æ¨¡å‹è¿›å…¥å¤æ‚çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µä¹‹å‰ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹åˆæ­¥å­¦ä¼šå¹¶å†…åŒ–ä¸€å¥—é«˜è´¨é‡ã€ç±»ä¼¼äººç±»çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚è¿™å°±åƒç»™æ¨¡å‹è¿›è¡Œâ€œé¢„çƒ­â€ï¼Œç¡®ä¿å®ƒåœ¨åç»­çš„ RL è®­ç»ƒä¸­èƒ½æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨å¥–åŠ±ä¿¡å·ï¼Œç”Ÿæˆæœ‰ä»·å€¼çš„æ¨ç†è¿‡ç¨‹ã€‚

ä»ç²¾ç‚¼è¿‡çš„ RL æç¤ºé›†ï¼ˆRL prompt setï¼‰ä¸­é€‰å–é—®é¢˜ã€‚åˆ©ç”¨â€œæç¤ºå·¥ç¨‹â€ï¼ˆprompt engineeringï¼‰æŠ€æœ¯ï¼Œä¸ºè¿™äº›é—®é¢˜æ„å»ºå‡ºå°‘é‡ä½†é«˜è´¨é‡çš„â€œé•¿ CoT æ¨ç†è·¯å¾„â€ã€‚è¿™äº›è·¯å¾„åŒ…å«ç»è¿‡ç²¾ç¡®éªŒè¯çš„æ¨ç†æ­¥éª¤ï¼Œé€‚ç”¨äºæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚ç±»ä¼¼æ‹’ç»é‡‡æ ·ï¼ˆRSï¼‰ï¼Œä½†å…¶ä¾§é‡ç‚¹æ˜¯é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¥â€œå¼•å¯¼â€æ¨¡å‹ç”Ÿæˆé•¿ CoT æ¨ç†è·¯å¾„ï¼Œè€Œéç®€å•åœ°ä»å¤§é‡éšæœºç”Ÿæˆä¸­é€‰æ‹©æœ€ä½³ç»“æœã€‚é€šè¿‡ä»¥ä¸Šæ­¥éª¤æˆ‘ä»¬å°±æ„å»ºå¥½äº†ä¸€ä¸ªç”¨äº SFT çš„æ•°æ®é›†ã€‚

##### Kimi RL

æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æ¨¡å‹åœ¨å‚è€ƒç­”æ¡ˆä¸Šçš„æœŸæœ›å¥–åŠ±ï¼ŒåŒæ—¶ä¸è®©æ¨¡å‹åç¦»åŸå§‹è¡Œä¸ºå¤ªå¤šï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼š

$$
\max_{\theta} \mathbb{E}_{(x,y^*) \sim \mathcal{D}} \left[ \mathbb{E}_{(y,z) \sim \pi_\theta} \left[ r(x, y, y^*) \right] - \tau \text{KL}(\pi_\theta(x) || \pi_{\theta_i}(x)) \right]
$$

å€Ÿé‰´äº† DPO çš„æ— å¥–åŠ±åå¥½ä¼˜åŒ–æ€æƒ³ï¼Œä¸ç›´æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡æ¯”è¾ƒå½“å‰ç­–ç•¥ä¸å‚è€ƒç­–ç•¥çš„å·®å¼‚ï¼Œé—´æ¥åœ°å®šä¹‰ä¸€ä¸ªâ€œä¼ªå¥–åŠ±â€ï¼Œå†ç”¨å¹³æ–¹æŸå¤±å»é€¼è¿‘å®ƒã€‚

è¿™é‡Œå‡è®¾å­˜åœ¨ä¸€ä¸ªâ€œç†æƒ³ç­–ç•¥â€$\pi^*$ï¼ˆå¯ä»¥ç†è§£ä¸ºäººç±»åå¥½åˆ†å¸ƒæˆ–ä¸“å®¶ç­–ç•¥ï¼‰ï¼Œç„¶åé€šè¿‡ DPO çš„æ€è·¯ï¼ŒæŠŠå¥–åŠ±å‡½æ•° $r$ ä¸ç­–ç•¥æ¯”å€¼è”ç³»èµ·æ¥ã€‚å…·ä½“åœ°è¯´ï¼Œ**å¥–åŠ±å‡å»ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•° $\tau \log Z$ï¼Œç­‰äº $\tau$ å€çš„ç†æƒ³ç­–ç•¥ä¸å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¯”å€¼**ã€‚è¿™ä¸ªæ¨å¯¼åŸºäºâ€œéå‚æ•°å‡è®¾â€ï¼Œæ„æ€æ˜¯ä¸æ˜¾å¼å»ºæ¨¡å¥–åŠ±å‡½æ•°ï¼Œè€Œæ˜¯è®©å®ƒéšå«åœ°ç”±ç­–ç•¥å·®å¼‚å†³å®šï¼ˆç±»ä¼¼ DPO çš„æ ¸å¿ƒæ€æƒ³ï¼‰ã€‚æœ€ç»ˆç›®çš„æ˜¯ä¸ºäº†â€œè§£å‡º $r$â€ï¼Œå³æŠŠå¥–åŠ±å‡½æ•°è¡¨è¾¾æˆç­–ç•¥çš„å‡½æ•°ã€‚

$
r(x, y, y^*) - \tau \log Z = \tau \log \frac{\pi^*(y, z|x)}{\pi_{\theta_i}(y, z|x)}
$

å› ä¸ºç›´æ¥ä¼˜åŒ–åŸå§‹ç›®æ ‡å¯èƒ½å›°éš¾ï¼Œè¿™é‡Œç”¨äº†ä¸€ä¸ª**å¹³æ–¹è¯¯å·®æŸå¤±**æ¥è¿‘ä¼¼ä¼˜åŒ–ã€‚å®ƒçš„ç›®æ ‡æ˜¯è®©å½“å‰ç­–ç•¥ $\pi_\theta$ çš„è¾“å‡ºï¼Œå°½å¯èƒ½åŒ¹é…â€œç†æƒ³ç­–ç•¥ $\pi^*$â€æ‰€å¯¹åº”çš„å¥–åŠ±è¡¨è¾¾å¼ã€‚æ³¨æ„è¿™é‡Œé‡‡æ ·æ˜¯ä»**å‚è€ƒç­–ç•¥ $\pi_{\theta_i}$** ä¸­è¿›è¡Œçš„ï¼Œè€Œä¸æ˜¯å½“å‰ç­–ç•¥ $\pi_\theta$ â€”â€” è¿™æ˜¯ä¸ºäº†ç¨³å®šè®­ç»ƒï¼Œé¿å…è‡ªä¸¾ï¼ˆbootstrappingï¼‰å¸¦æ¥çš„åå·®ã€‚æœ€ç»ˆæŸå¤± $L(\theta)$ æ˜¯å¯¹æ‰€æœ‰æ ·æœ¬å’Œé‡‡æ ·ç»“æœå–æœŸæœ›åçš„å¹³æ–¹è¯¯å·®ã€‚

$
L(\theta) = \mathbb{E}_{(x,y^*) \sim \mathcal{D}} \left[ \mathbb{E}_{(y,z) \sim \pi_{\theta_i}} \left[ \left( r(x, y, y^*) - \tau \log Z - \tau \log \frac{\pi_\theta(y, z|x)}{\pi_{\theta_i}(y, z|x)} \right)^2 \right] \right]
$

æœ€ç»ˆç”¨äºæ›´æ–°æ¨¡å‹å‚æ•° Î¸ çš„å¸¦æ­£åˆ™åŒ–çš„åŸºçº¿ç­–ç•¥æ¢¯åº¦ï¼š

$$
\frac{1}{k} \sum_{j=1}^{k} \left( \nabla_\theta \log \pi_\theta(y_j, z_j | x) \left( r(x, y_j, y^*) - \bar{r} \right) - \frac{\tau}{2} \nabla_\theta \left( \log \frac{\pi_\theta(y_j, z_j | x)}{\pi_{\theta_i}(y_j, z_j | x)} \right)^2 \right)
$$


å¯¹æ¯ä¸ªé‡‡æ ·å¾—åˆ°çš„ $(y_j, z_j)$ï¼Œè®¡ç®—å…¶æ¢¯åº¦è´¡çŒ®ã€‚æ¢¯åº¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š1. å¥–åŠ±é©±åŠ¨çš„ç­–ç•¥æ”¹è¿›ï¼›2. æ­£åˆ™åŒ–é©±åŠ¨çš„è¡Œä¸ºçº¦æŸã€‚æœ€åå–å¹³å‡ï¼ˆ$\frac{1}{k} \sum$ï¼‰ï¼Œå¾—åˆ°æœ€ç»ˆæ›´æ–°æ–¹å‘ã€‚

##### é•¿åº¦æ§åˆ¶

Kimi å›¢é˜Ÿè§‚å¯Ÿåˆ°ä¸€ä¸ªâ€œè¿‡åº¦æ€è€ƒâ€ç°è±¡ï¼Œå³æ¨¡å‹å“åº”çš„é•¿åº¦åœ¨ RL è®­ç»ƒæœŸé—´æ˜¾è‘—å¢åŠ ã€‚
è™½ç„¶è¿™ä¼šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼Œä½†è¿‡é•¿çš„æ¨ç†è¿‡ç¨‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶æˆæœ¬å¾ˆé«˜ï¼Œè€Œä¸”è¿‡åº¦æ€è€ƒé€šå¸¸ä¸è¢«äººç±»æ‰€åå¥½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬å¼•å…¥äº†ä¸€ä¸ªé•¿åº¦å¥–åŠ±æ¥æŠ‘åˆ¶ token é•¿åº¦çš„å¿«é€Ÿå¢é•¿ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ token æ•ˆç‡ã€‚

\[
\text{len\_reward}(i) = \begin{cases}
    \lambda & \text{If } r(x, y_i, y^*) = 1 \\
    \min(0, \lambda) & \text{If } r(x, y_i, y^*) = 0
\end{cases}\text{, where } \lambda = 0.5 - \frac{\text{len}(i) - \text{min\_len}}{\text{max\_len} - \text{min\_len}}.
\]

è¯¥é•¿åº¦æƒ©ç½šæœºåˆ¶é¼“åŠ±æ¨¡å‹åœ¨ç»™å‡ºæ­£ç¡®ç­”æ¡ˆçš„åŒæ—¶ï¼Œå°½é‡ç”Ÿæˆç®€æ´çš„å“åº”ã€‚å¯¹äºé”™è¯¯çš„ç­”æ¡ˆï¼Œå®ƒç»ä¸ä¼šç»™äºˆä»»ä½•æ­£å‘çš„é•¿åº¦å¥–åŠ±ï¼Œå¹¶ä¸”ä¼šå¯¹è¿‡é•¿çš„é”™è¯¯ç­”æ¡ˆæ–½åŠ é¢å¤–çš„æƒ©ç½šã€‚


#### é¢å¤–ç»†èŠ‚

é‡‡æ ·ç­–ç•¥ï¼š
- ä¸ºæ•°æ®é›†åˆ†é…éš¾åº¦æ ‡ç­¾ï¼Œä»æ˜“åˆ°éš¾
- é—®é¢˜çš„é‡‡æ ·æ¯”ä¾‹ä¸(1-success_rate)æˆæ­£æ¯”ï¼Œä»¥é¿å…é‡å¤å·²è§£å†³çš„é—®é¢˜ 

å¥–åŠ±ï¼š
- å¯¹äºä»£ç â€”â€”é‡‡ç”¨å…·æœ‰ ground truth è§£çš„é—®é¢˜ï¼Œç”Ÿæˆæ–°çš„æµ‹è¯•ç”¨ä¾‹ 
- å¯¹äºæ•°å­¦â€”â€”ä½¿ç”¨800kä¸ªæ ·æœ¬æ¥è®­ç»ƒä¸€ä¸ªCoTå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºç­”æ¡ˆç­‰ä»·æ€§æ£€æŸ¥ 

#### Scaling ç»“æœ

Kimi-k1.5 åœ¨æ€§èƒ½ä¸Šä¸â€œo1â€å¤§è‡´ç›¸å½“ï¼Œç”šè‡³å¯èƒ½æ›´ä¼˜:

<div align="center">
   <img src="images/14-21-Kimi-k1.5ä¸å…¶ä»–çš„æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.21 Kimi-k1.5çš„é•¿æ€ç»´é“¾ç»“æœ</p>
</div>

å…¶ä»–æœ‰è¶£çš„ç»“æœï¼š

<div align="center">
   <img src="images/14-22-Kimi-k1.5çš„é•¿æ€ç»´é“¾ç»“æœ.png" />
   <p>å›¾14.22 Kimi-k1.5çš„é•¿æ€ç»´é“¾ç»“æœ</p>
</div>

#### æ¶ˆèå®éªŒ

<div align="center">
   <img src="images/14-23-å’ŒReSTç”¨äºç­–ç•¥æ¢¯åº¦ä¼˜åŒ–çš„æ¯”è¾ƒ.png" />
   <p>å›¾14.22 å’ŒReSTç”¨äºç­–ç•¥æ¢¯åº¦ä¼˜åŒ–çš„æ¯”è¾ƒ</p>
</div>

> æ³¨æ„ï¼Œä¸Šè¿°åˆ†æ•°æ¥è‡ªä¸€ä¸ªå†…éƒ¨çš„ long-cot æ¨¡å‹ï¼Œå…¶æ¨¡å‹å°ºå¯¸è¿œå°äº k1.5 long-CoT æ¨¡å‹


### 14.3.3 Qwen 3ï¼šæ€ç»´æ¨¡å¼èåˆ

Qwen3 å®¶æ—æœ€å¤§å‹å·çš„æ¨¡å‹ Qwen3-235B-A22B æ€§èƒ½è¶…è¿‡äº† OpenAI-o1 å’Œ Deepseek-R1ï¼Œå“ªæ€•æ˜¯ Qwen3-32B ä¹Ÿä¸ o1 æ€§èƒ½ç›¸å½“ã€‚

<div align="center">
   <img src="images/14-24-Qwen3å’Œå…¶ä»–æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ.png" />
   <p>å›¾14.24 Qwen3å’Œå…¶ä»–æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒ</p>
</div>

Qwen3 çš„åè®­ç»ƒæµç¨‹ç²¾å¿ƒè®¾è®¡äº†ä¸¤ä¸ªæ ¸å¿ƒç›®æ ‡ï¼š

<div align="center">
   <img src="images/14-25-Qwen3ç³»åˆ—æ¨¡å‹çš„åè®­ç»ƒç®¡é“.png" />
   <p>å›¾14.25 Qwen3ç³»åˆ—æ¨¡å‹çš„åè®­ç»ƒç®¡é“</p>
</div>

- **æ€è€ƒæ§åˆ¶**ï¼šè¿™æ¶‰åŠä¸¤ç§ä¸åŒæ¨¡å¼çš„é›†æˆï¼Œå³â€œéæ€è€ƒâ€æ¨¡å¼å’Œâ€œæ€è€ƒâ€æ¨¡å¼ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿçµæ´»é€‰æ‹©æ¨¡å‹æ˜¯å¦è¿›è¡Œæ¨ç†ï¼Œå¹¶é€šè¿‡æŒ‡å®šæ€è€ƒè¿‡ç¨‹çš„ token é¢„ç®—æ¥æ§åˆ¶æ€è€ƒçš„æ·±åº¦
- **å¼ºåˆ°å¼±è’¸é¦**ï¼šè¿™æ—¨åœ¨ç®€åŒ–å’Œä¼˜åŒ–è½»é‡çº§æ¨¡å‹çš„è®­ç»ƒåæµç¨‹ã€‚ é€šè¿‡åˆ©ç”¨å¤§å‹æ¨¡å‹çš„çŸ¥è¯†ï¼Œå¤§å¤§é™ä½äº†æ„å»ºå°å‹æ¨¡å‹æ‰€éœ€çš„è®¡ç®—æˆæœ¬å’Œå¼€å‘å·¥ä½œé‡ã€‚

#### SFT + æ¨ç†å¼ºåŒ–å­¦ä¹  

æˆ‘ä»¬ç°åœ¨éƒ½çŸ¥é“è¿™ä¸ªå¥—è·¯äº†ï¼ŒQwen ä¹Ÿç”¨äº†å¾ˆå¤šã€‚ 

- æŒ‰éš¾åº¦è¿‡æ»¤ï¼ˆé€šè¿‡ best-of-nï¼Œä¾‹å¦‚ kimiï¼‰ 
    - ç§»é™¤æ¨¡å‹åœ¨æ²¡æœ‰ CoT çš„æƒ…å†µä¸‹å°±èƒ½æ­£ç¡®å›ç­”çš„é—®é¢˜ 
    - ç§»é™¤ä¸éªŒè¯æ•°æ®è¿‡äºç›¸ä¼¼çš„å†…å®¹ 
- æ‰‹åŠ¨è¿‡æ»¤ CoT çš„è´¨é‡ï¼ˆçŒœæµ‹ vs æ­£ç¡®å›ç­”ï¼‰ 
- ä½¿ç”¨ GRPO åœ¨ä»… 3995 ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ 

#### Qwen 3 ç‰¹æœ‰çš„æ–°å†…å®¹

æ€è€ƒæ¨¡å¼èåˆâ€”â€”æ§åˆ¶ CoT çš„é•¿åº¦ã€‚

1.æ··åˆå¸¦æ ‡ç­¾çš„éæ€è€ƒå’Œæ€è€ƒæ•°æ®

<div align="center">
   <img src="images/14-26-æ€è€ƒæ¨¡å¼èåˆé˜¶æ®µçš„SFTæ•°æ®ç¤ºä¾‹.png" />
   <p>å›¾14.26 æ€è€ƒæ¨¡å¼èåˆé˜¶æ®µçš„SFTæ•°æ®ç¤ºä¾‹</p>
</div>

2. é€šè¿‡ç‰¹æ®Šå­—ç¬¦ä¸²çš„æ—©åœ

æ€è€ƒæ¨¡å¼èåˆçš„ä¸€ä¸ªé¢å¤–ä¼˜åŠ¿æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹å­¦ä¼šä»¥éæ€è€ƒå’Œæ€è€ƒæ¨¡å¼è¿›è¡Œå“åº”ï¼Œå®ƒè‡ªç„¶ä¼šå‘å±•å‡ºå¤„ç†ä¸­é—´æƒ…å†µçš„èƒ½åŠ›â€”â€”æ ¹æ®ä¸å®Œæ•´çš„æ€è€ƒç”Ÿæˆå“åº”ã€‚ è¿™ç§èƒ½åŠ›ä¸ºå®ç°æ¨¡å‹æ€è€ƒè¿‡ç¨‹çš„é¢„ç®—æ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚ å…·ä½“æ¥è¯´ï¼Œå½“æ¨¡å‹çš„æ€è€ƒé•¿åº¦è¾¾åˆ°ç”¨æˆ·å®šä¹‰çš„é˜ˆå€¼æ—¶ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åœæ­¢æ€è€ƒè¿‡ç¨‹å¹¶æ’å…¥åœæ­¢æ€è€ƒæŒ‡ä»¤ï¼šâ€œè€ƒè™‘åˆ°ç”¨æˆ·çš„æ—¶é—´æœ‰é™ï¼Œæˆ‘å¿…é¡»ç›´æ¥æ ¹æ®æ€è€ƒç»™å‡ºè§£å†³æ–¹æ¡ˆã€‚\n</think>.\n\nâ€ã€‚ æ’å…¥æ­¤æŒ‡ä»¤åï¼Œæ¨¡å‹å°†æ ¹æ®å…¶è¿„ä»Šä¸ºæ­¢ç§¯ç´¯çš„æ¨ç†ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§èƒ½åŠ›å¹¶éæ˜¾å¼è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡åº”ç”¨æ€è€ƒæ¨¡å¼èåˆè‡ªç„¶äº§ç”Ÿçš„ã€‚

#### æµ‹è¯•æ—¶é—´æ‰©å±•ï¼ˆTest time scalingï¼ŒTTSï¼‰

è¿™å¼ å›¾è¡¨å±•ç¤ºäº†åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ï¼ˆAIME'24, AIME'25, LiveCodeBench (v5), GPQA Diamondï¼‰ä¸Šï¼Œä¸¤ç§æ¨¡å¼ï¼ˆâ€œæ€è€ƒæ¨¡å¼â€å’Œâ€œéæ€è€ƒæ¨¡å¼â€ï¼‰ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼ˆPass@1ï¼‰å¦‚ä½•éšâ€œæ€è€ƒé¢„ç®—â€ï¼ˆThinking Budgetï¼Œä»¥K tokensä¸ºå•ä½ï¼‰çš„å˜åŒ–è€Œå˜åŒ–ã€‚

<div align="center">
   <img src="images/14-27-Qwen3-235B-A22Béšthinking budgetçš„æ€§èƒ½è¡¨ç°.png" />
   <p>å›¾14.27 Qwen3-235B-A22Béšthinking budgetçš„æ€§èƒ½è¡¨ç°</p>
</div>

#### ä¸åŒé˜¶æ®µçš„ç»„æˆ 

ä¸‹å›¾å±•ç¤ºäº† Qwen3-32B æ¨¡å‹åœ¨ä¸åŒåè®­ç»ƒé˜¶æ®µï¼ˆPost-trainingï¼‰çš„æ€§èƒ½å˜åŒ–ï¼š

<div align="center">
   <img src="images/14-28-Qwen3-32Båœ¨ä¸åŒé˜¶æ®µçš„æ€§èƒ½.png" />
   <p>å›¾14.27 Qwen3-32Båœ¨ä¸åŒé˜¶æ®µçš„æ€§èƒ½</p>
</div>

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»¥é€šç”¨ä¸ºç›®çš„çš„ RLHF å¯¹æ•°å­¦/STEM èƒ½åŠ›ä¼šç•¥æœ‰ä¸‹é™ã€‚


Qwen 3 æå‡ºäº† **Thinking Mode Fusion**ï¼Œè¯•å›¾åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­èåˆâ€œæ€è€ƒâ€ä¸â€œä¸æ€è€ƒâ€ä¸¤ç§æ¨¡å¼ï¼š
*   **è®­ç»ƒ**: æ··åˆä½¿ç”¨å¸¦ `<think>` çš„æ•°æ®å’Œç›´æ¥è¾“å‡ºç­”æ¡ˆçš„æ•°æ®ã€‚
*   **æ•ˆæœ**: ç”¨æˆ·å¯ä»¥é€šè¿‡ Prompt æ§åˆ¶æ¨¡å‹æ˜¯å¦è¿›è¡Œé•¿æ¨ç†ã€‚
*   **æµ‹è¯•æ—¶è®¡ç®— (Test-time Compute)**: å¯ä»¥åœ¨æ¨ç†é˜¶æ®µé€šè¿‡æˆªæ–­ `<think>` è¿‡ç¨‹æ¥åŠ¨æ€è°ƒæ•´è®¡ç®—é‡å’Œæ€§èƒ½çš„å¹³è¡¡ã€‚

