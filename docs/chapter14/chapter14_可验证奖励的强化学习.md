# ç¬¬åå››ç« ï¼šå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RLVR)

åœ¨ä¹‹å‰çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº† RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚è™½ç„¶ RLHF æ˜¯ä½¿æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„å…³é”®ï¼Œä½†å®ƒé¢ä¸´ç€å·¨å¤§çš„æ‰©å±•æ€§æŒ‘æˆ˜ï¼šäººç±»åé¦ˆæ˜‚è´µã€ç¼“æ…¢ä¸”å®¹æ˜“è¢«â€œè¿‡åº¦ä¼˜åŒ–â€ï¼ˆGoodhart's Lawï¼‰ã€‚

æœ¬ç« æˆ‘ä»¬å°†ç›®å…‰è½¬å‘ **o1** å’Œ **DeepSeek R1** ç­‰æ¨ç†æ¨¡å‹èƒŒåçš„æ ¸å¿ƒæŠ€æœ¯â€”â€”**RLVR (Reinforcement Learning from Verifiable Rewards)**ã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**
1.  **ç®—æ³•æ¼”è¿›**ï¼šç†è§£ä» PPO åˆ° GRPO çš„æ¼”å˜é€»è¾‘ï¼Œä»¥åŠä¸ºä»€ä¹ˆ GRPO æ›´é€‚åˆå¤§æ¨¡å‹æ¨ç†è®­ç»ƒã€‚
2.  **å·¥ç¨‹å®ç°**ï¼šæ·±å…¥ PPO å’Œ GRPO çš„ä»£ç å®ç°ç»†èŠ‚ï¼ŒæŒæ¡ Advantage è®¡ç®—ä¸ Loss è®¾è®¡ã€‚
3.  **å‰æ²¿æ¡ˆä¾‹**ï¼šè§£æ„ DeepSeek R1ã€Kimi k1.5 å’Œ Qwen 3 çš„è®­ç»ƒæµæ°´çº¿ï¼Œç†è§£â€œå†·å¯åŠ¨æ•°æ®â€ã€â€œæ€ç»´é“¾ï¼ˆCoTï¼‰â€ä¸â€œé•¿åº¦æ§åˆ¶â€çš„å…³é”®ä½œç”¨ã€‚

---

## 14.1 ä¸ºä»€ä¹ˆéœ€è¦ RLVRï¼Ÿ

åœ¨ AlphaGo æˆ– AlphaFold ç­‰é¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå› ä¸ºå®ƒä»¬æ‹¥æœ‰**å®Œç¾çš„æ¨¡æ‹Ÿå™¨**å’Œ**æ˜ç¡®çš„å¥–åŠ±å‡½æ•°**ï¼ˆèµ¢/è¾“ï¼Œè›‹ç™½è´¨æŠ˜å èƒ½çº§ï¼‰ã€‚

åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¦‚æœæˆ‘ä»¬èƒ½æ‰¾åˆ°ç±»ä¼¼çš„é¢†åŸŸâ€”â€”**ç­”æ¡ˆå®¢è§‚ã€å¯éªŒè¯**ï¼ˆå¦‚æ•°å­¦é¢˜ã€ä»£ç ç”Ÿæˆï¼‰ï¼Œæˆ‘ä»¬å°±èƒ½åˆ©ç”¨å¤§è§„æ¨¡çš„è®¡ç®—èµ„æºæ¥æ›¿ä»£æ˜‚è´µçš„äººç±»æ ‡æ³¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ã€‚è¿™å°±æ˜¯ RLVR çš„æ ¸å¿ƒæ„¿æ™¯ã€‚

### 14.1.1 RLHF çš„å›°å¢ƒ

ä¼ ç»Ÿçš„ RLHF ä¾èµ–äººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„æˆå¯¹åå¥½åˆ¤æ–­ï¼ˆå¦‚ â€œA æ¯” B å¥½â€ï¼‰ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸‰å¤§æ ¹æœ¬æ€§é—®é¢˜ï¼š

- å¥–åŠ±å™ªå£°é«˜ï¼šäººç±»åˆ¤æ–­ä¸»è§‚ã€ä¸ä¸€è‡´ï¼Œä¸”æ˜“è¢«è¡¨é¢ä¿®è¾è¿·æƒ‘ï¼›
- éš¾ä»¥è§„æ¨¡åŒ–ï¼šé«˜è´¨é‡åå¥½æ•°æ®æ ‡æ³¨æˆæœ¬æé«˜ï¼Œæ— æ³•æ”¯æ’‘ä¸‡äº¿ token çº§è®­ç»ƒï¼›
- è¿‡ä¼˜åŒ–ï¼ˆOver-optimizationï¼‰ï¼šæ¨¡å‹å­¦ä¼šâ€œè®¨å¥½â€å¥–åŠ±æ¨¡å‹ï¼Œç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å†…å®¹ç©ºæ´ã€å†—é•¿ç”šè‡³å¹»è§‰çš„è¾“å‡ºã€‚

> RLHF ä¼˜åŒ–çš„æ˜¯ä»£ç†ç›®æ ‡ï¼ˆäººç±»åå¥½ï¼‰ï¼Œè€ŒéçœŸå®ç›®æ ‡ï¼ˆä»»åŠ¡æ­£ç¡®æ€§ï¼‰ã€‚


### 14.1.2 æˆåŠŸæ¡ˆä¾‹çš„å¯ç¤º

å›é¡¾ AlphaGoã€AlphaFold ç­‰ RL æˆåŠŸæ¡ˆä¾‹ï¼Œå…¶å…±åŒç‚¹æ˜¯ï¼š**å¥–åŠ±å‡½æ•°æ˜¯æ˜ç¡®ã€å¯éªŒè¯ã€å¯è‡ªåŠ¨è®¡ç®—çš„**ã€‚ä¾‹å¦‚ï¼š
- å›´æ£‹ï¼šæœ€ç»ˆæ˜¯å¦è·èƒœï¼ˆ0/1ï¼‰ï¼›
- è›‹ç™½è´¨æŠ˜å ï¼šé¢„æµ‹è›‹ç™½è´¨ç»“æ„ä¸çœŸå®ç»“æ„çš„ RMSDï¼ˆRoot Mean Square Deviationï¼Œå‡æ–¹æ ¹åå·®ï¼‰ è·ç¦»ã€‚

è¿™ç±»ä»»åŠ¡ä¸­ï¼ŒRL ç®—æ³•å¯ç›´æ¥ä¼˜åŒ–**çœŸå®ç›®æ ‡**ï¼Œæ— éœ€äººç±»ä¸­ä»‹ã€‚è¿™å¯å‘æˆ‘ä»¬ï¼š**èƒ½å¦å°† RL å¼•å…¥è¯­è¨€æ¨¡å‹çš„â€œå¯éªŒè¯ä»»åŠ¡â€ä¸­ï¼Ÿ**

### 14.1.3 RLVR çš„å®šä½

RLVR èšç„¦äºä¸€ç±»ç‰¹æ®Šä»»åŠ¡ï¼š**è¾“å‡ºå¯è¢«ç¨‹åºè‡ªåŠ¨åˆ¤åˆ†**ã€‚å…¸å‹åœºæ™¯åŒ…æ‹¬ï¼š
- **æ•°å­¦æ¨ç†**ï¼šç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†è§£ä¸€è‡´ï¼ˆå¦‚ GSM8Kã€MATHï¼‰ï¼›
- **ä»£ç ç”Ÿæˆ**ï¼šç”Ÿæˆçš„ç¨‹åºæ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼›
- **å½¢å¼åŒ–è¯æ˜**ï¼šè¯æ˜æ­¥éª¤æ˜¯å¦é€»è¾‘è‡ªæ´½ã€‚

åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå¥–åŠ±å‡½æ•° $R(z)$ å¯å®šä¹‰ä¸ºï¼š

$$
R(z) = 
\begin{cases}
1 & \text{è‹¥ } z \text{ æ­£ç¡®} \\
0 & \text{å¦åˆ™}
\end{cases}
$$

æˆ–æ›´ç²¾ç»†çš„**è¿‡ç¨‹å¥–åŠ±**ï¼ˆå¦‚æ¯æ­¥æ¨ç†å¾—åˆ†ï¼‰ã€‚è¿™ç§**é«˜ä¿¡å™ªæ¯”ã€å¯è§„æ¨¡åŒ–**çš„å¥–åŠ±ï¼Œæ­£æ˜¯ RL å¤§å±•èº«æ‰‹çš„èˆå°ã€‚

> âœ… **RLVR çš„æœ¬è´¨**ï¼šåœ¨é‚£äº›â€œå¯¹é”™å¯è¢«è‡ªåŠ¨åˆ¤å®šâ€çš„çª„åŸŸä»»åŠ¡ä¸­ï¼Œç»•è¿‡äººç±»åå¥½ï¼Œç›´æ¥ç”¨å½¢å¼åŒ–éªŒè¯æœºåˆ¶æä¾›å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°æ›´å¯é ã€å¯æ‰©å±•ã€å¯éªŒè¯çš„æ™ºèƒ½ä½“è®­ç»ƒ

ä¸‹é¢æ˜¯å¯¹ RLHF å’Œ RLVR çš„ç®€å•å¯¹æ¯”ï¼š

| ç»´åº¦ | RLHF | RLVR |
|------|------|------|
| å¥–åŠ±æ¥æº | äººç±»åå¥½ï¼ˆå¦‚ rankingï¼‰ | è‡ªåŠ¨éªŒè¯ï¼ˆå¦‚æµ‹è¯•ã€è¯æ˜ã€è§„åˆ™ï¼‰ |
| ä»»åŠ¡é¢†åŸŸ | é€šç”¨ã€å¼€æ”¾åŸŸï¼ˆå¦‚èŠå¤©ï¼‰ | çª„åŸŸã€ç»“æ„åŒ–ï¼ˆå¦‚ç¼–ç¨‹ã€æ•°å­¦ï¼‰ |
| å¥–åŠ±è´¨é‡ | ä¸»è§‚ã€æœ‰å™ªå£°ã€æˆæœ¬é«˜ | å®¢è§‚ã€ç²¾ç¡®ã€å¯æ‰©å±• |
| å¯¹é½ç›®æ ‡ | â€œè®©äººè§‰å¾—å¥½â€ | â€œåœ¨å½¢å¼æ„ä¹‰ä¸Šæ­£ç¡®â€ |


---

## 14.2 ç®—æ³•æ¼”è¿›ï¼šä» PPO åˆ° GRPO

è¦ç†è§£ç°åœ¨çš„ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹èƒŒåçš„ GRPO ç®—æ³•ï¼Œæˆ‘ä»¬å¿…é¡»å…ˆå›é¡¾å®ƒçš„å‰èº« PPOï¼Œå¹¶æ˜ç™½ä¸ºä»€ä¹ˆè¦æŠ›å¼ƒå®ƒã€‚

### 14.2.1 PPO

#### å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥ä¼˜åŒ–æ–¹æ³•çš„å‘å±•è„‰ç»œ

ä»åŸå§‹çš„ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰ â†’ åˆ°æ›´ç¨³å®šçš„TRPOï¼ˆTrust Region Policy Optimizationï¼‰ â†’ å†åˆ°æ›´å®ç”¨çš„PPOï¼ˆProximal Policy Optimizationï¼‰

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª**ç­–ç•¥**ï¼ˆpolicyï¼‰$\pi_\theta(a|s)$ï¼Œå®ƒç”¨å‚æ•° $\theta$ æ§åˆ¶æ™ºèƒ½ä½“å¦‚ä½•æ ¹æ®çŠ¶æ€ $s$ é€‰æ‹©åŠ¨ä½œ $a$ã€‚  
ç›®æ ‡æ˜¯ï¼š**æœ€å¤§åŒ–æœŸæœ›å›æŠ¥**ï¼ˆexpected returnï¼‰ï¼š

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
$$

å…¶ä¸­ $\tau = (s_1, a_1, s_2, a_2, ..., s_T)$ æ˜¯ä¸€æ¡è½¨è¿¹ï¼ˆtrajectoryï¼‰ï¼Œ $R(\tau)$ æ˜¯æ€»å¥–åŠ±ã€‚

æˆ‘ä»¬éœ€è¦è®¡ç®— $\nabla_\theta J(\theta)$ æ¥ç”¨æ¢¯åº¦ä¸Šå‡æ›´æ–° $\theta$ã€‚


ğŸ”¹ å°è¯• 1: ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰

åˆ©ç”¨**ä¼¼ç„¶æ¯”æŠ€å·§**ï¼ˆlikelihood ratio trickï¼‰ï¼Œå¯ä»¥æ¨å‡ºï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \nabla_\theta \log \pi_\theta(\tau) \right]
$$

è€Œ $\pi_\theta(\tau) = p(s_1) \prod_{t=1}^T \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)$ ï¼Œæ‰€ä»¥ $\nabla_\theta \log \pi_\theta(\tau) = \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t|s_t)$

äºæ˜¯å¾—åˆ°**REINFORCE**ç®—æ³•ï¼ˆæœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦ï¼‰ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=1}^T R_t \right) \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \right]
$$

å…¶ä¸­ $R_t = \sum_{k=t}^T \gamma^{k-t} r_k$ æ˜¯ä»æ—¶é—´ $t$ å¼€å§‹çš„æŠ˜æ‰£å›æŠ¥ã€‚

ç­–ç•¥æ¢¯åº¦å­˜åœ¨å“ªäº›é—®é¢˜ï¼š
- **é«˜æ–¹å·®ï¼ˆhigh varianceï¼‰**ï¼šå› ä¸ºæ•´ä¸ªè½¨è¿¹çš„æ€»å¥–åŠ± $R(\tau)$ è¢«ç”¨ä½œæ¯ä¸ªåŠ¨ä½œçš„â€œä¿¡å·â€ï¼Œä½†å¾ˆå¤šåŠ¨ä½œå…¶å®å’Œæœ€ç»ˆç»“æœæ— å…³ã€‚
- **æ›´æ–°ä¸ç¨³å®š**ï¼šä¸€æ¬¡æ›´æ–°å¯èƒ½å¤ªå¤§ï¼Œå¯¼è‡´ç­–ç•¥å´©æºƒï¼ˆâ€œcatastrophic collapseâ€ï¼‰ã€‚

> âœ… æ‰€ä»¥ç­–ç•¥æ¢¯åº¦**ç†è®ºä¸Šæ­£ç¡®ï¼Œä½†å®è·µä¸­éš¾ç”¨**ã€‚

ğŸ”¹ å°è¯• 2: TRPOï¼ˆTrust Region Policy Optimizationï¼‰

æ ¸å¿ƒæ€æƒ³ï¼šä¸è¦ç›´æ¥ç”¨åŸå§‹æ¢¯åº¦æ›´æ–°ï¼Œè€Œæ˜¯**æ¯æ¬¡åªå…è®¸ç­–ç•¥å˜åŠ¨ä¸€ç‚¹ç‚¹**ï¼Œç¡®ä¿æ–°ç­–ç•¥ $\pi_{\theta_{\text{new}}}$ å’Œæ—§ç­–ç•¥ $\pi_{\theta_{\text{old}}}$ è¶³å¤Ÿæ¥è¿‘ã€‚

å…·ä½“åšæ³•ï¼šè§£ä¸€ä¸ª**å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜**ï¼š

$$
\max_\theta \quad \mathbb{E}_{s,a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\text{old}}}(s,a) \right] \\
\text{subject to} \quad \mathbb{E}_s \left[ D_{\text{KL}} \left( \pi_{\theta_{\text{old}}}(\cdot|s) \,\|\, \pi_\theta(\cdot|s) \right) \right] \leq \delta
$$

- è¿™ä¸ªç›®æ ‡æ˜¯**è¿‘ä¼¼**ç­–ç•¥æ”¹è¿›ï¼ˆä½¿ç”¨é‡è¦æ€§é‡‡æ · + ä¼˜åŠ¿å‡½æ•° $A$ï¼‰
- çº¦æŸé¡¹é™åˆ¶ KL æ•£åº¦ä¸è¶…è¿‡ä¸€ä¸ªå°å¸¸æ•° $\delta$


TRPO ç‰¹ç‚¹ï¼š
- âœ… ç¨³å®šï¼Œç†è®ºä¿è¯å•è°ƒæ”¹è¿›
- âŒ å®ç°æå…¶å¤æ‚ï¼šéœ€è¦ç”¨å…±è½­æ¢¯åº¦ï¼ˆconjugate gradientï¼‰æˆ–äºŒé˜¶ä¼˜åŒ–ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰

> æ‰€ä»¥ TRPO æ˜¯â€œç†æƒ³ä½†ç¬¨é‡â€çš„æ–¹æ³•ã€‚


ğŸ”¹ å°è¯• 3: PPOï¼ˆProximal Policy Optimizationï¼‰

åŠ¨æœºï¼šèƒ½ä¸èƒ½**ç”¨ä¸€ä¸ªç®€å•çš„æ–¹æ³•ï¼Œè¿‘ä¼¼ TRPO çš„â€œå°æ­¥æ›´æ–°â€æ€æƒ³**ï¼Œè€Œä¸ç”¨è§£å¤æ‚çš„çº¦æŸä¼˜åŒ–ï¼Ÿ

PPO çš„æ ¸å¿ƒåˆ›æ–°ï¼š**è£å‰ªæ¦‚ç‡æ¯”ï¼ˆClipped Probability Ratioï¼‰**

å®šä¹‰**æ¦‚ç‡æ¯”**ï¼ˆlikelihood ratioï¼‰ï¼š

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

åœ¨ TRPO ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ› $r_t(\theta) \approx 1$ï¼ˆå³æ–°æ—§ç­–ç•¥è¾“å‡ºæ¦‚ç‡æ¥è¿‘ï¼‰ã€‚

PPO çš„æƒ³æ³•æ˜¯ï¼š**å¦‚æœ $r_t(\theta)$ å¤ªå¤§æˆ–å¤ªå°ï¼Œå°±æŠŠå®ƒâ€œè£å‰ªâ€æ‰**ï¼

äºæ˜¯æå‡º**è£å‰ªç›®æ ‡å‡½æ•°**ï¼ˆClipped Surrogate Objectiveï¼‰ï¼š

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) A_t, \ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t \right) \right]
$$

ç›´è§‚è§£é‡Šï¼š
- å¦‚æœ $A_t > 0$ï¼ˆè¿™ä¸ªåŠ¨ä½œå¥½ï¼‰ï¼š
  - æˆ‘ä»¬å¸Œæœ›å¢å¤§ $\pi_\theta(a_t|s_t)$ï¼Œå³è®© $r_t > 1$
  - ä½†å¦‚æœ $r_t > 1+\epsilon$ï¼Œè¯´æ˜æ›´æ–°å¤ªå¤§ â†’ è£å‰ªæ‰ï¼Œåªå– $1+\epsilon$
- å¦‚æœ $A_t < 0$ï¼ˆè¿™ä¸ªåŠ¨ä½œå·®ï¼‰ï¼š
  - æˆ‘ä»¬å¸Œæœ›å‡å° $\pi_\theta(a_t|s_t)$ï¼Œå³è®© $r_t < 1$
  - ä½†å¦‚æœ $r_t < 1-\epsilon$ï¼Œè¯´æ˜æƒ©ç½šå¤ªçŒ› â†’ è£å‰ªä¸º $1-\epsilon$

> ğŸ¯ è¿™æ ·ï¼ŒPPO **è‡ªåŠ¨é™åˆ¶äº†ç­–ç•¥æ›´æ–°çš„æ­¥é•¿**ï¼Œæ— éœ€æ˜¾å¼ KL çº¦æŸï¼


ğŸ” ä¸‰è€…å…³ç³»æ€»ç»“

| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | æ˜¯å¦çº¦æŸæ›´æ–°æ­¥é•¿ï¼Ÿ | å®ç°éš¾åº¦ | é€‚åˆ LLM å—ï¼Ÿ |
|------|--------|------------------|--------|-------------|
| **Policy Gradient** | ç›´æ¥æ¢¯åº¦ä¸Šå‡ | âŒ å¦ | ç®€å• | âŒï¼ˆæ–¹å·®å¤§ï¼‰ |
| **TRPO** | ç”¨ KL æ•£åº¦çº¦æŸæ›´æ–° | âœ… æ˜¯ï¼ˆç¡¬çº¦æŸï¼‰ | æéš¾ | âŒï¼ˆå†…å­˜/è®¡ç®—é«˜ï¼‰ |
| **PPO** | ç”¨è£å‰ªè¿‘ä¼¼å°æ­¥æ›´æ–° | âœ… æ˜¯ï¼ˆè½¯çº¦æŸï¼‰ | ä¸­ç­‰ | âœ…ï¼ˆä¸»æµé€‰æ‹©ï¼‰ |


####  PPO çš„ç—›ç‚¹

å½“æˆ‘ä»¬çœ‹ OPANAI å…³äº [PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html) ç®—æ³•çš„æ–‡æ¡£ï¼Œä»–çœ‹èµ·æ¥å¾ˆç®€å•ï¼š

<div align="center">
   <img src="images/14-1-ppoç®—æ³•ä¼ªä»£ç .png" />
   <p>å›¾14.1 ppoç®—æ³•ä¼ªä»£ç 
</p>
 </div>


ä½†åœ¨å®è·µä¸Šï¼ŒPPO çš„ç†è®ºå’Œå®ç°å®Œå…¨æ˜¯ä¸¤å›äº‹ã€‚PPO ç†è®ºç®€æ´ï¼Œä½†å®é™…è°ƒå‚å’Œå®ç°é™·é˜±æå¤šï¼ˆå¦‚ä»·å€¼å‡½æ•°è®­ç»ƒã€ä¼˜åŠ¿ä¼°è®¡ã€KL æ§åˆ¶ã€å¥–åŠ±å½’ä¸€åŒ–ç­‰ï¼‰ï¼Œæœ‰ç¯‡åšå®¢ç”šè‡³åˆ—å‡ºäº†[37ä¸ªPPOå®ç°ç»†èŠ‚](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)ï¼Œå‘ç°ä¸åŒçš„ PPO å˜ä½“åœ¨ RL çš„ benchmarks è¡¨ç°å‡ºäº†ä¸åŒçš„å¾—åˆ†ã€‚

<div align="center">
   <img src="images/14-2-ppoå®ç°ç»†èŠ‚å¯¹æ€§èƒ½çš„å½±å“.png" />
   <p>å›¾14.2 ppoå®ç°ç»†èŠ‚å¯¹æ€§èƒ½çš„å½±å“.png

</p>
 </div>

è€Œä¸”è¿˜æœ‰ä¸€ç¯‡è®ºæ–‡ï¼Œä¸“é—¨æ¢è®¨ä¸ºä»€ä¹ˆç»†èŠ‚å¯¹äº PPO å¦‚æ­¤é‡è¦ï¼Œè¯·å‚è€ƒ [Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO](https://arxiv.org/abs/2005.12729)ã€‚ä»¥åŠå¦‚æœä½ çœŸçš„æŠŠä»–ä»¬æç ¸äº†ï¼Œç”šè‡³æ²¡æœ‰æ­£ç¡®è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œä½†æ•ˆæœåè€Œæ›´å¥½ã€‚å¦‚æœä½ å»çœ‹ PPO çš„å®ç°ç»†èŠ‚ï¼Œä¼šå‘ç°æƒ…å†µéå¸¸å¤æ‚ï¼Œæ‰€ä»¥æˆ‘ä»¬ç¡®å®éœ€è¦é€šè¿‡ä»£ç çœ‹ä¸‹ PPO çš„å…·ä½“å®ç°ï¼š

å‚è€ƒ [alpaca_farm ä¸­å…³äº PPO çš„å®ç°](https://github.com/tatsu-lab/alpaca_farm/blob/30717ddae735365de756ee2085191b491a71788d/src/alpaca_farm/rl/ppo_trainer.py)ï¼Œè¯¥å®ç°éµå¾ªå…¸å‹çš„ on-policy RL å¾ªç¯ï¼Œå®ç°äº† PPO ç®—æ³•åœ¨ è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ ä¸Šçš„å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ŒåŒ…å«ï¼š

- Rolloutï¼ˆé‡‡æ ·ï¼‰ï¼šç”¨å½“å‰ç­–ç•¥ç”Ÿæˆ responses
- å¥–åŠ±è®¡ç®—ä¸å¡‘å½¢ï¼ˆReward Shapingï¼‰ï¼šç»“åˆä»»åŠ¡å¥–åŠ± + KL æƒ©ç½š
- ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
- æŸå¤±è®¡ç®—ï¼ˆPolicy + Value Loss with Clippingï¼‰ï¼šç”¨ PPO æŸå¤±å‡½æ•°ä¼˜åŒ–ç­–ç•¥ï¼ˆActorï¼‰å’Œä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
- æ—¥å¿—è®°å½•ä¸æ¨¡å‹ä¿å­˜

**å¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰ï¼š** å°†ç¨€ç–çš„ä»»åŠ¡å¥–åŠ±ï¼ˆåªåœ¨åºåˆ—æœ«å°¾ï¼‰ä¸å¯†é›†çš„KL æƒ©ç½šï¼ˆæ¯ä¸ª tokenï¼‰ç»“åˆèµ·æ¥ï¼Œå½¢æˆå¯è®­ç»ƒçš„ reward signal

```
def _shape_reward(self, rewards, responses, logprobs, ref_logprobs):
    # è®¡ç®— KL æ•£åº¦ï¼šç”¨ (logp - ref_logp) çš„æ­£å€¼éƒ¨åˆ†ï¼ˆå³æ–°ç­–ç•¥æ¯”å‚è€ƒç­–ç•¥æ›´â€œè‡ªä¿¡â€æ‰æƒ©ç½šï¼‰
    kl = torch.clamp(logprobs - ref_logprobs, min=0.0)

    # éä»»åŠ¡å¥–åŠ± = -Î² * KLï¼ˆÎ² ç”± self.kl_ctl æ§åˆ¶ï¼Œå¯åŠ¨æ€è°ƒæ•´ï¼‰
    non_score_rewards = -self.kl_ctl.value * kl

    # åˆå§‹åŒ–å¡‘å½¢å¥–åŠ±ï¼šå…ˆå¡«å…¥ KL æƒ©ç½šï¼ˆæ¯ä¸ª token éƒ½æœ‰ï¼‰
    shaped_rewards = non_score_rewards.clone()

    # æ‰¾åˆ°æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªé padding token çš„ä½ç½®ï¼ˆå³ EOS æˆ–çœŸå®ç»“å°¾ï¼‰
    terminal_positions = (responses != self.tokenizer.pad_token_id).sum(dim=1) - 1

    # åœ¨æœ€åä¸€ä¸ª token å¤„åŠ ä¸Šä»»åŠ¡å¥–åŠ±ï¼ˆå¦‚æ•°å­¦é¢˜æ˜¯å¦ç­”å¯¹ï¼‰
    shaped_rewards[list(range(rewards.size(0))), terminal_positions] += rewards

    return dict(shaped_rewards=shaped_rewards, non_score_rewards=non_score_rewards, kl=kl)
```


**å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼š** ç”¨ GAE ä¼°è®¡æ¯ä¸ª token çš„ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰ï¼Œæ›¿ä»£åŸå§‹å¥–åŠ±ï¼Œå¤§å¹…é™ä½ç­–ç•¥æ¢¯åº¦æ–¹å·®ã€‚

```
def _estimate_advantage(self, rewards, values):
    if self.args.whiten_rewards:
        rewards = torch_ops.whiten(rewards, shift_mean=False)  # å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰

    lastgaelam = 0
    advantages_reversed = []
    gen_length = self.args.response_len  # ç”Ÿæˆé•¿åº¦ï¼ˆå¦‚ 128ï¼‰

    # ä»åå¾€å‰è®¡ç®— GAEï¼ˆåå‘éå† tokenï¼‰
    for t in reversed(range(gen_length)):
        nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
        # TD error: Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
        delta = rewards[:, t] + self.args.gamma * nextvalues - values[:, t]
        # GAE: A_t = Î´_t + Î³Î» A_{t+1}
        lastgaelam = delta + self.args.gamma * self.args.lam * lastgaelam
        advantages_reversed.append(lastgaelam)

    advantages = torch.stack(advantages_reversed[::-1], dim=1)  # åè½¬å›æ­£å¸¸é¡ºåº
    returns = advantages + values  # Q(s,a) â‰ˆ A(s,a) + V(s)

    # ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ–ï¼ˆå‡å‡å€¼ã€é™¤æ ‡å‡†å·®ï¼‰â†’ é™ä½æ–¹å·®
    advantages = torch_ops.whiten(advantages, shift_mean=True)

    return dict(returns=returns, advantages=advantages)
```

**rolloutï¼ˆé‡‡æ ·è½¨è¿¹ï¼‰ï¼š** å®Œæˆä¸€æ¬¡å®Œæ•´çš„ é‡‡æ · â†’ è¯„ä¼° â†’ å¥–åŠ±è®¡ç®— â†’ ä¼˜åŠ¿ä¼°è®¡ æµç¨‹ï¼Œä¸ºåç»­ PPO æ›´æ–°å‡†å¤‡æ•°æ®

```
@torch.inference_mode()
def rollout(self, queries_data):
    self.policy.eval()
    unwrapped_policy = self.accelerator.unwrap_model(self.policy, keep_fp32_wrapper=True)
    self.ref_policy.eval()
    self.reward_model.eval()

    rollouts = []
    for batch in tqdm.tqdm(queries_data, desc="rollout"):
        # 1. ä»å½“å‰ç­–ç•¥ç”Ÿæˆ responses
        queries, masks = batch['queries'], batch['query_attn_masks']
        responses = unwrapped_policy.respond(queries, masks, temperature=...)  # ç”Ÿæˆ

        # 2. ç”¨å½“å‰ç­–ç•¥è®¡ç®— logprobs å’Œ valuesï¼ˆcritic è¾“å‡ºï¼‰
        policy_outputs = self.policy(queries, masks, responses, ...)  # forward

        # 3. ç”¨å‚è€ƒç­–ç•¥ï¼ˆSFT æ¨¡å‹ï¼‰è®¡ç®— ref_logprobsï¼ˆç”¨äº KLï¼‰
        ref_outputs = self.ref_policy(queries, masks, responses, ...)

        # 4. å°† response è½¬ä¸ºæ–‡æœ¬ï¼Œå†ç”¨ reward tokenizer é‡æ–° tokenize
        #    ï¼ˆå› ä¸º policy å’Œ reward model çš„ tokenizer å¯èƒ½ä¸åŒï¼‰
        text_queries = decode(queries); text_responses = decode(responses)
        text_sequences = [q + r for q, r in zip(text_queries, text_responses)]
        sequences = reward_tokenizer(text_sequences, ...)  # é‡æ–° tokenize

        # 5. ç”¨ reward model è®¡ç®—ä»»åŠ¡å¥–åŠ±
        reward_outputs = self.reward_model(**sequences)
        reward_outputs = self.post_reward(reward_outputs, responses)  # å¤„ç†æœªæ­£å¸¸ç»“æŸçš„åºåˆ—

        # 6. å¥–åŠ±å¡‘å½¢ï¼šåŠ å…¥ KL æƒ©ç½š
        shaped = self._shape_reward(rewards=reward_outputs['rewards'], ...)

        # 7. ä¿å­˜æ‰€æœ‰æ•°æ®åˆ° rollouts
        rollouts_batch.update(policy_outputs, ref_outputs, reward_outputs, shaped)
        rollouts.append(rollouts_batch.cpu())

    # åˆå¹¶æ‰€æœ‰ batch
    rollouts = common.merge_dict(rollouts, merge_fn=torch.cat)

    # 8. ç»Ÿä¸€è®¡ç®— GAEï¼ˆç”¨æ•´ä¸ª rollout æ•°æ®é›†ï¼Œæ›´ç¨³å®šï¼‰
    advantages = self._estimate_advantage(
        rewards=rollouts["shaped_rewards"].to(device),
        values=rollouts["values"].to(device),
    )

    return {**rollouts, **advantages}
```

**PPO æŸå¤±è®¡ç®—ï¼š** ä½¿ç”¨è£å‰ªæœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§

```
def compute_loss(self, rollouts):
    # æå–æ—§ç­–ç•¥æ•°æ®ï¼ˆfrom rolloutï¼‰
    values, old_logprob, returns, advantages, ... = rollouts

    # ç”¨å½“å‰ç­–ç•¥é‡æ–°è®¡ç®— logprobs å’Œ values
    outputs = self.policy(queries, masks, responses, ...)
    vpred = outputs["values"]      # æ–°çš„ value é¢„æµ‹
    logprob = outputs["logprobs"]  # æ–°çš„ log prob

    # --- Value Loss (Critic) ---
    # è£å‰ª value é¢„æµ‹ï¼ˆç±»ä¼¼ PPO è£å‰ªï¼‰
    vpredclipped = torch.clamp(vpred, values Â± cliprange_value)
    vf_losses1 = (vpred - returns) ** 2
    vf_losses2 = (vpredclipped - returns) ** 2
    vf_loss = 0.5 * max(vf_losses1, vf_losses2).mean()  # PPO-style value loss

    # --- Policy Loss (Actor) ---
    ratio = exp(logprob - old_logprob)  # æ–°æ—§ç­–ç•¥æ¦‚ç‡æ¯”
    pg_losses = -advantages * ratio
    pg_losses2 = -advantages * clip(ratio, 1-Îµ, 1+Îµ)
    pg_loss = max(pg_losses, pg_losses2).mean()  # PPO è£å‰ªç›®æ ‡

    # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + vf_coef * ä»·å€¼æŸå¤±
    loss = pg_loss + self.args.vf_coef * vf_loss

    # è®°å½•ç»Ÿè®¡é‡
    approxkl = 0.5 * (logprob - old_logprob)^2 çš„å‡å€¼
    entropy = outputs["entropies"].mean()

    return loss, stats
```


åœ¨è¯­è¨€æ¨¡å‹çš„ PPO è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå¥åº·ã€åˆç†çš„è®­ç»ƒæ›²çº¿æ˜¯ä»€ä¹ˆæ ·ï¼Ÿ


<div align="center">
   <img src="images/14-3-ppoè®­ç»ƒè¿‡ç¨‹æ›²çº¿.png" />
   <p>å›¾14.3 ppoè®­ç»ƒè¿‡ç¨‹æ›²çº¿</p>
 </div>

- Increasing overall rewardsï¼šæ€»å¥–åŠ±ä¸Šå‡ã€‚`kl_sum_seq` è¿™ä¸ªåå­—æœ‰ç‚¹è¯¯å¯¼æ€§ï¼Œå®ƒå®é™…ä¸Šä»£è¡¨çš„æ˜¯æ¯ä¸ªåºåˆ—çš„å¡‘å½¢å¥–åŠ±ï¼ˆshaped rewardsï¼‰çš„æ€»å’Œï¼Œè¿™ä¸ªâ€œå¡‘å½¢å¥–åŠ±â€ = ä»»åŠ¡å¥–åŠ±ï¼ˆå¦‚æ•°å­¦é¢˜ç­”å¯¹å¾—é«˜åˆ†ï¼‰ + KLæƒ©ç½šé¡¹ï¼ˆè´Ÿå€¼ï¼‰ï¼Œè¡¡é‡æ¨¡å‹çš„æ•´ä½“è¡¨ç°æ˜¯å¦åœ¨å˜å¥½ã€‚
- Incl. reward modelï¼š ä»»åŠ¡å¥–åŠ±ä¸Šå‡ã€‚è¿™ä¸ªæŒ‡æ ‡è¡¡é‡çš„æ˜¯æ¨¡å‹ç›´æ¥ä»å¥–åŠ±æ¨¡å‹é‚£é‡Œè·å¾—çš„ä»»åŠ¡å¥–åŠ±ï¼Œä¸åŒ…æ‹¬KLæƒ©ç½šã€‚å®ƒåæ˜ äº†æ¨¡å‹åœ¨æ ¸å¿ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜çš„å‡†ç¡®æ€§ã€éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ç­‰ã€‚
- Negative KL rewardsï¼š è´Ÿçš„KLå¥–åŠ±ã€‚è¿™ä¸ªæŒ‡æ ‡å°±æ˜¯ KL æƒ©ç½šé¡¹ï¼Œä¹Ÿå°±æ˜¯å‰é¢æåˆ°çš„ -Î² * KLã€‚è¯´æ˜æ¨¡å‹ç¡®å®åœ¨æ¢ç´¢å’Œæ”¹è¿›ï¼Œä½†æ²¡æœ‰å¤±æ§ã€‚è¿™æ˜¯PPOç®—æ³•â€œè¿‘ç«¯â€æ€æƒ³çš„ä½“ç°â€”â€”å…è®¸ä¸€å®šç¨‹åº¦çš„åç¦»ï¼Œä½†é™åˆ¶å…¶å¹…åº¦ã€‚

è¿™ä¸‰æ¡æ›²çº¿å…±åŒæç»˜äº†ä¸€ä¸ªå¥åº·çš„PPOè®­ç»ƒè¿‡ç¨‹ï¼šæ¨¡å‹åœ¨å¥–åŠ±æ¨¡å‹çš„å¼•å¯¼ä¸‹ï¼Œé€æ­¥å­¦ä¼šç”Ÿæˆæ›´å¥½çš„å›å¤ï¼ŒåŒæ—¶é€šè¿‡KLæƒ©ç½šä¿æŒä¸€å®šçš„ç¨³å®šæ€§ï¼Œé¿å…è¿‡åº¦åç¦»åˆå§‹çš„è‰¯å¥½è¡Œä¸ºã€‚

### 14.2.2 ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¦ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ï¼Ÿ

**1. ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ PPOï¼Ÿ**
PPO æ˜¯ç›®å‰æœ€æˆåŠŸçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œå°¤å…¶åœ¨è¯­è¨€æ¨¡å‹çš„å¯¹é½ï¼ˆAlignmentï¼‰ä¸­è¢«å¹¿æ³›åº”ç”¨ã€‚ä½†å®ƒæœ‰ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼š

- **å®ç°å¤æ‚ (complicated implementation)**ï¼šPPO ä¸æ˜¯ä¸€ä¸ªç®€å•çš„â€œå¼€ç®±å³ç”¨â€çš„ç®—æ³•ã€‚å®ƒåŒ…å«å¤šä¸ªå¤æ‚çš„ç»„ä»¶ï¼Œå¦‚ï¼š**Rollout é‡‡æ ·**ã€**å¥–åŠ±å¡‘å½¢**ã€ **ä¼˜åŠ¿ä¼°è®¡**ã€**æŸå¤±è®¡ç®—**ç­‰ã€‚è¿™äº›æ­¥éª¤éœ€è¦ç²¾å¿ƒè®¾è®¡å’Œè°ƒè¯•ï¼Œå¯¹äºæ–°æ‰‹æˆ–è¿½æ±‚å¿«é€Ÿè¿­ä»£çš„ç ”ç©¶è€…æ¥è¯´ï¼Œé—¨æ§›å¾ˆé«˜ã€‚

- **ä»·å€¼æ¨¡å‹ (Value model) çš„è´Ÿæ‹…**ï¼šPPO éœ€è¦ä¸€ä¸ªé¢å¤–çš„ **ä»·å€¼ç½‘ç»œï¼ˆValue Modelï¼‰** æ¥ä¼°è®¡çŠ¶æ€çš„ä»·å€¼ï¼ˆ`V(s)`ï¼‰ï¼Œä»è€Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆ`A = Q - V`ï¼‰ã€‚
    - **å†…å­˜æ¶ˆè€—å¤§ (memory hungry)**ï¼šä»·å€¼ç½‘ç»œä¸ç­–ç•¥ç½‘ç»œå…±äº«ä¸»å¹²ç»“æ„ï¼Œä½†éœ€è¦é¢å¤–çš„å‚æ•°å’Œè®¡ç®—èµ„æºã€‚
    - **é¢å¤–çš„è°ƒå‚ (additional tuning)**ï¼šä»·å€¼ç½‘ç»œæœ¬èº«ä¹Ÿéœ€è¦è®­ç»ƒå’Œä¼˜åŒ–ï¼Œè¿™å¢åŠ äº†æ•´ä¸ªç³»ç»Ÿçš„å¤æ‚æ€§å’Œè¶…å‚æ•°æœç´¢ç©ºé—´ã€‚ä½ éœ€è¦åŒæ—¶è°ƒä¼˜ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œï¼Œç¡®ä¿å®ƒä»¬ååŒå·¥ä½œã€‚

> âœ… **æ€»ç»“**ï¼šPPO è™½ç„¶å¼ºå¤§ä¸”æœ‰æ•ˆï¼Œä½†å…¶**å·¥ç¨‹å¤æ‚åº¦é«˜ã€èµ„æºæ¶ˆè€—å¤§ã€è°ƒå‚å›°éš¾**ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºæˆ–è¿½æ±‚é«˜æ•ˆå¼€å‘çš„åœºæ™¯ä¸‹ï¼Œæ˜¾å¾—ä¸å¤Ÿè½»ä¾¿ã€‚

**2. ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ DPOï¼Ÿ**

DPOï¼ˆDirect Preference Optimizationï¼‰æ˜¯è¿‘å¹´æ¥å…´èµ·çš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒç»•è¿‡äº†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ä»äººç±»åå¥½æ•°æ®ä¸­è¿›è¡Œä¼˜åŒ–ã€‚ä½†å…¶ä¹Ÿå­˜åœ¨ä»¥ä¸‹é™åˆ¶ï¼š

- **æ•°æ®å½¢å¼ä¸åŒ¹é… (Data not inherently pairwise)**ï¼šDPO çš„æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäº**æˆå¯¹æ¯”è¾ƒï¼ˆpairwise comparisonsï¼‰** æ•°æ®ï¼Œå³ç»™å®šä¸€ä¸ªæç¤ºï¼ˆpromptï¼‰ï¼Œæœ‰ä¸¤ä¸ªä¸åŒçš„å›å¤ï¼ˆresponse A å’Œ response Bï¼‰ï¼Œå¹¶æ ‡æ³¨å“ªä¸ªæ›´å¥½ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®å¹¶éå¤©ç„¶å°±æ˜¯æˆå¯¹çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œå¯éªŒè¯å¥–åŠ±â€ï¼ˆVerifiable Rewardsï¼‰é¢†åŸŸï¼Œæ•°æ®é€šå¸¸æ˜¯å•ä¸ªåºåˆ—åŠ ä¸Šä¸€ä¸ªå®¢è§‚çš„åˆ†æ•°ï¼ˆå¦‚æ•°å­¦é¢˜ç­”å¯¹å¾—1åˆ†ï¼Œç­”é”™å¾—0åˆ†ï¼‰ã€‚è¿™ç§**æ ‡é‡å¥–åŠ±ä¿¡å·**æ— æ³•ç›´æ¥ç”¨äº DPOã€‚

- **ç¦»çº¿ç®—æ³• (Offline)**ï¼šDPO æ˜¯ä¸€ç§**ç¦»çº¿ç®—æ³•**ã€‚å®ƒåœ¨å›ºå®šçš„ã€é¢„å…ˆæ”¶é›†å¥½çš„åå¥½æ•°æ®é›†ä¸Šè¿›è¡Œä¸€æ¬¡æ€§è®­ç»ƒã€‚è¿™ä¸ PPO çš„**åœ¨çº¿å­¦ä¹ **ç‰¹æ€§ä¸åŒã€‚PPO å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­ç”Ÿæˆæ–°çš„æ ·æœ¬ã€è·å¾—æ–°çš„åé¦ˆï¼Œå¹¶æ®æ­¤è¿­ä»£æ›´æ–°ç­–ç•¥ã€‚DPO â€œç†è®ºä¸Šå¯ä»¥é€šè¿‡è¿­ä»£ä½¿å…¶åœ¨çº¿åŒ–â€ï¼Œä½†è¿™ä¼šå¢åŠ å¤æ‚æ€§ï¼Œå¤±å»äº†å…¶ä½œä¸ºç®€å•ç¦»çº¿æ–¹æ³•çš„ä¼˜åŠ¿ã€‚

> âœ… **æ€»ç»“**ï¼šDPO åœ¨å¤„ç†**äººç±»åå¥½æ•°æ®**æ—¶éå¸¸ä¼˜é›…å’Œé«˜æ•ˆï¼Œä½†å®ƒ**ä¸é€‚ç”¨äºéæˆå¯¹çš„ã€å¯éªŒè¯çš„æ ‡é‡å¥–åŠ±åœºæ™¯**ï¼Œå¹¶ä¸”å…¶ç¦»çº¿ç‰¹æ€§é™åˆ¶äº†å®ƒåœ¨éœ€è¦æŒç»­æ¢ç´¢å’Œåœ¨çº¿å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚



### 14.2.3 GRPOï¼šå»æ‰äº†ä»·å€¼å‡½æ•°çš„ PPO

**GRPO (Group Relative Policy Optimization)** æ˜¯åœ¨ [DeepSeekMath](https://arxiv.org/pdf/2402.03300) è®ºæ–‡ä¸­æå‡ºå¹¶åœ¨ [Deepseek-R1](https://arxiv.org/abs/2501.12948) ä¸­å‘æ‰¬å…‰å¤§çš„ç®—æ³•ã€‚GRPO åœ¨ PPO çš„åŸºç¡€ä¸Šï¼Œç§»é™¤äº†ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰å’Œä¼˜åŠ¿è®¡ç®—ï¼ˆAdvantage Computationï¼‰ã€‚è¿™æ˜¯å¯¹ PPO æœ€å¤§çš„æ”¹åŠ¨ï¼Œä¹Ÿæ˜¯å…¶è½»é‡åŒ–çš„æ ¹æœ¬åŸå› ã€‚å¹¶ä¸”é‡‡ç”¨äº†ä¸€ç§å…¨æ–°çš„æ–¹å¼æ¥ä¼°ç®—â€œä¼˜åŠ¿â€â€”â€”å³ â€œç»„å†… z-scoreâ€ï¼ˆz-score within groupï¼‰ã€‚

<div align="center">
   <img src="images/14-4-ppoä¸grpoçš„å¯¹æ¯”.png" />
   <p>å›¾14.4 PPOä¸GRPOçš„å¯¹æ¯”</p>
 </div>

> âœ… ç®€å•æ¥è¯´ï¼ŒGRPO = PPO - Value Model + Group Z-Score Advantageã€‚


#### GRPO çš„ç›®æ ‡å‡½æ•°

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E} \left[ q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q) \right] \frac{1}{G} \sum_{i=1}^{G} \left( \min\left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon \right) A_i \right) - \beta \mathbb{D}_{KL} \left( \pi_\theta || \pi_{ref} \right) \right)
$$

- `min(...)`éƒ¨åˆ†ï¼šæ˜¯ PPO çš„ç»å…¸è£å‰ªç›®æ ‡å‡½æ•°ï¼Œç”¨äºæ›´æ–°ç­–ç•¥ $Ï€_Î¸$ã€‚
    - $\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}$ æ˜¯æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”ã€‚
    - $A_i$ æ˜¯ç¬¬ $i$ ä¸ªè¾“å‡º $o_i$ çš„â€œä¼˜åŠ¿â€ï¼Œè¿™æ˜¯ GRPO æœ€å¤§çš„åˆ›æ–°ç‚¹ã€‚
    - `clip(...)` æ˜¯ PPO çš„è£å‰ªæœºåˆ¶ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ã€‚
- $-Î² D_KL(...)$ éƒ¨åˆ†ï¼šè¿™æ˜¯ KL æ•£åº¦æƒ©ç½šé¡¹ï¼Œç”¨äºé˜²æ­¢æ–°ç­–ç•¥ `Ï€_Î¸` åç¦»å‚è€ƒç­–ç•¥ `Ï€_ref` å¤ªè¿œï¼Œä¿è¯ç”Ÿæˆç»“æœçš„ç¨³å®šæ€§ã€‚
    - `Î²` æ˜¯æ§åˆ¶ KL æƒ©ç½šå¼ºåº¦çš„è¶…å‚æ•°ã€‚

PPO çš„ç›®æ ‡å‡½æ•°ï¼š
$$
\min \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a), \text{ clip} \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a) \right)
$$

GRPO ä¸ PPO ä¸¤è€…çš„ç›®æ ‡å‡½æ•°ç»“æ„éå¸¸ç›¸ä¼¼ï¼Œéƒ½åŒ…å«æ¦‚ç‡æ¯”å’Œè£å‰ªã€‚æ ¸å¿ƒåŒºåˆ«åœ¨äº `A` çš„æ¥æºï¼š
- **PPO**: `A` æ˜¯é€šè¿‡ä»·å€¼ç½‘ç»œ `V(s)` å’Œ GAE è®¡ç®—å‡ºæ¥çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚ä¸”èµ„æºå¯†é›†çš„è¿‡ç¨‹ã€‚
- **GRPO**: `A` æ˜¯é€šè¿‡ç»„å†… z-score è®¡ç®—å‡ºæ¥çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€å¿«é€Ÿã€æ— éœ€é¢å¤–æ¨¡å‹çš„è¿‡ç¨‹ã€‚


#### KL æ•£åº¦çš„è®¡ç®—

$$
\mathbb{D}_{KL} \left( \pi_\theta || \pi_{ref} \right) = \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - \log \frac{\pi_{ref}(o_i|q)}{\pi_\theta(o_i|q)} - 1
$$

è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼çš„ KL æ•£åº¦å…¬å¼ã€‚å®ƒä¸æ˜¯æ ‡å‡†çš„ç§¯åˆ†å½¢å¼ï¼Œè€Œæ˜¯åœ¨æ¯ä¸ªé‡‡æ ·ç‚¹ $o_i$ ä¸Šè¿›è¡Œè¿‘ä¼¼ã€‚å®ƒçš„ä½œç”¨æ˜¯è¡¡é‡å½“å‰ç­–ç•¥ $Ï€_Î¸$ å’Œå‚è€ƒç­–ç•¥ $Ï€_{ref}$ åœ¨ç”Ÿæˆç‰¹å®šè¾“å‡º $o_i$ æ—¶çš„æ¦‚ç‡å·®å¼‚ã€‚

#### ç»„å†… z-score ä¼˜åŠ¿

$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}
$$

è¿™æ˜¯ GRPO çš„çµé­‚æ‰€åœ¨ï¼å®ƒå®Œå…¨æŠ›å¼ƒäº† PPO ä¸­å¤æ‚çš„ GAE è®¡ç®—ã€‚

**å¦‚ä½•è®¡ç®—ï¼Ÿ**
- å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ $q$ï¼Œä»æ—§ç­–ç•¥ $Ï€_{old}$ ä¸­**é‡‡æ ·ä¸€ç»„ï¼ˆGä¸ªï¼‰ä¸åŒçš„å›å¤** ${o_1, o_2, ..., o_G}$ã€‚
- ç”¨å¥–åŠ±æ¨¡å‹æˆ–å¯éªŒè¯è§„åˆ™ï¼Œä¸ºè¿™ G ä¸ªå›å¤**åˆ†åˆ«æ‰“åˆ†**ï¼Œå¾—åˆ°ä¸€ç»„å¥–åŠ± ${r_1, r_2, ..., r_G}$ã€‚
- è®¡ç®—è¿™ç»„å¥–åŠ±çš„**å‡å€¼ï¼ˆmeanï¼‰å’Œæ ‡å‡†å·®ï¼ˆstdï¼‰**ã€‚
- å°†æ¯ä¸ªå›å¤ $o_i$ çš„å¥–åŠ± $r_i$ å‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ ‡å‡†å·®ï¼Œå¾—åˆ°å®ƒçš„ $A_i$ã€‚

**ä¸ºä»€ä¹ˆå« â€œz-scoreâ€ï¼Ÿ**

åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œz-score è¡¨ç¤ºä¸€ä¸ªæ•°æ®ç‚¹è·ç¦»å¹³å‡å€¼æœ‰å¤šå°‘ä¸ªæ ‡å‡†å·®ã€‚è¿™é‡Œï¼Œ$A_i$ è¡¨ç¤ºå›å¤ $o_i$ çš„å¥–åŠ±åœ¨æœ¬ç»„ä¸­çš„â€œç›¸å¯¹ä¼˜åŠ£ç¨‹åº¦â€ã€‚å¦‚æœ $A_i > 0$ï¼Œè¯´æ˜è¿™ä¸ªå›å¤æ¯”ç»„å†…å¹³å‡æ°´å¹³å¥½ï¼›å¦‚æœ $A_i < 0$ï¼Œåˆ™è¯´æ˜å®ƒæ¯”å¹³å‡æ°´å¹³å·®ã€‚

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

- **ç®€å•é«˜æ•ˆ**ï¼šä¸éœ€è¦è®­ç»ƒé¢å¤–çš„ä»·å€¼ç½‘ç»œï¼Œä¹Ÿä¸éœ€è¦å¤æ‚çš„ GAE è®¡ç®—ã€‚
- **è‡ªå½’ä¸€åŒ–**ï¼šé€šè¿‡ç»„å†…æ¯”è¾ƒï¼Œè‡ªåŠ¨æ¶ˆé™¤äº†ä¸åŒé—®é¢˜ä¹‹é—´å¥–åŠ±å°ºåº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ•°å­¦é¢˜å¯èƒ½æœ€é«˜å¾—10åˆ†ï¼Œå¦ä¸€ä¸ªå¯èƒ½æœ€é«˜å¾—5åˆ†ï¼Œä½†å®ƒä»¬åœ¨åŒä¸€ç»„å†…æ¯”è¾ƒæ—¶ï¼Œz-score èƒ½å…¬å¹³åœ°åæ˜ ç›¸å¯¹å¥½åã€‚
- **é€‚ç”¨äºå¯éªŒè¯å¥–åŠ±**ï¼šå¯¹äºä¸€ä¸ªæ•°å­¦é¢˜ï¼Œä½ å¯ä»¥è®©æ¨¡å‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œç„¶åç”¨ç¨‹åºè‡ªåŠ¨åˆ¤æ–­æ¯ä¸ªç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆå¾—åˆ†ä¸º1æˆ–0ï¼‰ï¼Œå†ç”¨ z-score æ¥åŒºåˆ†å“ªä¸ªç­”æ¡ˆâ€œæ›´å¥½â€ã€‚

> åœ¨åœ¨çº¿å­¦ä¹ ï¼ˆè¾¹é‡‡æ ·è¾¹æ›´æ–°ï¼‰çš„åœºæ™¯ä¸‹ï¼ŒGRPO æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç§ä½¿ç”¨äº†ç»„å†…æ ‡å‡†åŒ–å¥–åŠ±çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚



#### ğŸ’» ä»£ç è§£è¯»ï¼šä¸€ä¸ªæç®€çš„ GRPO å®ç°

GRPO çš„å®ç°éå¸¸ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„ GAE è®¡ç®—ã€‚ä¸‹é¢æˆ‘ä»¬åŸºäº [nano-aha-moment](https://github.com/McGill-NLP/nano-aha-moment/blob/main/nano_r1_script.py) é¡¹ç›®ä¸­å¯¹äº GRPO ç®—æ³•çš„å®ç°ï¼Œå¯¹å…³é”®ä»£ç è¿›è¡Œåˆ†æã€‚å¦‚ä¸‹compute_pg_loss æ˜¯ä¸€ä¸ªå…¸å‹çš„ GRPO æŸå¤±è®¡ç®—å‡½æ•°ï¼š

```python
def compute_pg_loss(
    policy_model: Union[DeepSpeedEngine, PreTrainedModel], # å½“å‰è¦è®­ç»ƒçš„è¯­è¨€æ¨¡å‹
    batch: Dict[str, torch.Tensor], # ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
    total_response_len: torch.Tensor, # ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
    TEMPERATURE: float, # ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ï¼ˆå½±å“ log-prob è®¡ç®—ï¼‰
    KL_COEFFICIENT: float, # æ§åˆ¶ KL æƒ©ç½šå¼ºåº¦çš„è¶…å‚æ•°
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute the policy gradient loss with KL penalty between policy and reference models.

    This function:
    1. Calculates KL divergence penalty between the models
    2. Computes policy gradient loss using advantages
    3. Combines the losses with KL coefficient

    Args:
        policy_model: The model being trained
        batch: Dictionary containing:
            - input_ids: Tensor of shape [batch_size, seq_len]
            - attention_mask: Tensor of shape [batch_size, seq_len]
            - labels: Tensor of shape [batch_size, seq_len] with -100 for ignored positions
            - advantages: Tensor of shape [batch_size, seq_len]
            - ref_log_probs: Tensor of shape [batch_size, seq_len-1]
        total_response_len: Total number of valid tokens in the batch. This is a scalar tensor.

    Returns:
        Tuple containing:
            - loss: Combined policy gradient and KL penalty loss (scalar tensor)
            - metrics: Dictionary with detailed loss components:
                - policy_loss: Pure policy gradient loss
                - kl_penalty: KL divergence penalty
                - entropy: Policy entropy
    """

    # 1. ä» batch ä¸­æå–å…³é”®å¼ é‡
    input_ids = batch["input_ids"]  # [batch_size, seq_len]ï¼Œå®Œæ•´åºåˆ—ï¼ˆprompt + responseï¼‰
    attention_mask = batch["attention_mask"]  # [batch_size, seq_len]ï¼ŒæŒ‡ç¤ºæœ‰æ•ˆ token
    labels = batch["labels"]  # [batch_size, seq_len]ï¼Œé€šå¸¸ä¸ input_ids ç›¸åŒæˆ–å³ç§»ä¸€ä½
    labels_mask = batch["labels_mask"]  # [batch_size, seq_len]ï¼Œ1 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ response tokenï¼Œ0 è¡¨ç¤º prompt æˆ– padding
    advantages = batch["advantages"]  # [batch_size, seq_len]ï¼Œæ¯ä¸ª token çš„â€œä¼˜åŠ¿â€å€¼ï¼ˆæ¥è‡ªç»„å†…å½’ä¸€åŒ–ï¼‰
    ref_logps = batch["ref_log_probs"]  # [batch_size, seq_len-1]ï¼Œå‚è€ƒæ¨¡å‹åœ¨ response token ä¸Šçš„ log-probï¼ˆé•¿åº¦æ¯” input_ids å°‘ 1ï¼‰

    # 2. æ„å»ºæ¨¡å‹è¾“å…¥
    model_inputs = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "labels_mask": labels_mask,
    }

    # 3. è®¡ç®—å½“å‰ç­–ç•¥çš„ token log-probabilities
    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)  # è®© policy_model å¯¹ input_ids åšå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°æ¯ä¸ª token çš„ log-probabilityï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯ [batch_size, seq_len-1]ï¼Œå› ä¸ºæ¨¡å‹é¢„æµ‹çš„æ˜¯ input_ids[1:]

    # 4. å¯¹é½ mask å¹¶è®¡ç®— KL æƒ©ç½šé¡¹
    labels_mask = labels_mask[..., 1:].to(logps.dtype)  # å°† labels_mask ä¹Ÿå³ç§»ä¸€ä½ï¼Œä¸ logps å¯¹é½ï¼Œåªä¿ç•™ response token çš„ maskï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯ [batch_size, seq_len-1]ï¼Œ
    
    # ç›´æ¥è®¡ç®— KL éœ€è¦å¯¹æ•´ä¸ªè¯æ±‡è¡¨æ±‚å’Œï¼ˆsum(p * log(p/q))ï¼‰ï¼Œè®¡ç®—é‡æå¤§ã€‚ä½¿ç”¨äº† Bregman divergence çš„ä¸€ç§è¿‘ä¼¼ï¼Œè€Œè¿™ä¸ªè¿‘ä¼¼åªä¾èµ–äº logps å’Œ ref_logpsï¼ˆå³ token-level çš„ log-probï¼‰ï¼Œéå¸¸é«˜æ•ˆã€‚
    ref_logratio = ref_logps - logps
    kl_penalty = torch.exp(ref_logratio) - 1 - ref_logratio  # [batch_size, seq_len-1]
    kl_penalty = kl_penalty * labels_mask  # [batch_size, seq_len-1]ï¼Œåªå¯¹ response token è®¡ç®— KL æƒ©ç½šï¼Œå¿½ç•¥ prompt

    # 5. è®¡ç®—è¾…åŠ©ç»Ÿè®¡é‡ï¼ˆä¸å‚ä¸æ¢¯åº¦ï¼‰
    with torch.no_grad():
        entropy = -logps.sum() / labels_mask.sum()  # scalar
        zero_advantages = close_to_zero(advantages[..., 1:], labels_mask)  # scalar

    # 6. è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
    policy_loss = -logps * advantages[..., 1:]  # [batch_size, seq_len-1]ï¼Œadvantages[..., 1:]å– advantage ä»ç¬¬ 2 ä¸ª token å¼€å§‹ï¼Œä¸ logps å¯¹é½
    policy_loss = policy_loss * labels_mask  # [batch_size, seq_len-1]

    # 7. ç»„åˆæ€»æŸå¤±å¹¶å½’ä¸€åŒ–
    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len  # å°†ç­–ç•¥æŸå¤±å’Œ KL æƒ©ç½šåŠ æƒæ±‚å’Œï¼Œé™¤ä»¥ total_response_lenï¼Œå°†æ€»æŸå¤±å½’ä¸€åŒ–ä¸ºæ¯ä¸ªæœ‰æ•ˆ token çš„å¹³å‡æŸå¤±ï¼Œä½¿ loss å€¼åœ¨ä¸åŒ batch size ä¸‹å¯æ¯”ã€‚

    # 8. æ„å»ºè¿”å›çš„æŒ‡æ ‡å­—å…¸
    metrics = {
        "policy_loss": policy_loss.sum().item() / total_response_len.item(),
        "kl_penalty": kl_penalty.sum().item() / total_response_len.item(),
        "entropy": entropy.item() / total_response_len.item(),
        "zero_advantages_ratio": zero_advantages.item() / total_response_len.item(),
    }

    return loss, metrics

```

åœ¨ GRPO ä¸­ï¼Œä¼˜åŠ¿ï¼ˆAdvantageï¼‰çš„è®¡ç®—è¿‡ç¨‹æå…¶ç®€å•ï¼Œå…¶æ ¸å¿ƒå°±æ˜¯â€œç»„å†… z-score å½’ä¸€åŒ–â€ï¼Œå¹¶ä¸”ä¸ºäº†æ•°å€¼ç¨³å®šï¼Œæ·»åŠ äº†ä¸€ä¸ªå¾®å°çš„å¸¸æ•° 1e-4ã€‚ä¸‹é¢ä½¿å…¶å®ç°ä»£ç ï¼š

```python
# 1. æ•°æ®æ ¡éªŒä¸åˆ†ç»„

assert len(all_generations) == len(all_finish_reasons) # all_generations æ˜¯æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰å›å¤ï¼Œall_finish_reasons æ˜¯æ¯ä¸ªå›å¤çš„ç»“æŸåŸå› ï¼ˆå¦‚ "stop" æˆ– "length"ï¼‰ï¼Œsamples æ˜¯åŸå§‹è¾“å…¥æ ·æœ¬
assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE # GENERATIONS_PER_SAMPLE æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œè¡¨ç¤ºå¯¹æ¯ä¸ªè¾“å…¥æ ·æœ¬ sampleï¼Œè¦ç”Ÿæˆå¤šå°‘ä¸ªä¸åŒçš„å›å¤ï¼ˆä¾‹å¦‚ 3 ä¸ªï¼‰ã€‚æ‰€ä»¥æ€»å›å¤æ•° = æ ·æœ¬æ•° Ã— æ¯æ ·æœ¬ç”Ÿæˆæ•°ã€‚

# å°†æ‰€æœ‰ç”Ÿæˆçš„å›å¤æŒ‰æ ·æœ¬åˆ†ç»„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ GENERATIONS_PER_SAMPLE=3ï¼Œé‚£ä¹ˆ groups = [[0,1,2], [3,4,5], ...]ï¼Œå…¶ä¸­ [0,1,2] å¯¹åº”ç¬¬ä¸€ä¸ªæ ·æœ¬ç”Ÿæˆçš„ä¸‰ä¸ªå›å¤
groups = [
    list(range(i, i + GENERATIONS_PER_SAMPLE)) for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)
]

# 2. åˆå§‹åŒ–å­˜å‚¨å˜é‡

all_query_token_ids, all_responses_token_ids, all_samples, all_rewards = [], [], [], []
stats = { "response_lengths": [], "rewards": [], "non_stop_rate": [], }

# 3. æ ¸å¿ƒå¾ªç¯ï¼šå¯¹æ¯ä¸ªæ ·æœ¬åŠå…¶ç”Ÿæˆçš„å›å¤ç»„è¿›è¡Œå¤„ç†
# å¯¹äºå½“å‰æ ·æœ¬ sampleï¼Œè·å–å®ƒå¯¹åº”çš„ group_indicesï¼ˆå¦‚ [0,1,2]ï¼‰ï¼Œç„¶åæå–å‡ºè¯¥ç»„çš„ç»“æŸåŸå› ã€token ID å’Œè§£ç åçš„æ–‡æœ¬
for sample, group_indices in zip(samples, groups):
    finish_reasons = [all_finish_reasons[i] for i in group_indices]
    response_token_ids = [all_generations[i] for i in group_indices]
    responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)

    # å¯¹ç»„å†…çš„æ¯ä¸€ä¸ªå›å¤ respï¼Œè°ƒç”¨ compute_reward å‡½æ•°è®¡ç®—å…¶å¥–åŠ±åˆ†æ•°ã€‚compute_reward æ˜¯ä½ è‡ªå®šä¹‰çš„å‡½æ•°ï¼Œæ¯”å¦‚åˆ¤æ–­æ•°å­¦é¢˜æ˜¯å¦ç­”å¯¹ã€ä»£ç æ˜¯å¦èƒ½è¿è¡Œç­‰
    rewards_and_metrics = [compute_reward(resp, sample, EOS_TOKEN) for resp in responses]
    rewards, reward_metrics = zip(*rewards_and_metrics) # zip(*rewards_and_metrics) å°† (reward, metrics) å…ƒç»„è§£åŒ…æˆä¸¤ä¸ªåˆ—è¡¨ï¼šrewards å’Œ reward_metrics

    # 4. å…³é”®æ­¥éª¤ï¼šè®¡ç®—ç»„å†…å½’ä¸€åŒ–çš„â€œä¼˜åŠ¿â€ (Advantages)
    rewards = np.array(rewards)
    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4) # rewards - rewards.mean()ï¼šè®¡ç®—æ¯ä¸ªå›å¤çš„å¥–åŠ±ç›¸å¯¹äºç»„å†…å¹³å‡å€¼çš„åå·®; é™¤ä»¥ç»„å†…æ ‡å‡†å·®ï¼Œå¾—åˆ° z-score; å½“ç»„å†…æ‰€æœ‰å¥–åŠ±éƒ½ç›¸åŒæ—¶ï¼ˆæ ‡å‡†å·®ä¸º 0ï¼‰ï¼Œç›´æ¥é™¤ä»¥ 0 ä¼šå¯¼è‡´ NaN é”™è¯¯ã€‚åŠ ä¸Šä¸€ä¸ªæå°çš„å¸¸æ•° 1e-4 å¯ä»¥é¿å…è¿™ç§æƒ…å†µï¼Œä¿è¯è®¡ç®—ç¨³å®š

    # å°†æ¯ä¸ªå›å¤çš„æ ‡é‡ä¼˜åŠ¿å€¼ resp_adv æ‰©å±•ä¸ºä¸€ä¸ªåºåˆ—ï¼Œä½¿å…¶é•¿åº¦ä¸å›å¤çš„ token æ•°ç›¸åŒ,ç›®çš„æ˜¯ä¸ºäº†è®©ä¼˜åŠ¿ä¿¡å·å¯ä»¥ä¸æ¯ä¸ª token çš„ log-probability å¯¹é½ï¼Œä»è€Œè®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
    per_token_advantages = [[adv] * len(resp) for adv, resp in zip(advantages, response_token_ids)]

    # 5. æ”¶é›†æœ€ç»ˆæ•°æ®å¹¶è¿”å›
    # å°†å½“å‰ç»„çš„æ•°æ®ï¼ˆå¥–åŠ±ã€æ ·æœ¬ã€token IDï¼‰è¿½åŠ åˆ°å…¨å±€åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åç»­ç»Ÿä¸€å¤„ç†
    all_query_token_ids.extend([sample["input_ids"]] * GENERATIONS_PER_SAMPLE)
    all_responses_token_ids.extend(response_token_ids)
    all_advantages.extend(per_token_advantages)

    # è®°å½•ä¸€äº›æœ‰ç”¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚å¹³å‡å¥–åŠ±ã€æœªæ­£å¸¸ç»“æŸçš„æ¯”ä¾‹ã€å›å¤é•¿åº¦ç­‰
    stats["rewards"].extend(rewards)
    stats["non_stop_rate"].extend([fr != "stop" for fr in finish_reasons])
    stats["response_lengths"].extend([len(ids) for ids in response_token_ids])

    for rm in reward_metrics:
        for k, v in rm.items():
            stats.setdefault(f"reward_metrics/{k}", []).append(v)

# å°†æ‰€æœ‰æ•°æ®æ‰“åŒ…æˆä¸€ä¸ªå­—å…¸ episodes è¿”å›ï¼Œä¾›åç»­çš„ compute_pg_loss å‡½æ•°ä½¿ç”¨
episodes = {
        "all_query_token_ids": all_query_token_ids,
        "all_response_token_ids": all_responses_token_ids,
        "all_advantages": all_advantages,
    }

    return episodes, stats
```

#### GRPO çš„å®é™…æ•ˆæœ

GRPO çš„å®é™…æ•ˆæœå¦‚ä½•å‘¢ï¼Ÿä¸‹å›¾å±•ç¤ºäº†åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½ï¼š

<div align="center">
   <img src="images/14-5-grpoä¸å…¶ä»–è®­ç»ƒæ–¹æ³•åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”.png" />
   <p>å›¾14.5 GRPOä¸å…¶ä»–è®­ç»ƒæ–¹æ³•åœ¨ä¸¤ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”</p>
 </div>

å·¦å›¾ GSM8K æ˜¯ä¸€ä¸ªå°å­¦æ•°å­¦åº”ç”¨é¢˜æ•°æ®é›†ï¼Œå³å›¾ MATH è¿™æ˜¯ä¸€ä¸ªæ›´éš¾çš„é«˜ä¸­æ•°å­¦ç«èµ›é¢˜æ•°æ®é›†ã€‚Yè½´æ˜¯å‡†ç¡®ç‡ï¼ˆAcc %ï¼‰ï¼ŒXè½´æ˜¯è®­ç»ƒæ­¥æ•°ï¼ˆStepsï¼‰ã€‚å›¾ä¸­æœ‰å¤šæ¡æ›²çº¿ï¼Œä»£è¡¨ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼š

- RFT (Reinforcing Fine-Tuning)ï¼šè¿™æ˜¯æœ€åŸºç¡€çš„æ–¹æ³•ã€‚å®ƒåªå¥–åŠ±â€œæ­£ç¡®ç­”æ¡ˆâ€ï¼Œè€Œä¸è€ƒè™‘ç”Ÿæˆè¿‡ç¨‹ã€‚å¯ä»¥ç†è§£ä¸ºâ€œåªè¦ç»“æœå¯¹ï¼Œä¸ç®¡è¿‡ç¨‹â€ã€‚åœ¨å›¾ä¸­ç”¨ç´«è‰²çº¿è¡¨ç¤ºã€‚
- Online RFTï¼šè¿™æ˜¯ RFT çš„åœ¨çº¿ç‰ˆæœ¬ï¼Œå¯èƒ½æ„å‘³ç€å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šåŠ¨æ€åœ°é‡‡æ ·å’Œæ›´æ–°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„æ•°æ®é›†ã€‚åœ¨å›¾ä¸­ç”¨ç»¿è‰²çº¿è¡¨ç¤ºã€‚
- GRPO+OS (Group Relative Policy Optimization + Online Sampling)ï¼šè¿™æ˜¯æ ‡å‡†çš„ GRPO æ–¹æ³•ï¼Œå³æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„â€œç»„å†… z-score å½’ä¸€åŒ–â€ä¼˜åŠ¿è®¡ç®—ã€‚åœ¨å›¾ä¸­ç”¨æ©™è‰²çº¿è¡¨ç¤ºã€‚
- GRPO+PS (Group Relative Policy Optimization + Process Supervision)ï¼šè¿™æ˜¯åœ¨ GRPO åŸºç¡€ä¸Šï¼Œé¢å¤–åŠ å…¥äº†â€œè¿‡ç¨‹ç›‘ç£â€ï¼ˆProcess Supervisionï¼‰ã€‚è¿™æ„å‘³ç€ä¸ä»…å¥–åŠ±æœ€ç»ˆç­”æ¡ˆï¼Œè¿˜å¥–åŠ±æ­£ç¡®çš„è§£é¢˜æ­¥éª¤ã€‚åœ¨å›¾ä¸­ç”¨è“è‰²çº¿è¡¨ç¤ºã€‚

ä»ä¸¤å¼ å›¾ä¸­æˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

- GRPO æ˜¾è‘—ä¼˜äº RFTï¼šæ— è®ºæ˜¯ GSM8K è¿˜æ˜¯ MATHï¼Œæ©™è‰²çº¿ï¼ˆGRPO+OSï¼‰å’Œè“è‰²çº¿ï¼ˆGRPO+PSï¼‰éƒ½æ˜æ˜¾é«˜äºç´«è‰²çº¿ï¼ˆRFTï¼‰ã€‚è¿™è¯´æ˜ GRPO ç®—æ³•æœ¬èº«æ˜¯æœ‰æ•ˆçš„ï¼Œå®ƒèƒ½å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ï¼Œä»è€Œè·å¾—æ›´é«˜çš„å‡†ç¡®ç‡ã€‚
- è¿‡ç¨‹ç›‘ç£ï¼ˆPSï¼‰å¸¦æ¥é¢å¤–å¢ç›Šï¼šåœ¨å¤§å¤šæ•°è®­ç»ƒæ­¥æ•°ä¸‹ï¼Œè“è‰²çº¿ï¼ˆGRPO+PSï¼‰ç•¥é«˜äºæ©™è‰²çº¿ï¼ˆGRPO+OSï¼‰ã€‚è¿™è¡¨æ˜ï¼Œå¦‚æœèƒ½å¤Ÿæä¾›å…³äºâ€œè§£é¢˜è¿‡ç¨‹â€çš„ç›‘ç£ä¿¡å·ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯ä»¥å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚
- GRPO çš„ç¨³å®šæ€§ï¼šç›¸æ¯”äºæ³¢åŠ¨è¾ƒå¤§çš„ RFT å’Œ Online RFT æ›²çº¿ï¼ŒGRPO çš„æ›²çº¿ç›¸å¯¹æ›´å¹³æ»‘ï¼Œè¿™åæ˜ äº†å…¶ç®—æ³•è®¾è®¡çš„ç¨³å®šæ€§ã€‚



### 14.2.4 GRPO çš„æ½œåœ¨ç¼ºé™·ï¼šé•¿åº¦åå·® (Length Bias)

è™½ç„¶ GRPO æ•ˆæœæ‹”ç¾¤ï¼Œä½†å­¦æœ¯ç•Œï¼ˆå¦‚ "Dr. GRPO" è®ºæ–‡ï¼‰æŒ‡å‡ºå…¶æ•°å­¦ä¸Šå­˜åœ¨ç‘•ç–µï¼š
1.  **æœ‰åæ¢¯åº¦**ï¼šé™¤ä»¥æ ‡å‡†å·®ï¼ˆstdï¼‰ä½¿å¾—å®ƒä¸å†æ˜¯ä¸€ä¸ªæ— åçš„æ¢¯åº¦ä¼°è®¡ã€‚å®ƒä¼šè¿‡åº¦æ”¾å¤§é‚£äº›â€œå…¨å¯¹â€æˆ–â€œå…¨é”™â€çš„ç®€å•/å›°éš¾æ ·æœ¬çš„æƒé‡ï¼ˆå› ä¸º std å¾ˆå°ï¼‰ã€‚
2.  **é•¿åº¦æ¿€åŠ±**ï¼šå¦‚æœæ¨¡å‹å‘ç°è‡ªå·±ç­”é”™äº†ï¼ˆå¥–åŠ±ä¸ºè´Ÿï¼‰ï¼Œä¸ºäº†æœ€å¤§åŒ– $A_i$ï¼Œå®ƒä¼šå€¾å‘äºç”Ÿæˆ**æé•¿**çš„åºŸè¯ï¼ˆå¦‚æœå¥–åŠ±æœªåšé•¿åº¦å½’ä¸€åŒ–ï¼‰ã€‚åä¹‹ï¼Œå¦‚æœç­”å¯¹äº†ï¼Œå®ƒä¼šå€¾å‘äºæçŸ­çš„å›ç­”ã€‚

> **ğŸ’¡ é¡¿æ‚Ÿæ—¶åˆ» (Aha Moment) è¿˜æ˜¯ é•¿åº¦åç½®ï¼Ÿ**
> DeepSeek R1 æŠ¥å‘Šä¸­æåˆ°çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼ˆæ¨¡å‹åœ¨é•¿æ€ç»´é“¾ä¸­è‡ªæˆ‘çº é”™ï¼‰ä»¤äººå…´å¥‹ã€‚ä½†ä¹Ÿæœ‰ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§é•¿æ€ç»´é“¾çš„å‡ºç°ï¼Œéƒ¨åˆ†åŸå› å¯èƒ½æ˜¯ GRPO çš„å¥–åŠ±æœºåˆ¶æ— æ„ä¸­æ¿€åŠ±äº†æ¨¡å‹è¾“å‡ºæ›´é•¿çš„å†…å®¹ã€‚

---


## 14.3 æ¡ˆä¾‹ç ”ç©¶

### 14.3.1 DeepSeek R1

DeepSeek R1 çš„æˆåŠŸè¯æ˜äº†**çº¯å¼ºåŒ–å­¦ä¹ **åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚

### 14.4.1 R1-Zeroï¼šçº¯ç²¹çš„ RL
*   **è®¾ç½®**: ç›´æ¥åœ¨ Base æ¨¡å‹ï¼ˆDeepSeek-V3ï¼‰ä¸Šè¿è¡Œ GRPOã€‚
*   **å¥–åŠ±**:
    *   **å‡†ç¡®æ€§å¥–åŠ±**: ç­”æ¡ˆå¯¹ä¸å¯¹ï¼Ÿï¼ˆé€šè¿‡è§„åˆ™åŒ¹é…æˆ–ç¼–è¯‘å™¨éªŒè¯ï¼‰ã€‚
    *   **æ ¼å¼å¥–åŠ±**: å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨ `<think>` å’Œ `</think>` æ ‡ç­¾åŒ…è£¹æ€ç»´è¿‡ç¨‹ã€‚
*   **ç°è±¡**:
    *   **Aha Moment (é¡¿æ‚Ÿæ—¶åˆ»)**: æ¨¡å‹åœ¨è®­ç»ƒä¸­æœŸå¼€å§‹å­¦ä¼šè‡ªæˆ‘åæ€ï¼ˆSelf-correctionï¼‰ï¼Œä¾‹å¦‚â€œç­‰ç­‰ï¼Œæˆ‘ç®—é”™äº†ï¼Œåº”è¯¥é‡æ–°å°è¯•...â€ã€‚
    *   **è¯­è¨€æ··æ‚**: ç”±äºæ²¡æœ‰è¯­è¨€ä¸€è‡´æ€§çº¦æŸï¼Œæ¨¡å‹ä¼šåœ¨æ€ç»´é“¾ä¸­æ··åˆä½¿ç”¨ä¸­è‹±æ–‡ã€‚

####  R1 æ­£å¼ç‰ˆæµæ°´çº¿
ä¸ºäº†è§£å†³ R1-Zero çš„å¯è¯»æ€§é—®é¢˜å¹¶æå‡æ€§èƒ½ï¼ŒR1 é‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒï¼š
1.  **å†·å¯åŠ¨ (Cold Start)**: ä½¿ç”¨å°‘é‡é«˜è´¨é‡çš„é•¿æ€ç»´é“¾æ•°æ®ï¼ˆLong CoTï¼‰è¿›è¡Œ SFTã€‚è¿™è®©æ¨¡å‹åœ¨ RL ä¹‹å‰å°±å­¦ä¼šäº†â€œåƒäººä¸€æ ·æ€è€ƒâ€çš„æ ¼å¼ï¼Œè§£å†³äº†è¯­è¨€æ··æ‚å’Œä¸å¯è¯»é—®é¢˜ã€‚
2.  **æ¨ç† RL (Reasoning RL)**: ä½¿ç”¨ GRPO è¿›è¡Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼Œä¸“æ³¨äºæ•°å­¦ã€ä»£ç ç­‰å¯éªŒè¯ä»»åŠ¡ã€‚
3.  **é€šç”¨ RLHF**: åŠ å…¥é€šç”¨ä»»åŠ¡ï¼ˆå†™ä½œã€é—®ç­”ï¼‰ï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…ä¼šåšé¢˜ï¼Œè¿˜èƒ½ä»¥æ­¤ä¸ºåŸºç¡€è¿›è¡Œé€šç”¨å¯¹è¯ã€‚

**ğŸš€ è’¸é¦ (Distillation)**: R1 çš„å¦ä¸€ä¸ªå·¨å¤§è´¡çŒ®æ˜¯è¯æ˜äº†**å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯ä»¥è’¸é¦ç»™å°æ¨¡å‹**ã€‚ä½¿ç”¨ R1 ç”Ÿæˆçš„ 800k æ¡æ•°æ®å¾®è°ƒ Qwen-7Bï¼Œèƒ½ä½¿å…¶æ•°å­¦èƒ½åŠ›ä» 50% é£™å‡è‡³ 80%+ã€‚


### 14.3.2 Kimi k1.5ï¼šé•¿åº¦æ§åˆ¶ä¸ DPO å˜ä½“
Kimi k1.5 ä¸ R1 åŒæœŸå‘å¸ƒï¼Œå±•ç¤ºäº†å¦ä¸€ç§è·¯å¾„ï¼š
*   **ç®—æ³•**: ä½¿ç”¨äº†ä¸€ç§ç±»ä¼¼ DPO çš„ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡æœ€å°åŒ–å¹³æ–¹æŸå¤±æ¥é€¼è¿‘æœ€ä¼˜ç­–ç•¥ã€‚
*   **é•¿åº¦æ§åˆ¶**: ä»–ä»¬æ„è¯†åˆ°æ¨ç†æˆæœ¬æ˜¯å…³é”®ã€‚å¼•å…¥äº† **Length Reward**ï¼š
    *   å¦‚æœç­”å¯¹ï¼Œå¥–åŠ±è¶ŠçŸ­è¶Šå¥½ã€‚
    *   å¦‚æœç­”é”™ï¼Œå¥–åŠ±ä¿æŒä¸­ç­‰é•¿åº¦ï¼ˆé˜²æ­¢æ¨¡å‹å› æ— æ³•ç­”å¯¹è€Œç›´æ¥æ”¾å¼ƒè¾“å‡ºï¼Œé™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰ã€‚
*   **åŸºç¡€è®¾æ–½**: è®ºæ–‡è¯¦ç»†è®¨è®ºäº†å¦‚ä½•è§£è€¦è®­ç»ƒï¼ˆActorï¼‰å’Œæ¨ç†ï¼ˆRolloutï¼‰ï¼Œä½¿ç”¨ vLLM è¿›è¡Œé«˜æ•ˆé‡‡æ ·ã€‚

### 14.3.3 Qwen 3ï¼šæ€ç»´æ¨¡å¼èåˆ
Qwen 3 æå‡ºäº† **Thinking Mode Fusion**ï¼Œè¯•å›¾åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­èåˆâ€œæ€è€ƒâ€ä¸â€œä¸æ€è€ƒâ€ä¸¤ç§æ¨¡å¼ï¼š
*   **è®­ç»ƒ**: æ··åˆä½¿ç”¨å¸¦ `<think>` çš„æ•°æ®å’Œç›´æ¥è¾“å‡ºç­”æ¡ˆçš„æ•°æ®ã€‚
*   **æ•ˆæœ**: ç”¨æˆ·å¯ä»¥é€šè¿‡ Prompt æ§åˆ¶æ¨¡å‹æ˜¯å¦è¿›è¡Œé•¿æ¨ç†ã€‚
*   **æµ‹è¯•æ—¶è®¡ç®— (Test-time Compute)**: å¯ä»¥åœ¨æ¨ç†é˜¶æ®µé€šè¿‡æˆªæ–­ `<think>` è¿‡ç¨‹æ¥åŠ¨æ€è°ƒæ•´è®¡ç®—é‡å’Œæ€§èƒ½çš„å¹³è¡¡ã€‚

---

##  æ€»ç»“ä¸å±•æœ›

1.  **RLVR æ˜¯è¶‹åŠ¿**: åœ¨æ•°å­¦ã€ä»£ç ç­‰æ‹¥æœ‰ **Ground Truth** çš„é¢†åŸŸï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆOutcome-based RLï¼‰å·²ç»è¶…è¶Šäº†å•çº¯çš„ SFTã€‚
2.  **GRPO çš„èƒœåˆ©**: ç®€å•çš„ç®—æ³•å¾€å¾€æ›´æœ‰æ•ˆã€‚ç§»é™¤ Value Model æå¤§åœ°é™ä½äº†è®­ç»ƒé—¨æ§›ã€‚
3.  **æ•°æ®ä¸ºç‹**: æ— è®ºæ˜¯ R1 çš„å†·å¯åŠ¨æ•°æ®ï¼Œè¿˜æ˜¯ Kimi çš„éš¾åº¦ç­›é€‰ï¼Œé«˜è´¨é‡ã€é«˜éš¾åº¦çš„ Prompt é›†åˆæ˜¯ RL æˆåŠŸçš„å…³é”®ã€‚
4.  **è¿‡ç¨‹å¥–åŠ± (PRM) vs. ç»“æœå¥–åŠ± (ORM)**: ç›®å‰ R1 ç­‰æ¨¡å‹ä¸»è¦ä¾èµ–ç»“æœå¥–åŠ±ã€‚è™½ç„¶è¿‡ç¨‹å¥–åŠ±ï¼ˆæ¯ä¸€æ­¥éƒ½æ‰“åˆ†ï¼‰ç†è®ºä¸Šæ›´å¥½ï¼Œä½†æ ‡æ³¨æˆæœ¬æé«˜ä¸”éš¾ä»¥è‡ªåŠ¨åŒ–ï¼Œç›®å‰å°šæœªåœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­å±•ç°å‡ºè¶…è¶Š ORM çš„ç»Ÿæ²»åŠ›ã€‚


---

### ğŸ“š å‚è€ƒèµ„æ–™
*   [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948)
*   [Kimi k1.5 Technical Report](https://github.com/MoonshotAI/Kimi-k1.5)
*   [PPO Algorithm (OpenAI)](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
*   [Understanding GRPO (HuggingFace Blog)](https://huggingface.co/blog/grpo)
