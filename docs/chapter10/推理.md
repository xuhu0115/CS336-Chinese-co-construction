# 推理

推理是大型语言模型真正发挥价值的核心环节，是训练中积累的知识得以转化为实际输出的唯一途径。无论对话、搜索、问答或代码生成，所有能力都在推理中被激活与呈现。推理的速度、成本与稳定性不仅决定用户体验，更决定模型能否被大规模、低成本地部署，从而影响产品化与商业化能力。因此，深入理解推理机制与其系统瓶颈是让LLM从“能训练”走向“能落地”的关键一步。

# 10.1 推理与训练
训练与推理虽各自受不同瓶颈约束，却并非独立阶段；训练塑造能力，推理呈现能力，而推理的需求又反过来影响训练设计。二者共同构成了构建大规模语言模型的完整系统循环。

## 10.1.1 推理与训练的差异
在LLM中，训练与推理都涉及模型的前向计算，但其目标、计算模式和资源瓶颈存在本质差异。对于 `自回归生成任务`，假设需要生成第 $i$ 个token，则模型在该位置的预测依赖其之前的所有token：

$$
P(\text{token}_i \mid \text{token}_1, \text{token}*2, \dots, \text{token}*{i-1})
$$

**训练与推理的预测下一个词的依据对比**
| 阶段     | 下一步预测 $token_i$ 的输入来源                                                             |
| ------ | ---------------------------------------------------------------------- |
| **训练** | 来自真实标签（ground-truth）及其之前的输入序列，模型可利用完整序列并行计算，同时通过因果掩码保证每个位置只访问前面token。 |
| **推理** | 来自模型先前预测生成的token及其之前的文本，每步预测依赖前一步输出必须逐步生成（auto-regressive）。           |


**1. 训练阶段**

训练阶段的主要目标是 **优化模型参数**，训练需要执行完整的前向与反向传播，保留中间激活用于梯度计算，并在多 GPU 间同步梯度。因此训练过程高度依赖算力，瓶颈主要来自：

- 大规模矩阵乘法的**FLOPs**；
- 多张显卡并行运行涉及的**跨设备、统一设备内通信量**；
- 激活、梯度和优化器状态带来的**额外内存占用**。

> 训练阶段的显存占用主要由四部分组成：激活、梯度、优化器状态以及模型参数。

对于训练阶段的自回归 [Transformer](https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.1387.search.video_card.click&vd_source=17b9d3dcea2f2934b89c09ac53cc1d64)，我们输入完整目标序列并使用 `因果掩码（causal mask）`：每个位置只能访问前面token（真实标签），无法看到未来token。由于训练输入是已知的完整序列，模型可以将整个序列视作一个大批次，通过一次或少量大矩阵乘法**并行计算所有位置的前向表示**，同时因果掩码保证自注意力的顺序依赖。中间激活会被保留用于反向传播与参数更新。正因如此，尽管自注意力涉及成对位置的注意力计算，训练仍能在时间维度**高度并行**，这也是Transformer在大规模训练中高效利用GPU的关键原因。


**2. 推理阶段**

推理阶段的目标**使用固定参数生成输出序列**。推理通常采用**自回归生成方式**，生成每个`token`时，模型需要参考之前预测生成的所有token的信息，也就是每一层的Key、Value（KV cache）。

可以把LLM的推理过程想象成在写作文：每写一个字都要回头看前面已经写好的内容。随着上下文长度增加，这些“参考资料”也越来越多，占用的显存越来越大，而且每次访问都需要时间，就像翻阅厚厚笔记本一样慢。因此，**推理的速度并不主要受算力限制，而是受显存容量和内存带宽的制约**。

**总的来说就是：训练的瓶颈在算力，而推理的瓶颈在显存与带宽**。

> 为什么在自回归LLM的训练阶段，模型预测下一个token时通常以真实标签的前序tokens作为输入即`teacher-forcing`，而不是以模型先前预测的token作为输入？
>
>在训练阶段，使用teacher-forcing有两个主要原因：一方面，它提供稳定且明确的监督信号，便于模型快速收敛；另一方面，它允许对整段序列进行并行计算，从而大幅提高训练效率。如果在训练中将模型先前预测的token作为下一步输入，则可能导致隐藏状态中累积预测偏差。此外，由于采样（预测生成的token）操作是非微分的，会破坏标准的基于最大似然的逐位置损失计算和反向传播，从而增加训练难度。

**举例：训练 vs 推理并行对比图**
```
训练阶段（全序列并行）
----------------------------------------------------
输入序列：   x1    x2    x3    x4
模型前向：   ┌───┐ ┌───┐ ┌───┐ ┌───┐
             │f(x1)│ │f(x2)│ │f(x3)│ │f(x4)│  <- 并行计算
             └───┘ └───┘ └───┘ └───┘
输出序列：   y1    y2    y3    y4
说明：所有token同时计算，依赖使用causal mask保证不看未来。

推理阶段（自回归生成，逐token）
----------------------------------------------------
初始输入：  x1
Step 1:     y1 = Model(x1)
Step 2:     y2 = Model(x1, y1)
Step 3:     y3 = Model(x1, y1, y2)
Step 4:     y4 = Model(x1, y1, y2, y3)
说明：每一步生成依赖前面所有已生成的token，必须顺序执行。

```

## 10.1.2 训练与推理的联系

尽管训练与推理在操作机制、性能瓶颈和优化方向上存在明显差异，但二者在整个模型生命周期中紧密关联。

首先，**训练的根本目的在于优化推理行为**。
无论采用监督学习、预训练还是对齐方法，其结果最终体现在推理阶段的生成质量、稳定性和一致性上。推理是模型面向用户呈现所有能力的方式，因此训练过程的每一项设计——数据选择、损失函数、正则化方法——最终都影响推理表现。

其次，**推理本身是训练过程不可或缺的一部分**。
模型验证、能力评估、指令遵循测试等都依赖推理过程。在 RLHF 或基于奖励的训练中，模型必须通过推理生成响应，再利用这些响应计算奖励并更新参数。因此，从系统角度来看，推理并非仅出现在部署环节，而是贯穿训练的整个迭代周期。

最后，**训练中对模型结构的设计决定推理的可优化性**。
推理端的内存瓶颈要求模型具备良好的结构性特征，例如：

* GQA/MQA 减少 KV 头数量；
* 更低维度的 Key/Value 表示；
* 训练阶段的蒸馏或量化感知训练；
* 便于缓存或重计算的注意力结构。

这些设计往往必须在训练前便已确定，而无法在推理阶段临时补救。因此，推动模型推理效率的提升，需要在训练阶段就纳入系统性的结构设计考虑。


