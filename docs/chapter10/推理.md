# 推理

推理是大型语言模型真正发挥价值的核心环节，是训练中积累的知识得以转化为实际输出的唯一途径。无论对话、搜索、问答或代码生成，所有能力都在推理中被激活与呈现。推理的速度、成本与稳定性不仅决定用户体验，更决定模型能否被大规模、低成本地部署，从而影响产品化与商业化能力。因此，深入理解推理机制与其系统瓶颈是让LLM从“能训练”走向“能落地”的关键一步。

# 10.1 推理与训练
训练与推理虽各自受不同瓶颈约束，却并非独立阶段；训练塑造能力，推理呈现能力，而推理的需求又反过来影响训练设计。二者共同构成了构建大规模语言模型的完整系统循环。

## 10.1.1 推理与训练的差异
在LLM中，训练与推理都涉及模型的前向计算，但它们的目标、计算模式和资源瓶颈存在本质差异。对于`自回归推理`或`并行化训练`，假设需要生成第 $i$ 个token，则模型在该位置的条件概率可以统一写为：

$$
P(\text{token}_i \mid \text{token}_1, \text{token}_2, \dots, \text{token}_{i-1})
$$

>训练与推理在单步预测的数学形式上相同 —— 都是对词表做概率分布估计并从中选择，但**差别在于 $token_i$ 前文的来源（真实vs生成）以及是否把预测结果作为后续输入和是否能并行化**。


**训练与推理的预测下一个词的依据对比**
| 阶段     | 下一步预测 $token_i$ 的输入来源                                                             |
| ------ | ---------------------------------------------------------------------- |
| **训练** | 来自真实标签（ground-truth）及其之前的输入序列，模型可利用完整序列并行计算，同时通过因果掩码保证每个位置只访问前面token。 |
| **推理** | 来自模型先前预测生成的token及其之前的文本，每步预测依赖前一步输出必须逐步生成（auto-regressive）。           |


**1. 训练阶段**

训练阶段的主要目标是 **优化模型参数**，训练需要执行完整的前向与反向传播，保留中间激活用于梯度计算，并在多 GPU 间同步梯度。因此训练过程高度依赖算力，瓶颈主要来自：

- 大规模矩阵乘法的**FLOPs**；
- 多张显卡并行运行涉及的**跨设备、统一设备内通信量**；
- 激活、梯度和优化器状态带来的**额外内存占用**。

> 训练阶段的显存占用主要由四部分组成：激活、梯度、优化器状态以及模型参数。

对于训练阶段的自回归 [Transformer](https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.1387.search.video_card.click&vd_source=17b9d3dcea2f2934b89c09ac53cc1d64)，我们输入完整目标序列并使用 `因果掩码（causal mask）`：每个位置只能访问前面token（真实标签），无法看到未来token。由于训练输入是已知的完整序列，模型可以将整个序列视作一个大批次，通过一次或少量大矩阵乘法**并行计算所有位置的前向表示**，同时因果掩码保证自注意力的顺序依赖。中间激活会被保留用于反向传播与参数更新。正因如此，尽管自注意力涉及成对位置的注意力计算，训练仍能在时间维度**高度并行**，这也是Transformer在大规模训练中高效利用GPU的关键原因。


**2. 推理阶段**

推理阶段的目标**使用固定参数生成输出序列**。推理通常采用**自回归生成方式**，生成每个`token`时，模型需要参考之前预测生成的所有token的信息，也就是每一层的Key、Value（KV cache）。

可以把LLM的推理过程想象成在写作文：每写一个字都要回头看前面已经写好的内容。随着上下文长度增加，这些“参考资料”（KV cache）也越来越多，占用的显存越来越大，而且每次访问都需要时间，就像翻阅厚厚笔记本一样慢。因此，**推理的速度并不主要受算力限制，而是受显存容量和内存带宽的制约**。

**总的来说就是：训练的瓶颈在算力，而推理的瓶颈在显存与带宽**。

> 为什么在自回归LLM的训练阶段，模型预测下一个token时通常以真实标签的前序tokens作为输入即`teacher-forcing`，而不是以模型先前预测的token作为输入？
>
>在训练阶段，使用teacher-forcing有两个主要原因：一方面，它提供稳定且明确的监督信号，便于模型快速收敛；另一方面，它允许对整段序列进行并行计算，从而大幅提高训练效率。如果在训练中将模型先前预测的token作为下一步输入，则可能导致隐藏状态中累积预测偏差。此外，由于采样（预测生成的token）操作是非微分的，会破坏标准的基于最大似然的逐位置损失计算和反向传播，从而增加训练难度。

**举例：训练 vs 推理并行对比图**
```
训练阶段（全序列并行化，使用ground-tru原有的前文）
----------------------------------------------------
输入序列：     x1            x2           x3            x4
模型前向：    ┌────┐      ┌────┐       ┌────┐         ┌────┐
             │f(x1)│     │f(x1,x2)│   │f(x1,x2,x3)│  │f(x1,x2,x3,x4)│
             └────┘      └────┘       └────┘         └────┘
输出序列：     y1           y2           y3             y4
说明：
1. 所有token的预测y_i可并行计算（Transformer一次前向即可得到全部y_i）。
2. 位置i的预测条件为真实前文（x1...x{i-1}），因causal mask保证无法看到未来token。
3. 预测y_i仅用于计算训练损失例如cross-entropy，不会作为下一步输入。

----------------------------------------------------
推理阶段（自回归生成，逐token依赖模型生成的前文）
初始输入：   x1  
Step 1:     y1 = f(x1)
Step 2:     y2 = f(x1, y1)
Step 3:     y3 = f(x1, y1, y2)
Step 4:     y4 = f(x1, y1, y2, y3)
说明：
1每一步都必须等待上一步完成，无法并行。
2. 位置 i 的前文是模型自己已生成的token（不是ground-truth）。
3. y_i用作下一步输入并不断扩展序列。

----------------------------------------------------

```

在LLM的推理阶段，每一次生成一个token都依赖于之前已经生成的所有token，因此生成token的流程在时间维度上具有严格的顺序性，无法并行一次性输出多个token。但需要强调的是，这并不意味着推理内部的计算无法并行，每一步在计算新的KV、注意力权重以及前馈网络时，这些操作本质上仍是大型矩阵运算，完全可以在GPU上进行高度并行化处理。**因此，推理阶段的特点可以概括为在时间维度上是顺序的（自回归约束），在计算维度上是并行的（矩阵运算加速）**，训练阶段可以并行处理整个序列而推理阶段只能顺序处理生成token，但每个token的计算内部依然高效并行化。



## 10.1.2 训练与推理的联系

尽管训练与推理在操作机制、性能瓶颈和优化方向上存在明显差异，但二者在整个模型生命周期中紧密关联。

- **训练的根本目的在于优化推理行为**。
  无论采用监督学习、预训练、监督微调还是对齐方法，其结果最终体现在推理阶段的生成质量、稳定性和一致性上。推理是模型面向用户呈现所有能力的方式，因此训练过程的每一项设计——数据选择、损失函数、正则化方法等方法最终都影响推理表现。

- **推理本身是训练过程不可或缺的一部分**。
  模型验证、能力评估、指令遵循测试等都依赖推理过程。在训练中，模型必须通过推理生成响应，再利用这些响应计算奖励或者损失并更新参数。因此，从系统角度来看推理并非仅出现在部署环节，而是贯穿训练的整个迭代周期。

- **训练中对模型结构的设计决定推理的可优化性**。
  推理阶段受限于显存、KV缓存大小、延迟要求、带宽等多方面瓶颈，因此模型在训练阶段的结构设计必须为推理的可执行性和性能提供保障。
  
    - **结构设计影响推理的资源占用**。
    例如Transformer的结构、注意力复杂度、参数规模、MoE的专家布局、是否有KV压缩等，都会在推理时决定显存占用与延迟表现。设计不当的结构可能在训练时表现良好，但在推理时根本无法高效运行。
  
    - **结构设计影响推理的可优化空间**。
    是否能够使用FlashAttention、长上下文机制、推理加速策略等，都依赖模型结构在训练阶段是否为这些优化留下空间，一个推理友好的结构设计能在部署阶段获得更高吞吐、更低延迟和更强的扩展性。这些设计往往必须在训练前便已确定，而无法在推理阶段临时补救。因此，推动模型推理效率的提升，需要在训练阶段就纳入系统性的结构设计考虑。

>FlashAttention依然是全局注意力机制，但将原本需要一次性计算和存储的大型注意力矩阵拆分为多个可在高速片上缓存中处理的小块，通过分块计算显著减少显存读写量，从而降低内存访问开销并显著提升推理与训练效率。

# 10.2 推理原理分析

训练是一次性的成本，而推理却会被不断重复执行，随着大模型真正落地到各种应用场景，推理的规模正在急速扩大。Sam Altman曾公开提到，OpenAI现在每天要生成超过1000亿个词；像Cursor这种成熟的AI开发工具，每天也会产出数十亿行实际被用户采纳的代码。这些数字说明了一个事实：推理已经成为大模型成本中的主力，而非训练。为了更清晰地理解推理的效率好不好，我们通常从三个指标来衡量——`首标记生成时间`（TTFT）、`延迟`（Latency）、`吞吐量`（Throughput）。

  - TTFT：用户从发出请求到看到第一个token的时间，是交互体验中最关键的等待成本。
  - Latency：在首标记生成后，后续token的输出速度，决定了模型“说话是否流畅”。
  - Throughput：系统在单位时间内能生成的总token数，适用于批量处理和服务整体效率评估。

>需要注意的是，**高吞吐量并不等于低延迟**。在大规模推理系统中，系统总体吞吐量可以很高即每秒产出大量token，但个别对话请求的响应时间仍可能很长——原因包括上下文特别长、为该请求分配的计算资源较少、或请求在调度队列中等待等。`吞吐量`衡量的是系统层面的总体效率，而`延迟`关注的是单个用户或单次对话请求的体验，因为用户直接感受到的是其带来的等待。

## 10.2.1 Transfomer

<div align="center">
<img width="1260" height="500" alt="48108f8a684becfe656b925ff929365f" src="https://github.com/user-attachments/assets/3e0b268a-1700-4dcd-b7ef-aff88e3cd5c9" />
   <p>图10.1 Transfomer推理过程</p>
 </div>
 
在[自回归Transformer](https://jax-ml.github.io/scaling-book/inference/)的LLM推理中，一个核心优化点在于对已经存在的上下文，不需要每生成一个token就重新做完整前向计算。为此，推理通常分为两个阶段：**预填充**和**逐步生成**。

<div align="center">
<img width="1200" height="600" alt="2d8603a3413b3e848e256268422914ca" src="https://github.com/user-attachments/assets/bc301529-045d-4b35-ad59-dc440eea09c4" />
   <p>图10.2 自回归Transfomer推理</p>
 </div>

- **预填充阶段**，模型一次性处理用户输入的所有prompt token，并在每一层、每个注意力头中计算出这些token的Key与Value，记为 $K, V$ 。这些向量随后会被保存进**KV Cache**的结构中。保存KV的目的很简单——后续每生成一个新token时，我们只需要对这个新token做一次线性投影得到Query（Q），再让Q去与已经缓存好的所有K做点积，并用已有的V做加权求和即可。这样就不需要反复对旧token重新做线性映射与矩阵乘法，从而避免了大量重复计算。

- **生成阶段**，模型用上一步得到的新token embedding生成它的Query，执行

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V
$$


其中 $(K,V)$ 全部来自KV Cache，得到的输出logits是模型对**词汇表中每个token的未归一化分数**，其经过softmax归一化后就是词汇表的概率分布，用于采样生成下一个token。然后把这个新token在各层的新 $(K,V)$ 继续追加到缓存中，循环直到终止符或达到最大长度。

>KV Cache会占用显存，其规模大致与`token数 × 层数 × 注意力头数 × 隐藏层维度`成正比。

需要注意：**KV Cache只适用于自回归的推理模式**，因为在自回归设置中过去的token一旦计算完成就不会被未来token改变，故其对应的 $(K,V)$ 可以安全复用；而对于**双向注意力**如BERT、encoder-only等或者**频繁修改输入上下文**的场景如掩码语言模型、文本编辑器等，任一token的变动都会影响其他token的表示与Query–Key的相对关系，从而改变注意力权重即 $QK^\top$ 和应加权的 $V$ 。因此在这些非自回归推理场景中，已缓存的 $(K,V)$ 不能保持有效——一旦序列有改动就必须重新计算全部 $(K,V)$ ，其**不适合使用 KV Cache**。

>为什么当前大多数生成型模型如GPT系列、LLaMA系列、DeepSeek系列等采用自回归注意力架构，而不是像BERT那样的双向注意力?
>
>1. 高效逐步生成 自回归模型按顺序预测下一个token，可用KV Cache前面计算的Key、Value，避免重复计算，实现流式生成和在线交互的高效推理。
>2. 全局一致性 vs 计算开销 双向attention能访问前后上下文，提高理解和连贯性，但生成时需要多次全序列attention、迭代更新，计算和内存开销大，交互延迟高。  
>3. 工程折中与扩展性 自回归架构训练易扩展，虽然牺牲了即时全局修改能力，但在大规模部署中是最实用的折中方案，双向或非自回归方法则通过一些策略尝试弥补这些不足。


目前主流的大语言模型推理体系采用的是decoder-only、基于自回归的Transformer。这种结构在推理过程中可以使用KV Cache来复用历史token的Key、Value，从而避免重复计算。但随着上下文变长，KV Cache会带来显著的显存与带宽开销，因此近年来出现了许多为改进长上下文推理效率或降低KV负担等问题的[Transformer变体比如GQA、MHA、MLA、稀疏注意力机制、DSA等](https://github.com/xuhu0115/CS336-Chinese-co-construction/blob/main/docs/chapter4/chapter4_%E7%AC%AC%E5%9B%9B%E7%AB%A0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%92%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82.md)。

**MINMAX的线性注意力机制+局部注意力机制**


线性注意力机制通过将传统的点积Softmax注意力线性化，实现了在序列处理中的高效计算。在训练阶段，其复杂度为 $O(n d^2)$ ，并且在推理阶段可以通过递归更新累积项 $\sum K^\top V$ 来高效生成序列，这相比传统Softmax注意力在推理阶段仍然需要 $O(n^2 d)$ 的计算，显得尤为高效。


在自回归生成场景中，我们希望模型在生成每个词时**只能看到前面已经生成的词**，不能“偷看”后面的词。如果直接使用 $\phi(Q) (\sum K^\top V)$ 的方法，相当于把整句话的所有词的关键信息一次性混在一起再去计算每个词的输出，这会导致模型在计算当前词时**意外看到未来的词信息**，违反了自回归规则。为了避免这种情况，需要在计算时**一步一步累加**：每生成一个词，就把当前词的`key、value`加到累加器里，下一步生成下一个词时只使用已经累加过的前缀信息。这样，每个词的输出都只依赖前面已经生成的词，而看不到未来。

>逐步累加的方法**不能像训练或全句并行那样一次性快速计算所有词**，必须按顺序一点一点生成，因此推理速度比全序列并行慢。但相比局部注意力机制，线性注意力仍然在推理阶段具有明显的计算优势，因为它在每一步的计算量是线性增长的，而非平方增长。

因此，尽管线性注意力早在九年前就被提出由于其设计比较复杂，目前主流开源大语言模型——包括LLaMA3、Qwen2.5、DeepSeekV3和Mistra等仍未将线性注意力作为自回归生成的默认方案。

```python
import numpy as np

# ---- 配置 ----
L = 5          # 序列长度 
d = 2          # 特征维度
np.random.seed(42) # 设置随机种子以保证结果可复现

# 随机生成 Q, K, V 矩阵，形状都是(L, d)
Q = np.random.rand(L, d)
K = np.random.rand(L, d)
V = np.random.rand(L, d)

# 核映射φ(x)，这里用ReLU激活函数作为核函数的近似。
# 在线性注意力中，使用核函数来近似softmax
# 目的是将QK^T形式的二次复杂性O(L^2)降低为O(L * d)
def phi(x):
    # ReLU
    return np.maximum(x, 0)

# ---- 错误方式：右乘 K^T V (包含未来信息、非因果计算) ----
# phi(K)是形状(L, d)的矩阵，phi(K).T是形状(d, L)的矩阵，V是形状(L, d)的矩阵
KV_all = (phi(K).T @ V) 

# 计算输出 Y
# 分子：phi(Q) (L, d)乘以KV_all (d, d)得到(L, d)，这是未经归一化的Attention结果。
Numerator = phi(Q) @ KV_all

# 计算全局归一化因子Z
# phi(K).sum(axis=0, keepdims=True) 对K沿着序列长度L求和，得到 (1, d)
# .T 转换为 (d, 1)
# 这里的归一化因子 Z 也是全局计算的，没有考虑因果性。
Denominator = phi(Q) @ phi(K).sum(axis=0, keepdims=True).T

# Y_wrong：最终结果(L, d)，每一行Y_wrong[i]都受到了整个序列K和V的影响
Y_wrong = Numerator / Denominator
print("错误方式 (包含未来信息)：")
print(np.round(Y_wrong, 3))

# ---- 正确方式：前缀累加器 (只用前缀、因果计算) ----
# 这种方式是自回归模型在推理时应采用的线性注意力计算方法。
# S: 累加器，用于存储 K^T V 的前缀和，S的形状是(d, d)，用于累积K^T V的乘积
S = np.zeros((d, d)) 
# Z: 归一化因子累加器，Z的形状是(d, 1)，用于累积K向量之和
Z = np.zeros((d, 1)) 
Y_correct = []

# 遍历序列中的每一个位置i
for i in range(L):
    # 取出当前位置 i 的 K, V, Q 向量，并应用核映射
    # ki, vi, qi形状均为(d, 1)
    ki, vi, qi = phi(K[i:i+1]).T, V[i:i+1].T, phi(Q[i:i+1]).T
    
    # 增量更新 S：S += K[i]^T @ V[i]
    # S累积了从0到i位置的K^T V之和
    S += ki @ vi.T            
    
    # 增量更新 Z：Z += K[i]^T
    # Z累积了从0到i位置的K向量之和
    Z += ki                   
    
    # 计算当前位置i的输出y_i，分子：qi^T @ S_i；分母：qi^T @ Z_i
    # y_i形状是(1, d)，只依赖于i之前（含 i）的信息
    y_i = (qi.T @ S) / (qi.T @ Z)
    
    Y_correct.append(y_i.flatten())

Y_correct = np.array(Y_correct)
print("\n正确方式 (只用前缀)：")
print(np.round(Y_correct, 3))

# ---- 对比差异 ----
print("\n差异 (错误 - 正确)：")

# 理论上，Y_wrong[i]应该包含Y_correct[i]加上未来信息的影响。
print(np.round(Y_wrong - Y_correct, 3))
```

## 10.2.2 分析算术强度

其中：

- B表示批大小，表示一次处理的文本序列数量（推理通常B=1）；
- D表示模型的隐藏维度,即token embdding维度；
- F是前馈网络中的中间维度，通常约为D的四倍。

算术强度是一个核心指标，用于衡量一个计算任务的计算密集程度，并预测该任务的性能是受限于计算能力还是内存带宽，其计算原理为：

$$
I = \frac{\text{FLOPs}}{\text{Bytes Transferred}}
$$

- FLOPs：浮点运算次数，即算法或任务中所需的加法、乘法等操作的总量。
- Bytes Transferred：显存数据传输量，即执行该任务时，需要从主存例如GPU的HBM显存等或缓存中读取和写入的数据总量（以字节为单位）。
  
```python
"""
计算操作量与数据传输量的比值，来评估一个矩阵乘法操作是计算受限（Compute-bound）还是内存带宽受限（Memory-bound），并与特定硬件NVIDIA H100的理论算术强度进行比较。
"""
def arithmetic_intensity(B, D, F, bytes_per_elem=2):
    """
    计算矩阵乘法 X(BxD) @ W(DxF) 的算术强度

    返回值:
    flops: 浮点运算次数。
    bytes_transferred: 显存数据传输总量（读X + 读W + 写Y）。
    intensity: 算术强度。
    """

    # 统计进行矩阵乘法X @ W需要从显存中读取和写入的数据总量
    # 读取输入矩阵X(B x D)的数据量
    read_X = bytes_per_elem * B * D

    # 读取权重矩阵W(D x F)的数据量
    # bytes_per_elem->每个元素占有的字节数量，FP16时为2，FP32为4

    read_W = bytes_per_elem * D * F

    # 写入输出矩阵Y(B x F)的数据量
    # 矩阵乘法结果Y的维度是(B x D) @ (D x F) = (B x F)
    write_Y = bytes_per_elem * B * F

    # 总的显存数据传输量
    bytes_transferred = read_X + read_W + write_Y

    # FLOPs统计（浮点运算次数）
    # 矩阵乘法X(B, D) @ W(D, F) 的总FLOPs为2 * B * D * F
    flops = 2 * B * D * F

    # 衡量计算密集程度的关键指标
    intensity = flops / bytes_transferred

    return flops, bytes_transferred, intensity

def h100_intensity():
    """
    H100代表了硬件从内存受限切换到计算受限的理论“临界点”，硬件算术强度估计：
    定义为：峰值FLOPs吞吐量 / 峰值显存带宽(FLOPs/byte)
    """
    # 989 TFLOPs/s(FP16峰值性能，来自官方数据或规格估算)
    flops_per_second = 989e12

    # 3.35 TB/s（HBM3显存带宽峰值）
    memory_bandwidth = 3.35e12

    # 达到这个值硬件才能充分利用其计算能力，即不被内存带宽拖慢
    return flops_per_second / memory_bandwidth


# 示例运行
if __name__ == "__main__":
    B = 1         # 批大小B=1，对应单个token的生成推理情况
    D = 4096      # 隐藏层维度，例如 Llama2-7B/13B 的 D=4096
    F = 11008     # FFN中间的维度，通常为4 * D或8/3 * D

    # 计算给定参数下的操作特性
    flops, bytes_moved, intensity = arithmetic_intensity(B, D, F)

    print("==== 算术强度分析 ====")
    print(f"批大小 B = {B}")
    print(f"FLOPs = {flops:,.0f}（总浮点运算次数）")

    # 将字节数转换为MB
    print(f"显存数据传输量 = {bytes_moved/1e6:.3f} MB")
    print(f"算术强度 = {intensity:.4f} FLOPs/byte")

    # 获取H100的理论算术强度临界值
    h100_ai = h100_intensity()
    print(f"\nH100所需算术强度（临界值） ≈ {h100_ai:.1f} FLOPs/byte")

    # 比较操作的算术强度与硬件的临界值：
    # 如果intensity > h100_ai，操作的计算密度足够高，瓶颈是计算
    # 如果intensity < h100_ai，操作的计算密度不够高，瓶颈是数据传输
    if intensity > h100_ai:
        print("结论：计算受限 —— 意味着计算单元是瓶颈。")
    else:
        # 在生成推理阶段，B=1使得算术强度很低，因此常被内存带宽限制。
        print("结论：内存带宽受限 —— 这正是生成推理中B=1时的常见情况。")

    print("\n尝试不同batch size，观察强度变化：")
    # 随着B增大，FLOPs增加速度快于 bytes_transferred的增加速度（因为权重W只读一次）。
    # 当B增大时，强度intensity会逐渐增大，操作会从Memory-bound转向Compute-bound。
    for B_test in [1, 16, 64, 256, 512]:
        _, _, ai = arithmetic_intensity(B_test, D, F)
        print(f"B={B_test:4d} → 算术强度 = {ai:.1f}")
```

- **算力饱和与“拼车效应”**
增大批次大小 $B$ 是让GPU从“空闲”转向“饱和”的关键手段。可以把GPU推理想象成一辆大客车：模型权重是沉重的车身，输入数据是乘客。如果一次只拉一个乘客（ $B=1$ ），每趟行程都要拖动几十GB的车身，非常浪费；而一次拉满 $B$ 个乘客，固定成本就被分摊。在底层这意味着矩阵运算的并行性得到充分利用，算术强度显著提高——每从显存搬运一次数据，就能进行更多计算，从而充分发挥GPU的并行能力，提高整体吞吐量。

```
LLM推理阶段（自回归生成 + KV Cache）

          ┌─────────────┐
          │ 输入请求队列 │  ← 多个请求
          └─────┬───────┘
                │  
        ┌───────┴─────────┐
        │ 拼合多条请求一起 │  ← 提高总体吞吐
        └───────┬─────────┘
                │
        ┌───────┴─────────┐
        │  GPU 计算单元    
        │  ┌─────────┐    
        │  │Token_1  │<── K[K_1] V[V_1]
        │  └─────────┘  
        │  ┌─────────┐   
        │  │Token_2  │<── 必须等Token_1完成
        │  └─────────┘
        |       |
        |       ↓   KV cache更新为K[K1,K2] V[V1,V2]
        │  ┌─────────┐
        │  │Token_3  │<── 必须等Token_2完成
        │  └─────────┘
        |       |
        |       ↓   KV cache更新为K[K1,K2,K3] V[V1,V2,V3]
        |    ......
        └───────┬─────────┘
                │
        ┌───────┴─────────┐
        │   KV Cache      │  ← 存储已生成token的键值对
        │   Token_1...N   │  ← 每步生成都要访问全部历史
        └───────┬─────────┘
                │
          单条请求延迟 ↑
          （串行生成 + 记忆负担）

说明：
1. 动态合批提升系统吞吐，但单条请求延迟仍受KV Cache访问限制。
2. 随序列长度增长，KV Cache越大，延迟越高。
3. 每生成一个新的token直接在原有KV Cache上追加这个的KV即键值对，不会生成新的cache。
```
- **串行诅咒与记忆负担**
然而，LLM的自回归生成特性决定了它无法像传统神经网络那样完全并行，也不同于训练阶段可以并行计算token。无论批次多大，生成过程都像“成语接龙”：必须先写完上一个字，才能决定下一个字。更麻烦的是为了保持上下文连贯，GPU每生成一个token，都需要访问此前所有token的状态。KV Cache存储了每个已生成token的键值对，用于下一步注意力计算。随着序列长度增加，这个“记忆库”会越来越大，使原本轻量的计算变成频繁查阅历史信息的重负，严重影响单条请求的延迟。

- **内存墙与速度瓶颈**
这就引出了推理的终极瓶颈——内存墙。在解码阶段，GPU的计算单元往往空闲等待显存将庞大的KV Cache数据搬运过来，而实际计算时间非常短。虽然动态合批可以通过同时处理多条请求提升整体吞吐量，但这只是让“大客车一次拉更多乘客”，并不能让车速本身变快。对于单个用户，其生成延迟仍然被显存带宽限制：GPU算力再强，如果数据读取速度供不上，生成速度就无法提升。

*解码阶段会依据模型输出的词汇表概率分布，选取一个token作为生成结果如取最大概率、按分布采样等，并将其作为下一步的输入继续生成。*

>总的来说就是，总结来说，批量 $B$ 可以显著提升系统总体吞吐，但自回归生成和不断增长的`KV Cache`决定了单条请求的延迟最终受显存带宽限制。随着生成推进，即使系统仍能对多条请求进行合批，单条请求自身每一步解码的计算仍然只能处理1个`token`，并且随着KV Cache变大，其算术强度不断下降，性能表现会逐渐逼近单独处理即`B≈1`时的有效计算强度。

## 10.2.3 延迟 vs 吞吐量

# 10.3 新范式研究

# 思考

# 参考文献
