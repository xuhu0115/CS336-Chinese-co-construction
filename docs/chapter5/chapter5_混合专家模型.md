# 第五章 专家混合模型
专家混合模型（MoE）是当前LLM领域中一项至关重要的技术，它有效地解决了模型规模与计算成本之间的矛盾。MoE的核心在于将部分网络层替换为多个并行的“专家”子网络，并通过一个门控、路由机制例如为每个输入token选择 $top-k$ 个专家等实现稀疏激活。这种机制允许模型在不显著增加训练和推理计算量的前提下，大幅扩展其总参数规模和表达能力，从而实现了模型容量（参数数量）与计算效率之间的动态平衡。正是凭借MoE这一机制，Switch Transformer、GLaM和DeepSeek等模型得以问世并展现出卓越性能。然而，MoE在实际应用中依然会遭遇负载均衡、跨设备并行、训练不稳定和路由机制设计等工程挑战。接下来，我们将深入剖析MoE的核心概念、工作原理以及实际应用，并提供解决这些工程挑战的实用思路，旨在以更高的计算效率，正确应用这一机制来扩展LLM的能力。
## 5.1 分析MoE

  ### 5.1.1 概念直观理解
  ### 5.1.2 数学与概率视角
  ### 5.1.3 基本变体
## 5.2 MoE与LLM

  ### 5.2.1 路由算法与负载均衡
  ### 5.2.2 训练不稳定性
  ### 5.2.3 推理与部署
## 5.3 DeepSeek创新与实战复现

  ### 5.3.1 DeepSeek的创新关键点
  ### 5.3.2 小规模复现实验
## 5.4 近期MoE研究

## 思考
## 参考文献

