
# ç¬¬ä¸‰ç« ï¼špytorch ä¸èµ„æºæ ¸ç®—

ç¬¬äºŒç« ä¸­æˆ‘ä»¬è®¨è®ºäº†åˆ†è¯ï¼ˆTokenizationï¼‰ã€‚æœ¬èŠ‚è¯¾æˆ‘ä»¬å°†æ·±å…¥**æ¨¡å‹è®­ç»ƒçš„æœºåˆ¶**ã€‚æˆ‘ä»¬ä¸ä¼šç›´æ¥è®²è§£ Transformer æ¶æ„ï¼ˆé‚£æ˜¯ä¸‹èŠ‚è¯¾çš„å†…å®¹ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡æ„å»ºç®€å•çš„çº¿æ€§æ¨¡å‹ï¼ŒæŒæ¡è®­ç»ƒä»»ä½•æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ½é€šç”¨çš„â€œåŸè¯­â€ï¼ˆPrimitivesï¼‰ã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**
1.  **æœºåˆ¶ï¼ˆMechanicsï¼‰ï¼š** æŒæ¡ PyTorch çš„åŸºç¡€æ“ä½œï¼ˆè‡ªåº•å‘ä¸Šï¼šå¼ é‡ -> æ„å»ºæ¨¡å‹ -> ä¼˜åŒ–å™¨ -> è®­ç»ƒå¾ªç¯ï¼‰ã€‚
2.  **æ€ç»´æ¨¡å¼ï¼ˆMindsetï¼‰ï¼š** å…»æˆ**èµ„æºæ ¸ç®—**çš„ä¹ æƒ¯ã€‚ä½ éœ€è¦æ¸…æ¥šæ¯ä¸€è¡Œä»£ç æ¶ˆè€—äº†å¤šå°‘å†…å­˜ï¼ˆGBï¼‰å’Œç®—åŠ›ï¼ˆFLOPsï¼‰ã€‚
3.  **ç›´è§‰ï¼ˆIntuitionsï¼‰ï¼š** ç†è§£ä¸ºä»€ä¹ˆå¤§æ¨¡å‹éœ€è¦ç‰¹å®šçš„ç¡¬ä»¶å’Œç®—æ³•ä¼˜åŒ–ã€‚


---

## 3.1 ä¸ºä»€ä¹ˆéœ€è¦èµ„æºæ ¸ç®—ï¼Ÿ

åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œèµ„æºæ¶ˆè€—ç›´æ¥è½¬åŒ–ä¸ºæˆæœ¬å’Œæ—¶é—´ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªå®é™…é—®é¢˜æ¥ç†è§£å…¶é‡è¦æ€§ï¼š

#### åœºæ™¯ä¸€ï¼šæ—¶é—´ä¼°ç®—

> **é—®é¢˜ï¼š** å‡è®¾ä½ æ˜¯ä¸€ä¸ª AI å·¥ç¨‹å¸ˆï¼Œè€æ¿é—®ä½ ï¼šâ€œåœ¨ 1024 å¼  H100 æ˜¾å¡ä¸Šï¼Œè®­ç»ƒä¸€ä¸ª 70Bï¼ˆ700äº¿å‚æ•°ï¼‰çš„æ¨¡å‹ï¼Œæ•°æ®é‡æ˜¯ 15Tï¼ˆ15ä¸‡äº¿ tokensï¼‰ï¼Œå¤§æ¦‚è¦å¤šä¹…ï¼Ÿâ€

å¦‚æœä½ ç›´æ¥è·‘å»å†™ä»£ç æµ‹è¯•ï¼Œé‚£å¯èƒ½å‡ å¤©ç”šè‡³å‡ ä¸ªæœˆéƒ½å‡ºä¸æ¥ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¼šâ€œNapkin mathâ€ï¼ˆé¤å·¾çº¸è®¡ç®—ï¼Œå³**å¿«é€Ÿä¼°ç®—**ï¼‰ã€‚

##### ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æ€»å·¥ä½œé‡
FLOPsï¼ˆFloating Point Operationsï¼Œæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰æ˜¯è¡¡é‡è®¡ç®—å¤æ‚åº¦çš„å¸¸ç”¨æŒ‡æ ‡ï¼Œè¡¨ç¤ºä¸€ä¸ªç®—æ³•æˆ–æ¨¡å‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ‰€éœ€çš„æµ®ç‚¹åŠ å‡ä¹˜é™¤ç­‰è¿ç®—çš„æ€»æ¬¡æ•°ã€‚è€Œè®­ç»ƒæ¨¡å‹æœ¬è´¨ä¸Šæ˜¯è¿›è¡Œæµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰ã€‚æœ‰ä¸€ä¸ªç»éªŒå…¬å¼ï¼š
$$æ€»è®¡ç®—é‡ \approx 6 \times \text{å‚æ•°é‡} \times \text{Tokenæ•°é‡}$$

> ä¸ºä»€ä¹ˆæ˜¯ **6** å€ï¼Ÿ
> *   **å‰å‘ä¼ æ’­ (Forward Pass):** è®¡ç®—ä¸€æ¬¡çº¦ä¸º $2 \times$ å‚æ•°é‡ï¼ˆä¹˜æ³•+åŠ æ³•ï¼‰ã€‚
> *   **åå‘ä¼ æ’­ (Backward Pass):** è®¡ç®—æ¢¯åº¦çº¦ä¸ºå‰å‘ä¼ æ’­çš„ 2 å€å·¥ä½œé‡ï¼Œå³ $4 \times$ å‚æ•°é‡ã€‚

ä»£å…¥æ•°æ®ï¼š
$$6 \times (70 \times 10^9) \times (15 \times 10^{12}) \approx 6.3 \times 10^{24} \text{ FLOPs}$$

##### ç¬¬äºŒæ­¥ï¼šè®¡ç®—ç¡¬ä»¶ç®—åŠ›

æŸ¥é˜… [NVIDIA H100 çš„ç™½çš®ä¹¦](https://www.nvidia.com/en-sg/data-center/h100/)ï¼Œå…¶ FP16/BF16 çš„å³°å€¼ç®—åŠ›çº¦ä¸º **1979 TFLOPS**ï¼ˆæ¯ç§’ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼‰ã€‚

![3-1-H100](images/3-1-H100.png)

ä½†æ˜¯è¦æ³¨æ„ï¼Œè¿™ä¸ªå€¼æ˜¯ NVIDIA H100 GPU åœ¨ä½¿ç”¨ FP16 æˆ– BF16 æ•°æ®ç±»å‹ã€ä¸”å¯ç”¨ç»“æ„åŒ–ç¨€ç–ï¼ˆStructured Sparsityï¼‰æ—¶å¯è¾¾åˆ°çš„ç†è®ºæœ€å¤§è®¡ç®—ååé‡ã€‚å¯¹äºæˆ‘ä»¬è®­ç»ƒæ™®é€šçš„ç¨ å¯†æ¨¡å‹ï¼ˆå¦‚æ ‡å‡†çš„ LLaMA-1åˆ°LLaMA-3ç³»åˆ—ã€Qwen3-0.6B/1.7B/4B/8B/14B/32Bç­‰ï¼‰ï¼Œç†è®ºå³°å€¼å‡åŠï¼Œçº¦ 990 TFLOPSã€‚

![3-1-H100-sparsity.png](images/3-1-H100-sparsity.png)


> ç»“æ„åŒ–ç¨€ç–ï¼Œæ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©çš„æ–¹æ³•ï¼Œé€šå¸¸æ˜¯å¯¹ç¨ å¯†æ¨¡å‹æŒ‰ç…§50%çš„ç¨€ç–åº¦è¿›è¡Œå‰ªæï¼ˆç¨€ç–å½¢å¼ä¸ºn:mï¼Œå³mä¸ªè¿ç»­æƒé‡é‡Œå¿…é¡»å‰ªæ‰nä¸ªï¼Œç±»å‹æœ‰ 2:4ã€4:8ã€8:16)ã€‚è¿˜æœ‰ä¸€ç§çµæ´»åº¦æ›´é«˜çš„éç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œå¯ä»¥æŒ‰ç…§ç™¾åˆ†æ¯”è¿›è¡Œå‰ªæï¼Œä½†é€šå¸¸ç›¸å¯¹äºç»“æ„åŒ–å‰ªææ€§èƒ½æ›´å·®ã€‚

![3-1-puning.png](images/3-1-puning.png)

ä¸Šè¿° 990 TFLOPS æ˜¯ H100 çš„ç†è®ºå³°å€¼ï¼Œä½†å®é™…è¿è¡Œæ¨¡å‹æ—¶ï¼Œç”±äºå„ç§è½¯ç¡¬ä»¶å¼€é”€ï¼Œä½ å‡ ä¹ä¸å¯èƒ½è¾¾åˆ° 100% **æ¨¡å‹ç®—åŠ›åˆ©ç”¨ç‡ (MFU, Model FLOPs Utilization)**ï¼Œé€šå¸¸æŒ‰ 30%â€“60% çš„åˆ©ç”¨ç‡ä¼°ç®—æ›´ç°å®ã€‚è¿™é‡Œå– 50% ç”¨ä½œåç»­ä¼°è®¡ã€‚

- å•å¡å®é™…ç®—åŠ› $\approx (990 \times 10^{12}) \times 0.5 \approx 5 \times 10^{14} \text{ FLOPS}$
- 1024 å¼ å¡æ€»ç®—åŠ› $\approx 5 \times 10^{17} \text{ FLOPS}$ã€‚

##### ç¬¬ä¸‰æ­¥ï¼šå¾—å‡ºç»“æœ
$$ \text{æ—¶é—´} = \frac{\text{æ€»å·¥ä½œé‡}}{\text{æ€»ç®—åŠ›}} = \frac{6.3 \times 10^{24}}{5 \times 10^{17}} \approx 1.26 \times 10^7 \text{ ç§’} \approx \textbf{146 å¤©} $$

\
è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å·¥ç¨‹ã€‚å¦‚æœä¸è¿›è¡Œä¼˜åŒ–ï¼Œéœ€è¦äº”ä¸ªæœˆæ‰èƒ½è®­ç»ƒå®Œï¼Œè¿™è¿˜æ˜¯è®­ç»ƒä¸ä¸­æ–­çš„æƒ…å†µä¸‹éœ€è¦çš„æ—¶é—´ã€‚

---

#### åœºæ™¯äºŒï¼šå†…å­˜ä¼°ç®—

> **é—®é¢˜ï¼š** åœ¨ 8 å¼  H100 GPU ä¸Šï¼Œä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼ˆæœ´ç´ å®ç°ï¼Œå³æ‰€æœ‰æ•°æ®éƒ½ç”¨ float32 å­˜å‚¨ï¼Œæ²¡æœ‰æ··åˆç²¾åº¦æˆ–å‹ç¼©ï¼‰ï¼Œä½ èƒ½è®­ç»ƒçš„æœ€å¤§æ¨¡å‹å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ

å¾ˆå¤šåˆå­¦è€…è®¤ä¸ºï¼š8 å¼  H100ï¼ˆæ¯å¼  80GB æ˜¾å­˜ï¼‰ï¼Œæ˜¾å­˜æ€»å…± 640GBã€‚å‚æ•°é‡‡ç”¨ `float32`å­˜å‚¨ï¼Œä¸€ä¸ªå‚æ•° 4 å­—èŠ‚ï¼Œæ‰€ä»¥ $640GB / 4$ å­—èŠ‚ = 1600äº¿å‚æ•°ï¼ˆ160Bï¼‰ï¼Ÿ**é”™ï¼**

è®­ç»ƒæ—¶ï¼Œæ˜¾å­˜é‡Œå­˜çš„ä¸åªæ˜¯**å‚æ•° (Parameters)**ï¼Œè¿˜æœ‰ï¼š
- **æ¢¯åº¦ (Gradients):** å’Œå‚æ•°ä¸€æ ·å¤§ï¼Œéœ€è¦ 4 å­—èŠ‚
- **ä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer States):** æ¯”å¦‚å¸¸ç”¨çš„ AdamW ä¼˜åŒ–å™¨ï¼Œéœ€è¦å­˜å‚¨æ¯ä¸ªå‚æ•°çš„ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰

å› æ­¤ï¼Œæ¯ä¸ªå‚æ•°åœ¨è®­ç»ƒæ—¶å¤§çº¦å ç”¨ 16 å­—èŠ‚ï¼š
*   å‚æ•° (FP32): 4 bytes
*   æ¢¯åº¦ (FP32): 4 bytes
*   ä¼˜åŒ–å™¨çŠ¶æ€ (FP32): 8 bytes (2 ä¸ªå˜é‡ Ã— 4 bytes)

$$ \text{æœ€å¤§å‚æ•°é‡} = \frac{640 \times 10^9 \text{ bytes}}{16 \text{ bytes/param}} \approx \textbf{400 äº¿ (40B)} $$

å½“ç„¶ï¼Œè¿™ä¸ªä¼°ç®—è¾ƒä¸ºç²—ç•¥ï¼Œå› ä¸ºå®ƒæœªè€ƒè™‘ä¾èµ–äºæ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„æ¿€æ´»å€¼å†…å­˜å ç”¨ï¼Œæ‰€ä»¥å¦‚æœä¸è€ƒè™‘æ··åˆç²¾åº¦æˆ–å‹ç¼©ï¼Œå®é™…èƒ½è®­ç»ƒçš„æ¨¡å‹å¯èƒ½ä¼šæ›´å°ã€‚

---

## 3.2 å¼ é‡ (Tensors)

### 3.2.1 å¼ é‡åŸºç¡€

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¼ é‡ï¼ˆtensorï¼‰æ˜¯å­˜å‚¨ä¸€åˆ‡çš„åŸºç¡€æ•°æ®ç»“æ„ï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€è¾“å…¥æ•°æ®ã€ä¸­é—´æ¿€æ´»å€¼ç­‰éƒ½ä»¥å¼ é‡å½¢å¼å­˜åœ¨ã€‚æ›´å¤šå…³äºå¼ é‡çš„çŸ¥è¯†å¯è§ [PyTorch docs on tensors](https://docs.pytorch.org/docs/stable/tensors.html)ã€‚

PyTorchæä¾›äº†å¤šç§åˆ›å»ºå¼ é‡çš„æ–¹æ³•ï¼š

```
# åŸºç¡€åˆ›å»ºæ–¹å¼
x = torch.tensor([[1., 2, 3], [4, 5, 6]])  # ä»Pythonåˆ—è¡¨åˆ›å»º
x = torch.zeros(4, 8)  # 4x8çš„é›¶çŸ©é˜µ
x = torch.ones(4, 8)   # 4x8çš„å…¨1çŸ©é˜µ
x = torch.randn(4, 8)  # 4x8çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°

# åˆ†é…ä½†ä¸åˆå§‹åŒ–å€¼ï¼ˆç”¨äºè‡ªå®šä¹‰åˆå§‹åŒ–çš„å€¼ï¼‰
x = torch.empty(4, 8)  # æœªåˆå§‹åŒ–çš„4x8çŸ©é˜µ
nn.init.trunc_normal_(x, mean=0, std=1, a=-2, b=2)  # æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œåˆ†å¸ƒå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼Œæˆªæ–­èŒƒå›´ï¼Œåªä¿ç•™[-2, 2]åŒºé—´å†…çš„å€¼ï¼Œè¶…å‡ºæ­¤èŒƒå›´çš„å€¼ä¼šè¢«é‡æ–°é‡‡æ ·
```

### 3.2.2 å¼ é‡çš„æ“ä½œ

#### å¼ é‡å­˜å‚¨æœºåˆ¶
PyTorchå¼ é‡ä¸ä»…ä»…æ˜¯æ•°æ®ï¼Œè¿˜åŒ…æ‹¬å…ƒæ•°æ®ï¼ˆmetadataï¼‰æè¿°å¦‚ä½•è®¿é—®åº•å±‚å­˜å‚¨ï¼š

```
x = torch.tensor([
    [0., 1, 2, 3],
    [4, 5, 6, 7],
    [8, 9, 10, 11],
    [12, 13, 14, 15],
])

# æ­¥é•¿ï¼ˆstrideï¼‰ï¼šåœ¨æ¯ä¸ªç»´åº¦ä¸Šç§»åŠ¨æ—¶è·³è¿‡çš„å…ƒç´ æ•°
assert x.stride(0) == 4  # è¡Œç»´åº¦ï¼šç§»åŠ¨åˆ°ä¸‹ä¸€è¡Œéœ€è·³è¿‡4ä¸ªå…ƒç´ 
assert x.stride(1) == 1  # åˆ—ç»´åº¦ï¼šç§»åŠ¨åˆ°ä¸‹ä¸€åˆ—éœ€è·³è¿‡1ä¸ªå…ƒç´ 

# è®¡ç®—å…ƒç´ ä½ç½®ï¼šè¡Œç´¢å¼•rï¼Œåˆ—ç´¢å¼•c
r, c = 1, 2
index = r * x.stride(0) + c * x.stride(1)  # ä½ç½®6
```

#### å¼ é‡è§†å›¾

è®¸å¤šæ“ä½œä¸ä¼šå¤åˆ¶æ•°æ®ï¼Œè€Œæ˜¯åˆ›å»ºåŸå§‹å¼ é‡çš„è§†å›¾ï¼ˆviewï¼‰ï¼š

```
x = torch.tensor([[1., 2, 3], [4, 5, 6]])

# è¿™äº›æ“ä½œä¸å¤åˆ¶æ•°æ®
y = x[0]           # è·å–ç¬¬0è¡Œ
y = x[:, 1]        # è·å–ç¬¬1åˆ—
y = x.view(3, 2)   # é‡å¡‘ä¸º3x2
y = x.transpose(1, 0)  # è½¬ç½®

# éªŒè¯æ˜¯å¦å…±äº«å­˜å‚¨
assert same_storage(x, y)  # same_storageæ£€æŸ¥åº•å±‚å­˜å‚¨æ˜¯å¦ç›¸åŒ

# æ³¨æ„ï¼šä¿®æ”¹xä¼šå½±å“y
x[0][0] = 100
assert y[0][0] == 100  # å€¼ä¹Ÿè¢«ä¿®æ”¹
```

> æ³¨æ„ï¼šè§†å›¾æ“ä½œæ˜¯ä¸æ¶ˆè€—é¢å¤–å†…å­˜å’Œè®¡ç®—ï¼Œä½†éè¿ç»­å¼ é‡ï¼ˆnon-contiguous tensorï¼‰å¯èƒ½é™åˆ¶è¿›ä¸€æ­¥æ“ä½œã€‚ä¾‹å¦‚ï¼Œè½¬ç½®åçš„å¼ é‡æ˜¯éè¿ç»­çš„ï¼š

```
x = torch.tensor([[1., 2, 3], [4, 5, 6]])
y = x.transpose(1, 0)
assert not y.is_contiguous()  # éè¿ç»­å¼ é‡

# å°è¯•é‡å¡‘ä¼šå¤±è´¥
try:
    y.view(2, 3)
except RuntimeError as e:
    assert "view size is not compatible with input tensor's size and stride" in str(e)

# è§£å†³æ–¹æ¡ˆï¼šå…ˆä½¿å¼ é‡è¿ç»­
y = x.transpose(1, 0).contiguous().view(2, 3)
assert not same_storage(x, y)  # ç°åœ¨åˆ›å»ºäº†æ–°å­˜å‚¨
```

#### é€å…ƒç´ æ“ä½œï¼ˆElement-wise Operationsï¼‰

è¿™äº›æ“ä½œå¯¹æ¯ä¸ªå…ƒç´ ç‹¬ç«‹åº”ç”¨å‡½æ•°ï¼Œè¿”å›ç›¸åŒå½¢çŠ¶çš„æ–°å¼ é‡ï¼š

```
x = torch.tensor([1, 4, 9])
assert torch.equal(x.pow(2), torch.tensor([1, 16, 81]))
assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3]))
assert torch.equal(x + x, torch.tensor([2, 8, 18]))

# å®ç”¨æ“ä½œï¼šä¸Šä¸‰è§’çŸ©é˜µï¼ˆç”¨äºå› æœæ³¨æ„åŠ›æ©ç ï¼‰
x = torch.ones(3, 3).triu()
# ç»“æœï¼š
# [[1, 1, 1],
#  [0, 1, 1],
#  [0, 0, 1]]
```

#### çŸ©é˜µä¹˜æ³•ï¼ˆMatrix Multiplicationï¼‰

æœ€åï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼šçŸ©é˜µä¹˜æ³•ã€‚

```
# åŸºæœ¬çŸ©é˜µä¹˜æ³•
x = torch.ones(16, 32)  # 16x32çŸ©é˜µ
w = torch.ones(32, 2)   # 32x2æƒé‡çŸ©é˜µ
y = x @ w               # ç»“æœï¼š16x2çŸ©é˜µ
assert y.size() == torch.Size([16, 2])
```

é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šå¯¹æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬å’Œåºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°æ‰§è¡Œæ“ä½œã€‚

![](images/3-3-tensor-matmul.png)

ä¸‹é¢æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯¹å‰ä¸¤ä¸ªç»´åº¦ï¼ˆæ‰¹æ¬¡å’Œåºåˆ—ï¼‰ä¸­çš„æ¯ä¸ªæ ·æœ¬æ‰§è¡ŒçŸ©é˜µä¹˜æ³•

```
# æ‰¹é‡å¤„ç†ï¼ˆæ·±åº¦å­¦ä¹ ä¸­çš„å¸¸è§æ¨¡å¼ï¼‰
x = torch.ones(4, 8, 16, 32)  # æ‰¹æ¬¡Ã—åºåˆ—Ã—æ ·æœ¬Ã—ç‰¹å¾
w = torch.ones(32, 2)         # æƒé‡çŸ©é˜µ
y = x @ w                     # ç»“æœï¼š4x8x16x2
```

### 3.2.3 ä½¿ç”¨ Einops åº“å¯¹å¼ é‡æ“ä½œè¿›è¡Œä¼˜åŒ–

åœ¨å¤„ç† Transformer æ—¶ï¼Œå¼ é‡ç»´åº¦é€šå¸¸æ˜¯ `[Batch, Seq, Head, Hidden]`ã€‚ç”¨ PyTorch åŸç”Ÿçš„ `.view()` å’Œ `.transpose()` å¾ˆå®¹æ˜“ææ™•ã€‚æ¨èå·¥å…· **Einops** è®©ä»£ç åƒå†™å…¬å¼ä¸€æ ·æ¸…æ™°ã€‚

#### Jaxtypingï¼šç±»å‹æ³¨è§£ä¸­çš„ç»´åº¦å‘½å

```python
# ä¼ ç»Ÿæ–¹å¼ï¼ˆå®¹æ˜“å†™é”™ç»´åº¦é¡ºåºï¼‰
x = torch.ones(2, 2, 1, 3)  # batch seq heads hidden

# Jaxtypingæ–¹å¼ï¼ˆåœ¨ç±»å‹æ³¨è§£ä¸­å‘½åç»´åº¦ï¼‰
from jaxtyping import Float
x: Float[torch.Tensor, "batch seq heads hidden"] = torch.ones(2, 2, 1, 3)
```

#### Einsteinæ±‚å’Œï¼ˆeinsumï¼‰

å¹¿ä¹‰çŸ©é˜µä¹˜æ³•ï¼Œå…·æœ‰æ¸…æ™°çš„ç»´åº¦è·Ÿè¸ªï¼š

```
from einops import einsum

# ä¼ ç»Ÿæ–¹å¼
z = x @ y.transpose(-2, -1)  # batch, sequence, sequence

# Einopsæ–¹å¼
z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2")
# æœªåœ¨è¾“å‡ºä¸­å‘½åçš„ç»´åº¦ï¼ˆhiddenï¼‰ä¼šè¢«è‡ªåŠ¨æ±‚å’Œ
```


#### å½’çº¦æ“ä½œï¼ˆreduceï¼‰

åœ¨æŒ‡å®šç»´åº¦ä¸Šåº”ç”¨èšåˆå‡½æ•°ï¼š

```
from einops import reduce

x: Float[torch.Tensor, "batch seq hidden"] = torch.ones(2, 3, 4)

# ä¼ ç»Ÿæ–¹å¼
y = x.mean(dim=-1)  # åœ¨hiddenç»´åº¦ä¸Šå–å¹³å‡

# Einopsæ–¹å¼
y = reduce(x, "... hidden -> ...", "mean")  # ...è¡¨ç¤ºä»»æ„å‰å¯¼ç»´åº¦
```

#### é‡æ’æ“ä½œï¼ˆrearrangeï¼‰

å°†ä¸€ä¸ªç»´åº¦æ‹†åˆ†ä¸ºå¤šä¸ªç»´åº¦ï¼Œæˆ–å°†å¤šä¸ªç»´åº¦åˆå¹¶ï¼š

```
from einops import rearrange

# æƒ…æ™¯ï¼šhiddenç»´åº¦å®é™…ä¸Šæ˜¯headsÃ—hidden1çš„æ‰å¹³åŒ–è¡¨ç¤º
x: Float[torch.Tensor, "batch seq total_hidden"] = torch.ones(2, 3, 8)
w: Float[torch.Tensor, "hidden1 hidden2"] = torch.ones(4, 4)

# 1. æ‹†åˆ†total_hiddenä¸ºheadså’Œhidden1
x = rearrange(x, "... (heads hidden1) -> ... heads hidden1", heads=2)

# 2. åº”ç”¨å˜æ¢
x = einsum(x, w, "... hidden1, hidden1 hidden2 -> ... hidden2")

# 3. åˆå¹¶ç»´åº¦
x = rearrange(x, "... heads hidden2 -> ... (heads hidden2)")
```

å°½ç®¡Einopså¢åŠ äº†å°‘é‡è¯­æ³•å¼€é”€ï¼Œä½†å…¶æ¸…æ™°çš„ç»´åº¦å‘½åæ˜¾è‘—é™ä½äº†è°ƒè¯•éš¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ¨¡å‹æ¶æ„ä¸­ï¼Œæ›´å¤šç¤ºä¾‹è§ [Einops tutorial](https://einops.rocks/1-einops-basics/)ã€‚

## 3.3 å†…å­˜ï¼ˆMemoryï¼‰


å¼ é‡çš„å†…å­˜å ç”¨ç”±ä¸¤ä¸ªå› ç´ å†³å®šï¼š

- å…ƒç´ æ•°é‡ï¼šå¼ é‡çš„å½¢çŠ¶ï¼ˆå¦‚4Ã—8çŸ©é˜µæœ‰32ä¸ªå…ƒç´ ï¼‰
- æ•°æ®ç±»å‹ï¼šæ¯ä¸ªå…ƒç´ å ç”¨çš„å­—èŠ‚æ•°

### 3.3.1 æµ®ç‚¹ç±»å‹

åœ¨ LLM è®­ç»ƒä¸­ï¼Œé€‰æ‹©æ•°æ®ç±»å‹æœ¬è´¨ä¸Šæ˜¯åœ¨åšæ˜¾å­˜å ç”¨ã€è®¡ç®—é€Ÿåº¦ä¸æ•°å€¼ç¨³å®šæ€§ä¹‹é—´çš„æƒè¡¡ï¼ˆTrade-offï¼‰ã€‚æˆ‘ä»¬å…³æ³¨çš„å¤§å¤šæ•°å¼ é‡ï¼ˆå‚æ•°ã€æ¢¯åº¦ã€æ¿€æ´»å€¼ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰éƒ½ä»¥æµ®ç‚¹æ•°å½¢å¼è¿›è¡Œå­˜å‚¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥ç†è§£ä¸åŒçš„æµ®ç‚¹æ•°æ®ç±»å‹ï¼š

1. Float32 (FP32 / å•ç²¾åº¦æµ®ç‚¹æ•°)

![3-1-H100](images/3-2-fp32.png)

*   **è§„æ ¼**ï¼šå ç”¨ **4 å­—èŠ‚** (32 bits)ã€‚ç»“æ„ä¸ºï¼š1ä½ç¬¦å·ä½ + **8ä½æŒ‡æ•°ä½** + 23ä½å°¾æ•°ä½ã€‚
*   **åœ°ä½**ï¼šPyTorch çš„é»˜è®¤æ•°æ®ç±»å‹ï¼Œä¹Ÿæ˜¯ç§‘å­¦è®¡ç®—é¢†åŸŸçš„â€œé»„é‡‘æ ‡å‡†â€ã€‚
*   **ä¼˜ç‚¹**ï¼šæ•°å€¼ç²¾åº¦é«˜ï¼ŒåŠ¨æ€èŒƒå›´å¤§ï¼Œè®­ç»ƒæœ€ç¨³å®šï¼Œå‡ ä¹ä¸ä¼šå‡ºç°æ•°å€¼æº¢å‡ºé—®é¢˜ã€‚
*   **ç¼ºç‚¹**ï¼šå¯¹äºå¤§æ¨¡å‹è€Œè¨€å¤ªâ€œå¥¢ä¾ˆâ€ã€‚å®ƒå ç”¨çš„æ˜¾å­˜æ˜¯ 16ä½æ ¼å¼çš„ä¸¤å€ï¼Œä¸”åœ¨ç°ä»£ GPUï¼ˆå¦‚ H100ï¼‰ä¸Šçš„è®¡ç®—ååé‡è¿œä½äºä½ç²¾åº¦æ ¼å¼ã€‚
*   **ç‚¼ä¸¹ç”¨é€”**ï¼šé€šå¸¸ç”¨äºå­˜å‚¨**å‚æ•°çš„ä¸»å‰¯æœ¬ (Master Weights)** å’Œ **ä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer States)**ï¼Œä»¥ç¡®ä¿åœ¨æ¢¯åº¦ç´¯ç§¯å’Œå‚æ•°æ›´æ–°æ—¶ä¸ä¼šå› ä¸ºç²¾åº¦ä¸¢å¤±è€Œå¯¼è‡´æ¨¡å‹æ— æ³•æ”¶æ•›ã€‚



2. Float16 (FP16 / åŠç²¾åº¦æµ®ç‚¹æ•°)

![3-1-H100](images/3-2-fp16.png)

*   **è§„æ ¼**ï¼šå ç”¨ **2 å­—èŠ‚** (16 bits)ã€‚ç»“æ„ä¸ºï¼š1ä½ç¬¦å·ä½ + **5ä½æŒ‡æ•°ä½** + 10ä½å°¾æ•°ä½ã€‚
*   **ä¼˜ç‚¹**ï¼šæ˜¾å­˜å ç”¨æ¯” FP32 å‡å°‘ä¸€åŠï¼Œè®¡ç®—é€Ÿåº¦å¿«ã€‚
*   **è‡´å‘½ç¼ºé™·**ï¼š**åŠ¨æ€èŒƒå›´ï¼ˆDynamic Rangeï¼‰å¤ªçª„**ã€‚
    *   ç”±äºæŒ‡æ•°ä½åªæœ‰ 5 ä½ï¼Œå®ƒæ— æ³•è¡¨ç¤ºéå¸¸å°çš„æ•°ï¼ˆä¼šå‘ç”Ÿä¸‹æº¢ Underflowï¼Œç›´æ¥å˜æˆ 0ï¼‰æˆ–éå¸¸å¤§çš„æ•°ï¼ˆä¼šå‘ç”Ÿä¸Šæº¢ Overflowï¼Œå˜æˆ Infinityï¼‰ã€‚
    *   ä¾‹å¦‚ï¼šåœ¨ FP16 ä¸­ï¼Œ`1e-8` è¿™æ ·çš„å°æ•°ä¼šè¢«ç›´æ¥å½“ä½œ `0` å¤„ç†ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚
*   **ç”¨é€”**ï¼šè¿™æ˜¯ä¸Šä¸€ä»£ GPUï¼ˆå¦‚ V100ï¼‰æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸»æµã€‚ä¸ºäº†è§£å†³æº¢å‡ºé—®é¢˜ï¼Œå¿…é¡»ä½¿ç”¨å¤æ‚çš„**æŸå¤±ç¼©æ”¾ (Loss Scaling)** æŠ€æœ¯ã€‚ç›®å‰åœ¨ LLM è®­ç»ƒä¸­æ­£é€æ¸è¢« BF16 å–ä»£ã€‚

3. BFloat16 (BF16 / Brain Floating Point)

![3-1-H100](images/3-2-bf16.png)

*   **è§„æ ¼**ï¼šå ç”¨ **2 å­—èŠ‚** (16 bits)ã€‚ç»“æ„ä¸ºï¼š1ä½ç¬¦å·ä½ + **8ä½æŒ‡æ•°ä½** + 7ä½å°¾æ•°ä½ã€‚
*   **æ¥æº**ï¼šç”± Google Brain ä¸“ä¸ºæ·±åº¦å­¦ä¹ è®¾è®¡ã€‚
*   **è®¾è®¡é€»è¾‘**ï¼š**â€œè¦èŒƒå›´ï¼Œä¸è¦ç²¾åº¦â€**ã€‚
    *   æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯ç¥ç»ç½‘ç»œï¼‰å¯¹å°æ•°ç‚¹çš„åå‡ ä½ç²¾åº¦ä¸æ•æ„Ÿï¼Œä½†å¯¹æ•°å€¼çš„èŒƒå›´éå¸¸æ•æ„Ÿã€‚
    *   BF16 ç›´æ¥æˆªæ–­äº† FP32 çš„å°¾æ•°ï¼Œä½†**ä¿ç•™äº†å’Œ FP32 ç›¸åŒçš„ 8 ä½æŒ‡æ•°ä½**ã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   æ‹¥æœ‰å’Œ FP32 ä¸€æ ·å®½å¹¿çš„åŠ¨æ€èŒƒå›´ï¼Œ**ä¸éœ€è¦ Loss Scaling** ä¹Ÿèƒ½ç¨³å®šè®­ç»ƒã€‚
    *   æ˜¾å­˜å ç”¨å’Œ FP16 ä¸€æ ·å°‘ã€‚
    *   åœ¨ A100/H100 ç­‰æ–°ç¡¬ä»¶ä¸Šè®¡ç®—é€Ÿåº¦æå¿«ã€‚
*   **ç”¨é€”**ï¼š**å½“å‰ LLM è®­ç»ƒçš„ç»å¯¹ä¸»æµé€‰æ‹©**ã€‚é€šå¸¸ç”¨äºå­˜å‚¨**æ¿€æ´»å€¼ (Activations)** ä»¥åŠè¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­çš„çŸ©é˜µä¹˜æ³•è®¡ç®—ã€‚

4. FP8 (8ä½æµ®ç‚¹æ•°)

![3-1-H100](images/3-2-fp8.png)

*   **è§„æ ¼**ï¼šå ç”¨ **1 å­—èŠ‚** (8 bits)ã€‚
*   **å˜ä½“**ï¼š
    *   **E4M3**ï¼š4ä½æŒ‡æ•°ï¼Œ3ä½å°¾æ•°ï¼ˆç²¾åº¦ç¨é«˜ï¼ŒèŒƒå›´ç¨å°ï¼‰ã€‚
    *   **E5M2**ï¼š5ä½æŒ‡æ•°ï¼Œ2ä½å°¾æ•°ï¼ˆèŒƒå›´ç¨å¤§ï¼Œç²¾åº¦æ›´ä½ï¼‰ã€‚
*   **ä¼˜ç‚¹**ï¼šæè‡´çš„æ˜¾å­˜å‹ç¼©å’Œè®¡ç®—ååé‡ã€‚
*   **ç¡¬ä»¶é™åˆ¶**ï¼šä»…åœ¨ NVIDIA H100 åŠæ›´æ–°çš„æ¶æ„ï¼ˆé…åˆ Transformer Engineï¼‰ä¸Šæ‰è¢«åŸç”Ÿæ”¯æŒã€‚
*   **ç‚¼ä¸¹ç”¨é€”**ï¼šç›®å‰ä¸»è¦ç”¨äº**æ¨ç†é˜¶æ®µçš„é‡åŒ– (Quantization)**ã€‚è™½ç„¶ç†è®ºä¸Šå¯ä»¥ç”¨äºè®­ç»ƒï¼ˆH100 æ”¯æŒï¼‰ï¼Œä½†ç”±äºç²¾åº¦æä½ï¼Œè®­ç»ƒæä¸ç¨³å®šï¼Œå±äºæ¯”è¾ƒå‰æ²¿çš„ç ”ç©¶é¢†åŸŸã€‚

\
**ğŸªœ ä¸ºä»€ä¹ˆ BF16 ä¼˜äº FP16ï¼Ÿ**
> åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸å…³å¿ƒå°æ•°ç‚¹åç¬¬10ä½çš„ç²¾ç¡®åº¦ï¼ˆç²¾åº¦ï¼‰ï¼Œä½†éå¸¸å…³å¿ƒèƒ½å¦è¡¨ç¤ºéå¸¸å¤§æˆ–éå¸¸å°çš„æ•°ï¼ˆåŠ¨æ€èŒƒå›´ï¼‰ã€‚FP16 çš„æŒ‡æ•°ä½å¤ªå°‘ï¼Œå¯¼è‡´è®­ç»ƒä¸­ç»å¸¸å‡ºç° `NaN` æˆ– `0`ï¼ˆä¸‹æº¢ï¼‰ã€‚BF16 æˆªæ–­äº† FP32 çš„å°¾æ•°ï¼Œä¿ç•™äº†æŒ‡æ•°ä½ï¼Œå› æ­¤å®ƒèƒ½è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´ä¸ FP32 ä¸€æ ·å¤§ï¼Œæå¤§åœ°æå‡äº†è®­ç»ƒç¨³å®šæ€§ã€‚

### 3.3.2 å°†å¼ é‡ï¼ˆtensorsï¼‰ä»CPUå†…å­˜ç§»åŠ¨åˆ°GPUå†…å­˜

é»˜è®¤æƒ…å†µä¸‹ï¼Œå¼ é‡æ˜¯å­˜å‚¨åœ¨ CPU å†…å­˜ä¸Šçš„ï¼Œè®©æˆ‘ä»¬æ¥æ£€æŸ¥ä¸‹ï¼š

```
x = torch.zeros(32, 32)
assert x.device == torch.device("cpu")
```

ä½†æ˜¯ï¼Œä¸ºäº†åˆ©ç”¨ GPU çš„å¤§è§„æ¨¡å¹¶è¡Œæ€§è®¡åŠ é€Ÿè®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬ç§»è‡³ GPU å†…å­˜ä¸­ã€‚

![](images/3-2-cpu-gpu.png)

ä¸Šå›¾å·¦ä¾§åŒ…å«ä¸­å¤®å¤„ç†å™¨ï¼ˆCPUï¼‰å’Œç³»ç»Ÿå†…å­˜ï¼ˆRAMï¼‰ï¼Œå³ä¾§åŒ…å«å›¾å½¢å¤„ç†å™¨ï¼ˆGPUï¼‰åŠå…¶ä¸“ç”¨çš„é«˜é€Ÿæ˜¾å­˜ï¼ˆDRAMï¼‰ï¼ŒGPUå†…éƒ¨ç”±å¤šä¸ªâ€œæµå¼å¤šå¤„ç†å™¨â€ï¼ˆstreaming multiprocessorï¼‰ç»„æˆï¼Œè¿™æ˜¯å…¶å¹¶è¡Œè®¡ç®—èƒ½åŠ›çš„æ¥æºã€‚CPUå’ŒGPUé€šè¿‡â€œPCI BUSâ€ï¼ˆPCIæ€»çº¿ï¼‰ç›¸è¿ï¼Œæ•°æ®åœ¨CPUå’ŒGPUä¹‹é—´ä¼ è¾“éœ€è¦ç»è¿‡è¿™æ¡æ€»çº¿ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œå› ä¸ºå®ƒçš„å¸¦å®½è¿œå°äºGPUå†…éƒ¨çš„å†…å­˜å¸¦å®½ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬åº”å°½é‡å‡å°‘CPUå’ŒGPUä¹‹é—´çš„æ•°æ®ä¼ è¾“


åœ¨ PyTorch ä¸­ï¼Œæƒ³è¦å°†å¼ é‡ä» CPU ç§»è‡³ GPU ï¼Œéœ€è¦ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

- é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸‹æˆ‘ä»¬å½“å‰æœºå™¨ä¸Šæœ‰æ²¡æœ‰GPUï¼š
    ```
    if not torch.cuda.is_available():
            return
    ```

- æ¥ä¸‹æ¥ï¼Œè·å–GPUä¿¡æ¯:
    ```
    num_gpus = torch.cuda.device_count() # è·å–GPUæ•°é‡
    for i in range(num_gpus):
        properties = torch.cuda.get_device_properties(i) # è·å–ç¬¬iå—GPUçš„å±æ€§ï¼ˆå¦‚å‹å·ã€æ˜¾å­˜å¤§å°ç­‰ï¼‰
    memory_allocated = torch.cuda.memory_allocated() # è·å–å½“å‰å·²åˆ†é…ç»™PyTorchçš„æ˜¾å­˜æ€»é‡
    ```
- ç§»åŠ¨å¼ é‡åˆ°GPUä¸Šæœ‰ä¸¤ç§æ–¹æ³•ï¼š

    - æ–¹æ³•ä¸€ï¼šç§»åŠ¨ç°æœ‰å¼ é‡
        ```
        y = x.to("cuda:0") # å°†å¼ é‡xç§»åŠ¨åˆ°ç¼–å·ä¸º0çš„GPUä¸Š
        assert y.device == torch.device("cuda", 0) # éªŒè¯ç§»åŠ¨æˆåŠŸ
        ```
    - æ–¹æ³•äºŒï¼šç›´æ¥åœ¨GPUä¸Šåˆ›å»ºå¼ é‡
        ```
        z = torch.zeros(32, 32, device="cuda:0") # åœ¨GPUä¸Šç›´æ¥åˆ›å»ºä¸€ä¸ª32x32çš„é›¶çŸ©é˜µ
        ```
    è¿™ç§æ–¹æ³•æ›´é«˜æ•ˆï¼Œå› ä¸ºå®ƒé¿å…äº†å…ˆåœ¨CPUåˆ›å»ºå†ç§»åŠ¨çš„è¿‡ç¨‹ã€‚



- æœ€åï¼ŒéªŒè¯å†…å­˜åˆ†é…

    ```
    new_memory_allocated = torch.cuda.memory_allocated() # å†æ¬¡æŸ¥è¯¢å½“å‰æ˜¾å­˜å ç”¨
    memory_used = new_memory_allocated - memory_allocated # è®¡ç®—æ–°å¢çš„æ˜¾å­˜å ç”¨
    assert memory_used == 2 * (32 * 32 * 4) # éªŒè¯ï¼šæ–°å¢äº†2ä¸ª32x32çš„çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ æ˜¯4å­—èŠ‚çš„float
    ```


### 3.3.3 å¼ é‡çš„å­˜å‚¨

åœ¨ PyTorch ä¸­ï¼Œå¼ é‡æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**ä¸€ç»´æ•°ç»„ï¼ˆStorageï¼‰** åŠ ä¸Š **å…ƒæ•°æ®ï¼ˆMetadataï¼‰**ã€‚
å…ƒæ•°æ®ä¸­æœ€é‡è¦çš„æ˜¯ **Shapeï¼ˆå½¢çŠ¶ï¼‰** å’Œ **Strideï¼ˆæ­¥é•¿ï¼‰**ã€‚

> **æ¦‚å¿µè§£é‡Šï¼šStrideï¼ˆæ­¥é•¿ï¼‰**
> å‡è®¾ä½ æœ‰ä¸€ä¸ª $4 \times 4$ çš„çŸ©é˜µå­˜æˆäº†ä¸€è¡Œã€‚`stride` å‘Šè¯‰ç¨‹åºï¼šå¦‚æœä½ æƒ³åœ¨ç¬¬ 0 ç»´ï¼ˆè¡Œï¼‰ç§»åŠ¨ä¸€æ­¥ï¼Œç”±äºä¸€è¡Œæœ‰ 4 ä¸ªå…ƒç´ ï¼Œä½ éœ€è¦åœ¨å†…å­˜é‡Œè·³è¿‡ 4 ä¸ªä½ç½®ï¼›å¦‚æœåœ¨ç¬¬ 1 ç»´ï¼ˆåˆ—ï¼‰ç§»åŠ¨ä¸€æ­¥ï¼Œåªéœ€è¦è·³è¿‡ 1 ä¸ªä½ç½®ã€‚

![](images/3-3-tensor-memory.png)

**å…³é”®ç‚¹ï¼š** å¾ˆå¤šæ“ä½œï¼ˆå¦‚ `view`, `transpose`, `slice`ï¼‰**ä¸ä¼šå¤åˆ¶æ•°æ®**ï¼Œåªæ˜¯ä¿®æ”¹äº† strideã€‚è¿™å« **Zero-copy**ï¼Œéå¸¸é«˜æ•ˆã€‚

```python
x = torch.randn(32, 32)
y = x.view(1024) # åªæ˜¯æ”¹å˜äº†çœ‹å¾…æ•°æ®çš„æ–¹å¼ï¼Œå†…å­˜åœ°å€æ²¡å˜
z = x.transpose(0, 1) # ä¿®æ”¹äº† strideï¼Œæ²¡æœ‰å¤åˆ¶æ•°æ®
```

**å‘ç‚¹ï¼š** å¦‚æœä½ å¯¹è½¬ç½®åçš„å¼ é‡è°ƒç”¨ `.view()`ï¼Œå¯èƒ½ä¼šæŠ¥é”™ã€‚å› ä¸ºè½¬ç½®åå†…å­˜ä¸å†è¿ç»­ï¼ˆContiguousï¼‰ã€‚ä½ éœ€è¦å…ˆè°ƒç”¨ `.contiguous()`ï¼Œè¿™ä¼šè§¦å‘æ•°æ®å¤åˆ¶ï¼ˆæ¶ˆè€—å†…å­˜å’Œæ—¶é—´ï¼‰ã€‚


---

## 3. è®¡ç®—æ•ˆç‡

### 3.1 æµ®ç‚¹è¿ç®—æ€»æ•°ï¼ˆFLOPsï¼‰

FLOPï¼ˆFloating Point Operationï¼‰æ˜¯æµ®ç‚¹è¿ç®—çš„åŸºæœ¬å•ä½ï¼Œå¦‚åŠ æ³•æˆ–ä¹˜æ³•ã€‚ç†è§£FLOPså¯¹æ€§èƒ½åˆ†æè‡³å…³é‡è¦ï¼š

- FLOPsï¼ˆå°å†™sï¼‰ï¼šæ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ€»æ•°ï¼ˆè¡¡é‡è®¡ç®—é‡ï¼‰
- FLOP/sï¼ˆæ¯ç§’æµ®ç‚¹è¿ç®—ï¼‰ï¼šç¡¬ä»¶æ€§èƒ½æŒ‡æ ‡ï¼Œä¹Ÿå†™ä½œFLOPSï¼ˆå¤§å†™Sï¼‰

#### 3.1.1 çŸ©é˜µä¹˜æ³•çš„è®¡ç®—é‡

æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ˜¯çŸ©é˜µä¹˜æ³•ï¼ˆMatMulï¼‰ã€‚å‡è®¾è®¡ç®— $C = A \times B$ï¼Œå…¶ä¸­ $A$ æ˜¯ $(M, K)$ï¼Œ$B$ æ˜¯ $(K, N)$ï¼Œè¾“å‡ºçŸ©é˜µ $C$ æœ‰ $M \times N$ ä¸ªå…ƒç´ ã€‚

å¯¹äºç»“æœä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
- ä¸€è¡Œä¹˜ä¸€åˆ—ï¼š$K$ æ¬¡ä¹˜æ³•ã€‚
- ç´¯åŠ ç»“æœï¼š$K$ æ¬¡åŠ æ³•ã€‚

å› æ­¤ï¼ŒçŸ©é˜µä¹˜æ³•çš„è®¡ç®—é‡ï¼š**æ€» FLOPs** $\approx 2 \times M \times N \times K$ã€‚ 

#### 3.1.2 MFU (Model FLOPs Utilization)

MFU = (å®é™…FLOP/s) / (ç¡¬ä»¶å³°å€¼FLOP/s)ï¼Œå³ï¼š

$$ \text{MFU} = \frac{\text{å®æµ‹ FLOPS}}{\text{ç¡¬ä»¶ç†è®ºå³°å€¼ FLOPS}} $$
*   **< 20%**: ä»£ç å†™å¾—å¾ˆçƒ‚ï¼Œæˆ–è€…å—é™äºå†…å­˜å¸¦å®½ï¼ˆMemory Boundï¼‰ã€‚
*   **50% - 70%**: ä¼˜ç§€çš„æ°´å¹³ï¼ˆCompute Boundï¼‰ã€‚

---

## 4. æ¨¡å‹æ„å»ºä¸è®­ç»ƒåŸºç¡€


ä¸ºä»€ä¹ˆè®­ç»ƒ LLM æå…¶æ˜‚è´µï¼Ÿæˆ‘ä»¬éœ€è¦è®¡ç®—å‰å‘ä¼ æ’­ï¼ˆForwardï¼‰å’Œåå‘ä¼ æ’­ï¼ˆBackwardï¼‰ã€‚

### 4.1 æ¢¯åº¦è®¡ç®—åŸç†

#### 4.1.1 çº¿æ€§æ¨¡å‹çš„æ¨å¯¼
å‡è®¾æ¨¡å‹ä¸º $Y = XW$ï¼Œå…¶ä¸­ $X$ æ˜¯ $(B, D)$ï¼Œ$W$ æ˜¯ $(D, K)$ã€‚
1.  **å‰å‘ä¼ æ’­ (Forward)**:
    *   è®¡ç®— $X \times W$ã€‚
    *   FLOPs $\approx 2BDK$ã€‚
    *   è®°å‚æ•°é‡ $N = D \times K$ï¼Œåˆ™å‰å‘ FLOPs $\approx 2 \times B \times N$ã€‚

2.  **åå‘ä¼ æ’­ (Backward)**:
    æˆ‘ä»¬éœ€è¦è®¡ç®—ä¸¤ä¸ªæ¢¯åº¦ï¼š
    *   å¯¹æƒé‡çš„æ¢¯åº¦ $\nabla W = X^T \times \nabla Y$ã€‚è®¡ç®—é‡ $\approx 2BDK$ã€‚
    *   å¯¹è¾“å…¥çš„æ¢¯åº¦ $\nabla X = \nabla Y \times W^T$ï¼ˆä¸ºäº†ä¼ ç»™ä¸Šä¸€å±‚ï¼‰ã€‚è®¡ç®—é‡ $\approx 2BDK$ã€‚
    *   **åå‘æ€» FLOPs** $\approx 4BDK = 4 \times B \times N$ã€‚

> 6N æ³•åˆ™ï¼š$ è®­ç»ƒæ€» FLOPs = Forward (2N) + Backward (4N) = 6N \times Tokenæ•° $

è¿™å°±æ˜¯æ–‡ç« å¼€å¤´ä¼°ç®—å…¬å¼çš„ç”±æ¥ã€‚

---

### 4.2 æ¨¡å‹ä¸ä¼˜åŒ–å™¨ (Model & Optimizer)

ç°åœ¨æˆ‘ä»¬åŠ¨æ‰‹å†™ä»£ç ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç®€å•çš„æ·±åº¦çº¿æ€§ç½‘ç»œï¼ˆDeep Linear Networkï¼‰ï¼Œå¹¶æ‰‹åŠ¨å®ç°ä¼˜åŒ–å™¨ã€‚

#### 4.2.1 å‚æ•°åˆå§‹åŒ– (Initialization)

å¦‚æœä½ ç›´æ¥ç”¨ `torch.randn` è¿›è¡Œæ ‡å‡†é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–å‚æ•°ï¼Œéšç€å±‚æ•°å˜æ·±ï¼Œæ•°å€¼ä¼šå˜å¾—éå¸¸å¤§ï¼ˆçˆ†ç‚¸ï¼‰æˆ–éå¸¸å°ï¼ˆæ¶ˆå¤±ï¼‰ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ **Xavier (Kaiming) åˆå§‹åŒ–**ã€‚é€šè¿‡é™¤ä»¥ $\sqrt{\text{è¾“å…¥ç»´åº¦}}$ æ¥ç¼©æ”¾æƒé‡ï¼Œä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚

```python
class Linear(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        # âŒ é”™è¯¯åšæ³•ï¼šw = torch.randn(...)
        # âœ… æ­£ç¡®åšæ³•ï¼šXavier åˆå§‹åŒ–ï¼Œä¿æŒæ–¹å·®ä¸€è‡´
        self.weight = nn.Parameter(
            torch.randn(input_dim, output_dim) / np.sqrt(input_dim)
        )

    def forward(self, x):
        return x @ self.weight
```


#### 4.2.2 æ•°æ®åŠ è½½

LLM çš„æ•°æ®é›†é€šå¸¸æ˜¯ TB çº§åˆ«çš„ï¼ˆå¦‚ LLaMA æ•°æ®é›† 2.8TBï¼‰ï¼Œæ— æ³•ä¸€æ¬¡æ€§è¯»å…¥å†…å­˜ã€‚ä½¿ç”¨ `numpy.memmap`ã€‚å®ƒå°†ç£ç›˜æ–‡ä»¶æ˜ å°„åˆ°è™šæ‹Ÿå†…å­˜ï¼Œåªæœ‰å½“ä»£ç çœŸæ­£è®¿é—®æŸå—æ•°æ®æ—¶ï¼Œæ“ä½œç³»ç»Ÿæ‰ä¼šå°†å…¶åŠ è½½åˆ°ç‰©ç†å†…å­˜ä¸­ã€‚

```python
import numpy as np

def load_massive_data():
    # å‡è®¾ data.npy æœ‰ 1TB
    # mode='r' è¡¨ç¤ºåªè¯»ï¼Œä¸ä¼šæŠŠæ•´ä¸ªæ–‡ä»¶åŠ è½½è¿› RAM
    data = np.memmap("data.npy", dtype=np.int32, mode='r')
    
    # åƒæ“ä½œæ™®é€šæ•°ç»„ä¸€æ ·æ“ä½œå®ƒï¼ŒOS ä¼šå¤„ç†ç¼ºé¡µä¸­æ–­å’ŒåŠ è½½
    batch = data[0:1024] 
    return torch.tensor(batch)
```

è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é€šå¸¸æ˜¯å·¨å‹æ•´æ•°åºåˆ—ï¼ˆåˆ†è¯å™¨è¾“å‡ºï¼‰ï¼š

```
def data_loading():
    # 1. ä¿å­˜æ•°æ®ä¸ºnumpyæ•°ç»„
    orig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32)
    orig_data.tofile("data.npy")
    
    # 2. ä½¿ç”¨memmapæ‡’åŠ è½½ï¼ˆä¸ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
    data = np.memmap("data.npy", dtype=np.int32)
    
    # 3. é«˜æ•ˆæ‰¹å¤„ç†
    B = 2  # æ‰¹å¤§å°
    L = 4  # åºåˆ—é•¿åº¦
    x = get_batch(data, batch_size=B, sequence_length=L, device=get_device())
```

å¯ä»¥é€šè¿‡å›ºå®šå†…å­˜ï¼ˆPinned Memoryï¼‰çš„æ–¹æ³•è¿›è¡Œä¼˜åŒ–ï¼Œå›ºå®šå†…å­˜å…è®¸æ•°æ®ä¼ è¾“ä¸è®¡ç®—å¹¶è¡Œï¼Œå¤§å¹…æå‡ååé‡ã€‚

```
def get_batch(data, batch_size, sequence_length, device):
    # 1. é‡‡æ ·èµ·å§‹ä½ç½®
    start_indices = torch.randint(len(data) - sequence_length, (batch_size,))
    
    # 2. ç´¢å¼•æ•°æ®
    x = torch.tensor([data[start:start+sequence_length] for start in start_indices])
    
    # 3. å›ºå®šå†…å­˜ï¼ˆå…è®¸å¼‚æ­¥GPUä¼ è¾“ï¼‰
    if torch.cuda.is_available():
        x = x.pin_memory()  # å›ºå®šåˆ°CPUå†…å­˜é¡µ
    
    # 4. å¼‚æ­¥ä¼ è¾“åˆ°GPU
    x = x.to(device, non_blocking=True)
    return x
```


#### 4.2.3 ä¼˜åŒ–å™¨ (Optimizer)

æˆ‘ä»¬ä»¥ AdaGrad ä¸ºä¾‹ï¼ˆè™½ç„¶ç°åœ¨å¸¸ç”¨ AdamWï¼Œä½†åŸç†ç±»ä¼¼ï¼‰ã€‚ä¼˜åŒ–å™¨ä¸ä»…è¦æ›´æ–°å‚æ•°ï¼Œè¿˜è¦è®°ä½æ¯ä¸ªå‚æ•°çš„å†å²æ¢¯åº¦ä¿¡æ¯ï¼ˆçŠ¶æ€ï¼‰ã€‚

```
class AdaGrad(torch.optim.Optimizer):
    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                grad = p.grad.data
                # è·å–çŠ¶æ€ï¼ˆæ¢¯åº¦å¹³æ–¹å’Œï¼‰
                state = self.state[p]
                if 'sum_squared_grad' not in state:
                    state['sum_squared_grad'] = torch.zeros_like(p.data)
                
                # æ›´æ–°çŠ¶æ€ï¼šç´¯åŠ æ¢¯åº¦çš„å¹³æ–¹
                state['sum_squared_grad'] += grad ** 2
                
                # æ›´æ–°å‚æ•°ï¼šé™¤ä»¥ æ ¹å·ä¸‹(çŠ¶æ€)
                std = state['sum_squared_grad'].sqrt() + 1e-10
                p.data -= group['lr'] * grad / std
```

ä»¥ä¸¤å±‚çº¿æ€§æ¨¡å‹ä¸ºä¾‹ï¼ˆç»´åº¦Dï¼Œå±‚æ•°2ï¼Œæ‰¹å¤§å°Bï¼‰è¿›è¡Œä¼˜åŒ–å™¨å†…å­˜åˆ†æï¼š

```
# 1. å‚æ•°æ•°é‡
num_parameters = (D * D * 2) + D  # ä¸¤å±‚éšè—å±‚ + è¾“å‡ºå±‚

# 2. æ¿€æ´»å€¼æ•°é‡ï¼ˆå‰å‘ä¼ æ’­ä¸­éœ€è¦ä¿å­˜ç”¨äºåå‘ä¼ æ’­ï¼‰
num_activations = B * D * 2  # æ¯å±‚æ¯ä¸ªæ ·æœ¬çš„æ¿€æ´»å€¼

# 3. æ¢¯åº¦æ•°é‡ï¼ˆä¸å‚æ•°ç›¸åŒï¼‰
num_gradients = num_parameters

# 4. ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdaGradéœ€è¦å­˜å‚¨æ¢¯åº¦å¹³æ–¹ï¼‰
num_optimizer_states = num_parameters

# 5. æ€»å†…å­˜ï¼ˆå‡è®¾FP32ï¼Œ4å­—èŠ‚/å…ƒç´ ï¼‰
total_memory = 4 * (num_parameters + num_activations + num_gradients + num_optimizer_states)
```
ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚Adamçš„åŠ¨é‡å’Œæ–¹å·®ï¼‰é€šå¸¸å ç”¨ä¸å‚æ•°ç›¸åŒç”šè‡³æ›´å¤šçš„å†…å­˜ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ8å—H100ï¼ˆæ¯å—80GBï¼‰åªèƒ½è®­ç»ƒçº¦400äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆä½¿ç”¨AdamWï¼‰ã€‚

#### 4.2.4 è®­ç»ƒå¾ªç¯ï¼šæ•´åˆæ‰€æœ‰ç»„ä»¶

ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯æ•´åˆäº†æ‰€æœ‰å‰è¿°ç»„ä»¶ï¼š

```
def train_loop():
    # 1. ç”Ÿæˆåˆæˆæ•°æ®ï¼ˆçº¿æ€§å‡½æ•°ï¼‰
    D = 16
    true_w = torch.arange(D, dtype=torch.float32, device=get_device())
    def get_batch(B: int) -> tuple[torch.Tensor, torch.Tensor]:
        x = torch.randn(B, D).to(get_device())
        true_y = x @ true_w
        return x, true_y
    
    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = Cruncher(dim=D, num_layers=0).to(get_device())
    optimizer = SGD(model.parameters(), lr=0.01)
    
    # 3. è®­ç»ƒå¾ªç¯
    for t in range(num_train_steps):
        # è·å–æ•°æ®
        x, y = get_batch(B=4)
        
        # å‰å‘ä¼ æ’­ï¼ˆè®¡ç®—æŸå¤±ï¼‰
        pred_y = model(x)
        loss = F.mse_loss(pred_y, y)
        
        # åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
        loss.backward()
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)  # é‡Šæ”¾æ¢¯åº¦å†…å­˜
```


å¤§å‹æ¨¡å‹è®­ç»ƒå¯èƒ½æŒç»­æ•°å‘¨ï¼Œå´©æºƒä¸å¯é¿å…ã€‚ä¸ºäº†é˜²æ­¢è®­ç»ƒå´©æºƒï¼Œéœ€è¦åŠæ—¶ä¿å­˜è¿›åº¦ï¼š

```
def checkpointing():
    model = Cruncher(dim=64, num_layers=3).to(get_device())
    optimizer = AdaGrad(model.parameters(), lr=0.01)
    
    # 1. ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint = {
        "model": model.state_dict(),        # æ¨¡å‹å‚æ•°
        "optimizer": optimizer.state_dict(), # ä¼˜åŒ–å™¨çŠ¶æ€
        "step": 1000,                       # è®­ç»ƒæ­¥æ•°
        "loss": 0.123,                      # å½“å‰æŸå¤±
    }
    torch.save(checkpoint, "model_checkpoint.pt")
    
    # 2. åŠ è½½æ£€æŸ¥ç‚¹
    loaded_checkpoint = torch.load("model_checkpoint.pt")
    model.load_state_dict(loaded_checkpoint["model"])
    optimizer.load_state_dict(loaded_checkpoint["optimizer"])
```


#### 4.2.5 æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision)
ä¸ºäº†å…¼é¡¾é€Ÿåº¦ä¸ç²¾åº¦ï¼Œç°ä»£è®­ç»ƒé€šå¸¸é‡‡ç”¨æ··åˆç­–ç•¥ï¼š
*   **FP32**: å­˜å‚¨ä¸»æƒé‡ï¼ˆMaster Weightsï¼‰ï¼Œç¡®ä¿æ›´æ–°ç´¯ç§¯çš„ç²¾åº¦ã€‚
*   **BF16/FP16**: ç”¨äºå‰å‘å’Œåå‘ä¼ æ’­çš„è®¡ç®—ï¼Œä»¥åŠå­˜å‚¨æ¿€æ´»å€¼ã€‚

åœ¨éœ€è¦æ—¶ä½¿ç”¨é«˜ç²¾åº¦ï¼Œå…¶ä»–æ—¶å€™ç”¨ä½ç²¾åº¦ï¼š
```
def mixed_precision_training():
    # 1. å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼šFP32
    model = Cruncher(dim=64, num_layers=3).to(torch.float32)
    
    # 2. å‰å‘ä¼ æ’­ï¼šBF16
    with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
        output = model(input_bf16)
        loss = loss_fn(output, target)
    
    # 3. åå‘ä¼ æ’­ï¼šFP32æ¢¯åº¦
    loss.backward()
    
    # 4. ä¼˜åŒ–å™¨æ›´æ–°ï¼šFP32
    optimizer.step()
```

PyTorch æä¾›äº†è‡ªåŠ¨æ··åˆç²¾åº¦å·¥å…· `torch.amp` (Automatic Mixed Precision) æ¥è‡ªåŠ¨ç®¡ç†è¿™ä¸€è¿‡ç¨‹ã€‚

```
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()  # é˜²æ­¢æ¢¯åº¦ä¸‹æº¢

for input, target in data_loader:
    optimizer.zero_grad()
    
    # è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
    with autocast():
        output = model(input)
        loss = loss_fn(output, target)
    
    # ç¼©æ”¾æŸå¤±é˜²æ­¢æ¢¯åº¦ä¸‹æº¢
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

### ğŸ“š å‚è€ƒèµ„æ–™
*   [PyTorch Docs on Tensors](https://pytorch.org/docs/stable/tensors.html)
*   [Einops Tutorial](https://einops.rocks/) (å¼ºçƒˆæ¨èç”¨äºå¤„ç†å¤æ‚çš„ç»´åº¦å˜æ¢)
*   [FlashAttention Paper](https://arxiv.org/abs/2205.14135) (å…³äº IO æ„ŸçŸ¥çš„é«˜æ•ˆè®¡ç®—)
*   [NVIDIA H100 Datasheet](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet)