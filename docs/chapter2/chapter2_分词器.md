# ç¬¬äºŒç«  åˆ†è¯å™¨
åˆ†è¯å™¨æ˜¯è¿æ¥äººç±»è‡ªç„¶è¯­è¨€ä¸æœºå™¨è®¡ç®—çš„å…³é”®ï¼Œè´Ÿè´£å°†éç»“æ„åŒ–çš„æ–‡æœ¬é«˜æ•ˆåœ°ç¼–ç ä¸ºæ¨¡å‹å¯ç†è§£çš„`æ•°å­—åºåˆ—ï¼ˆTokensï¼‰`ã€‚å…¶åœ°ä½è‡³å…³é‡è¦ï¼Œå®ƒå†³å®šäº†æ¨¡å‹çš„è¾“å…¥å½¢å¼ã€‚æˆ‘ä»¬å¯ä»¥å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†ä¸ºä¸€ä½é˜…è¯»è€…ï¼Œåˆ†è¯å™¨åˆ™æ‰§è¡Œäº†æ–‡æœ¬çš„â€œè®¤çŸ¥æ‹†è§£â€å³å°†è¿ç»­çš„æ–‡æœ¬æµåˆ‡åˆ†ä¸ºå…·å¤‡æœ€å°è¯­ä¹‰æˆ–åŠŸèƒ½æ„ä¹‰çš„åŸºæœ¬å•å…ƒï¼Œç±»ä¼¼äºäººç±»é˜…è¯»æ—¶å¯¹è¯æ±‡æˆ–éŸ³èŠ‚çš„æœ¬èƒ½è¯†åˆ«ã€‚æœ¬ç« èŠ‚å°†å‰–æåˆ†è¯å™¨å®ç°æ–‡æœ¬â€œæ•°å­—åŒ–â€çš„æ ¸å¿ƒåŸç†ï¼Œå¹¶æ¢è®¨BPEç­‰ä¸»æµåˆ†è¯ç®—æ³•çš„å®é™…åº”ç”¨ã€‚

<div align="center">
   <img width="800" height="500" alt="1" src="https://github.com/user-attachments/assets/bc838c76-7eff-4479-a760-ef404fc48e89" />
   <p>å›¾2.1 åˆ†è¯å™¨ä¸LLM</p>
 </div>
 
## 2.1 è®­ç»ƒåˆ†è¯å™¨
åœ¨æˆ‘ä»¬æŠŠæµ·é‡æ•°æ®å–‚ç»™å¤§æ¨¡å‹ä¹‹å‰ï¼Œå¿…é¡»å…ˆç»è¿‡ä¸€é“å…³é”®å·¥åºâ€”â€”åˆ†è¯ã€‚åˆ†è¯å™¨å¸¸è¢«è§†ä¸ºLLMçš„ä¸€éƒ¨åˆ†ï¼Œä½†å®ƒå…¶å®æ‹¥æœ‰ç‹¬ç«‹çš„è®­ç»ƒç”Ÿå‘½å‘¨æœŸã€‚æˆ‘ä»¬éœ€è¦åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç»Ÿè®¡æ„å»ºå‡ºä¸€å¥—é«˜æ•ˆçš„**è¯å…ƒâ€”â€”æ•°å­—**ç¦»æ•£åºåˆ—è½¬åŒ–`è¯è¡¨ï¼ˆvocabï¼‰`ï¼Œè¿™ä¸ªæ˜ å°„è¿‡ç¨‹å†³å®šäº†æ¨¡å‹çœ¼ä¸­çš„ä¸–ç•Œæ˜¯ç”±å­—ã€è¯è¿˜æ˜¯æ›´ç¢çš„ç‰‡æ®µç»„æˆçš„ï¼Œç›´æ¥å½±å“åç»­æ¨¡å‹å¯¹è¯­ä¹‰çš„ç†è§£æ•ˆç‡ã€‚ä¹Ÿæ­£å› å¦‚æ­¤ï¼Œåˆ†è¯å™¨è™½ç‹¬ç«‹è®­ç»ƒå´ä¸LLMä¿æŒç€â€œå¼ºè€¦åˆâ€çš„å…³ç³»ã€‚

```
è®­ç»ƒä¸€ä¸ªç”¨äºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†è¯å™¨å¯ä»¥æ‹†æˆå››æ­¥ï¼šå‡†å¤‡è¯­æ–™ â†’ åˆå§‹åŒ–åŸºç¡€å•å…ƒ(å¯çœç•¥) â†’ ç»Ÿè®¡å¹¶è¿­ä»£åˆå¹¶ â†’ è¾“å‡ºäº§ç‰©å¹¶ç”¨äºç¼–ç ã€è§£ç ã€‚
```

<div align="center">
   
![1](https://github.com/user-attachments/assets/14b349a5-167d-4f66-825b-2951e87d0dc6)

   <p>å›¾2.2 åˆ†è¯å™¨è®­ç»ƒæµç¨‹</p>
 </div>
 
### 2.1.1 å‡†å¤‡è¯­æ–™

1. åœ¨å‡†å¤‡è¯­æ–™é˜¶æ®µï¼Œåº”å°½é‡æ”¶é›†è¦†ç›–ç›®æ ‡åº”ç”¨åœºæ™¯çš„å¤šæ ·åŒ–æ–‡æœ¬ï¼Œä»¥ä¾¿è®­ç»ƒå‡ºçš„è¯è¡¨å¯¹ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚
   
   - å‡†å¤‡ä¸åŒç±»å‹çš„æ–‡æœ¬ä¿¡æ¯æ¯”å¦‚å°è¯´ã€æ•£æ–‡ã€è¯—æ­Œç­‰ä¸åŒæè¿°é£æ ¼çš„ä¿¡æ¯ã€‚
   - å¤šç§è¯­è¨€çš„æ–‡æœ¬ä¿¡æ¯æ¯”å¦‚ä¸­æ–‡ã€è‹±æ–‡ã€éŸ©è¯­ã€æ³•è¯­ç­‰ã€‚
     
2. å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œæ¸…æ´—å’Œæ ‡å‡†åŒ–æ˜¯å¿…é¡»çš„æ­¥éª¤ï¼ŒåŒ…å«å»é™¤æˆ–å±è”½æ— å…³å…ƒæ•°æ®ã€ä¿®æ­£æˆ–åˆ é™¤ä¹±ç ä¸éæ³•å­—ç¬¦ã€ç»Ÿä¸€å­—ç¬¦ç¼–ç ä¸ºUTF-8ï¼Œå¹¶å¯¹é‡å¤æˆ–è¿‘é‡å¤æ ·æœ¬è¿›è¡Œå»é‡ä»¥å‡å°‘è®­ç»ƒåç§»ã€‚
3. å¯¹å¸¦æœ‰æ•æ„Ÿä¿¡æ¯æˆ–éšç§çš„è¯­æ–™è¦æå‰è¿›è¡Œè„±æ•å¤„ç†ä¸åˆè§„æ£€æŸ¥ï¼Œæ˜ç¡®å“ªäº›ä¿¡æ¯ä¸å¯ç”¨äºè®­ç»ƒå¹¶è®°å½•æ•°æ®æ¥æºä¸è®¸å¯ã€‚
      å¤„ç†ç¤ºä¾‹åŸºäºpythonå®ç°ï¼š
   
```python
   import re
   
   def desensitize(text):
       # 1.å¸¸è§ä¸­æ–‡å§“åï¼ˆç®€å•è§„åˆ™ï¼š2~3å­—ï¼Œå…¨ä¸­æ–‡ï¼‰
       text = re.sub(r'([\u4e00-\u9fa5]{2,3})(çš„)', r'[NAME]\2', text)
   
       # 2.æ‰‹æœºå·è„±æ•ï¼ˆ11ä½æ•°å­—ï¼‰
       text = re.sub(r'1[3-9]\d{9}', '[PHONE]', text)
   
       # 3.åœ°å€è„±æ•ï¼ˆå¦‚ â€œå±…ä½äºé‡åº†xxâ€¦åœ°æ–¹â€ï¼‰
       # æ•è·ï¼ˆå±…ä½äºã€ç°å±…ä½äºã€ç°å±…äºï¼‰å­—æ®µåé¢çš„â€œé‡åº†â€â€œåŒ—äº¬â€â€œä¸Šæµ·â€â€œå¹¿å·â€ ç­‰ +ä»»æ„å­—ç¬¦
       text = re.sub(r'(å±…ä½äº|ç°å±…ä½äº|ç°å±…äº)([\u4e00-\u9fa5A-Za-z0-9]+)', r'\1[PLACE]', text)
       return text
   
   # æµ‹è¯•
   text="å°æ˜çš„è”ç³»ç”µè¯æ˜¯13312311111ï¼Œç°åœ¨å±…ä½äºé‡åº†ä¸¤æ±Ÿæ–°åŒºæŸå°åŒºã€‚"
   print(f"å¤„ç†å‰:{text}")
   print(f"è„±æ•åï¼š{desensitize(text)}")
```
   
å¤„ç†å‰
      
>å°æ˜çš„è”ç³»ç”µè¯æ˜¯13312311111ï¼Œç°åœ¨å±…ä½äºé‡åº†ä¸¤æ±Ÿæ–°åŒºæŸå°åŒºã€‚
         
è„±æ•å
      
>[NAME]çš„è”ç³»ç”µè¯æ˜¯[PHONE]ï¼Œç°å±…ä½äº[PLACE]ã€‚
   
å¦‚æœå¥å­ä¸­å‡ºç°å§“åã€ç”µè¯å·ç ã€ä½å€ç­‰ç‰¹å®šä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯è¿™æ ·æ•°æ®è„±æ•ä¸ä»…æ˜¯ä¿æŠ¤éšç§ä¸åˆè§„çš„éœ€è¦ï¼ŒåŒæ—¶ä¹Ÿèƒ½è®©tokenizerçš„ç»Ÿè®¡è¿‡ç¨‹æ›´å¹²å‡€ã€æ›´ç¨³å®šã€‚å¤§é‡ç‹¬ç‰¹ä¸”é«˜åŸºæ•°çš„ä¿¡æ¯å¦‚*å§“åã€ç”µè¯å·ç ã€èº«ä»½è¯å·ç­‰*å¦‚æœä¸å¤„ç†ï¼Œä¼šåœ¨è¯­æ–™ä¸­ä»¥å‡ ä¹ä¸é‡å¤çš„å½¢å¼å‡ºç°ï¼Œä½¿åˆ†è¯å™¨åœ¨è®­ç»ƒæ—¶è¢«è¿™äº›â€œåªå‡ºç°ä¸€æ¬¡çš„éšæœºå­—ç¬¦ä¸²â€å¹²æ‰°ï¼Œä»è€Œäº§ç”Ÿå¤§é‡ä½ä»·å€¼çš„tokenç‰‡æ®µï¼Œé€šè¿‡è„±æ•æ›¿æ¢è¿™ç±»ä¿¡æ¯åæ¨¡å‹èƒ½å¤Ÿæ›´ä¸“æ³¨äºå­¦ä¹ çœŸæ­£é«˜é¢‘ã€æœ‰è§„å¾‹çš„è¯­è¨€ç»“æ„ï¼Œä½¿è¯è¡¨æ›´åŠ ç²¾ç®€åˆ†è¯æ•ˆæœæ›´ä¸€è‡´ï¼Œæ³›åŒ–èƒ½åŠ›ä¹Ÿæ›´å¼ºã€‚

å¦‚æœä¸‹æ¸¸ä»»åŠ¡æœ¬èº«éœ€è¦è¯†åˆ«çœŸå®å®ä½“ï¼ˆå¦‚ä¿¡æ¯æŠ½å–ç­‰ï¼‰ï¼Œè¿‡åº¦è„±æ•ä¼šå‰Šå¼±è®­ç»ƒä¿¡å·ã€‚å› æ­¤éœ€è¦åœ¨**ä¿æŠ¤éšç§**ä¸**ä¿ç•™å…³é”®è¯­ä¹‰ä¿¡æ¯**ä¹‹é—´è¿›è¡Œåˆç†çš„ç­–ç•¥é€‰æ‹©ä¸æƒè¡¡ã€‚
   
>tokenæ˜¯LLMçš„åŸºæœ¬è¾“å…¥å•ä½ï¼Œç”±åˆ†è¯å™¨æ ¹æ®ç»Ÿè®¡è§„åˆ™æŠŠæ–‡æœ¬æ‹†æˆçš„å­è¯ã€å­—ç¬¦æˆ–å­—èŠ‚ï¼Œå†æ˜ å°„æˆæ•°å­—IDã€‚

4. åœ¨å¤šè¯­è¨€æˆ–æ··åˆè¯­æ–™åœºæ™¯ä¸­ï¼Œåº”å½“ç»Ÿè®¡æ¯ç§è¯­è¨€çš„å æ¯”å¹¶è€ƒè™‘æ˜¯å¦å¯¹ä½èµ„æºè¯­è¨€åšè¿‡é‡‡æ ·æˆ–ä¸“é—¨ä¿ç•™ï¼Œä»¥é¿å…è¯è¡¨è¢«é«˜é¢‘è¯­è¨€ä¸»å¯¼å¯¼è‡´ä½é¢‘è¯­è¨€è¡¨ç°å·®å³è¯­æ–™ç±»å‹ã€è¯­è¨€ä¸å¹³è¡¡ä¼šå¯¼è‡´**tokenç¢ç‰‡åŒ–**ã€**å ç”¨æ›´å¤štoken**å¹¶é™ä½èµ„æºè¯­è¨€æ€§èƒ½ã€‚
   
   ä¾‹å¦‚å‡†å¤‡ä¸€ç§å¯ä»¥æ”¯æŒå››ç§è¯­è¨€çš„åˆ†è¯å™¨ï¼Œè¿™é‡Œå‡è®¾æå‰æ”¶é›†åˆ°çš„åŸå§‹æœªç»è¿‡ç¬¬7æ­¥å¤„ç†çš„å„è¯­è¨€åŸå§‹è¯­æ–™å æ¯”å¦‚ä¸‹ï¼š
   
| è¯­è¨€ | è¯­æ–™é‡ |
| :--- | ---: |
| ä¸­æ–‡ | 200 GB |
| è‹±æ–‡ | 150 GB |
| æ³•è¯­ | 10 GB |
| éŸ©æ–‡ | 5 GB |

>è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„å¤šè¯­è¨€è¯­æ–™ä¸å¹³è¡¡åœºæ™¯ã€‚è‹¥å°†ä¸Šè¿°è¯­æ–™ä¸ç»å¤„ç†ç›´æ¥æ··åˆè®­ç»ƒåˆ†è¯å™¨ï¼Œå…¶ç»Ÿè®¡è¿‡ç¨‹ä¼šè¢«ä¸­æ–‡å’Œè‹±æ–‡ä¸»å¯¼ï¼Œå¯¼è‡´æ³•è¯­ä¸éŸ©æ–‡çš„å¸¸è§å­—ä¸²åœ¨åˆå¹¶é˜¶æ®µéš¾ä»¥è¿›å…¥é«˜é¢‘ç»Ÿè®¡ï¼Œä»è€Œæ— æ³•å æ®è¶³å¤Ÿçš„è¯è¡¨ç©ºé—´ï¼Œæœ€ç»ˆåœ¨`vocab`ä¸­ä¼šå‡ºç°å¤§é‡è¢«åˆ‡å¾—è¿‡ç¢çš„`token`ï¼Œå½¢æˆä¸¥é‡ç¢ç‰‡åŒ–ï¼Œä¸‹æ¸¸LLMåœ¨æ³•è¯­ä¸éŸ©æ–‡ä»»åŠ¡ä¸Šä¼šå› æ­¤è¡¨ç°æ˜¾è‘—åŠ£åŒ–ã€‚

å› æ­¤åœ¨å‡†å¤‡è¯­æ–™çš„ç¬¬4æ­¥åº”å…ˆæŒ‰è¯­è¨€ç»Ÿè®¡è¯­æ–™å æ¯”ï¼Œå¹¶æ ¹æ®ç›®æ ‡èƒ½åŠ›è®¾å®šåˆç†çš„é‡‡æ ·ç­–ç•¥ã€‚ä¾‹å¦‚å°†è¯­æ–™æ¯”ä¾‹è°ƒæ•´ä¸º`ä¸­æ–‡:è‹±æ–‡:æ³•è¯­:éŸ©æ–‡=4:4:1:1`æˆ–è€…`é‡‡ç”¨å®Œå…¨å‡è¡¡ç­–ç•¥`ã€‚é€šè¿‡å¯¹é«˜èµ„æºè¯­è¨€ä¸‹é‡‡æ ·æˆ–å¯¹ä½èµ„æºè¯­è¨€è¿‡é‡‡æ ·ã€å¢å¼ºï¼Œå¯ä»¥è·å¾—æ›´ç¬¦åˆç›®æ ‡åˆ†å¸ƒçš„è®­ç»ƒè¯­æ–™ï¼Œå†ä½¿ç”¨éªŒè¯é›†è¯„ä¼°å„è¯­è¨€çš„tokenè¦†ç›–ç‡ã€å¹³å‡å…¶ç¢ç‰‡åŒ–ç¨‹åº¦ï¼Œä»¥ç¡®ä¿æœ€ç»ˆè¯è¡¨åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­å…·å¤‡ç¨³å®šä¸”å‡è¡¡çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
    
5. å»ºè®®ä¿ç•™ä¸€å°éƒ¨åˆ†æœªå‚ä¸è®­ç»ƒçš„éªŒè¯è¯­æ–™æ¯”å¦‚è®­ç»ƒé›†:éªŒè¯é›†ï¼99:1ï¼Œç”¨æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°åˆ†è¯å™¨å¯¹çœŸå®æ–‡æœ¬çš„ç¼–ç æ•ˆç‡ä¸å¹³å‡tokené•¿åº¦ç­‰ç»Ÿè®¡æŒ‡æ ‡ã€‚

### 2.1.2 åˆå§‹åŒ–åŸºç¡€å•å…ƒ

1. é¢„åˆ†è¯çš„ä¸»è¦ä»»åŠ¡æ˜¯å°†åŸå§‹æ–‡æœ¬åˆ‡åˆ†æˆå¯ç»Ÿè®¡ã€å¯åˆå¹¶çš„åŸºç¡€å•å…ƒï¼Œä¾‹å¦‚å­—ç¬¦ã€å­—èŠ‚æˆ–Unicodeç‰‡æ®µã€‚å¸¸è§ç­–ç•¥åŒ…æ‹¬åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ‡åˆ†ã€æŒ‰Unicodeç±»åˆ«åˆ’åˆ†ï¼Œæˆ–ç›´æ¥é‡‡ç”¨å­—èŠ‚çº§åˆ‡åˆ†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯å¹¶ä¸æ˜¯æ‰€æœ‰çš„åˆ†è¯å™¨éƒ½éœ€è¦ç”¨æˆ·æ˜¾å¼è¿›è¡Œé¢„åˆ†è¯ã€‚*ä¾‹å¦‚åŸºäºSentencePieceçš„åˆ†è¯å™¨å°†æ ‡å‡†åŒ–å’Œé¢„åˆ†è¯é€»è¾‘å†…ç½®ï¼Œå› æ­¤æ— éœ€åœ¨å¤–éƒ¨é¢å¤–æ‰§è¡Œé¢„åˆ†è¯æ­¥éª¤ã€‚*

   - åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ‡åˆ†ç­–ç•¥ï¼šä¸€ä¸ªå®Œæ•´çš„å¥å­ä¸­é‡åˆ°ç©ºæ ¼æˆ–è€…æ ‡ç‚¹ï¼ˆ.,!?[]{}...ï¼‰å¯ä»¥åˆ†ä¸ºç‹¬ç«‹çš„tokensï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºå¤§å¤šæ•°é¢„åˆ†è¯å¤„ç†è¿‡ç¨‹ã€‚
     
```python
      
   # åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹åˆ‡åˆ†çš„å®ç°ç¤ºä¾‹
   import re
         
   def part(text):
   # å°†æ ‡ç‚¹ç¬¦å·å•ç‹¬æ‹†å¼€ï¼Œå¹¶æŒ‰ç…§ç©ºæ ¼è¿›è¡Œåˆ†å‰²
   text = re.sub(r'([.,!?;:()"\'\[\]{}])', r' \1 ', text)
   tokens = text.split()
   return tokens
         
   # æµ‹è¯•
   s = "I like DataWhale."
   print(part(s))
   
```
   
è¾“å…¥
>I like DataWhale.
         
è¾“å‡ºtokenåˆ’åˆ†
>['I', 'like', 'DataWhale', '.']

   - Unicodeç±»åˆ«åˆ’åˆ†ç­–ç•¥ï¼šæ ¹æ®å­—ç¬¦ç±»å‹æ¯”å¦‚å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ã€ä¸­æ–‡ã€ç‰¹æ®Šå­—ç¬¦ç­‰è‡ªåŠ¨åˆ‡åˆ†tokenï¼Œä¸åŒUnicodeç±»åˆ«ä¼šå±äºä¸åŒtokenå—ï¼Œè¿™ç§æ–¹æ³•å¤©ç„¶é€‚ç”¨äºå¤šç§è¯­è¨€æ··åˆçš„æ–‡æœ¬ï¼Œæä¾›äº†å¯é çš„åŸºçº¿åˆ‡åˆ†ã€‚
        
```python
      # Unicodeç±»åˆ«åˆ’åˆ†token
      import unicodedata
      def unicode_category_type(ch):
          """æ ¹æ®Unicodeç±»åˆ«å°†å­—ç¬¦åˆ’ä¸ºï¼šä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€å…¶ä»–"""
          if '\u4e00' <= ch <= '\u9fff':
              return "CJK"
          if ch.isdigit():
              return "DIGIT"
          if ch.isalpha():
              return "ALPHA"
          return "OTHER"
      
      def tokenize_unicode_category(text):
          if not text:
              return []
      
          tokens = []
          current = text[0]
          current_type = unicode_category_type(current)
      
          for ch in text[1:]:
              ch_type = unicode_category_type(ch)
              if ch_type == current_type and ch_type != "OTHER":
                  # åŒç±»å­—ç¬¦ï¼Œç»§ç»­åˆå¹¶
                  current += ch
              else:
                  # Unicodeä¸åŒç±» â†’ åˆ‡åˆ†
                  tokens.append(current)
                  current = ch
                  current_type = ch_type
          tokens.append(current)
      
          # æœ€åå†æŠŠ"OTHER"ç±»å‹ï¼ˆæ ‡ç‚¹ç­‰ï¼‰æ‹†å¼€
          final_tokens = []
          for t in tokens:
              if unicode_category_type(t[0]) == "OTHER":
                  final_tokens.extend(list(t))
              else:
                  final_tokens.append(t)
          return final_tokens
      
      # æµ‹è¯•
      s = "Helloï¼ŒDataWhaleæˆç«‹äº2018å¹´12æœˆ6æ—¥ï¼è‡³ä»Šå·²æœ‰7å¹´çš„å†å²äº†~"
      print(tokenize_unicode_category(s))

```
     
è¾“å…¥
>Helloï¼ŒDataWhaleæˆç«‹äº2018å¹´12æœˆ6æ—¥ï¼è‡³ä»Šå·²æœ‰7å¹´çš„å†å²äº†~

è¾“å‡º
>['Hello', 'ï¼Œ', 'DataWhale', 'æˆç«‹äº', '2018', 'å¹´', '12', 'æœˆ', '6', 'æ—¥', 'ï¼', 'è‡³ä»Šå·²æœ‰', '7', 'å¹´çš„å†å²äº†', '~']

   - å­—èŠ‚çº§åˆ‡åˆ†ç­–ç•¥ï¼šå…ˆå°†æ¯ä¸ªå­—ç¬¦æ‹†æˆ[UTF-8å­—èŠ‚åºåˆ—](https://datatracker.ietf.org/doc/html/rfc3629)ï¼Œä¸ä¾èµ–è¯­è¨€ç§ç±»ã€å­—ç¬¦ï¼ŒæŒ‰ç…§å•ä¸ªå­—èŠ‚åºåˆ—å¾—åˆ°ä¸€ä¸ªç‹¬ç«‹çš„tokenã€‚

```python
     
      def tokenize_byte_level(text):
          tokens = []
          for ch in text:
              # å­—ç¬¦å¯¹åº”çš„UTF-8å­—èŠ‚åºåˆ—
              utf8_bytes = ch.encode("utf-8")
              hex_bytes = [f"{b:02X}" for b in utf8_bytes]
      
              # æ‰“å°è½¬æ¢è¿‡ç¨‹
              print(f"{ch} è½¬åŒ–ä¸ºUTF-8å­—èŠ‚åºåˆ—ï¼š{hex_bytes}")
      
              # åŠ å…¥tokenåˆ—è¡¨
              tokens.extend(hex_bytes)
          return tokens
      
      # æµ‹è¯•
      s = "All for learnersï¼"
      print(tokenize_byte_level(s))

```

è¾“å…¥
>All for learnersï¼

è¾“å‡ºtokenåˆ’åˆ†
>['41', '6C', '6C', '20', '66', '6F', '72', '20', '6C', '65', '61', '72', '6E', '65', '72', '73', 'EF', 'BC', '81']

     
**Unicodeä¸UTF-8çš„è”ç³»ï¼š**

  `Unicode`å°±åƒç»™å…¨ä¸–ç•Œæ‰€æœ‰å­—ç¬¦å‘çš„â€œèº«ä»½è¯å·â€ï¼Œä¸ç®¡æ˜¯è‹±æ–‡Aã€æ±‰å­—â€œä¸­â€ã€è¿˜æ˜¯emoji ğŸ˜„ç­‰ä¸åŒç±»å‹çš„å­—ç¬¦éƒ½åœ¨Unicodeé‡Œæœ‰ä¸€ä¸ªå”¯ä¸€ç¼–å·æ¯”å¦‚Aæ˜¯U+0041ï¼Œâ€œä¸­â€æ˜¯U+4E2Dã€‚ä½†â€œèº«ä»½è¯å·â€æœ¬èº«åªæ˜¯ä¸€ä¸ªæŠ½è±¡ç¼–å·ï¼Œç”µè„‘ä¸èƒ½ç›´æ¥å­˜å‚¨ã€‚

   `UTF-8`å°±åƒæŠŠè¿™ä¸ªå­—ç¬¦å¯¹åº”çš„â€œèº«ä»½è¯å·â€å†™è¿›ç”µè„‘çš„å…·ä½“æ–¹å¼ã€‚å®ƒè§„å®šè¿™ä¸ªå­—ç¬¦çš„ç¼–å·åº”è¯¥ç”¨å‡ ä¸ªå­—èŠ‚ã€æŒ‰ä»€ä¹ˆè§„åˆ™å†™ä¸‹æ¥ã€‚è‹±æ–‡å¸¸ç”¨å­—ç¬¦åœ¨UTF-8ä¸­åªéœ€è¦1ä¸ªå­—èŠ‚ï¼Œè€Œä¸­æ–‡é€šå¸¸éœ€è¦3ä¸ªå­—èŠ‚ã€‚ä¸ç®¡æ˜¯Unicodeè¿˜æ˜¯UTF-8éƒ½å¯ä»¥è¡¨ç¤ºä¸åŒç±»åˆ«çš„å­—ç¬¦ï¼Œä¸¤è€…é…åˆèµ·æ¥è®©è‡ªç„¶è¯­è¨€å¯ä»¥è¢«è®¡ç®—æœºå‡†ç¡®å­˜å‚¨ã€ä¼ è¾“å’Œè§£æï¼Œæ˜¯äººæœºäº¤äº’ä¹‹é—´çš„â€œæ¡¥æ¢â€ã€‚

<div align="center">
<img width="1236" height="612" alt="da455deb32b22e4285d51b04dfb9a063" src="https://github.com/user-attachments/assets/97e514fd-5def-405d-95c2-9a9208d6f067" />
   <p>å›¾2.3 tokenåºåˆ—é•¿åº¦å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“</p>
 </div>
 
åœ¨LLMçš„tokenåˆ’åˆ†ä¸­ï¼Œå¸¸è§ç­–ç•¥åŒ…æ‹¬åŸºäºè§„åˆ™çš„é¢„åˆ†è¯æŒ‰ç©ºæ ¼ä»¥åŠæ ‡ç‚¹åˆ‡åˆ†ã€æŒ‰Unicodeç±»åˆ«çš„åˆ†æ®µå¦‚è¿ç»­æ±‰å­—ã€è¿ç»­æ‹‰ä¸å­—æ¯æˆ–æ•°å­—...ä»¥åŠæ›´åº•å±‚çš„UTF-8å­—èŠ‚çº§åˆ‡åˆ†ã€‚å‰ä¸¤ç±»æ–¹æ³•åœ¨å¤„ç†ç¼ºä¹æ˜¾å¼åˆ†éš”ç¬¦æˆ–é•¿æ®µåŒç±»å­—ç¬¦<ins>ä¾‹å¦‚è¿ç»­çš„ä¸­æ–‡é•¿å¥ã€æ‹¼æ¥çš„ä»£ç æ ‡è¯†ç¬¦æˆ–å‹ç¼©åçš„å­—ç¬¦ä¸²æ—¶</ins>å­˜åœ¨å±€é™ï¼Œé¢„å¤„ç†é˜¶æ®µéš¾ä»¥æœ‰æ•ˆæ–­å¥ï¼Œåˆ†è¯å™¨å¯èƒ½è¢«è¿«é€€åŒ–ä¸ºå­—ç¬¦çº§åˆ‡åˆ†ï¼Œä»è€ŒæŠŠæ–‡æœ¬æ˜ å°„æˆæ›´é•¿çš„`token IDåºåˆ—`ï¼Œå¢åŠ `Transformer`çš„è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…ï¼Œå…¶ä¸­æ³¨æ„åŠ›çš„å¤æ‚åº¦è¿‘ä¼¼ä¸º $O(n^2)$ ã€‚
 
ç›¸æ¯”ä¹‹ä¸‹ï¼ŒUTF-8å­—èŠ‚çº§ç­–ç•¥å…·æœ‰æœ€å¼ºçš„é€šç”¨æ€§ï¼Œå®ƒæŠŠä»»æ„æ–‡æœ¬ç»Ÿä¸€æ‹†ä¸ºå­—èŠ‚åºåˆ—ï¼Œä»è€Œä»æ ¹æœ¬ä¸Šå‡å°‘æœªç™»å½•è¯ï¼ˆOOVï¼‰é—®é¢˜å¹¶è¦†ç›–ä»»æ„å­—ç¬¦é›†ã€‚ä½†å› ä¸ºå®ƒä»¥æœ€ç»†ç²’åº¦å¼€å§‹ï¼Œè®­ç»ƒæ—¶é€šå¸¸éœ€è¦æ›´å¤šè½®çš„å…±ç°ç»Ÿè®¡ä¸åˆå¹¶æ¥æŠŠé›¶æ•£å­—èŠ‚å‹ç¼©ä¸ºç´§å‡‘ä¸”å…·è¯­ä¹‰çš„tokenï¼Œæ‰èƒ½åœ¨Transformerè®¡ç®—æ•ˆç‡ä¸è¯­ä¹‰è¡¨å¾ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

>æœªç™»å½•è¯æ˜¯å½“LLMæ¨¡å‹åœ¨å¤„ç†æ–°çš„ã€å®é™…åº”ç”¨çš„æ–‡æœ¬æ—¶ï¼Œå¦‚æœé‡åˆ°ä¸€ä¸ªè¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„Tokenï¼Œé‚£ä¹ˆè¿™ä¸ªTokenå°±è¢«è§†ä¸ºä¸€ä¸ªOOVã€‚

2. å¯¹å¤§å¤šæ•°ä»¥ç©ºæ ¼ä¸ºè¯è¾¹ç•Œçš„è¯­è¨€ï¼Œå¯å…ˆç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰å•è¯è¾¹ç•Œå’Œæ ‡ç‚¹è¿›è¡Œåˆæ­¥åˆ‡å‰²ï¼Œè€Œå¯¹ä¸­æ–‡ã€æ—¥æ–‡ç­‰ä¸ä»¥ç©ºæ ¼ä¸ºè¯ç•Œçš„è¯­è¨€åˆ™é€šå¸¸é‡‡ç”¨é€å­—ç¬¦æˆ–åŸºäºå­—çš„åˆå§‹å•å…ƒæ¥ä¿è¯è¦†ç›–æ€§ã€‚
   
   å­—èŠ‚çº§é¢„åˆ†è¯çš„å¥½å¤„ï¼š
   - tokenåˆ©ç”¨ç‡æ›´é«˜ï¼Œæé«˜BPEåˆå¹¶tokençš„è‡ªç”±åº¦ä»¥åŠå°½å¯èƒ½åˆå¹¶å…±ç°é¢‘ç‡é«˜çš„å•ä¸ªå­—ç¬¦ï¼Œæé«˜æ–‡æœ¬ä¿¡æ¯å‹ç¼©ç‡ã€‚
   - å¯å…¼å®¹å¤„ç†å¤šç§è¯­è¨€ã€‚
   - å­¦åˆ°æ›´å¤šé«˜é¢‘ç‰‡æ®µï¼Œå‡å°‘æœªç™»å½•è¯çš„å‡ºç°æƒ…å†µï¼Œæ¨¡å‹æ¨ç†æ›´å¿«ï¼ˆtokenæ•°å°‘ï¼‰ã€‚
     
>æ–‡æœ¬å‹ç¼©ç‡ï¼šæŒ‡ä¸€æ®µæ–‡å­—è¢«è½¬æ¢æˆtokenï¼ˆæ•°å­—åŒ–ï¼‰åï¼Œç”¨å¤šå°‘tokenæ¥è¡¨ç¤ºå†…å®¹çš„ç´§å‡‘ç¨‹åº¦ï¼ŒåŒæ ·å†…å®¹ä½¿ç”¨çš„tokenè¶Šå°‘ï¼Œå‹ç¼©ç‡å°±è¶Šé«˜ã€‚

3. é¢„åˆ†è¯ç”Ÿæˆçš„åŸºç¡€å•å…ƒåºåˆ—å°†ä½œä¸ºåç»­ç»Ÿè®¡åˆå¹¶çš„è¾“å…¥ï¼ŒåŠ¡å¿…ä¿å­˜è¯¥åºåˆ—ä¸å¯¹åº”ä½ç½®ä¿¡æ¯ä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹åå¤é«˜æ•ˆæ›´æ–°ã€‚
   
   å®ç°ç¤ºä¾‹ï¼š
   
   ```python
   def btp_hex_list(text):
       """
       UTF-8å­—èŠ‚çº§é¢„åˆ†è¯ï¼Œè¿”å›ï¼š
           1. tokens: æ¯ä¸ªå­—ç¬¦çš„å­—èŠ‚åºåˆ—+ä½ç½®ä¿¡æ¯
           2. t: æ‰€æœ‰å­—èŠ‚çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²åˆ—è¡¨
       """
       tokens = []
       t = []
       for idx, char in enumerate(text):
           utf8_bytes = char.encode('utf-8')
           hex_bytes = ' '.join(f"{b:02X}" for b in utf8_bytes)
           tokens.append({
               'char': char,
               'bytes': hex_bytes, # å•ä¸ªå­—ç¬¦å¯¹åº”çš„UTF-8å­—èŠ‚åºåˆ—
               'start': idx,   # æ–‡æœ¬ä¿¡æ¯èµ·å§‹ä½ç½®
               'end': idx + 1   # æ–‡æœ¬ä¿¡æ¯ç»“æŸä½ç½®
           })
           # å°†æ¯ä¸ªå­—èŠ‚æ‹†æˆå•ä¸ªåå…­è¿›åˆ¶å­—ç¬¦ä¸²
           t.extend([f"{b:02X}" for b in utf8_bytes])
       return tokens, t
   
   # æµ‹è¯•
   text = "Hiï¼Œä½ å¥½ğŸ‹"
   tokens, t = btp_hex_list(text)
   for i in tokens:
       print(i)
   print(t)
   ```

### 2.1.3 ç»Ÿè®¡å¹¶è¿­ä»£æ›´æ–° 

1. **å­è¯å€™é€‰ç»Ÿè®¡**

éå†è¯­æ–™ä»¥æ”¶é›†ç”¨äºåç»­å†³ç­–çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå…·ä½“æ–¹æ³•éšç®—æ³•ä¸åŒè€Œå¼‚ï¼š

   - BPEï¼šç»Ÿè®¡å½“å‰ç¬¦å·å­—ç¬¦ã€å­è¯åºåˆ—ä¸­ç›¸é‚»å¯¹çš„å‡ºç°é¢‘æ¬¡ï¼Œæ¯æ¬¡è´ªå¿ƒåˆå¹¶å‡ºç°é¢‘æ¬¡æœ€é«˜çš„ç›¸é‚»å¯¹ï¼Œè¿­ä»£æ„å»ºè¯è¡¨â€”â€”å…¶å†³ç­–ä»…åŸºäºé¢‘ç‡ç»Ÿè®¡ã€‚
   - WordPieceï¼šè¯„ä¼°åˆå¹¶æˆ–ä¿ç•™æŸäº›å­è¯å¯¹å¯¹è¯­æ–™ä¼¼ç„¶å³è¯­è¨€æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ï¼Œé€‰æ‹©èƒ½æ˜¾è‘—æå‡è¯­æ–™æ‹Ÿåˆåº¦çš„åˆå¹¶æ“ä½œã€‚
   - Unigramï¼šä»ä¸€ä¸ªè¿‡å¤§çš„ç§å­è¯è¡¨å‡ºå‘ï¼Œåˆå§‹åŒ–æ¯ä¸ªtokençš„æ¦‚ç‡ã€‚
       
| ç®—æ³• | é€‚ç”¨åœºæ™¯ | å¸¸è§åˆ†è¯å™¨ç»“åˆ | å…¸å‹LLMæ¡†æ¶ |
| --- | --- | --- | --- |
| [BPE](https://arxiv.org/pdf/1508.07909)| å¿«é€Ÿã€ç®€å•ï¼Œé«˜é¢‘å­è¯å‹ç¼©å¥½ï¼›<br>å¤šè¯­è¨€ã€å¤§è¯­æ–™é€‚ç”¨ | å­—èŠ‚çº§BPE<br>æˆ–ä»£ç ç‚¹çº§BPE| GPTç³»åˆ—ã€RoBERTa|
| [WordPiece](https://huggingface.co/learn/llm-course/en/chapter6/6)| å¹³è¡¡OOVä¸è¯è¡¨å¤§å°ï¼›<br>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¸¸ç”¨ | ä»£ç ç‚¹ã€å­—ç¬¦çº§å®ç°<br>Google BERTåŸå§‹å®ç° | [BERT](https://arxiv.org/abs/1810.04805)ã€DistilBERTã€RoBERTa |
| [Unigram](https://arxiv.org/abs/1808.06226)| çµæ´»é€‚åº”è¯­æ–™åˆ†å¸ƒï¼›<br>å¤šè¯­è¨€ã€ä½é¢‘è¯å‹å¥½ | SentencePiece Unigramæ¨¡å¼<br>æ”¯æŒbyte-fallback | T5ã€mT5ã€å…¶ä»–ä½¿ç”¨ SentencePiece çš„æ¨¡å‹ |
| [SentencePiece](https://arxiv.org/pdf/1804.10959)| è·¨è¯­è¨€ã€é€šç”¨tokenizerï¼›<br>å¯ç§»æ¤ã€å¤šè¯­ç§é€‚ç”¨ | ä»£ç ç‚¹çº§BPEã€Unigram<br>byte-fallback | T5ã€mT5ã€å„ç§å¼€æºæ¨¡å‹å’Œè‡ªå®šä¹‰LLM |

æ€»ä½“æ¥è¯´ï¼Œä»¥ä¸Šå››ç§å­è¯åˆ†è¯ç®—æ³•å„æœ‰ç‰¹ç‚¹ï¼Œæ²¡æœ‰å“ªä¸€ç§æ˜¯ç»å¯¹æœ€å¥½çš„ã€‚é€‰æ‹©ç®—æ³•æ—¶åº”æ ¹æ®å…·ä½“çš„æ–‡æœ¬å†…å®¹ï¼ˆè¯­æ–™åˆ†å¸ƒï¼‰ã€ä»»åŠ¡ç±»å‹ï¼ˆç†è§£æˆ–ç”Ÿæˆï¼‰ã€è¯è¡¨è§„æ¨¡ä»¥åŠæ˜¯å¦éœ€è¦å¤„ç†å¤šè¯­è¨€æ¥å†³å®šï¼Œè¿™æ ·æ‰èƒ½è®©è®­ç»ƒå‡ºæ¥çš„LLMæ¨¡å‹å‘æŒ¥æœ€ä½³æ€§èƒ½ã€‚

2. **è¿­ä»£**
BPEã€WordPieceã€Unigramã€SentencePieceè¿™å››ç§è¿­ä»£ç®—æ³•ç®€è¦åˆ†æï¼š

   - BPEç®—æ³•ï¼šå¯ä»¥å€ŸåŠ©ç¬¬ä¸€æ­¥å­è¯å€™é€‰ç»Ÿè®¡æ•°æ®ä½œä¸ºåˆå§‹åŒ–æ•°æ®ï¼Œè¿›è¡Œå•ä¸ªtokenåˆå¹¶å½¢æˆæ–°çš„tokenï¼Œç„¶ååœ¨å¤šæ¬¡è¿­ä»£è¿‡ç¨‹ä¸­åŠ¨æ€ç»Ÿè®¡å…±ç°æ¬¡æ•°ï¼Œå¾—åˆ°æ–°çš„tokenã€‚
   - WordPieceç®—æ³•ï¼šä¼šåœ¨è¿­ä»£è¿‡ç¨‹ä¸­åŠ¨æ€ç»Ÿè®¡å½“å‰è¯æ±‡è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å­è¯å¯¹çš„å‡ºç°æƒ…å†µã€‚å…¶å…³é”®å¹¶éç®€å•åˆå¹¶é¢‘ç‡æœ€é«˜çš„å¯¹å­ï¼Œè€Œæ˜¯ä¼˜å…ˆé€‰æ‹©èƒ½æœ€å¤§æå‡è¯­æ–™æ•´ä½“ä¼¼ç„¶çš„å­è¯å¯¹ï¼Œä»è€Œå½¢æˆæ›´æœ‰è¡¨å¾æ„ä¹‰çš„tokenï¼Œä¸€ä¸ªå¸¸ç”¨çš„è¿‘ä¼¼è¯„åˆ†ä¸ºï¼š

$$
\text{score}(A,B)=\frac{P(A,B)}{P(A) \times P(B)}
$$

> è¯¥æ¯”å€¼è¡¡é‡Aä¸Bçš„â€œå…³è”æ€§â€æ˜¯å¦å¼ºäºç‹¬ç«‹å‡ºç°æ—¶çš„æœŸæœ›ã€‚å¦‚æœscore>1ï¼Œåˆ™è¯´æ˜Aä¸Bçš„ç»“åˆæ¯”éšæœºç‹¬ç«‹å‡ºç°æ›´æœ‰ä¹‰ï¼Œæ›´å¯èƒ½è¢«WordPieceåˆå¹¶ã€‚ã€Googleæ²¡æœ‰å…¬å¼€WordPieceç®—æ³•è¯¦æƒ…ï¼Œè¿™é‡Œå‚è€ƒ[huggingfaceç›¸å…³åŸç†ä»‹ç»](https://huggingface.co/learn/llm-course/en/chapter6/6)ã€‘

   - Unigramç®—æ³•ï¼šå®ƒåŸºäºä¸€ä¸ªå­è¯æ¦‚ç‡è¯­è¨€æ¨¡å‹ï¼Œå°†ä¸€ä¸ªå¥å­çš„æ¦‚ç‡å®šä¹‰ä¸ºå…¶æ‰€æœ‰å¯èƒ½åˆ†è¯æ–¹å¼æ¦‚ç‡çš„æ€»å’Œã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è¿­ä»£ä¼˜åŒ–å­è¯æ¦‚ç‡ï¼Œä½¿å¾—æ•´ä¸ªè¯­æ–™çš„ä¼¼ç„¶æœ€å¤§åŒ–ã€‚ç®—æ³•é‡‡ç”¨æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰æ–¹æ³•ï¼Œä¸»è¦åŒ…å«ä¸¤æ­¥ï¼š

      â‘ Eæ­¥ï¼ˆæœŸæœ›æ­¥ï¼‰ï¼šåœ¨å½“å‰è¯è¡¨å’Œå­è¯æ¦‚ç‡ä¸‹ï¼Œä¸ºè¯­æ–™ä¸­çš„æ¯ä¸ªå¥å­è®¡ç®—æœ€å¯èƒ½çš„åˆ†è¯æ–¹å¼æˆ–å‰nä¸ªé«˜æ¦‚ç‡åˆ†è¯æ–¹æ¡ˆï¼Œå¹¶æ®æ­¤ä¼°è®¡æ¯ä¸ªå­è¯åœ¨è¯­æ–™ä¸­çš„æœŸæœ›ä½¿ç”¨æ¬¡æ•°ã€‚

      â‘¡Mæ­¥ï¼ˆæœ€å¤§åŒ–æ­¥ï¼‰ï¼šæ ¹æ®Eæ­¥çš„ç»Ÿè®¡ç»“æœï¼Œæ›´æ–°æ¯ä¸ªå­è¯çš„æ¦‚ç‡ï¼Œä½¿æ•´ä½“è¯­æ–™çš„ä¼¼ç„¶æœ€å¤§åŒ–ã€‚

      â‘¢åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹ä¼šå‰ªæï¼ˆæ·˜æ±°ï¼‰æ¦‚ç‡è¾ƒä½çš„tokenå¦‚ä¸¢å¼ƒåº•éƒ¨10%~20%ï¼Œä»è€Œé€æ­¥æ”¶æ•›åˆ°ä¸€ä¸ªè¾ƒå°ä¸”ä¼˜åŒ–åçš„è¯è¡¨ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„ç›®æ ‡è¯è¡¨å¤§å°ã€‚è¿™ç§æ–¹æ³•ç›¸æ¯”BPEæˆ–WordPieceæ›´ä¾èµ–æ¦‚ç‡å»ºæ¨¡ï¼Œèƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒé•¿åº¦çš„å­è¯ï¼Œå¹¶è‡ªç„¶ä¿ç•™æœ€èƒ½è§£é‡Šè¯­æ–™çš„é«˜é¢‘æ®µã€‚

`æœ€å¤§åŒ–è¯­æ–™ä¼¼ç„¶`æŒ‡çš„æ˜¯åœ¨è®­ç»ƒåˆ†è¯å™¨æ—¶ï¼Œå­¦ä¹ ä¸€å¥—èƒ½è®©è®­ç»ƒè¯­æ–™æ•´ä½“æ¦‚ç‡æœ€å¤§çš„`token`åˆ’åˆ†æ–¹å¼å’Œè¯è¡¨ï¼Œä½¿è¯­æ–™ä¸­çš„å­—ç¬¦åºåˆ—å¯ä»¥è¢«æ›´é«˜æ¦‚ç‡ã€æ›´å¸¸è§çš„tokenç»„åˆè¡¨ç¤ºã€‚é€šä¿—åœ°è¯´å°±æ˜¯æŠŠæ–‡æœ¬åˆ‡æˆæœ€é¡ºç•…ã€æœ€ç¬¦åˆè¯­è¨€ç»Ÿè®¡è§„å¾‹çš„ç‰‡æ®µï¼Œè®©åŒä¸€å¥è¯åœ¨ä¸åŒåœ°æ–¹éƒ½èƒ½è¢«ç¨³å®šã€ç´§å‡‘åœ°åˆ‡åˆ†æˆä¸€è‡´çš„tokenåºåˆ—ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼šåˆ†è¯å™¨ä¼˜åŒ–çš„æ˜¯è¾“å…¥çš„è¡¨ç¤ºæ–¹å¼ï¼Œå®ƒæœ¬èº«å¹¶ä¸ä¼šè®©æ¨¡å‹â€œç†è§£â€è‡ªç„¶è¯­è¨€â€”â€”çœŸæ­£â€œç†è§£â€è¯­è¨€çš„èƒ½åŠ›æ˜¯åœ¨åç»­`Transformer`å‚ä¸çš„è®­ç»ƒä¸­è·å¾—çš„ã€‚ä½†æ›´åˆç†çš„tokenåˆ’åˆ†ä¼šè®©è¾“å…¥åˆ†å¸ƒæ›´æ¸…æ™°ä¸€è‡´ï¼Œä»è€Œé—´æ¥æå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚

   - SentencePieceç®—æ³•ï¼šå®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åˆ†è¯å·¥å…·å’Œå®ç°åº“ï¼Œèƒ½å¤Ÿç›´æ¥ä»åŸå§‹æ–‡æœ¬è®­ç»ƒå­è¯æ¨¡å‹ï¼Œå› æ­¤æ— éœ€ç”¨æˆ·åœ¨å¤–éƒ¨æ˜¾å¼æ‰§è¡Œé¢„åˆ†è¯æ­¥éª¤ã€‚å®ƒåœ¨å†…éƒ¨ä¼šå°†ç©ºæ ¼ã€è¯è¾¹ç•Œç­‰ä¿¡æ¯ç¼–ç ä¸ºç‰¹æ®Šå­—ç¬¦ï¼ˆè®­ç»ƒè¾“å‡ºä¸­å¸¸è§çš„`â–`ç”¨äºè¡¨ç¤ºè¯é¦–ç©ºæ ¼ï¼‰ï¼Œä»è€Œå¯ä»¥å°†ç©ºæ ¼ä¹Ÿä½œä¸ºå»ºè¡¨å¯¹è±¡ä¹‹ä¸€ï¼Œæ¥ç€ä¼šåœ¨è¿™äº›åˆå§‹tokenä¸Šåº”ç”¨BPEæˆ–Unigramç®—æ³•ï¼Œç”Ÿæˆæœ€ç»ˆçš„tokenè¯è¡¨åŠæ˜ å°„ã€‚

è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä¸­éœ€è¦ä¿æŒç‰¹æ®Šæ§åˆ¶tokenï¼ˆå¦‚`<PAD>`ã€`<UNK>`ã€`<CLS>`ã€`<MASK>`ç­‰ï¼‰ï¼Œåœ¨åˆ†è¯å™¨è¿­ä»£æ›´æ–°è¿‡ç¨‹ä¸­ä¸å‚ä¸ä¿®æ”¹ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿å®ƒä»¬çš„è¯â€”â€”æ•°å­—æ˜ å°„ä¿æŒå›ºå®šï¼Œç¼–ç åçš„ç¦»æ•£æ•°å­—åºåˆ—èƒ½å¤Ÿå‡†ç¡®è¿˜åŸä¸ºåŸå§‹æ–‡æœ¬ã€‚åŒæ—¶ï¼Œè¿™äº›tokenä¸ä¼šåœ¨ç»Ÿè®¡åˆå¹¶æˆ–æ¦‚ç‡ä¼˜åŒ–ä¸­è¢«æ‹†åˆ†æˆ–è¦†ç›–ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘ç¢ç‰‡åŒ–tokençš„å‡ºç°ã€‚æ— è®ºä½¿ç”¨BPEã€WordPieceã€SentencePieceè¿˜æ˜¯Unigramç­‰ç®—æ³•ï¼Œè¿™ä¸€ç­–ç•¥éƒ½é€‚ç”¨æœ‰åŠ©äºä¿æŠ¤å…³é”®tokençš„å®Œæ•´æ€§ï¼Œä¿è¯æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„ä¸€è‡´æ€§ã€‚

4. **é€šç”¨ç»ˆæ­¢æ¡ä»¶**
   ä¸åŒç®—æ³•å¯èƒ½é‡‡ç”¨ä¸åŒç­–ç•¥ï¼Œä½†æ ¸å¿ƒç›®æ ‡éƒ½æ˜¯å¾—åˆ°ä¸€ä¸ªå¯æ§å¤§å°ä¸”é«˜æ•ˆçš„è¯è¡¨ï¼š

   - **è¯è¡¨å¤§å°é™åˆ¶**ï¼šå½“è¯è¡¨è¾¾åˆ°é¢„è®¾å¤§å°å¦‚32kã€50kã€100kã€256k...æ—¶åœæ­¢ã€‚
   - **è¿­ä»£æ­¥æ•°é™åˆ¶**ï¼šè¾¾åˆ°æœ€å¤§åˆå¹¶æˆ–è®­ç»ƒæ­¥æ•°ï¼ˆåˆå¹¶æ¬¡æ•°ã€EMè¿­ä»£æ¬¡æ•°ï¼‰æ—¶åœæ­¢ã€‚
   - **ä½è´¡çŒ®å€™é€‰ç»ˆæ­¢**ï¼š

     - å¯¹BPEã€WordPieceï¼šæœ€é«˜åˆå¹¶é¢‘ç‡ä½äºé˜ˆå€¼æ—¶åœæ­¢ã€‚
     - å¯¹Unigramã€SPMï¼šå­è¯æ¦‚ç‡ä½äºé˜ˆå€¼ï¼Œåˆ é™¤åå‰©ä½™è¯è¡¨ç¨³å®šå³å¯åœæ­¢ã€‚
   
   - **è®­ç»ƒã€éªŒè¯é›†æŒ‡æ ‡æ”¶æ•›**ï¼šç”¨äºç›‘æ§ä¸è¯„ä¼°ç­‰æŒ‡æ ‡ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾ç›®æ ‡æˆ–æ”¶æ•›ã€‚

5. **å¤§è§„æ¨¡è¯­æ–™ä¼˜åŒ–**

   - åˆ†å¸ƒå¼ç»Ÿè®¡æˆ–åˆ†ç‰‡è®­ç»ƒã€‚
   - ç²¾ç¡®ã€è¿‘ä¼¼è®¡æ•°ã€‚
   - ç¡®å®šæ€§æ’åº+å›ºå®šéšæœºç§å­ä¿è¯è®­ç»ƒå¯å¤ç°ã€‚

6. **ç›‘æ§ä¸è¯„ä¼°æŒ‡æ ‡**

   - å¹³å‡tokené•¿åº¦å³æ¯è¯æˆ–æ¯å­—ç¬¦ã€æœªç™»å½•è¯ç‡ï¼ˆOOVï¼‰ã€‚
   - é«˜é¢‘tokenç¤ºä¾‹åˆç†æ€§ã€‚
   - å‹ç¼©ç‡ï¼ˆæ€»å­—ç¬¦æ•°æˆ–æ€»å­—èŠ‚æ•°/æ€»tokenæ•°ï¼‰ã€‚

> **æ€»ç»“**ï¼šåˆ†è¯å™¨è®­ç»ƒçš„æ ¸å¿ƒæ˜¯<ins>è¿­ä»£æ›´æ–°å€™é€‰å­è¯ â†’ æ§åˆ¶è¯è¡¨å¤§å°æˆ–æ”¶æ•›æŒ‡æ ‡ â†’ ç›‘æ§è´¨é‡æŒ‡æ ‡</ins>ï¼Œä¸åŒç®—æ³•ä»…åœ¨â€œå€™é€‰ç”Ÿæˆæ–¹å¼â€å’Œâ€œè¿­ä»£æ›´æ–°ç­–ç•¥â€ä¸Šæœ‰å·®å¼‚ã€‚


### 2.1.4 è¾“å‡ºäº§ç‰©å¹¶ç”¨äºç¼–ç ä¸è§£ç 

1. **å¯¼å‡ºæ ¸å¿ƒäº§ç‰©**
   è®­ç»ƒå®Œæˆåéœ€è¦å¯¼å‡ºè‡³å°‘ä¸¤ä¸ªå…³é”®æ–‡ä»¶ï¼š

   - **vocabæ–‡ä»¶**ï¼šè®°å½•æ‰€æœ‰tokenåŠå…¶å¯¹åº”çš„idï¼Œæ˜¯ç¼–ç å™¨å’Œè§£ç å™¨çš„æ ¸å¿ƒç´¢å¼•ã€‚
   - **mergesæ–‡ä»¶**ï¼šæŒ‰é¡ºåºè®°å½•æ‰€æœ‰å­è¯åˆå¹¶è§„åˆ™æˆ–æ¦‚ç‡æ¨¡å‹ã€‚äºŒè€…å…±åŒå†³å®štokenizerçš„ç¼–ç ä¸è§£ç é€»è¾‘ï¼Œå¹¶ç¡®ä¿ç¼–ç çš„å¯é€†ã€‚

2. **ä¸‹æ¸¸ä½¿ç”¨å‰çš„éªŒè¯ä¸è¯„ä¼°**

   - å°†tokenizeråº”ç”¨äºä¸€éƒ¨åˆ†éªŒè¯é›†åï¼Œå»ºè®®ç»Ÿè®¡ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š`å¹³å‡tokenæ•°ä¸æœ€å¤§é•¿åº¦åˆ†å¸ƒ`ï¼Œç›´æ¥å½±å“æ˜¾å­˜å ç”¨ã€è®­ç»ƒé€Ÿåº¦å’Œæ¨ç†æ•ˆç‡ï¼›`ç¢ç‰‡åŒ–æƒ…å†µ`ï¼Œæ£€æŸ¥å…³é”®å®ä½“ã€ä¸“ä¸šæœ¯è¯­æ˜¯å¦è¢«æ‹†å¾—è¿‡ç¢ï¼Œé¿å…å½±å“æ¨¡å‹ç†è§£ï¼›`è·¨è¯­è¨€tokenå¹³è¡¡åº¦`ï¼Œå¤šè¯­è¨€ä»»åŠ¡ä¸­éœ€ç¡®ä¿ä¸åŒè¯­è¨€çš„å¸¸è§æ¨¡å¼éƒ½æœ‰è¶³å¤Ÿçš„tokenæ”¯æŒã€‚

   - å¦‚æœåç»­éœ€è¦æ‰©è¡¨å¦‚åŠ å…¥æ–°é¢†åŸŸæœ¯è¯­ã€ä¸“ä¸šè¯æˆ–å“ç‰Œåç­‰ï¼Œå»ºè®®ä¼˜å…ˆé‡‡ç”¨è¿™äº›æ–¹å¼è€Œéå®Œå…¨é‡è®­tokenizerï¼š**å¢é‡è®­ç»ƒ**ã€**åŠ å…¥æ–°çš„mergesé¡¹**ã€**æ¸…ç†æä½é¢‘token**ã€‚

   - æ‰©è¡¨ååº”è¿›è¡Œä¸€æ¬¡**å›å½’æµ‹è¯•**ï¼Œç¡®ä¿ï¼šä¸æ—§æ¨¡å‹ä¿æŒå…¼å®¹ä¸”æ ¹æ®æ•°å­—åŒ–ç¼–ç å¯ä»¥è¿˜åŸå›åˆ°æœ€å¼€å§‹çš„è¾“å…¥æ–‡æœ¬ã€ä¸å‘ç”Ÿtokenåˆ†é…å†²çªæˆ–tokenè€—å°½é—®é¢˜ã€‚

3. **ç‰ˆæœ¬ç®¡ç†ä¸å¯å¤ç°æ€§ä¿è¯**
   è¯è¡¨ä¸mergesæ–‡ä»¶åº”çº³å…¥ä¸¥æ ¼çš„ç‰ˆæœ¬æ§åˆ¶ï¼ŒåŒ…æ‹¬ï¼šè¯­ä¹‰åŒ–ç‰ˆæœ¬å·ï¼ˆå¦‚1.2.0 ï¼‰ã€æ¯æ¬¡ä¿®æ”¹çš„å˜æ›´æ—¥å¿—ã€åœ¨è®­ç»ƒè„šæœ¬å’Œæ¨ç†pipelineä¸­æ˜¾å¼å›ºå®štokenizerç‰ˆæœ¬ï¼Œé˜²æ­¢æ¨¡å‹è®­ç»ƒé˜¶æ®µä¸æ¨ç†éƒ¨ç½²é˜¶æ®µä½¿ç”¨ä¸åŒtokenizerï¼Œå¯¼è‡´ç»“æœä¸å¯å¤ç°æˆ–æ€§èƒ½ä¸‹é™ã€‚

## 2.2 å¸¸ç”¨çš„åˆ†è¯å™¨
åœ¨NLPçš„å‘å±•å†ç¨‹ä¸­ï¼Œåˆ†è¯ç­–ç•¥ç»å†äº†å‡ æ¬¡é‡è¦çš„æ¼”å˜ã€‚æˆ‘ä»¬ä¸»è¦å…³æ³¨å››ç§æœ€å…¸å‹çš„èŒƒå¼ï¼šå­—ç¬¦ã€å­—èŠ‚ã€è¯çº§ã€BPEåˆ†è¯å™¨ï¼Œä»¥åŠç»“åˆè¯¾ç¨‹[lecture1](https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json)å„ä¸ªåˆ†è¯å™¨ä¼ªä»£ç è½¬åŒ–ä¸ºpythonä»£ç å®è·µã€‚

### 2.2.1 å­—ç¬¦åˆ†è¯å™¨

**åŸç†ä»‹ç»**

è¿™æ˜¯æœ€ç›´è§‚ã€æœ€ç®€å•çš„åˆ†è¯æ–¹å¼ï¼Œå®ƒå°†æ–‡æœ¬æ‹†è§£ä¸ºæœ€å°çš„å­—ç¬¦å•ä½å³å•ä¸ªå­—ç¬¦å½¢å¦‚è‹±è¯­ä¸­çš„å­—æ¯ï¼ˆa, b, cï¼‰æˆ–è€…ä¸­æ–‡é‡Œçš„å•å­—ï¼ˆä½ ï¼Œå¥½ï¼‰ã€‚

  - **ä¼˜ç‚¹ï¼š**
      - **è¯è¡¨æå°ï¼š** è‹±è¯­åªéœ€åŒ…å«26ä¸ªå­—æ¯+ç¬¦å·ï¼›ä¸­æ–‡åªéœ€åŒ…å«å¸¸ç”¨æ±‰å­—ï¼ˆçº¦å‡ åƒä¸ªï¼‰ã€‚
      - **æ— OOVé—®é¢˜ï¼š** ä»»ä½•ç”Ÿåƒ»è¯éƒ½æ˜¯ç”±åŸºç¡€å­—ç¬¦ç»„æˆçš„ï¼Œä¸ä¼šå‡ºç°â€œæœªçŸ¥è¯â€ã€‚
  - **ç¼ºç‚¹ï¼š**
      - **åºåˆ—è¿‡é•¿ï¼š** ä¸€å¥è¯å˜æˆå­—ç¬¦åï¼Œé•¿åº¦ä¼šå¢åŠ æ•°å€ï¼Œå¤§å¤§æ¶ˆè€—LLMå®è´µçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä»è€ŒåŠ å¤§LLMçš„transformerè®¡ç®—æ˜¾å­˜æ¶ˆè€—ã€‚
      - **è¯­ä¹‰ç¨€ç–ï¼š** å•ä¸ªå­—ç¬¦ï¼ˆå¦‚tï¼‰é€šå¸¸ä¸å…·å¤‡ç‹¬ç«‹çš„è¯­ä¹‰ï¼Œæ¨¡å‹éœ€è¦æ›´æ·±çš„ç½‘ç»œå±‚æ•°æ¥ç»„åˆå‡ºæ„ä¹‰ã€‚

   å®ç°ç¤ºä¾‹ï¼š
   ```python
      # å­—ç¬¦Tokenizer
   class CharacterTokenizer:
       def __init__(self):
           pass  # ä¸éœ€è¦é¢å¤–å‚æ•°ï¼Œç›´æ¥ç”¨ordã€chr
   
       def encode(self, text):
           """
           å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºå­—ç¬¦ç´¢å¼•åˆ—è¡¨ï¼ˆUnicode code pointsï¼‰
           """
           return [ord(ch) for ch in text]
   
       def decode(self, indices):
           """
           å°†ç´¢å¼•åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²
           """
           return ''.join([chr(i) for i in indices])
   
   # æµ‹è¯•ä»£ç 
   if __name__ == "__main__":
       tokenizer = CharacterTokenizer()
       string = "hiï¼Œå¾ˆå¥½çš„ï¼Œterrificï¼ğŸ‹"  # æµ‹è¯•å­—ç¬¦ä¸²
   
       # ç¼–ç 
       indices = tokenizer.encode(string)
       print("ç¼–ç ID:", indices)
   
       # è§£ç 
       reconstructed_string = tokenizer.decode(indices)
       print("è§£ç :", reconstructed_string)
   
       # éªŒè¯æ˜¯å¦å¯é€†
       assert string == reconstructed_string, "å­—ç¬¦ç¼–ç ã€è§£ç ä¸ä¸€è‡´!"
   
       # è®¡ç®—è¯æ±‡é‡ï¼ˆæœ€å¤§Unicode code point+1ï¼‰
       vocabulary_size = max(indices) + 1
       print("è¯æ±‡é‡ï¼ˆä¸Šé™ï¼‰", vocabulary_size)
   
       # ç®€å•å‹ç¼©ç‡è®¡ç®—
       def get_compression_ratio(text, indices):
           # å‹ç¼©ç‡ = åŸå­—ç¬¦ä¸²å­—èŠ‚æ•°/ç¼–ç ç´¢å¼•å­—èŠ‚æ•°
           import sys
           original_bytes = len(text.encode('utf-8'))
           encoded_bytes = len(indices) * 4  # å‡è®¾æ¯ä¸ªUnicode code pointç”¨4å­—èŠ‚å­˜å‚¨
           return original_bytes / encoded_bytes
   
       compression_ratio = get_compression_ratio(string, indices)
       print("å‹ç¼©æ¯”ç‡:", compression_ratio)
   ```

   
è¾“å…¥
> hiï¼Œå¾ˆå¥½çš„ï¼Œterrificï¼ğŸ‹

è¾“å‡º
> ç¼–ç ID: [104, 105, 65292, 24456, 22909, 30340, 65292, 116, 101, 114, 114, 105, 102, 105, 99, 65281, 128011]
> 
> å‹ç¼©æ¯”ç‡: 0.47058823529411764


### 2.2.2 å­—èŠ‚åˆ†è¯å™¨

**åŸç†ä»‹ç»**

è®¡ç®—æœºåº•å±‚å­˜å‚¨æ–‡æœ¬æœ¬è´¨ä¸Šéƒ½æ˜¯**å­—èŠ‚**ï¼Œåœ¨UTF-8ç¼–ç ä¸­ï¼Œè‹±æ–‡é€šå¸¸å 1ä¸ªå­—èŠ‚ï¼Œæ±‰å­—é€šå¸¸å 3ä¸ªå­—èŠ‚ã€‚å­—èŠ‚åˆ†è¯å™¨ç›´æ¥å¯¹äºŒè¿›åˆ¶å­—èŠ‚è¿›è¡Œæ“ä½œã€‚

  - **æ ¸å¿ƒé€»è¾‘ï¼š** ä¸å†ç»´æŠ¤â€œå­—ç¬¦â€çš„è¯è¡¨ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªå¤§å°ä¸º256çš„åŸºç¡€è¯è¡¨ï¼ˆ0x00åˆ°0xFFï¼‰ã€‚
  - **åº”ç”¨ï¼š** ç°ä»£LLMå¦‚GPT-4, Llamaé€šå¸¸ä¸å•ç‹¬ä½¿ç”¨çº¯å­—èŠ‚åˆ†è¯ï¼Œè€Œæ˜¯å°†å­—èŠ‚ä½œä¸ºBPEçš„åŸºç¡€å•ä½å³BBPEï¼Œè¿™æ ·å¯ä»¥å½»åº•è§£å†³è·¨è¯­è¨€å’Œç‰¹æ®Šç¬¦å·å¦‚emoji ğŸŒç­‰çš„ç¼–ç é—®é¢˜ã€‚

   ```python
   # Byte-level Tokenizerå®ç°
   class ByteTokenizer:
       def __init__(self):
           # vocabå°±æ˜¯0~255çš„256ä¸ªå€¼
           self.vocab_size = 256
   
       def encode(self, text: str):
           # å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºUTF-8å­—èŠ‚åºåˆ— è½¬ä¸ºintåˆ—è¡¨
           return list(text.encode("utf-8"))
   
       def decode(self, indices):
           # å°†intåˆ—è¡¨â†’bytesâ†’UTF-8 å­—ç¬¦ä¸²
           return bytes(indices).decode("utf-8")
   
   
   # è®¡ç®—å‹ç¼©ç‡
   def get_compression_ratio(text: str, indices):
       input_byte_len = len(text.encode("utf-8"))  # åŸå§‹å­—èŠ‚åºåˆ—é•¿åº¦
       token_len = len(indices)                   # tokenæ•°é‡
       return input_byte_len / token_len if token_len > 0 else 1
   
   # æµ‹è¯•
   if __name__ == "__main__":
   
       print("ä»¥ä¸‹æµ‹è¯•å•å­—èŠ‚ä¸å¤šå­—èŠ‚ UTF-8 å­—ç¬¦ï¼š")
       assert bytes("a", encoding="utf-8") == b"a"
       assert bytes("ğŸŒ", encoding="utf-8") == b"\xf0\x9f\x8c\x8d"
       print("æµ‹è¯•é€šè¿‡ï¼šUTF-8å•å­—èŠ‚ä¸å¤šå­—èŠ‚éªŒè¯å®Œæ¯•\n")
   
       tokenizer = ByteTokenizer()
       string = "Hello, ğŸŒ! ä½ å¥½!"
       print("åŸå§‹å­—ç¬¦ä¸²ï¼š", string)
       indices = tokenizer.encode(string)
       print("ç¼–ç åçš„byte tokenåºåˆ—ï¼š", indices)
   
       reconstructed_string = tokenizer.decode(indices)
       print("è§£ç ç»“æœï¼š", reconstructed_string)
   
       assert string == reconstructed_string
       print("\nRound-tripæµ‹è¯•é€šè¿‡")
   
       vocabulary_size = tokenizer.vocab_size
       print("\nè¯è¡¨å¤§å°:", vocabulary_size)
   
       compression_ratio = get_compression_ratio(string, indices)
       print("å‹ç¼©ç‡compression_ratio:", compression_ratio)
   
       assert compression_ratio == 1
       print("å‹ç¼©ç‡æµ‹è¯•é€šè¿‡ï¼ˆbyte tokenizeræ— å‹ç¼©ï¼‰ã€‚")
   ```

è¾“å…¥
> Hello, ğŸŒ! ä½ å¥½!

è¾“å‡º
> åŸå§‹å­—ç¬¦ä¸²ï¼š Hello, ğŸŒ! ä½ å¥½!
> 
>ç¼–ç åçš„byte tokenåºåˆ—ï¼š [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]
> 
>è§£ç ç»“æœï¼š Hello, ğŸŒ! ä½ å¥½!

å€¼å¾—æ³¨æ„çš„æ˜¯**å­—èŠ‚çº§åˆ†è¯å™¨çš„å‹ç¼©æ¯”æ’ç­‰äº1**ï¼ŒåŸå› åœ¨äºï¼š

- è¾“å…¥æ–‡æœ¬ä¸­å•ä¸ªå­—ç¬¦é¦–å…ˆè¢«ç¼–ç ä¸ºUTF-8å­—èŠ‚åºåˆ—ï¼›
- å­—èŠ‚çº§åˆ†è¯å™¨å°†æ¯ä¸€ä¸ªUTF-8å­—èŠ‚ï¼ˆ0-255ï¼‰ç›´æ¥ä½œä¸ºä¸€ä¸ªtokenï¼›
- å› æ­¤`tokenæ•°é‡=UTF-8å­—èŠ‚æ•°`ã€‚

æ‰€ä»¥

$$
compression_{ratio}
= \frac{\text{UTF-8 å­—èŠ‚é•¿åº¦}}{\text{token æ•°é‡}}
= \frac{N}{N}
= 1
$$

**ä¹Ÿå°±æ˜¯è¯´ï¼Œå­—èŠ‚çº§åˆ†è¯å™¨å®Œå…¨ä¸å…·å¤‡å‹ç¼©èƒ½åŠ›å³æ¯ä¸ªå­—èŠ‚å¯¹åº”ä¸€ä¸ªtokenï¼Œä¸ä¼šäº§ç”Ÿæ›´é•¿æˆ–æ›´çŸ­çš„è¯ç‰‡æ®µã€‚**

### 2.2.3 è¯çº§åˆ†è¯å™¨

**åŸç†ä»‹ç»**

åœ¨æ·±åº¦å­¦ä¹ æ—©æœŸï¼ˆå¦‚RNNæ—¶ä»£ï¼‰è¿™æ˜¯æœ€ä¸»æµçš„æ–¹æ³•ã€‚å®ƒåŸºäºç©ºæ ¼ï¼ˆè‹±æ–‡ï¼‰æˆ–åˆ†è¯ç®—æ³•ï¼ˆä¸­æ–‡ï¼‰å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå…·å¤‡ç‹¬ç«‹è¯­ä¹‰çš„â€œè¯â€ã€‚

  - **ä¼˜ç‚¹ï¼š** Tokenä¿ç•™äº†å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯æ¯”å¦‚"apple" ç›´æ¥å¯¹åº”ä¸€ä¸ªToken ID...ã€‚
  - **ç¼ºç‚¹ï¼š**
      - **è¯è¡¨çˆ†ç‚¸ï¼š** è‹±è¯­ä¸­ `look, looks, looked, looking` ä¼šè¢«è§†ä¸º4ä¸ªå®Œå…¨ä¸åŒçš„IDï¼Œå¯¼è‡´è¯è¡¨å·¨å¤§å‡ åä¸‡ç”šè‡³ä¸Šç™¾ä¸‡ã€‚
      - **OOV é—®é¢˜ä¸¥é‡ï¼š** é‡åˆ°æ²¡è§è¿‡çš„è¯å¦‚äººåã€æ–°é€ è¯ç­‰ï¼Œåªèƒ½æ ‡è®°ä¸º`<UNK>` ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ä»è€Œå½±å“LLMçš„è¡¨ç°èƒ½åŠ›ã€‚

   å®ç°ç¤ºä¾‹ï¼š
   ```python
   import regex
   
   # deepseek tokenizerä¸­ä½¿ç”¨çš„ç»å…¸æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
   TOKENIZER_REGEX =  r"\p{L}+|\p{N}+|[^\p{L}\p{N}\s]+|\s+"
   
   # å‹ç¼©ç‡è®¡ç®—
   def get_compression_ratio(text: str, segments):
       byte_len = len(text.encode("utf-8"))
       token_count = len(segments)
       return byte_len / token_count if token_count > 0 else 1
   
   
   # Word-level Tokenizerå®ç°
   class WordTokenizer:
       def __init__(self, pattern=r"\w+|."):
           """
           pattern: æ­£åˆ™è¡¨è¾¾å¼ï¼ˆé»˜è®¤åŸºç¡€ç‰ˆï¼šæŠŠè¿ç»­å­—æ¯æ•°å­—åˆæˆä¸€ä¸ªè¯ï¼‰
           """
           self.pattern = pattern
           self.word2id = {}
           self.id2word = {}
   
       def build_vocab(self, texts):
           """
           æ ¹æ®è®­ç»ƒæ–‡æœ¬åˆ—è¡¨å»ºç«‹è¯è¡¨
           """
           vocab = set()
           for text in texts:
               segments = regex.findall(self.pattern, text)
               vocab.update(segments)
   
           vocab = sorted(vocab)
           self.word2id = {w: i for i, w in enumerate(vocab)}
           self.id2word = {i: w for w, i in self.word2id.items()}
   
       def encode(self, text):
           """
           æ–‡æœ¬ â†’ å­—ç¬¦ä¸²ç‰‡æ®µ â†’ token idåˆ—è¡¨
           æœªç™»å½•è¯ UNK = -1
           """
           segments = regex.findall(self.pattern, text)
           return [self.word2id.get(seg, -1) for seg in segments], segments
   
       def decode(self, ids):
           """
           token ID â†’ åŸå§‹ç‰‡æ®µ â†’ æ‹¼æˆå­—ç¬¦ä¸²
           """
           return "".join(self.id2word.get(i, "<UNK>") for i in ids)
   
   # æµ‹è¯•
   if __name__ == "__main__":
   
       string = "It's so supercalifragilisticexpialidocious!ğŸ‘‹ğŸ‘‹"
       print("åŸå§‹å­—ç¬¦ä¸²ï¼š", string)
   
       # ä½¿ç”¨åŸºç¡€æ­£åˆ™åˆ†è¯ï¼ˆåŸºäºç©ºæ ¼å’Œæ ‡ç‚¹åˆ‡åˆ†ï¼‰
       basic_segments = regex.findall(r"\w+|.", string)
       print("åŸºç¡€æ­£åˆ™åˆ†è¯ç»“æœï¼š")
       print(basic_segments)
   
       # ä½¿ç”¨deepseeké£æ ¼æ­£åˆ™
       segments = regex.findall(TOKENIZER_REGEX, string)
       print(f"deepseeké£æ ¼åˆ†è¯ç»“æœï¼š{segments}")
   
       # æ„å»ºè¯è¡¨
       tokenizer = WordTokenizer(pattern=TOKENIZER_REGEX)
       tokenizer.build_vocab([string])
   
       print("è¯è¡¨å¤§å°ï¼š", len(tokenizer.word2id))
   
       # ç¼–ç 
       ids, segs = tokenizer.encode(string)
       print(f"ç¼–ç token IDsï¼š{ids}")
   
       # å­—èŠ‚åºåˆ—
       byte_tokens = [b for b in string.encode("utf-8")]
       print(f"UTF-8å­—èŠ‚åºåˆ—ï¼š{byte_tokens}")
   
       print(f"ç¼–ç segmentsï¼š{segs}")
   
       # è§£ç 
       decoded = tokenizer.decode(ids)
       print("è§£ç ç»“æœï¼š", decoded)
   
       # å‹ç¼©ç‡
       ratio = get_compression_ratio(string, segs)
       print("å‹ç¼©ç‡ï¼š", ratio)
   ```
è¾“å…¥   
>It's so supercalifragilisticexpialidocious!ğŸ‘‹ğŸ‘‹

è¾“å‡º
>åŸºç¡€æ­£åˆ™åˆ†è¯ç»“æœï¼š
>
>['It', "'", 's', ' ', 'so', ' ', 'supercalifragilisticexpialidocious', '!', 'ğŸ‘‹', 'ğŸ‘‹']
>
>deepseeké£æ ¼åˆ†è¯ç»“æœï¼š['It', "'", 's', ' ', 'so', ' ', 'supercalifragilisticexpialidocious', '!ğŸ‘‹ğŸ‘‹']
>
>è¯è¡¨å¤§å°ï¼š 7
>
>ç¼–ç token IDsï¼š[3, 2, 4, 0, 5, 0, 6, 1]
>
>å‹ç¼©ç‡ï¼š 6.375

### 2.2.4 BPEåˆ†è¯å™¨

**åŸç†ä»‹ç»**

è¿™æ˜¯ç›®å‰LLMï¼ˆGPT, BERT, Llamaç­‰ï¼‰æœ€ä¸»æµçš„åˆ†è¯ç®—æ³•ï¼ŒBPEæ˜¯ä¸€ç§è¯•å›¾åœ¨<ins>å­—ç¬¦çº§(ç²’åº¦å¤ªç»†)</ins>å’Œ<ins>è¯çº§(ç²’åº¦å¤ªç²—)</ins>ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

  - **æ ¸å¿ƒæ€æƒ³ï¼š** ç»Ÿè®¡è¯­æ–™ä¸­ç›¸é‚»å­—ç¬¦å¯¹å‡ºç°çš„é¢‘ç‡ï¼Œè¿­ä»£åœ°å°†**æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹**åˆå¹¶æˆä¸€ä¸ªæ–°çš„Tokenã€‚

  - **è¿‡ç¨‹ï¼š**
    1. åˆå§‹åŒ–ï¼šå°†å•è¯æ‹†æˆå­—ç¬¦åºåˆ—ã€‚
    2. ç»Ÿè®¡ï¼šè®¡ç®—æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡ï¼ˆå¦‚'e' å’Œ's'ç»å¸¸ä¸€èµ·å‡ºç°ï¼‰ã€‚
    3. åˆå¹¶ï¼šå°†é¢‘ç‡æœ€é«˜çš„å¯¹ï¼ˆ'e', 's'ï¼‰åˆå¹¶ä¸ºæ–° Token ('es')ã€‚
    4. å¾ªç¯ï¼šé‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯è¡¨å¤§å°ã€‚

   å®ç°å®ä¾‹ï¼šç®€æ˜“ç‰ˆBPEè®­ç»ƒè¿‡ç¨‹
   
   ```python
   import regex
   from collections import Counter
   
   # DeepSeeké£æ ¼æ­£åˆ™
   DEEPSEEK_REGEX = r"\p{L}+|\p{N}+|[^\p{L}\p{N}\s]+|\s+"
   
   # ä½¿ç”¨grapheme clusterä¿æŒemojiä¸è¢«æ‹†åˆ†
   def split_graphemes(token):
       return tuple(regex.findall(r'\X', token))
   
   # BPEè®­ç»ƒå‡½æ•°
   def train_bpe(texts, num_merges=50):
       """
       texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äºè®­ç»ƒBPEï¼‰
       num_merges: BPE è¿­ä»£åˆå¹¶çš„æ¬¡æ•°
       """
       # 1.æ„å»ºåˆå§‹vocabï¼ˆå­—ç¬¦çº§+</w>ç»“æŸç¬¦ï¼‰
       vocab = Counter()
       for text in texts:
           tokens = regex.findall(DEEPSEEK_REGEX, text)
           for token in tokens:
               chars = split_graphemes(token) + ('</w>',)
               vocab[chars] += 1
       merges = []
       for _ in range(num_merges):
           # ç»Ÿè®¡ç›¸é‚»pairå‡ºç°æ¬¡æ•°
           pairs = Counter()
           for word, freq in vocab.items():
               for i in range(len(word)-1):
                   pairs[(word[i], word[i+1])] += freq
           if not pairs:
               break
   
           # æ‰¾åˆ°æœ€å¸¸è§pair
           best_pair = max(pairs, key=pairs.get)
           merges.append(best_pair)
   
           # åˆå¹¶æ‰€æœ‰vocabä¸­çš„è¯¥pair
           new_vocab = {}
           for word, freq in vocab.items():
               w = []
               i = 0
               while i < len(word):
                   if i < len(word)-1 and (word[i], word[i+1]) == best_pair:
                       w.append(word[i]+word[i+1])
                       i += 2
                   else:
                       w.append(word[i])
                       i += 1
               new_vocab[tuple(w)] = freq
           vocab = new_vocab
       return merges, vocab
   
   # BPE Tokenizerç±»
   class BPETokenizer:
       def __init__(self, merges):
           self.merges = merges
   
       def encode_word(self, token):
           # åˆå§‹åˆ†æˆå­—ç¬¦+</w>
           word = list(split_graphemes(token)) + ['</w>']
           # æŒ‰mergeé¡ºåºä¾æ¬¡åˆå¹¶
           for pair in self.merges:
               i = 0
               new_word = []
               while i < len(word):
                   if i < len(word)-1 and (word[i], word[i+1]) == pair:
                       new_word.append(word[i]+word[i+1])
                       i += 2
                   else:
                       new_word.append(word[i])
                       i += 1
               word = new_word
           return word
   
       def encode(self, text):
           tokens = regex.findall(DEEPSEEK_REGEX, text)
           bpe_tokens = []
           for t in tokens:
               bpe_tokens.extend(self.encode_word(t))
           return bpe_tokens
   
       def decode(self, tokens):
           # æ‹¼æ¥tokenså¹¶å»æ‰ç»“å°¾</w>
           text = ''.join(tokens).replace('</w>', '')
           return text
   
   # æµ‹è¯•
   if __name__ == "__main__":
       train_texts = ["è¿™åªçŒ«ğŸˆå¾ˆå¯çˆ±", "the quick brown fox jumps over the lazy ğŸ•â€ğŸ¦º"]
       merges, vocab = train_bpe(train_texts, num_merges=20)
       print("BPEåˆå¹¶:", merges)
       tokenizer = BPETokenizer(merges)
       test_text = "æ•æ·çš„æ£•è‰²ç‹ç‹¸ğŸ¦Š"
       encoded = tokenizer.encode(test_text)
       print("ç¼–ç :", encoded)
       decoded = tokenizer.decode(encoded)
       print("è§£ç :", decoded)
   ```

è¾“å…¥
>test_text = "æ•æ·çš„æ£•è‰²ç‹ç‹¸ğŸ¦Š"

è¾“å‡º
>BPEåˆå¹¶: [(' ', '</w>'), ('t', 'h'), ('th', 'e'), ('the', '</w>'), ('è¿™', 'åª'), ('è¿™åª', 'çŒ«'), ('è¿™åªçŒ«', '</w>'), ('ğŸˆ', '</w>'), ('å¾ˆ', 'å¯'), ('å¾ˆå¯', 'çˆ±'), ('å¾ˆå¯çˆ±', '</w>'), ('q', 'u'), ('qu', 'i'), ('qui', 'c'), ('quic', 'k'), ('quick', '</w>'), ('b', 'r'), ('br', 'o'), ('bro', 'w'), ('brow', 'n')]
>
>ç¼–ç : ['æ•', 'æ·', 'çš„', 'æ£•', 'è‰²', 'ç‹', 'ç‹¸', '</w>', 'ğŸ¦Š', '</w>']

åœ¨BPEç¼–ç é˜¶æ®µï¼Œå¦‚æœæ²¡æœ‰`</w>`ç®—æ³•å¯èƒ½æŠŠ`the`é”™è¯¯åœ°æ‹†æˆ'th'ã€'e'æˆ–åœ¨åç»­åˆå¹¶æ—¶ä¸å…¶ä»–tokené”™è¯¯åˆå¹¶ã€‚åŠ ä¸Š`</w>`åï¼Œ`the`ä¼šè¢«è¡¨ç¤ºä¸º['t', 'h', 'e', '</w>']ï¼ŒBPEå°±çŸ¥é“è¿™æ˜¯ä¸€ä¸ªå®Œæ•´å•è¯çš„ç»“å°¾ä¸ä¼šè·¨å•è¯é”™è¯¯åˆå¹¶ï¼Œé‚£ä¹ˆè§£ç é˜¶æ®µå»æ‰`</w>`å°±èƒ½æŠŠtokenæ‹¼å›`the`ï¼Œä¿è¯åŸæ–‡æ¢å¤æ­£ç¡®ã€‚

*å› æ­¤ï¼Œ`</w>`çš„æ ¸å¿ƒä½œç”¨æ˜¯ä¿è¯å•è¯å®Œæ•´æ€§ï¼Œå¹¶è®©ç¼–ç å¯é€†å³å¯ä»¥ä»ç›¸åº”çš„æ•°å­—åºåˆ—è½¬åŒ–ä¸ºåŸæ–‡ã€‚*

**å››ç§åˆ†è¯å™¨å¯¹æ¯”è¡¨**

| åˆ†è¯å™¨ç±»å‹ | ç²’åº¦ | è¯è¡¨å¤§å° | æœªç™»å½•è¯ (OOV) | åºåˆ—é•¿åº¦ | ä»£è¡¨æ¨¡å‹ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| å­—ç¬¦çº§ | ç»† | å° (100-5k) | æ—  | éå¸¸é•¿ | Char-RNN |
| è¯çº§ | ç²— | æå¤§ (>100k) | ä¸¥é‡ | çŸ­ | Word2Vec, GloVe |
| **BPE** | **ä¸­ (è‡ªé€‚åº”)** | **é€‚ä¸­ (30k-100k)** | **æå°‘** | **é€‚ä¸­** | **GPT-4, Llama 3** |

é™¤äº†`åˆ†è¯å™¨çš„é€‰æ‹©`ä¸`è®­ç»ƒè¯­æ–™`ç›´æ¥å½±å“LLMçš„è¾“å…¥ç¨€ç–åº¦ä¸è¡¨ç¤ºæ•ˆç‡ã€‚ç”¨å¤§è§„æ¨¡ã€é«˜è´¨é‡ä¸”å¤šæ ·çš„è¯­æ–™è®­ç»ƒåˆ†è¯å™¨é€šå¸¸ä¼š`å‡å°‘tokenç¢ç‰‡åŒ–`å³ç”Ÿæˆæ›´å¸¸è§ã€æ›´ç¨³å®šçš„å­è¯å•å…ƒï¼Œä½¿å¾—åŒä¸€æ®µæ–‡å­—è¢«ç¼–ç ä¸ºæ›´å°‘çš„tokenï¼ŒåŒæ—¶åœ¨å›ºå®šçš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸‹å•ä½tokenæ‰¿è½½æ›´å¤šå®é™…ä¿¡æ¯ï¼Œè¿™æ„å‘³ç€æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çª—å£å†…â€œçœ‹åˆ°â€æ›´å¤šå†…å®¹â€”â€”ä»è€Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£å› ä¸Šä¸‹æ–‡é•¿åº¦å—é™å¼•èµ·çš„ä¿¡æ¯ä¸¢å¤±ã€‚

>æ³¨æ„ä¸Šè¿°æƒ…å†µè¿™ä¾èµ–äºè¯­æ–™çš„è¦†ç›–ä¸è´¨é‡ï¼›è‹¥è¯­æ–™åé¢‡æˆ–è¿‡åº¦åˆå¹¶ç½•è§è¯ï¼Œåè€Œå¯èƒ½æŸå®³å°‘æ•°è¯­è¨€æˆ–ä¸“ä¸šæœ¯è¯­çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

## 3 åˆ†æDeepSeekçš„åˆ†è¯å™¨

DeepSeekæ¨¡å‹å°¤å…¶æ˜¯Coderç³»åˆ—ï¼Œå¯¹ä»£ç å’Œä¸­è‹±æ–‡éƒ½è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ï¼Œæˆ‘ä»¬å°†åŠ è½½DeepSeek Coderæ¨¡å‹çš„å®˜æ–¹åˆ†è¯å™¨ã€‚
### 3.1 åŠ è½½DeepSeek Tokenizer
è¯·ç¡®ä¿transformersåº“å·²å®‰è£…

```python
# å®‰è£…transformersåº“
pip install transformers torch
```

æˆ‘ä»¬å°†åŠ è½½`deepseek-ai/deepseek-coder-6.7b-instruct`çš„åˆ†è¯å™¨ã€‚

```python
from transformers import AutoTokenizer
# ä½¿ç”¨DeepSeek Coderç³»åˆ—æ¨¡å‹çš„åˆ†è¯å™¨
MODEL_NAME = "deepseek-ai/deepseek-coder-6.7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {MODEL_NAME} çš„åˆ†è¯å™¨ã€‚")
print(f"åˆ†è¯å™¨è¯è¡¨å¤§å°V: {len(tokenizer.get_vocab())}")
```

### 3.2 DeepSeekåˆ†è¯å™¨çš„å¤„ç†é€»è¾‘
DeepSeekçš„åˆ†è¯å™¨åŸºäºBPEåœ¨å¤„ç†ä¸­ã€è‹±æ–‡å’Œä»£ç æ—¶å…·æœ‰å…¶ç‹¬ç‰¹çš„ç­–ç•¥ã€‚

ä¸¾ä¾‹åˆ†æ: ä¸­æ–‡æ–‡æœ¬å¤„ç†
è§‚å¯ŸDeepSeekå¦‚ä½•å¤„ç†ä¸­æ–‡çŸ­è¯­ï¼Œé€šå¸¸å®ƒä¹Ÿä¼šä½¿ç”¨å­è¯æˆ–å•ä¸ªæ±‰å­—Tokenæ¥æé«˜æ•ˆç‡ã€‚
```python
chinese_text = "æ³¨æ„åŠ›æœºåˆ¶æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ ğŸš€ ğŸš€"
# ç¼–ç 
encoded_ids = tokenizer.encode(chinese_text, add_special_tokens=False)
# è§£ç å›Tokenå­—ç¬¦ä¸² (ç”¨äºè§‚å¯Ÿå­è¯)
tokens = tokenizer.convert_ids_to_tokens(encoded_ids)
print(f"\nåŸæ–‡: {chinese_text}")
print(f"ç¼–ç : {tokens}")
print(f"IDs:{encoded_ids}")
```

å¾—åˆ°tokenå­—ç¬¦ä¸²åˆ’åˆ†ç»“æœå¯èƒ½åœ¨æ˜¾ç¤ºä¸Šä¸åŸæ–‡æœ‰æ‰€å·®å¼‚è¿™å¹¶ä¸æ˜¯ç¼–ç æœ¬èº«å‡ºé”™ï¼Œè€Œæ˜¯å› ä¸ºLLMæ‰€ç”¨çš„**è¯è¡¨**åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­<ins>å¯¹æŸäº›å­—ç¬¦æˆ–å­è¯çš„è¦†ç›–ä¸è¶³</ins>ï¼ˆä¾‹å¦‚BPEè®­ç»ƒä¸å¤Ÿå……åˆ†ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•ç”Ÿæˆå¯¹åº”çš„tokenï¼Œä»è€Œåœ¨å¯è¯»å½¢å¼ä¸Šçœ‹èµ·æ¥åƒâ€œä¹±ç â€ã€‚å¯ä»¥é€šè¿‡å¢åŠ è®­ç»ƒè¯­æ–™é‡æˆ–è¿›è¡Œå……åˆ†çš„BPEè®­ç»ƒï¼Œå¯ä»¥å­¦ä¹ åˆ°æ›´å®Œæ•´çš„tokenæ˜ å°„è¯è¡¨ï¼Œä»è€Œè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰å­—ç¬¦éƒ½èƒ½è¢«æ­£ç¡®ç¼–ç å’Œè§£ç ã€‚æ¥ä¸‹æ¥æ˜¯ç›¸åº”çš„è§£å†³åŠæ³•å³è®­ç»ƒBPEï¼š

```python
"""
DeepSeek-V3 Tokenizerç®€æ˜“å®ç°ç¤ºä¾‹
ï¼ˆæ ¸å¿ƒåŒ…å«ï¼šå­—èŠ‚çº§BPE+DeepSeeké£æ ¼æ­£åˆ™é¢„åˆ†è¯ï¼‰
"""
import regex as re
from collections import Counter
from typing import List, Tuple, Dict, Iterable
import json
import base64


# é…ç½®ï¼šDeepSeek æ­£åˆ™æ¨¡å¼ï¼ˆé¢„åˆ†è¯ï¼‰
# \p{L}+   è¿ç»­å­—æ¯ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€æ‰€æœ‰ Unicode å­—æ¯ï¼‰
# \p{N}+   è¿ç»­æ•°å­—
# [^\p{L}\p{N}\s]+  éå­—æ¯æ•°å­—ç©ºç™½çš„å­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ã€emojiï¼‰
# \s+      è¿ç»­ç©ºç™½ç¬¦
DEEPSEEK_REGEX = r"\p{L}+|\p{N}+|[^\p{L}\p{N}\s]+|\s+"


# åŸºç¡€å‡½æ•°ï¼šé¢„åˆ†è¯ä¸å­—èŠ‚å¤„ç†
def pretokenize(text:str):
    """æŒ‰DeepSeeké£æ ¼çš„æ­£åˆ™è¿›è¡Œé¢„åˆ†è¯"""
    return re.findall(DEEPSEEK_REGEX, text)

def bytes2tokens(b:bytes):
    """
    å°†UTF-8å­—èŠ‚åºåˆ—è½¬ä¸ºlatin1å¯è¡¨ç¤ºçš„tokenåˆ—è¡¨ã€‚
    æ¯ä¸ªå­—èŠ‚0â€“255éƒ½èƒ½è¢«latin1æ¥æ˜ å°„åˆ°å­—ç¬¦ã€‚
    """
    return [bytes([x]).decode('latin1') for x in b]

def tokens2bytes(tokens):
    """å°†latin1 tokenåˆ—è¡¨é‡æ–°è½¬å›åŸå§‹bytes"""
    return b''.join([t.encode('latin1') for t in tokens])


# BPEè®­ç»ƒç›¸å…³
def build_corpus(texts):
    """
    æ„å»ºbyte-levelè¯­æ–™ã€‚
    æ­¥éª¤ï¼šé¢„åˆ†è¯ â†’ UTF-8ç¼–ç  â†’ åˆ†è§£ä¸ºå•å­—èŠ‚ â†’ ä½œä¸ºåˆå§‹tokenåºåˆ—ã€‚
    """
    corpus = []
    for text in texts:
        for chunk in pretokenize(text):
            corpus.append(bytes2tokens(chunk.encode('utf-8')))
    return corpus

def pair_freq(corpus: List[List[str]]):
    """ç»Ÿè®¡æ‰€æœ‰tokenåºåˆ—ä¸­ç›¸é‚»token pairçš„å‡ºç°é¢‘ç‡"""
    pairs = Counter()
    for word in corpus:
        for i in range(len(word)-1):
            pairs[(word[i], word[i+1])] += 1
    return pairs

def merge_pair(word: List[str], pair: Tuple[str,str]):
    """å°†æŒ‡å®šçš„token pairåˆå¹¶æˆä¸€ä¸ªtoken"""
    a, b = pair
    merged = []
    i = 0
    while i < len(word):
        if i < len(word)-1 and word[i]==a and word[i+1]==b:
            merged.append(a+b)   # åˆå¹¶ä¸ºä¸€ä¸ªæ–°token
            i += 2
        else:
            merged.append(word[i])
            i += 1
    return merged

def train_bpe(texts: Iterable[str], vocab_size: int=5000, num_merges: int=None) -> Tuple[List[Tuple[str,str]], List[str]]:
    """
    è®­ç»ƒå­—èŠ‚çº§BPE
    """
    corpus = build_corpus(texts)
    base_tokens = [bytes([i]).decode('latin1') for i in range(256)]
    merges: List[Tuple[str,str]] = []
    merged_set = set()
    cur_vocab_size = 256

    # è‹¥æœªæŒ‡å®šåˆå¹¶æ¬¡æ•°ï¼Œåˆ™ç”±target vocabæ¥å†³å®š
    merge_steps = num_merges or (vocab_size - 256)

    for _ in range(merge_steps):
        pfreq = pair_freq(corpus)
        if not pfreq:
            break

        # æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„pair
        best_pair, _ = pfreq.most_common(1)[0]

        if cur_vocab_size + 1 > vocab_size:
            break

        merges.append(best_pair)

        # å¯¹æ•´ä¸ªè¯­æ–™è¿›è¡Œåˆå¹¶æ›¿æ¢
        corpus = [merge_pair(word, best_pair) for word in corpus]

        # å°†æ–°tokenè®°å…¥è¯è¡¨
        merged_set.add(best_pair[0]+best_pair[1])
        cur_vocab_size += 1

    # è¿½åŠ ç‰¹æ®Štoken
    special_tokens = ["<pad>", "<bos>", "<eos>", "<unk>"]

    # vocab = ç‰¹æ®Štoken+ 256 byte token +BPEåˆå¹¶çš„æ–°token
    vocab_tokens = special_tokens + base_tokens + sorted(merged_set)

    return merges, vocab_tokens



# Tokenizerç±»
class DeepSeekV3Tokenizer:
    def __init__(self, merges: List[Tuple[str,str]], vocab_tokens: List[str]):
        self.merges = merges
        self.vocab_tokens = vocab_tokens

        # token â†” idæ˜ å°„
        self.token2id = {tok:i for i, tok in enumerate(vocab_tokens)}
        self.id2token = {i:tok for tok,i in self.token2id.items()}

        # merges pair â†’ æ’åºindex
        self.ranks = {pair:i for i,pair in enumerate(merges)}

        # ç‰¹æ®Štoken
        self.pad_token = "<pad>"
        self.bos_token = "<bos>"
        self.eos_token = "<eos>"
        self.unk_token = "<unk>"

    def encode_chunk(self, chunk: str) -> List[str]:
        """
        å¯¹ä¸€ä¸ªé¢„åˆ†è¯åšBPEç¼–ç ï¼š
        - è½¬å­—èŠ‚token
        - é€æ­¥åº”ç”¨merges
        - å¤„ç†OOVï¼šæœªçŸ¥tokenæ‹†å›å­—èŠ‚æˆ–æ ‡è®°ä¸º<unk>
        """
        tokens = bytes2tokens(chunk.encode('utf-8'))

        # åº”ç”¨PEå¹¶è§„åˆ™
        for pair in self.merges:
            new_tokens = []
            i = 0
            a,b = pair
            while i < len(tokens):
                if i<len(tokens)-1 and tokens[i]==a and tokens[i+1]==b:
                    new_tokens.append(a+b)
                    i+=2
                else:
                    new_tokens.append(tokens[i])
                    i+=1
            tokens = new_tokens

        # OOV tokenæ‹†å›å­—èŠ‚
        out = []
        for t in tokens:
            if t in self.token2id:
                out.append(t)
            else:
                # æ‹†åˆ†æˆå­—èŠ‚tokenï¼Œå¦‚æœå­—èŠ‚tokenä¹Ÿä¸åœ¨è¯è¡¨ â†’ <unk>
                out.extend([ch if ch in self.token2id else self.unk_token for ch in t])
        return out

    def encode(self, text: str, add_bos=False, add_eos=False, print_chunks=False):
        """
        ç¼–ç å®Œæ•´æ–‡æœ¬ï¼š
        - å…ˆé¢„åˆ†è¯
        - å†é€chunkç¼–ç 
        - å¯é€‰æ‰“å°ä¸­é—´è¿‡ç¨‹
        """
        ids = []

        if add_bos:
            ids.append(self.token2id[self.bos_token])
            if print_chunks: print(f"[Special] <bos> -> {self.token2id[self.bos_token]}")

        for chunk in pretokenize(text):
            toks = self.encode_chunk(chunk)
            chunk_ids = [self.token2id.get(t, self.token2id[self.unk_token]) for t in toks]

            if print_chunks:
                readable = []
                for t in toks:
                    try:
                        # å°è¯•æ¢å¤utf-8
                        r = tokens2bytes([t]).decode('utf-8', errors='ignore')
                        readable.append(r if r else t.encode('latin1').hex())
                    except:
                        readable.append(t.encode('latin1').hex())

                print(f"[Chunk] \"{chunk}\" -> {readable} -> IDs: {chunk_ids}")

            ids.extend(chunk_ids)

        if add_eos:
            ids.append(self.token2id[self.eos_token])
            if print_chunks: print(f"[Special] <eos> -> {self.token2id[self.eos_token]}")
        return ids

    def decode(self, ids: Iterable[int]):
        """
        å°†IDåºåˆ—è¿˜åŸä¸ºutf-8æ–‡æœ¬ï¼š
        """
        byte_seq = bytearray()
        for i in ids:
            tok = self.id2token.get(i, self.unk_token)
            if tok in {self.pad_token, self.bos_token, self.eos_token}:
                continue
            byte_seq.extend(tokens2bytes(list(tok)))
        return byte_seq.decode('utf-8', errors='replace')

    def save(self, vocab_path: str, merges_path: str):

        # ä¿å­˜vocabï¼ˆtoken2idï¼‰
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(self.token2id, f, ensure_ascii=False, indent=2)

        # ä¿å­˜mergesï¼šæ¯ä¸ªtokenç”¨base64
        merges_b64 = []
        for a, b in self.merges:
            a_bytes = a.encode('latin1')
            b_bytes = b.encode('latin1')
            merges_b64.append((
                base64.b64encode(a_bytes).decode('ascii'),
                base64.b64encode(b_bytes).decode('ascii')
            ))

        with open(merges_path, 'w', encoding='utf-8') as f:
            json.dump(merges_b64, f, ensure_ascii=False, indent=2)

    @classmethod
    def load(cls, vocab_path: str, merges_path: str):

        # åŠ è½½vocab
        with open(vocab_path, 'r', encoding='utf-8') as f:
            token2id = json.load(f)
        vocab_tokens = [None] * (max(token2id.values()) + 1)
        for tok, idx in token2id.items():
            vocab_tokens[idx] = tok

        # åŠ è½½mergesï¼ˆbase64 â†’ bytes â†’ latin1ï¼‰
        with open(merges_path, 'r', encoding='utf-8') as f:
            merges_b64 = json.load(f)

        merges = []
        for a_b64, b_b64 in merges_b64:
            a = base64.b64decode(a_b64).decode('latin1')
            b = base64.b64decode(b_b64).decode('latin1')
            merges.append((a, b))
        return cls(merges, vocab_tokens)


# æä¾›è®­ç»ƒå‡½æ•°
def train_tokenizer(texts, vocab_size=5000, num_merges=None):
    merges, vocab_tokens = train_bpe(texts, vocab_size=vocab_size, num_merges=num_merges)
    return DeepSeekV3Tokenizer(merges, vocab_tokens)

# ç¤ºä¾‹
if __name__ == "__main__":
    texts = [
        "Transformeræ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ã€‚",
        "DeepSeekåˆ†è¯å™¨æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰å¤šè¯­è¨€ã€‚",
        "Hello, ä¸–ç•Œ! ğŸŒğŸš€",
    ]

    print("è®­ç»ƒ Tokenizer (vocab_size=1024)")
    tokenizer = train_tokenizer(texts, vocab_size=1024)
    print(f"å®Œæˆè®­ç»ƒï¼Œè¯è¡¨å¤§å°: {len(tokenizer.vocab_tokens)}")
    print("-"*50)

    txt = "æ³¨æ„åŠ›æœºåˆ¶æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ ğŸš€ ğŸš€"
    print(f"ç¼–ç æ–‡æœ¬: {txt}")
    ids = tokenizer.encode(txt, add_bos=True, add_eos=True, print_chunks=True)

    print("-"*50)
    print("Token ID:", ids)
    decoded = tokenizer.decode(ids)
    print("è§£ç ç»“æœ:", decoded)
    print("æ˜¯å¦å¯é€†:", decoded == txt)
```
è¾“å…¥æµ‹è¯•æ ·ä¾‹
>æ³¨æ„åŠ›æœºåˆ¶æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ ğŸš€ ğŸš€

è¾“å‡º
>tokensæ˜ å°„idï¼Œä»¥åŠæ¯ä¸ªåˆ’åˆ†tokenå¯¹åº”çš„ç¼–ç ï¼Œå¹¶ä¸”å¯¹äºä¸åŒä½ç½®çš„ç©ºæ ¼å’ŒemojiğŸš€å¯¹åº”çš„ç¼–ç ä»¥åŠæ˜ å°„IDæ˜¯ç›¸åŒçš„ã€‚

ä»ä»¥ä¸Šä»£ç çš„è¿è¡Œç»“æœå¯ä»¥çœ‹åˆ°ï¼Œåˆ†è¯å™¨ä¸­çš„`token â†” id`æ˜ å°„åªæè¿°â€œè¿™ä¸ªtokençš„å†…å®¹â€ï¼Œå¹¶ä¸åŒ…å«å®ƒåœ¨å¥å­ä¸­çš„ä»»ä½•ä½ç½®ä¿¡æ¯ã€‚BPEæˆ–å…¶ä»–åŸºäºç»Ÿè®¡å’Œæ¦‚ç‡çš„åˆ†è¯ç®—æ³•ï¼Œå…¶æœ¬è´¨éƒ½æ˜¯ä¾æ®è¯­æ–™ä¸­çš„å…±ç°é¢‘ç‡æˆ–æ¦‚ç‡åˆ†å¸ƒï¼Œå†³å®šå¦‚ä½•å°†å¸¸è§çš„å­—ç¬¦ã€å­—èŠ‚æˆ–å­ä¸²åˆå¹¶æˆæ›´é•¿ã€æ›´é«˜é¢‘çš„tokenã€‚è¿™ç±»ç®—æ³•æœ¬èº«å¹¶ä¸ç†è§£å¥å­çš„è¯­ä¹‰ï¼Œå®ƒæ›´åƒä¸€ä¸ªçº¯ç»Ÿè®¡æ¨¡å—ï¼Œé€šè¿‡é¢‘ç‡æˆ–æ¦‚ç‡åŸåˆ™å¯¹å­—ç¬¦åºåˆ—è¿›è¡Œåˆ‡åˆ†ä¸åˆå¹¶ï¼Œä¸ºä¸Šå±‚æ¨¡å‹*å¦‚LLM*æä¾›ç¨³å®šè€Œç´§å‡‘çš„ç¦»æ•£è¾“å…¥å•å…ƒã€‚çœŸæ­£çš„è¯­ä¹‰ç†è§£ç”±ä¸‹æ¸¸çš„`Transformer`æ¿å—åœ¨ä¸Šä¸‹æ–‡ä¸­é€šè¿‡å­¦ä¹ è·å¾—ï¼Œå¹¶ä¾èµ–ä½ç½®ç¼–ç æ¥å»ºæ¨¡é¡ºåºä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç”±`åˆ†è¯å™¨`å®Œæˆã€‚

>ä¸ºä»€ä¹ˆDeepSeekè¦ç”¨latin1ç¼–è§£ç ?
>
>åœ¨DeepSeekçš„åˆ†è¯æµç¨‹ä¸­æœ€ç»ˆå¤„ç†å¾—åˆ°çš„æ˜¯æ•°å­—åŒ–çš„tokenï¼Œä½†åœ¨BPEåˆ†è¯å™¨è®­ç»ƒé˜¶æ®µéœ€è¦æŒ‰â€œå­—ç¬¦â€æ“ä½œã€‚å¦‚æœç›´æ¥ç”¨ UTF-8ç¼–è§£ç ï¼Œæ±‰å­—æˆ–emojiç­‰å¤šå­—èŠ‚å­—ç¬¦åœ¨æ‹†åˆ†ä¸ºå•å­—èŠ‚æ—¶ä¼šå‡ºç°ä¸å®Œæ•´åºåˆ—ï¼ŒPythonä¼šæŠ¥é”™æˆ–æ›¿æ¢ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚è€Œlatin-1æ˜¯å•å­—èŠ‚ç¼–ç ï¼Œå®ƒæŠŠæ¯ä¸ªå­—èŠ‚ï¼ˆ0â€“255ï¼‰æœºæ¢°æ˜ å°„ä¸ºä¸€ä¸ªUnicodeå­—ç¬¦ï¼Œä¿è¯ä»»æ„å­—èŠ‚åºåˆ—éƒ½èƒ½å®Œæ•´ã€å¯é€†åœ°ä¿å­˜ï¼Œä»è€Œè®©BPEæˆ–å…¶ä»–å­è¯ç®—æ³•èƒ½æŠŠå­—èŠ‚å½“ä½œå­—ç¬¦åˆå¹¶è€Œä¸ä¸¢æ•°æ®ã€‚ç®€å•æ¥è¯´ä½¿ç”¨latin-1æ˜¯ä¸ºäº†åœ¨åˆ†è¯å™¨ä¸­å®‰å…¨åœ°æŠŠåŸå§‹å­—èŠ‚å½“ä½œå­—ç¬¦å¤„ç†ï¼Œç¡®ä¿ç¼–ç å™¨é˜¶æ®µä¿¡æ¯å®Œæ•´ã€‚

### 3.3 å¿«é€Ÿä½“éªŒDeepSeekåˆ†è¯å™¨

<div align="center">
   <img width="1788" height="1077" alt="fdfd8c74f27d0832c80a9d4587daeb16" src="https://github.com/user-attachments/assets/7e7b40df-e9c0-43ed-896a-40679a8046c8" />
   <p>å›¾2.4 DeepSeekåˆ†è¯å™¨</p>
   </div>
   
   è¿™é‡Œè¾“å…¥æ–‡æœ¬ä¿¡æ¯ä¸º*ä½ å¥½ ï¼Œhello,  world !  ğŸŒ ï¼*ã€‚
   
>é€šè¿‡è¿™é‡Œçš„å¯è§†åŒ–å±•ç¤ºDeepSeekçš„åˆ†è¯å™¨è¾“å‡ºï¼Œå³è¾¹çš„æœ€åä¸€æ è¡¨ç¤ºçš„æ˜¯å¯¹åº”token IDæ˜ å°„ï¼Œå¯ä»¥çœ‹åˆ°å¯¹äºå•ç‹¬åˆ’åˆ†ä¸ºä¸€ç»„tokençš„ç©ºæ ¼ï¼Œå°±ç®—ä½ç½®ä¸åŒå…¶IDå€¼å‡ä¸º223ã€‚

## 4æ€è€ƒ
1ï¼‰æœ‰ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰ç‰¹å¾èƒ½å¤Ÿå¢å¼ºLLMçš„ç†è§£èƒ½åŠ›ï¼Œä½†å¹¶éé€‚ç”¨äºæ‰€æœ‰è¯­è¨€ä»»åŠ¡ã€‚é‚£ä¹ˆæ˜¯å¦å¯ä»¥åœ¨è§†è§‰è¡¨å¾ä¸ç¦»æ•£ token ä¹‹é—´å¯»æ±‚ä¸€ç§åŠ¨æ€â€œå¹³è¡¡ç‚¹â€ï¼šåŒæ—¶ä¸ºæ¨¡å‹æä¾›ä¸¤ç±»è¡¨å¾æ–¹å¼ï¼Œå¹¶å€Ÿé‰´MoEçš„æ€æƒ³è®¾è®¡è½»é‡çº§åŠ¨æ€è·¯ç”±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡æˆ–æ–‡æœ¬ç‰‡æ®µä¸­è‡ªåŠ¨é€‰æ‹©æˆ–èåˆæœ€åˆé€‚çš„*è¯â€”â€”æ•°å­—*æ˜ å°„è¡¨å½¢å¼ï¼Œä»è€Œæ˜¾è‘—æå‡è·¨åœºæ™¯çš„é€‚é…èƒ½åŠ›ï¼Ÿ

> æ–‡æœ¬tokençš„ç¦»æ•£æ€§é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ï¼Œè§†è§‰tokenå¯æä¾›é«˜å¯†åº¦çš„è¿ç»­å‹ç¼©è¡¨å¾ä½†å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰è¯­è¨€åœºæ™¯ï¼›å› æ­¤æ¢ç´¢ä¸€ç§MoEé£æ ¼çš„å¤šè¡¨å¾æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½æŒ‰ä»»åŠ¡åŠ¨æ€é€‰æ‹©æ–‡æœ¬ã€è§†è§‰æˆ–æ··åˆè¡¨å¾ï¼Œä»¥è·å¾—æ›´ä¸°å¯Œä¸”å…·åœºæ™¯é€‚é…æ€§çš„è¡¨ç¤ºæˆ–è®¸ä¹Ÿå€¼å¾—æ€è€ƒã€‚

2ï¼‰èƒ½å¦è®¾è®¡ä¸€ç§â€œè‡ªé€‚åº”åˆ†è¯å™¨â€ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå…ˆä¸LLMåˆ†å¼€è®­ç»ƒï¼Œå¹¶é€šè¿‡ä¸€ç§ç‰¹æ®Šæœºåˆ¶å°†è®­ç»ƒå¥½çš„åˆ†è¯å™¨ä¸æ¨¡å‹ç»“åˆï¼Œä½¿å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä»èƒ½åŠ¨æ€å­¦ä¹ å’Œä¼˜åŒ–tokenåˆ’åˆ†ç­–ç•¥ï¼Ÿ

>æ¯”å¦‚è€ƒè™‘ä¸€ç§åé¦ˆé©±åŠ¨çš„è¯è¡¨åŠ¨æ€å¢å¼ºæ–¹æ³•ï¼Œ**æ ¸å¿ƒæ˜¯è·¨æ¨¡å‹è¯­ä¹‰è¡¨ç¤ºçš„è’¸é¦ä¸è¿ç§»**ã€‚å®ƒä¸ä½¿ç”¨ä¼ ç»Ÿçš„è¾“å‡ºæ¦‚ç‡è’¸é¦ï¼Œè€Œæ˜¯ç”±æ•™å¸ˆæ¨¡å‹æ ¹æ®ç”¨æˆ·åé¦ˆï¼Œæå–æ–°æ¦‚å¿µçš„ç²¾å‡†è¯­ä¹‰å‘é‡ã€‚é€šè¿‡æ˜ å°„é€‚é…å™¨å°†è¯¥å‘é‡æŠ•å½±åˆ°å­¦ç”Ÿæ¨¡å‹çš„Embeddingç©ºé—´ï¼Œå®ç°å¯¹å­¦ç”Ÿè¯è¡¨çŸ©é˜µçš„å³æ—¶â€œè¡¥ä¸â€ï¼Œä»è€Œè®©å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿé›¶æ ·æœ¬åœ°è¯†åˆ«å¹¶å¤„ç†æ–°Tokenã€‚

3ï¼‰å€ŸåŠ©å¾®åˆ†å­è¯æ¨¡å—ã€å…ƒå­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œè®©åˆ†è¯å™¨èƒ½å¤Ÿä»å°‘é‡å¯¹è¯æˆ–ä»»åŠ¡æ ·æœ¬ä¸­è‡ªåŠ¨å‘ç°æœ€åˆé€‚çš„tokenåˆ’åˆ†æ–¹å¼ï¼Œä»è€Œé™ä½ä¸‹æ¸¸ä»»åŠ¡å¯¹æ•°æ®çš„ä¾èµ–åŒæ—¶æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Ÿ

> è¿™ç§æ–¹å¼æœ‰ç‚¹åƒåŠç›‘ç£å­¦ä¹ ï¼Œåˆ†è¯å™¨è‡ªå·±åœ¨â€œå­¦ä¹ æ€ä¹ˆå­¦ä¹ â€ï¼Œè¿™æ ·å³ä½¿åªçœ‹åˆ°å°‘é‡å¯¹è¯æ ·æœ¬ï¼Œå®ƒä¹Ÿèƒ½æ‰¾åˆ°æ›´åˆé€‚çš„tokenåˆ’åˆ†æ–¹å¼ï¼Œè®©æ¨¡å‹ç†è§£è¯­è¨€æ›´é«˜æ•ˆï¼Œä¹Ÿæ›´ä¸å®¹æ˜“è¢«æ–°è¯æˆ–å°‘é‡æ•°æ®éš¾ä½ã€‚

## å‚è€ƒèµ„æ–™
- [hugging faceä»‹ç»å››ç§åˆ†è¯å™¨ç®—æ³•](https://huggingface.co/learn/llm-course/en/chapter6/1)
- [BEPç®—æ³•](https://arxiv.org/pdf/1508.07909)
- [CS336çš„Lecture1è¯¾ç¨‹èµ„æ–™](https://stanford-cs336.github.io/spring2025-lectures/?trace=var%2Ftraces%2Flecture_01.json&source=lecture_01.py&line=238)

